{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating Pythonic Code",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmg5awbq44BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__OZFzfv49Gl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "061e3998-da8f-4f23-ee43-2b320482c919"
      },
      "source": [
        "sys.path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_CmFWmj4-PB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddd1b3a3-c172-4c61-c642-2c1ca344072d"
      },
      "source": [
        "import os\n",
        "STLIBLOC = '/usr/local/lib/python3.6/dist-packages'\n",
        "os.listdir(STLIBLOC)\n",
        "max_files = 100\n",
        "count = 0\n",
        "with open(\"input.txt\",\"a\",encoding=\"utf-8\") as f:\n",
        "  print(\"Inside with..\")\n",
        "  for path, directories, files in os.walk(STLIBLOC):\n",
        "    print(\"Path\",path)\n",
        "    print(\"Directories\",directories)\n",
        "    print(\"Files\",files)\n",
        "    count+=1\n",
        "    print(\"Incrementing count..\")\n",
        "    if count>max_files:\n",
        "      print(\"Breaking..\")\n",
        "      break\n",
        "    for i in files:\n",
        "      if \".py\" in i:\n",
        "        print(\"Found files.\")\n",
        "        try:\n",
        "          print(\"In try\")\n",
        "          with open(os.path.join(path, i),\"r\") as data_f:\n",
        "            contents = data_f.read()\n",
        "            print(contents)\n",
        "          f.write(contents)\n",
        "          f.write('\\n')\n",
        "        except Exception as e:\n",
        "          print(str(e))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inside with..\n",
            "Path /usr/local/lib/python3.6/dist-packages\n",
            "Directories ['tensorflow', 'tensorflow-1.14.0.dist-info', 'google_colab-1.0.0.dist-info', 'google', 'certifi-2019.6.16.dist-info', 'traitlets-4.3.2.dist-info', 'backcall-0.1.0.dist-info', 'uritemplate', 'psycopg2', 'attr', 'msgpack-0.5.6.dist-info', '_plotly_utils', 'google_pasta-0.1.7.dist-info', 'tensorboardcolab-0.0.22.dist-info', 'mkl-2019.0.dist-info', 'wcwidth-0.1.7.dist-info', 'fbprophet-0.5.dist-info', 'bz2file-0.98.dist-info', 'pyglet', 'backcall', 'sklearn', 'xlrd', '_plotly_future_', 'pywt', 'keras', 'prompt_toolkit', 'missingno', 'wasabi-0.2.2.dist-info', 'kaggle-1.5.5.dist-info', 'importlib_metadata-0.20.dist-info', 'wrapt', 'future-0.16.0.dist-info', 'astor', 'image', 'jupyter_core', 'autograd', 'tensorflow_probability', 'llvmlite-0.29.0.dist-info', 'more_itertools', 'spacy', 'ideep4py', 'pycocotools', 'matplotlib_venn', 'pytz-2018.9.dist-info', 'idna', 'python_dateutil-2.5.3.dist-info', 'sqlalchemy', 'cvxopt-1.2.3.dist-info', 'promise-2.2.1.dist-info', 'PyOpenGL-3.1.0.dist-info', 'distributed-1.25.3.dist-info', 'pandocfilters-1.4.2.dist-info', 'cycler-0.10.0.dist-info', 'decorator-4.4.0.dist-info', 'backports.weakref-1.0.post1.dist-info', 'autograd-1.3.dist-info', 'keras_vis-0.4.1.dist-info', 'yellowbrick-0.9.1.dist-info', 'pymystem3-0.2.0.dist-info', 'ptyprocess', 'opt_einsum-3.0.1.dist-info', 'cupy', 'scs', 'urllib3', 'cvxpy-1.0.25.dist-info', 'vega_datasets-0.7.0.dist-info', 'Werkzeug-0.15.6.dist-info', 'pickleshare-0.7.5.dist-info', 'dm_sonnet-1.35.dist-info', 'termcolor-1.1.0.dist-info', 'seaborn-0.9.0.dist-info', 'mido', 'Jinja2-2.10.1.dist-info', 'openpyxl', 'chardet-3.0.4.dist-info', 'tlz', 'widgetsnbextension', 'markdown', 'contextlib2-0.5.5.dist-info', 'numexpr', 'django', 'text_unidecode-1.2.dist-info', 'typing-3.7.4.1.dist-info', 'jedi', 'mistune-0.8.4.dist-info', 'google_cloud_datastore-1.8.0.dist-info', 'Keras_Preprocessing-1.1.0.dist-info', 'matplotlib-3.0.3.dist-info', 'xlrd-1.1.0.dist-info', 'hyperopt-0.1.2.dist-info', 'xlwt', 'fastdtw', 'torchsummary', 'httpimport-0.5.16.dist-info', 'pandas_profiling', 'sphinxcontrib', 'atomicwrites', 'psutil', 'Click-7.0.dist-info', 'bleach-3.1.0.dist-info', 'lucid', 'xlwt-1.3.0.dist-info', 'traitlets', 'psutil-5.4.8.dist-info', 'openpyxl-2.5.9.dist-info', 'branca', 'astor-0.8.0.dist-info', 'send2trash', 'lucid-0.3.8.dist-info', 'folium-0.8.3.dist-info', 'babel', 'ipython_genutils', 'wordcloud-1.5.0.dist-info', 'murmurhash-1.0.2.dist-info', 'markupsafe', 'tensorflow_estimator', 'dopamine_rl-1.0.5.dist-info', 'fancyimpute-0.4.3.dist-info', 'numpy-1.16.5.dist-info', 'imageio-2.4.1.dist-info', 'attrs-19.1.0.dist-info', 'keras_preprocessing', 'stable_baselines', 'bs4', 'resampy-0.2.2.dist-info', 'scikit_image-0.15.0.dist-info', 'okgrade', 'client', 'semantic_version-2.8.2.dist-info', 'docs', 'gast-0.2.2.dist-info', 'botocore-1.12.224.dist-info', 'more_itertools-7.2.0.dist-info', 'pycparser', 'Cython-0.29.13.dist-info', 'pymystem3', 'yellowbrick', 'convertdate-2.1.3.dist-info', 'branca-0.3.1.dist-info', 'nltk', 'dataclasses-0.6.dist-info', 'editdistance-0.5.3.dist-info', 'greenlet-0.4.15.dist-info', 'google_auth-1.4.2.dist-info', 'numexpr-2.7.0.dist-info', 'fastdtw-0.3.2.dist-info', 'itsdangerous', 'nbformat', 's3transfer-0.2.1.dist-info', 'humanize', 'Pillow-4.3.0.dist-info', 'imutils', 'nltk-3.2.5.dist-info', 'defusedxml-0.6.0.dist-info', 'albumentations', 'music21', 'docutils', 'python_utils', 'boto3', 'rsa', 'grpcio-1.15.0.dist-info', 'missingno-0.4.2.dist-info', 'pyparsing-2.4.2.dist-info', 'magenta', 'zmq-0.0.0.dist-info', 'tensorboardcolab', 'tables', 'flask', 'sklearn_pandas', 'pandas', 'atomicwrites-1.3.0.dist-info', 'pygments', 'crcmod-1.7.dist-info', 'distributed', 'filelock-3.0.12.dist-info', 'numpy', 'audioread', 'nibabel', 'oauth2client', 'pycparser-2.19.dist-info', 'featuretools-0.4.1.dist-info', 'google_resumable_media-0.4.0.dist-info', 'pystan', 'gunicorn-19.9.0.dist-info', 'gdown', 'imagesize-1.1.0.dist-info', 'ipython-5.5.0.dist-info', 'imblearn', 'httplib2', 'editdistance', 'music21-5.5.0.dist-info', 'sqlparse-0.3.0.dist-info', 'tqdm-4.28.1.dist-info', 'jupyter_core-4.5.0.dist-info', 'umap', 'nbformat-4.4.0.dist-info', 'toolz-0.10.0.dist-info', 'tflearn-0.3.2.dist-info', 'pytest-3.6.4.dist-info', 'PyDrive-1.3.1.dist-info', 'python_chess-0.23.11.dist-info', 'atari_py-0.1.15.dist-info', 'pyarrow', 'protobuf-3.7.1.dist-info', 'xarray', 'crcmod', 'mpl_toolkits', 'tensorboard-1.14.0.dist-info', 'cachetools-3.1.1.dist-info', 'google_auth_oauthlib', 'lightgbm-2.2.3.dist-info', 'zict-1.0.0.dist-info', 'test', 'kaggle', 'nbconvert-5.6.0.dist-info', 'backports', 'srsly-0.1.0.dist-info', 'lxml-4.2.6.dist-info', 'slugify', 'torchvision-0.3.0.dist-info', 'pycocotools-2.0.0.dist-info', 'intel_openmp-2019.0.dist-info', 'sphinx', 'pymc3', 'webencodings', 'snowballstemmer-1.9.1.dist-info', 'fa2-0.3.5.dist-info', 'astropy-3.0.5.dist-info', 'google_cloud_storage-1.16.1.dist-info', 'google_cloud_translate-1.5.0.dist-info', 'palettable', 'zipp-0.6.0.dist-info', 'pydotplus-2.0.2.dist-info', 'plotly-4.1.1.dist-info', 'preshed', 'packaging', 'pyasn1-0.4.7.dist-info', 'fix_yahoo_finance', 'examples', 'prometheus_client-0.7.1.dist-info', 'gunicorn', 'bottleneck', 'rpy2', 'pretty_midi', 'pandas_datareader-0.7.4.dist-info', 'altair-3.2.0.dist-info', 'imgaug-0.2.9.dist-info', 'portpicker-1.2.0.dist-info', 'Send2Trash-1.5.0.dist-info', 'gensim', 'plotly', 'prettytable-0.7.2.dist-info', 'pexpect-4.7.0.dist-info', 'pydot-1.3.0.dist-info', 'urllib3-1.24.3.dist-info', 'pymongo', 'oauthlib', 'oauth2client-4.1.3.dist-info', 'mizani-0.5.4.dist-info', 'tflearn', 'oauthlib-3.1.0.dist-info', 'umap_learn-0.3.10.dist-info', 'fastrlock-0.4.dist-info', 'nibabel-2.3.3.dist-info', 'docutils-0.15.2.dist-info', 'fastai-1.0.57.dist-info', 's3fs', 'requests_oauthlib-1.2.0.dist-info', 'googleapis_common_protos-1.6.0.dist-info', 'jinja2', 'ipython_genutils-0.2.0.dist-info', 'librosa-0.6.3.dist-info', 'coveralls', 'caffe2', 'chardet', 'Theano-1.0.4.dist-info', 'ephem-3.7.7.0.dist-info', 'google-2.0.2.dist-info', 'sql', 'terminado-0.8.2.dist-info', 'nbconvert', 'osqp', 'datascience', 'future', 'osqppurepy', 'httplib2-0.11.3.dist-info', 'requests_oauthlib', 'mesh_tensorflow-0.0.5.dist-info', 'humanize-0.5.1.dist-info', 'notebook', 'mpmath', 'pathlib-1.0.1.dist-info', 'tfds_nightly-1.2.0.dev201909050105.dist-info', 'pysndfile', 'intervaltree-2.1.0.dist-info', 'prompt_toolkit-1.0.16.dist-info', 'boto3-1.9.224.dist-info', 'fix_yahoo_finance-0.0.22.dist-info', 'pyrsistent', 'sympy', 'moviepy', 'certifi', 'gspread-3.0.1.dist-info', 'Flask-1.1.1.dist-info', 'olefile', 'textgenrnn-1.4.1.dist-info', 'tzlocal-1.5.1.dist-info', 'gym', 'docopt-0.6.2.dist-info', 'pandas_datareader', 'fastrlock', 'jieba-0.39.dist-info', 'py', 'np_utils-0.5.11.1.dist-info', 'gspread_dataframe-3.0.3.dist-info', 'cmake-3.12.0.dist-info', 'gdown-3.6.4.dist-info', 'tensorflow_estimator-1.14.0.dist-info', 'coverage-3.7.1.dist-info', 'absl_py-0.8.0.dist-info', 'click', 'dask-1.1.5.dist-info', 'descartes-1.1.0.dist-info', 'textgenrnn', 'torch', 'lxml', 'ipywidgets', 'boto-2.49.0.dist-info', 'numba-0.40.1.dist-info', 'pyglet-1.4.3.dist-info', 'mlxtend-0.14.0.dist-info', 'plotnine', 'magenta-0.3.19.dist-info', 'pyzmq-17.0.0.dist-info', 'msgpack', 'imblearn-0.0.dist-info', 'olefile-0.46.dist-info', 'torchsummary-1.5.1.dist-info', 'prometheus_client', 'libfuturize', 'glob2', 'vis', 'jpeg4py', 'jupyter_client', 'backports.tempfile-1.0.dist-info', 'kiwisolver-1.1.0.dist-info', 'cloudpickle-0.6.1.dist-info', 'knnimpute-0.1.0.dist-info', 'scs-2.1.1.post2.dist-info', 'importlib_metadata', 'patsy', 'sqlparse', 'lightgbm', 'dopamine', 'piptools', 'lmdb-0.97.dist-info', 'resampy', 'Bottleneck-1.2.1.dist-info', 'imgaug', 'srsly', 'ptyprocess-0.6.0.dist-info', 'pyasn1_modules', 'webencodings-0.5.1.dist-info', 'audioread-2.1.8.dist-info', 'palettable-3.2.0.dist-info', 's3transfer', 'intervaltree', 'itsdangerous-1.1.0.dist-info', 'googlesearch', 'jsonschema-2.6.0.dist-info', 'setuptools_git-1.2.dist-info', 'pystan-2.19.0.0.dist-info', 'smart_open-1.8.4.dist-info', 'uritemplate-3.0.0.dist-info', 'opencv_python-3.4.5.20.dist-info', 'fastcache-1.1.0.dist-info', 'qtconsole-4.5.5.dist-info', 'cupy_cuda100-5.4.0.dist-info', 'geopy-1.17.0.dist-info', 'mpi4py', 'testpath', 'jupyter_client-5.3.1.dist-info', 'inflect-2.1.0.dist-info', 'toolz', 'alabaster-0.7.12.dist-info', 'plotlywidget', 'numba', 'progressbar2-3.38.0.dist-info', 'beautifulsoup4-4.6.3.dist-info', 'torch-1.1.0.dist-info', 'mir_eval', 'tables-3.4.4.dist-info', 'widgetsnbextension-3.5.1.dist-info', 'torchvision', 'sortedcontainers', 'defusedxml', 'MarkupSafe-1.1.1.dist-info', 'werkzeug', 'multitasking-0.0.9.dist-info', 'thinc', '.libs_cffi_backend', 'pyasn1', 'Sphinx-1.8.5.dist-info', 'jupyter_console', 'bin', 'dateutil', 'knnimpute', 'jsonschema', 'github2pypi', 'mizani', 'cvxpy', 'Keras-2.2.5.dist-info', 'PyWavelets-1.0.3.dist-info', 'pip_tools-3.9.0.dist-info', 'thinc-7.0.8.dist-info', 'html5lib-1.0.1.dist-info', 'community-1.0.0b1.dist-info', 'retrying-1.3.3.dist-info', 'chainermn', 'dill-0.3.0.dist-info', 'entrypoints-0.3.dist-info', 'joblib-0.13.2.dist-info', 'torchtext-0.3.1.dist-info', 'astropy', 'pyrsistent-0.15.4.dist-info', 'botocore', 'atari_py', 'grpc', 'holidays-0.9.11.dist-info', 'google_cloud_core-1.0.3.dist-info', 'wrapt-1.11.2.dist-info', 'graphviz-0.10.1.dist-info', 'opt_einsum', 'gensim-3.6.0.dist-info', 'plac-0.9.6.dist-info', 'chainer', 'tensorflow_hub', 'h5py-2.8.0.dist-info', 'pysndfile-1.3.7.dist-info', 'feather', 'google_auth_oauthlib-0.4.0.dist-info', 'rsa-4.0.dist-info', 'rtmidi', 'prefetch_generator-1.0.1.dist-info', 'multiprocess', 'PyYAML-3.13.dist-info', 'pydrive', 'jmespath-0.9.4.dist-info', 'pexpect', 'chainer-5.4.0.dist-info', 'sonnet', 'tensorflow_probability-0.7.0.dist-info', 'jax-0.1.44.dist-info', 'natsort-5.5.0.dist-info', 'mpmath-1.1.0.dist-info', 'xgboost', 'graph_nets', 'tweepy-3.6.0.dist-info', 'tensor2tensor-1.11.0.dist-info', 'pretty_midi-0.2.8.dist-info', 'cffi', 'joblib', 'fastai', 'HeapDict-1.0.0.dist-info', 'packaging-19.1.dist-info', 'Shapely-1.6.4.post2.dist-info', '__pycache__', 'jpeg4py-0.1.4.dist-info', 'SQLAlchemy-1.3.8.dist-info', 'pyarrow-0.14.1.dist-info', 'notebook-5.2.2.dist-info', 'jdcal-1.4.1.dist-info', 'h5py', 'requests', 'datascience-0.10.6.dist-info', 'ipython_sql-0.3.9.dist-info', 'absl', 'googledrivedownloader-0.4.dist-info', 'pasta', 's3fs-0.3.4.dist-info', 'idna-2.8.dist-info', 'llvmlite', 'parso-0.5.1.dist-info', 'fsspec', 'fbprophet', 'graph_nets-1.0.4.dist-info', 'pyemd-0.5.1.dist-info', 'mido-1.2.6.dist-info', 'Pygments-2.1.3.dist-info', 'gevent', 'jax', 'et_xmlfile', 'ephem', 'nvidia_ml_py3-7.352.0.dist-info', 'murmurhash', 'pandas-0.24.2.dist-info', 'ideep4py-2.0.0.post3.dist-info', 'skimage', 'multitasking', 'snowballstemmer', 'python_rtmidi-1.3.0.dist-info', 'matplotlib', 'bleach', 'descartes', 'progressbar', 'xgboost-0.90.dist-info', 'python_utils-2.3.0.dist-info', 'yaml', 'six-1.12.0.dist-info', 'easydict', 'geographiclib', 'ipykernel-4.6.1.dist-info', 'cffi-1.12.3.dist-info', 'mir_eval-0.5.dist-info', 'shapely', 'Babel-2.7.0.dist-info', 'dlib-19.16.0.dist-info', 'tensorflow_metadata-0.14.0.dist-info', 'tensorboard', 'albumentations-0.1.12.dist-info', 'wordcloud', 'gspread', 'apiclient', 'sklearn_pandas-1.8.0.dist-info', 'imageio', 'jedi-0.15.1.dist-info', 'requests-2.21.0.dist-info', 'moviepy-0.2.3.5.dist-info', 'rpy2-2.9.5.dist-info', 'scikit_learn-0.21.3.dist-info', 'folium', 'ecos-2.0.7.post1.dist-info', 'google_auth_httplib2-0.0.3.dist-info', 'mesh_tensorflow', 'cloudpickle', 'natsort', 'mpi4py-3.0.2.dist-info', 'google_drive_downloader', 'nisext', 'Django-2.2.5.dist-info', 'py-1.8.0.dist-info', 'ecos', 'vega_datasets', 'simplegeneric-0.8.1.dist-info', 'zict', 'torchtext', 'xarray-0.11.3.dist-info', 'gridfs', 'statsmodels-0.10.1.dist-info', 'scipy-1.3.1.dist-info', 'pymongo-3.9.0.dist-info', 'colab', 'convertdate', 'smart_open', 'geopy', 'textblob', 'text_unidecode', 'blis', 'pandas_gbq', 'python_slugify-3.0.3.dist-info', 'jaxlib-0.1.26.dist-info', 'bs4-0.0.1.dist-info', 'fastcache', 'sympy-1.1.1.dist-info', 'cmake', 'networkx-2.3.dist-info', 'psycopg2-2.7.6.1.dist-info', 'PySocks-1.7.0.dist-info', 'bson', 'promise', 'PIL', 'pluggy', 'jupyter_console-5.2.0.dist-info', 'easydict-1.9.dist-info', 'parso', 'matplotlib_venn-0.11.5.dist-info', 'pymc3-3.7.dist-info', 'google_api_python_client-1.7.11.dist-info', 'chess', 'np_utils', 'pydot_ng', 'multiprocess-0.70.8.dist-info', 'pytz', 'jieba', 'imutils-0.5.3.dist-info', 'imbalanced_learn-0.4.3.dist-info', 'tweepy', 'networkx', 'ipykernel', 'pyemd', 'tblib', 'pandas_gbq-0.4.1.dist-info', 'wasabi', 'alabaster', 'hyperopt', 'gast', 'jupyter-1.0.0.dist-info', 'community', 'terminado', 'tabulate-0.8.3.dist-info', 'plotnine-0.5.1.dist-info', 'graphviz', 'IPython', 'libpasteurize', 'gevent-1.4.0.dist-info', 'pluggy-0.7.1.dist-info', 'preshed-2.0.1.dist-info', 'bokeh-1.0.4.dist-info', 'Cython', '_pytest', 'setuptools_git', 'dill', 'tblib-1.4.0.dist-info', 'html5lib', 'qtconsole', 'tensorflow_datasets', 'sklearn-0.0.dist-info', 'google_api_core-1.14.2.dist-info', 'cupyx', 'tests', 'scipy', 'osqp-0.6.1.dist-info', 'jaxlib', 'et_xmlfile-1.0.1.dist-info', 'tornado', 'pyasn1_modules-0.2.6.dist-info', 'tensorflow_metadata', 'lmdb', 'sphinxcontrib_websupport-1.1.2.dist-info', 'theano', 'mlxtend', 'pydot_ng-2.0.0.dist-info', 'kapre', 'tzlocal', 'kapre-0.1.3.1.dist-info', 'altair', 'tensor2tensor', 'seaborn', 'cv2', 'boto', 'gym-0.10.11.dist-info', 'cachetools', 'tensorflow_hub-0.6.0.dist-info', 'Keras_Applications-1.0.8.dist-info', 'coverage', 'textblob-0.15.3.dist-info', 'fsspec-0.4.4.dist-info', 'blis-0.2.4.dist-info', 'spacy-2.1.8.dist-info', 'coveralls-0.5.dist-info', 'semantic_version', 'gin_config-0.2.1.dist-info', 'fa2', 'opencv_contrib_python-3.4.3.18.dist-info', 'gin', 'python_louvain-0.13.dist-info', 'cymem', 'testpath-0.4.2.dist-info', 'cymem-2.0.2.dist-info', 'dask', 'bokeh', 'pydotplus', 'fastprogress', 'keras_applications', 'zmq', 'pyximport', 'statsmodels', 'daft-0.0.4.dist-info', 'ipywidgets-7.5.1.dist-info', 'librosa', 'OpenGL', 'cvxopt', 'Markdown-3.1.1.dist-info', 'googleapiclient', 'geographiclib-1.49.dist-info', 'en_core_web_sm-2.1.0.dist-info', 'google_cloud_bigquery-1.14.0.dist-info', 'tqdm', 'okgrade-0.4.3.dist-info', 'lunardate-0.2.0.dist-info', 'stable_baselines-2.2.1.dist-info', 'en_core_web_sm', 'glob2-0.7.dist-info', 'patsy-0.5.1.dist-info', 'feather_format-0.4.0.dist-info', 'fastprogress-0.1.21.dist-info', 'pandas_profiling-1.4.1.dist-info', 'sortedcontainers-2.1.0.dist-info', 'past', 'tornado-4.5.3.dist-info', 'prefetch_generator', 'google_cloud_language-1.2.0.dist-info', 'wcwidth', 'image-1.5.27.dist-info', 'jmespath', 'featuretools', 'fancyimpute', 'pip', 'pkg_resources', 'setuptools', 'wheel', 'wheel-0.33.6.dist-info', 'pip-19.2.3.dist-info', 'setuptools-41.2.0.dist-info']\n",
            "Files ['google_colab-1.0.0-py3.6-nspkg.pth', '_ecos.cpython-36m-x86_64-linux-gnu.so', 'heapdict.py', 'google_cloud_language-1.2.0-py3.6-nspkg.pth', 'dataclasses.py', 'pydot.py', 'google_resumable_media-0.4.0-py3.6-nspkg.pth', 'google_cloud_core-1.0.3-py3.6-nspkg.pth', 'socks.py', 'cython.py', 'pandocfilters.py', 'gspread_dataframe.py', 'protobuf-3.7.1-py3.6-nspkg.pth', 'sphinxcontrib_websupport-1.1.2-py3.7-nspkg.pth', 'pathlib.py', 'prettytable.py', 'google_api_core-1.14.2-py3.6-nspkg.pth', 'retrying.py', 'google_auth_httplib2.py', 'google_cloud_storage-1.16.1-py3.6-nspkg.pth', 'matplotlib-3.0.3-py3.6-nspkg.pth', 'greenlet.cpython-36m-x86_64-linux-gnu.so', 'pyparsing.py', 'tabulate.py', 'pickleshare.py', 'lunardate.py', 'inflect.py', 'jdcal.py', 'pytest.py', 'google_cloud_bigquery-1.14.0-py3.6-nspkg.pth', 'plac.py', 'googleapis_common_protos-1.6.0-py3.6-nspkg.pth', 'mistune.py', 'plac_ext.py', 'decorator.py', 'nvidia_smi.py', 'filelock.py', 'pylab.py', '_scs_direct.cpython-36m-x86_64-linux-gnu.so', 'google_cloud_translate-1.5.0-py3.6-nspkg.pth', 'plac_tk.py', 'dlib.cpython-36m-x86_64-linux-gnu.so', 'termcolor.py', '_scs_indirect.cpython-36m-x86_64-linux-gnu.so', 'pynvml.py', 'kiwisolver.cpython-36m-x86_64-linux-gnu.so', 'dot_parser.py', 'six.py', 'contextlib2.py', 'cycler.py', '_pyrsistent_version.py', '_cvxcore.cpython-36m-x86_64-linux-gnu.so', '_scs_python.cpython-36m-x86_64-linux-gnu.so', 'plac_core.py', 'pvectorc.cpython-36m-x86_64-linux-gnu.so', 'holidays.py', 'portpicker.py', 'ipykernel_launcher.py', '_cffi_backend.cpython-36m-x86_64-linux-gnu.so', 'daft.py', 'docopt.py', 'imagesize.py', 'bz2file.py', 'google_auth-1.4.2-py2.7-nspkg.pth', 'simplegeneric.py', 'jupyter.py', 'typing.py', 'httpimport.py', 'zipp.py', 'google_cloud_datastore-1.8.0-py3.6-nspkg.pth', 'entrypoints.py', 'sockshandler.py', '_multiprocess.cpython-36m-x86_64-linux-gnu.so', 'easy_install.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "import collections\n",
            "\n",
            "def doc(s):\n",
            "    if hasattr(s, '__call__'):\n",
            "        s = s.__doc__\n",
            "    def f(g):\n",
            "        g.__doc__ = s\n",
            "        return g\n",
            "    return f\n",
            "\n",
            "class heapdict(collections.MutableMapping):\n",
            "    __marker = object()\n",
            "\n",
            "    @staticmethod\n",
            "    def _parent(i):\n",
            "        return ((i - 1) >> 1)\n",
            "\n",
            "    @staticmethod\n",
            "    def _left(i):\n",
            "        return ((i << 1) + 1)\n",
            "\n",
            "    @staticmethod\n",
            "    def _right(i):\n",
            "        return ((i+1) << 1)    \n",
            "    \n",
            "    def __init__(self, *args, **kw):\n",
            "        self.heap = []\n",
            "        self.d = {}\n",
            "        self.update(*args, **kw)\n",
            "\n",
            "    @doc(dict.clear)\n",
            "    def clear(self):\n",
            "        self.heap.clear()\n",
            "        self.d.clear()\n",
            "\n",
            "    @doc(dict.__setitem__)\n",
            "    def __setitem__(self, key, value):\n",
            "        if key in self.d:\n",
            "            self.pop(key)\n",
            "        wrapper = [value, key, len(self)]\n",
            "        self.d[key] = wrapper\n",
            "        self.heap.append(wrapper)\n",
            "        self._decrease_key(len(self.heap)-1)\n",
            "\n",
            "    def _min_heapify(self, i):\n",
            "        l = self._left(i)\n",
            "        r = self._right(i)\n",
            "        n = len(self.heap)\n",
            "        if l < n and self.heap[l][0] < self.heap[i][0]:\n",
            "            low = l\n",
            "        else:\n",
            "            low = i\n",
            "        if r < n and self.heap[r][0] < self.heap[low][0]:\n",
            "            low = r\n",
            "\n",
            "        if low != i:\n",
            "            self._swap(i, low)\n",
            "            self._min_heapify(low)\n",
            "\n",
            "    def _decrease_key(self, i):\n",
            "        while i:\n",
            "            parent = self._parent(i)\n",
            "            if self.heap[parent][0] < self.heap[i][0]: break\n",
            "            self._swap(i, parent)\n",
            "            i = parent\n",
            "\n",
            "    def _swap(self, i, j):\n",
            "        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]\n",
            "        self.heap[i][2] = i\n",
            "        self.heap[j][2] = j\n",
            "\n",
            "    @doc(dict.__delitem__)\n",
            "    def __delitem__(self, key):\n",
            "        wrapper = self.d[key]\n",
            "        while wrapper[2]:\n",
            "            parentpos = self._parent(wrapper[2])\n",
            "            parent = self.heap[parentpos]\n",
            "            self._swap(wrapper[2], parent[2])\n",
            "        self.popitem()\n",
            "\n",
            "    @doc(dict.__getitem__)\n",
            "    def __getitem__(self, key):\n",
            "        return self.d[key][0]\n",
            "\n",
            "    @doc(dict.__iter__)\n",
            "    def __iter__(self):\n",
            "        return iter(self.d)\n",
            "\n",
            "    def popitem(self):\n",
            "        \"\"\"D.popitem() -> (k, v), remove and return the (key, value) pair with lowest\\nvalue; but raise KeyError if D is empty.\"\"\"\n",
            "        wrapper = self.heap[0]\n",
            "        if len(self.heap) == 1:\n",
            "            self.heap.pop()\n",
            "        else:\n",
            "            self.heap[0] = self.heap.pop(-1)\n",
            "            self.heap[0][2] = 0\n",
            "            self._min_heapify(0)\n",
            "        del self.d[wrapper[1]]\n",
            "        return wrapper[1], wrapper[0]    \n",
            "\n",
            "    @doc(dict.__len__)\n",
            "    def __len__(self):\n",
            "        return len(self.d)\n",
            "\n",
            "    def peekitem(self):\n",
            "        \"\"\"D.peekitem() -> (k, v), return the (key, value) pair with lowest value;\\n but raise KeyError if D is empty.\"\"\"\n",
            "        return (self.heap[0][1], self.heap[0][0])\n",
            "\n",
            "del doc\n",
            "__all__ = ['heapdict']\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "import re\n",
            "import sys\n",
            "import copy\n",
            "import types\n",
            "import inspect\n",
            "import keyword\n",
            "\n",
            "__all__ = ['dataclass',\n",
            "           'field',\n",
            "           'Field',\n",
            "           'FrozenInstanceError',\n",
            "           'InitVar',\n",
            "           'MISSING',\n",
            "\n",
            "           # Helper functions.\n",
            "           'fields',\n",
            "           'asdict',\n",
            "           'astuple',\n",
            "           'make_dataclass',\n",
            "           'replace',\n",
            "           'is_dataclass',\n",
            "           ]\n",
            "\n",
            "# Conditions for adding methods.  The boxes indicate what action the\n",
            "# dataclass decorator takes.  For all of these tables, when I talk\n",
            "# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I'm\n",
            "# referring to the arguments to the @dataclass decorator.  When\n",
            "# checking if a dunder method already exists, I mean check for an\n",
            "# entry in the class's __dict__.  I never check to see if an attribute\n",
            "# is defined in a base class.\n",
            "\n",
            "# Key:\n",
            "# +=========+=========================================+\n",
            "# + Value   | Meaning                                 |\n",
            "# +=========+=========================================+\n",
            "# | <blank> | No action: no method is added.          |\n",
            "# +---------+-----------------------------------------+\n",
            "# | add     | Generated method is added.              |\n",
            "# +---------+-----------------------------------------+\n",
            "# | raise   | TypeError is raised.                    |\n",
            "# +---------+-----------------------------------------+\n",
            "# | None    | Attribute is set to None.               |\n",
            "# +=========+=========================================+\n",
            "\n",
            "# __init__\n",
            "#\n",
            "#   +--- init= parameter\n",
            "#   |\n",
            "#   v     |       |       |\n",
            "#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n",
            "# +=======+=======+=======+\n",
            "# | False |       |       |\n",
            "# +-------+-------+-------+\n",
            "# | True  | add   |       |  <- the default\n",
            "# +=======+=======+=======+\n",
            "\n",
            "# __repr__\n",
            "#\n",
            "#    +--- repr= parameter\n",
            "#    |\n",
            "#    v    |       |       |\n",
            "#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n",
            "# +=======+=======+=======+\n",
            "# | False |       |       |\n",
            "# +-------+-------+-------+\n",
            "# | True  | add   |       |  <- the default\n",
            "# +=======+=======+=======+\n",
            "\n",
            "\n",
            "# __setattr__\n",
            "# __delattr__\n",
            "#\n",
            "#    +--- frozen= parameter\n",
            "#    |\n",
            "#    v    |       |       |\n",
            "#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n",
            "# +=======+=======+=======+\n",
            "# | False |       |       |  <- the default\n",
            "# +-------+-------+-------+\n",
            "# | True  | add   | raise |\n",
            "# +=======+=======+=======+\n",
            "# Raise because not adding these methods would break the \"frozen-ness\"\n",
            "# of the class.\n",
            "\n",
            "# __eq__\n",
            "#\n",
            "#    +--- eq= parameter\n",
            "#    |\n",
            "#    v    |       |       |\n",
            "#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n",
            "# +=======+=======+=======+\n",
            "# | False |       |       |\n",
            "# +-------+-------+-------+\n",
            "# | True  | add   |       |  <- the default\n",
            "# +=======+=======+=======+\n",
            "\n",
            "# __lt__\n",
            "# __le__\n",
            "# __gt__\n",
            "# __ge__\n",
            "#\n",
            "#    +--- order= parameter\n",
            "#    |\n",
            "#    v    |       |       |\n",
            "#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n",
            "# +=======+=======+=======+\n",
            "# | False |       |       |  <- the default\n",
            "# +-------+-------+-------+\n",
            "# | True  | add   | raise |\n",
            "# +=======+=======+=======+\n",
            "# Raise because to allow this case would interfere with using\n",
            "# functools.total_ordering.\n",
            "\n",
            "# __hash__\n",
            "\n",
            "#    +------------------- unsafe_hash= parameter\n",
            "#    |       +----------- eq= parameter\n",
            "#    |       |       +--- frozen= parameter\n",
            "#    |       |       |\n",
            "#    v       v       v    |        |        |\n",
            "#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n",
            "# +=======+=======+=======+========+========+\n",
            "# | False | False | False |        |        | No __eq__, use the base class __hash__\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | False | False | True  |        |        | No __eq__, use the base class __hash__\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | False | True  | False | None   |        | <-- the default, not hashable\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | True  | True  | False | add    | raise  | Not frozen, but hashable\n",
            "# +-------+-------+-------+--------+--------+\n",
            "# | True  | True  | True  | add    | raise  | Frozen, so hashable\n",
            "# +=======+=======+=======+========+========+\n",
            "# For boxes that are blank, __hash__ is untouched and therefore\n",
            "# inherited from the base class.  If the base is object, then\n",
            "# id-based hashing is used.\n",
            "#\n",
            "# Note that a class may already have __hash__=None if it specified an\n",
            "# __eq__ method in the class body (not one that was created by\n",
            "# @dataclass).\n",
            "#\n",
            "# See _hash_action (below) for a coded version of this table.\n",
            "\n",
            "\n",
            "# Raised when an attempt is made to modify a frozen class.\n",
            "class FrozenInstanceError(AttributeError): pass\n",
            "\n",
            "# A sentinel object for default values to signal that a default\n",
            "# factory will be used.  This is given a nice repr() which will appear\n",
            "# in the function signature of dataclasses' constructors.\n",
            "class _HAS_DEFAULT_FACTORY_CLASS:\n",
            "    def __repr__(self):\n",
            "        return '<factory>'\n",
            "_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n",
            "\n",
            "# A sentinel object to detect if a parameter is supplied or not.  Use\n",
            "# a class to give it a better repr.\n",
            "class _MISSING_TYPE:\n",
            "    pass\n",
            "MISSING = _MISSING_TYPE()\n",
            "\n",
            "# Since most per-field metadata will be unused, create an empty\n",
            "# read-only proxy that can be shared among all fields.\n",
            "_EMPTY_METADATA = types.MappingProxyType({})\n",
            "\n",
            "# Markers for the various kinds of fields and pseudo-fields.\n",
            "class _FIELD_BASE:\n",
            "    def __init__(self, name):\n",
            "        self.name = name\n",
            "    def __repr__(self):\n",
            "        return self.name\n",
            "_FIELD = _FIELD_BASE('_FIELD')\n",
            "_FIELD_CLASSVAR = _FIELD_BASE('_FIELD_CLASSVAR')\n",
            "_FIELD_INITVAR = _FIELD_BASE('_FIELD_INITVAR')\n",
            "\n",
            "# The name of an attribute on the class where we store the Field\n",
            "# objects.  Also used to check if a class is a Data Class.\n",
            "_FIELDS = '__dataclass_fields__'\n",
            "\n",
            "# The name of an attribute on the class that stores the parameters to\n",
            "# @dataclass.\n",
            "_PARAMS = '__dataclass_params__'\n",
            "\n",
            "# The name of the function, that if it exists, is called at the end of\n",
            "# __init__.\n",
            "_POST_INIT_NAME = '__post_init__'\n",
            "\n",
            "# String regex that string annotations for ClassVar or InitVar must match.\n",
            "# Allows \"identifier.identifier[\" or \"identifier[\".\n",
            "# https://bugs.python.org/issue33453 for details.\n",
            "_MODULE_IDENTIFIER_RE = re.compile(r'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)')\n",
            "\n",
            "class _InitVarMeta(type):\n",
            "    def __getitem__(self, params):\n",
            "        return self\n",
            "\n",
            "class InitVar(metaclass=_InitVarMeta):\n",
            "    pass\n",
            "\n",
            "\n",
            "# Instances of Field are only ever created from within this module,\n",
            "# and only from the field() function, although Field instances are\n",
            "# exposed externally as (conceptually) read-only objects.\n",
            "#\n",
            "# name and type are filled in after the fact, not in __init__.\n",
            "# They're not known at the time this class is instantiated, but it's\n",
            "# convenient if they're available later.\n",
            "#\n",
            "# When cls._FIELDS is filled in with a list of Field objects, the name\n",
            "# and type fields will have been populated.\n",
            "class Field:\n",
            "    __slots__ = ('name',\n",
            "                 'type',\n",
            "                 'default',\n",
            "                 'default_factory',\n",
            "                 'repr',\n",
            "                 'hash',\n",
            "                 'init',\n",
            "                 'compare',\n",
            "                 'metadata',\n",
            "                 '_field_type',  # Private: not to be used by user code.\n",
            "                 )\n",
            "\n",
            "    def __init__(self, default, default_factory, init, repr, hash, compare,\n",
            "                 metadata):\n",
            "        self.name = None\n",
            "        self.type = None\n",
            "        self.default = default\n",
            "        self.default_factory = default_factory\n",
            "        self.init = init\n",
            "        self.repr = repr\n",
            "        self.hash = hash\n",
            "        self.compare = compare\n",
            "        self.metadata = (_EMPTY_METADATA\n",
            "                         if metadata is None or len(metadata) == 0 else\n",
            "                         types.MappingProxyType(metadata))\n",
            "        self._field_type = None\n",
            "\n",
            "    def __repr__(self):\n",
            "        return ('Field('\n",
            "                f'name={self.name!r},'\n",
            "                f'type={self.type!r},'\n",
            "                f'default={self.default!r},'\n",
            "                f'default_factory={self.default_factory!r},'\n",
            "                f'init={self.init!r},'\n",
            "                f'repr={self.repr!r},'\n",
            "                f'hash={self.hash!r},'\n",
            "                f'compare={self.compare!r},'\n",
            "                f'metadata={self.metadata!r},'\n",
            "                f'_field_type={self._field_type}'\n",
            "                ')')\n",
            "\n",
            "    # This is used to support the PEP 487 __set_name__ protocol in the\n",
            "    # case where we're using a field that contains a descriptor as a\n",
            "    # defaul value.  For details on __set_name__, see\n",
            "    # https://www.python.org/dev/peps/pep-0487/#implementation-details.\n",
            "    #\n",
            "    # Note that in _process_class, this Field object is overwritten\n",
            "    # with the default value, so the end result is a descriptor that\n",
            "    # had __set_name__ called on it at the right time.\n",
            "    def __set_name__(self, owner, name):\n",
            "        func = getattr(type(self.default), '__set_name__', None)\n",
            "        if func:\n",
            "            # There is a __set_name__ method on the descriptor, call\n",
            "            # it.\n",
            "            func(self.default, owner, name)\n",
            "\n",
            "\n",
            "class _DataclassParams:\n",
            "    __slots__ = ('init',\n",
            "                 'repr',\n",
            "                 'eq',\n",
            "                 'order',\n",
            "                 'unsafe_hash',\n",
            "                 'frozen',\n",
            "                 )\n",
            "\n",
            "    def __init__(self, init, repr, eq, order, unsafe_hash, frozen):\n",
            "        self.init = init\n",
            "        self.repr = repr\n",
            "        self.eq = eq\n",
            "        self.order = order\n",
            "        self.unsafe_hash = unsafe_hash\n",
            "        self.frozen = frozen\n",
            "\n",
            "    def __repr__(self):\n",
            "        return ('_DataclassParams('\n",
            "                f'init={self.init!r},'\n",
            "                f'repr={self.repr!r},'\n",
            "                f'eq={self.eq!r},'\n",
            "                f'order={self.order!r},'\n",
            "                f'unsafe_hash={self.unsafe_hash!r},'\n",
            "                f'frozen={self.frozen!r}'\n",
            "                ')')\n",
            "\n",
            "\n",
            "# This function is used instead of exposing Field creation directly,\n",
            "# so that a type checker can be told (via overloads) that this is a\n",
            "# function whose type depends on its parameters.\n",
            "def field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n",
            "          hash=None, compare=True, metadata=None):\n",
            "    \"\"\"Return an object to identify dataclass fields.\n",
            "\n",
            "    default is the default value of the field.  default_factory is a\n",
            "    0-argument function called to initialize a field's value.  If init\n",
            "    is True, the field will be a parameter to the class's __init__()\n",
            "    function.  If repr is True, the field will be included in the\n",
            "    object's repr().  If hash is True, the field will be included in\n",
            "    the object's hash().  If compare is True, the field will be used\n",
            "    in comparison functions.  metadata, if specified, must be a\n",
            "    mapping which is stored but not otherwise examined by dataclass.\n",
            "\n",
            "    It is an error to specify both default and default_factory.\n",
            "    \"\"\"\n",
            "\n",
            "    if default is not MISSING and default_factory is not MISSING:\n",
            "        raise ValueError('cannot specify both default and default_factory')\n",
            "    return Field(default, default_factory, init, repr, hash, compare,\n",
            "                 metadata)\n",
            "\n",
            "\n",
            "def _tuple_str(obj_name, fields):\n",
            "    # Return a string representing each field of obj_name as a tuple\n",
            "    # member.  So, if fields is ['x', 'y'] and obj_name is \"self\",\n",
            "    # return \"(self.x,self.y)\".\n",
            "\n",
            "    # Special case for the 0-tuple.\n",
            "    if not fields:\n",
            "        return '()'\n",
            "    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n",
            "    return f'({\",\".join([f\"{obj_name}.{f.name}\" for f in fields])},)'\n",
            "\n",
            "\n",
            "def _create_fn(name, args, body, *, globals=None, locals=None,\n",
            "               return_type=MISSING):\n",
            "    # Note that we mutate locals when exec() is called.  Caller\n",
            "    # beware!  The only callers are internal to this module, so no\n",
            "    # worries about external callers.\n",
            "    if locals is None:\n",
            "        locals = {}\n",
            "    return_annotation = ''\n",
            "    if return_type is not MISSING:\n",
            "        locals['_return_type'] = return_type\n",
            "        return_annotation = '->_return_type'\n",
            "    args = ','.join(args)\n",
            "    body = '\\n'.join(f' {b}' for b in body)\n",
            "\n",
            "    # Compute the text of the entire function.\n",
            "    txt = f'def {name}({args}){return_annotation}:\\n{body}'\n",
            "\n",
            "    exec(txt, globals, locals)\n",
            "    return locals[name]\n",
            "\n",
            "\n",
            "def _field_assign(frozen, name, value, self_name):\n",
            "    # If we're a frozen class, then assign to our fields in __init__\n",
            "    # via object.__setattr__.  Otherwise, just use a simple\n",
            "    # assignment.\n",
            "    #\n",
            "    # self_name is what \"self\" is called in this function: don't\n",
            "    # hard-code \"self\", since that might be a field name.\n",
            "    if frozen:\n",
            "        return f'object.__setattr__({self_name},{name!r},{value})'\n",
            "    return f'{self_name}.{name}={value}'\n",
            "\n",
            "\n",
            "def _field_init(f, frozen, globals, self_name):\n",
            "    # Return the text of the line in the body of __init__ that will\n",
            "    # initialize this field.\n",
            "\n",
            "    default_name = f'_dflt_{f.name}'\n",
            "    if f.default_factory is not MISSING:\n",
            "        if f.init:\n",
            "            # This field has a default factory.  If a parameter is\n",
            "            # given, use it.  If not, call the factory.\n",
            "            globals[default_name] = f.default_factory\n",
            "            value = (f'{default_name}() '\n",
            "                     f'if {f.name} is _HAS_DEFAULT_FACTORY '\n",
            "                     f'else {f.name}')\n",
            "        else:\n",
            "            # This is a field that's not in the __init__ params, but\n",
            "            # has a default factory function.  It needs to be\n",
            "            # initialized here by calling the factory function,\n",
            "            # because there's no other way to initialize it.\n",
            "\n",
            "            # For a field initialized with a default=defaultvalue, the\n",
            "            # class dict just has the default value\n",
            "            # (cls.fieldname=defaultvalue).  But that won't work for a\n",
            "            # default factory, the factory must be called in __init__\n",
            "            # and we must assign that to self.fieldname.  We can't\n",
            "            # fall back to the class dict's value, both because it's\n",
            "            # not set, and because it might be different per-class\n",
            "            # (which, after all, is why we have a factory function!).\n",
            "\n",
            "            globals[default_name] = f.default_factory\n",
            "            value = f'{default_name}()'\n",
            "    else:\n",
            "        # No default factory.\n",
            "        if f.init:\n",
            "            if f.default is MISSING:\n",
            "                # There's no default, just do an assignment.\n",
            "                value = f.name\n",
            "            elif f.default is not MISSING:\n",
            "                globals[default_name] = f.default\n",
            "                value = f.name\n",
            "        else:\n",
            "            # This field does not need initialization.  Signify that\n",
            "            # to the caller by returning None.\n",
            "            return None\n",
            "\n",
            "    # Only test this now, so that we can create variables for the\n",
            "    # default.  However, return None to signify that we're not going\n",
            "    # to actually do the assignment statement for InitVars.\n",
            "    if f._field_type == _FIELD_INITVAR:\n",
            "        return None\n",
            "\n",
            "    # Now, actually generate the field assignment.\n",
            "    return _field_assign(frozen, f.name, value, self_name)\n",
            "\n",
            "\n",
            "def _init_param(f):\n",
            "    # Return the __init__ parameter string for this field.  For\n",
            "    # example, the equivalent of 'x:int=3' (except instead of 'int',\n",
            "    # reference a variable set to int, and instead of '3', reference a\n",
            "    # variable set to 3).\n",
            "    if f.default is MISSING and f.default_factory is MISSING:\n",
            "        # There's no default, and no default_factory, just output the\n",
            "        # variable name and type.\n",
            "        default = ''\n",
            "    elif f.default is not MISSING:\n",
            "        # There's a default, this will be the name that's used to look\n",
            "        # it up.\n",
            "        default = f'=_dflt_{f.name}'\n",
            "    elif f.default_factory is not MISSING:\n",
            "        # There's a factory function.  Set a marker.\n",
            "        default = '=_HAS_DEFAULT_FACTORY'\n",
            "    return f'{f.name}:_type_{f.name}{default}'\n",
            "\n",
            "\n",
            "def _init_fn(fields, frozen, has_post_init, self_name):\n",
            "    # fields contains both real fields and InitVar pseudo-fields.\n",
            "\n",
            "    # Make sure we don't have fields without defaults following fields\n",
            "    # with defaults.  This actually would be caught when exec-ing the\n",
            "    # function source code, but catching it here gives a better error\n",
            "    # message, and future-proofs us in case we build up the function\n",
            "    # using ast.\n",
            "    seen_default = False\n",
            "    for f in fields:\n",
            "        # Only consider fields in the __init__ call.\n",
            "        if f.init:\n",
            "            if not (f.default is MISSING and f.default_factory is MISSING):\n",
            "                seen_default = True\n",
            "            elif seen_default:\n",
            "                raise TypeError(f'non-default argument {f.name!r} '\n",
            "                                'follows default argument')\n",
            "\n",
            "    globals = {'MISSING': MISSING,\n",
            "               '_HAS_DEFAULT_FACTORY': _HAS_DEFAULT_FACTORY}\n",
            "\n",
            "    body_lines = []\n",
            "    for f in fields:\n",
            "        line = _field_init(f, frozen, globals, self_name)\n",
            "        # line is None means that this field doesn't require\n",
            "        # initialization (it's a pseudo-field).  Just skip it.\n",
            "        if line:\n",
            "            body_lines.append(line)\n",
            "\n",
            "    # Does this class have a post-init function?\n",
            "    if has_post_init:\n",
            "        params_str = ','.join(f.name for f in fields\n",
            "                              if f._field_type is _FIELD_INITVAR)\n",
            "        body_lines.append(f'{self_name}.{_POST_INIT_NAME}({params_str})')\n",
            "\n",
            "    # If no body lines, use 'pass'.\n",
            "    if not body_lines:\n",
            "        body_lines = ['pass']\n",
            "\n",
            "    locals = {f'_type_{f.name}': f.type for f in fields}\n",
            "    return _create_fn('__init__',\n",
            "                      [self_name] + [_init_param(f) for f in fields if f.init],\n",
            "                      body_lines,\n",
            "                      locals=locals,\n",
            "                      globals=globals,\n",
            "                      return_type=None)\n",
            "\n",
            "\n",
            "def _repr_fn(fields):\n",
            "    return _create_fn('__repr__',\n",
            "                      ('self',),\n",
            "                      ['return self.__class__.__qualname__ + f\"(' +\n",
            "                       ', '.join([f\"{f.name}={{self.{f.name}!r}}\"\n",
            "                                  for f in fields]) +\n",
            "                       ')\"'])\n",
            "\n",
            "\n",
            "def _frozen_get_del_attr(cls, fields):\n",
            "    # XXX: globals is modified on the first call to _create_fn, then\n",
            "    # the modified version is used in the second call.  Is this okay?\n",
            "    globals = {'cls': cls,\n",
            "              'FrozenInstanceError': FrozenInstanceError}\n",
            "    if fields:\n",
            "        fields_str = '(' + ','.join(repr(f.name) for f in fields) + ',)'\n",
            "    else:\n",
            "        # Special case for the zero-length tuple.\n",
            "        fields_str = '()'\n",
            "    return (_create_fn('__setattr__',\n",
            "                      ('self', 'name', 'value'),\n",
            "                      (f'if type(self) is cls or name in {fields_str}:',\n",
            "                        ' raise FrozenInstanceError(f\"cannot assign to field {name!r}\")',\n",
            "                       f'super(cls, self).__setattr__(name, value)'),\n",
            "                       globals=globals),\n",
            "            _create_fn('__delattr__',\n",
            "                      ('self', 'name'),\n",
            "                      (f'if type(self) is cls or name in {fields_str}:',\n",
            "                        ' raise FrozenInstanceError(f\"cannot delete field {name!r}\")',\n",
            "                       f'super(cls, self).__delattr__(name)'),\n",
            "                       globals=globals),\n",
            "            )\n",
            "\n",
            "\n",
            "def _cmp_fn(name, op, self_tuple, other_tuple):\n",
            "    # Create a comparison function.  If the fields in the object are\n",
            "    # named 'x' and 'y', then self_tuple is the string\n",
            "    # '(self.x,self.y)' and other_tuple is the string\n",
            "    # '(other.x,other.y)'.\n",
            "\n",
            "    return _create_fn(name,\n",
            "                      ('self', 'other'),\n",
            "                      [ 'if other.__class__ is self.__class__:',\n",
            "                       f' return {self_tuple}{op}{other_tuple}',\n",
            "                        'return NotImplemented'])\n",
            "\n",
            "\n",
            "def _hash_fn(fields):\n",
            "    self_tuple = _tuple_str('self', fields)\n",
            "    return _create_fn('__hash__',\n",
            "                      ('self',),\n",
            "                      [f'return hash({self_tuple})'])\n",
            "\n",
            "\n",
            "def _is_classvar(a_type, typing):\n",
            "    # This test uses a typing internal class, but it's the best way to\n",
            "    # test if this is a ClassVar.\n",
            "    return type(a_type) is typing._ClassVar\n",
            "\n",
            "\n",
            "def _is_initvar(a_type, dataclasses):\n",
            "    # The module we're checking against is the module we're\n",
            "    # currently in (dataclasses.py).\n",
            "    return a_type is dataclasses.InitVar\n",
            "\n",
            "\n",
            "def _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n",
            "    # Given a type annotation string, does it refer to a_type in\n",
            "    # a_module?  For example, when checking that annotation denotes a\n",
            "    # ClassVar, then a_module is typing, and a_type is\n",
            "    # typing.ClassVar.\n",
            "\n",
            "    # It's possible to look up a_module given a_type, but it involves\n",
            "    # looking in sys.modules (again!), and seems like a waste since\n",
            "    # the caller already knows a_module.\n",
            "\n",
            "    # - annotation is a string type annotation\n",
            "    # - cls is the class that this annotation was found in\n",
            "    # - a_module is the module we want to match\n",
            "    # - a_type is the type in that module we want to match\n",
            "    # - is_type_predicate is a function called with (obj, a_module)\n",
            "    #   that determines if obj is of the desired type.\n",
            "\n",
            "    # Since this test does not do a local namespace lookup (and\n",
            "    # instead only a module (global) lookup), there are some things it\n",
            "    # gets wrong.\n",
            "\n",
            "    # With string annotations, cv0 will be detected as a ClassVar:\n",
            "    #   CV = ClassVar\n",
            "    #   @dataclass\n",
            "    #   class C0:\n",
            "    #     cv0: CV\n",
            "\n",
            "    # But in this example cv1 will not be detected as a ClassVar:\n",
            "    #   @dataclass\n",
            "    #   class C1:\n",
            "    #     CV = ClassVar\n",
            "    #     cv1: CV\n",
            "\n",
            "    # In C1, the code in this function (_is_type) will look up \"CV\" in\n",
            "    # the module and not find it, so it will not consider cv1 as a\n",
            "    # ClassVar.  This is a fairly obscure corner case, and the best\n",
            "    # way to fix it would be to eval() the string \"CV\" with the\n",
            "    # correct global and local namespaces.  However that would involve\n",
            "    # a eval() penalty for every single field of every dataclass\n",
            "    # that's defined.  It was judged not worth it.\n",
            "\n",
            "    match = _MODULE_IDENTIFIER_RE.match(annotation)\n",
            "    if match:\n",
            "        ns = None\n",
            "        module_name = match.group(1)\n",
            "        if not module_name:\n",
            "            # No module name, assume the class's module did\n",
            "            # \"from dataclasses import InitVar\".\n",
            "            ns = sys.modules.get(cls.__module__).__dict__\n",
            "        else:\n",
            "            # Look up module_name in the class's module.\n",
            "            module = sys.modules.get(cls.__module__)\n",
            "            if module and module.__dict__.get(module_name) is a_module:\n",
            "                ns = sys.modules.get(a_type.__module__).__dict__\n",
            "        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n",
            "            return True\n",
            "    return False\n",
            "\n",
            "\n",
            "def _get_field(cls, a_name, a_type):\n",
            "    # Return a Field object for this field name and type.  ClassVars\n",
            "    # and InitVars are also returned, but marked as such (see\n",
            "    # f._field_type).\n",
            "\n",
            "    # If the default value isn't derived from Field, then it's only a\n",
            "    # normal default value.  Convert it to a Field().\n",
            "    default = getattr(cls, a_name, MISSING)\n",
            "    if isinstance(default, Field):\n",
            "        f = default\n",
            "    else:\n",
            "        if isinstance(default, types.MemberDescriptorType):\n",
            "            # This is a field in __slots__, so it has no default value.\n",
            "            default = MISSING\n",
            "        f = field(default=default)\n",
            "\n",
            "    # Only at this point do we know the name and the type.  Set them.\n",
            "    f.name = a_name\n",
            "    f.type = a_type\n",
            "\n",
            "    # Assume it's a normal field until proven otherwise.  We're next\n",
            "    # going to decide if it's a ClassVar or InitVar, everything else\n",
            "    # is just a normal field.\n",
            "    f._field_type = _FIELD\n",
            "\n",
            "    # In addition to checking for actual types here, also check for\n",
            "    # string annotations.  get_type_hints() won't always work for us\n",
            "    # (see https://github.com/python/typing/issues/508 for example),\n",
            "    # plus it's expensive and would require an eval for every stirng\n",
            "    # annotation.  So, make a best effort to see if this is a ClassVar\n",
            "    # or InitVar using regex's and checking that the thing referenced\n",
            "    # is actually of the correct type.\n",
            "\n",
            "    # For the complete discussion, see https://bugs.python.org/issue33453\n",
            "\n",
            "    # If typing has not been imported, then it's impossible for any\n",
            "    # annotation to be a ClassVar.  So, only look for ClassVar if\n",
            "    # typing has been imported by any module (not necessarily cls's\n",
            "    # module).\n",
            "    typing = sys.modules.get('typing')\n",
            "    if typing:\n",
            "        if (_is_classvar(a_type, typing)\n",
            "            or (isinstance(f.type, str)\n",
            "                and _is_type(f.type, cls, typing, typing.ClassVar,\n",
            "                             _is_classvar))):\n",
            "            f._field_type = _FIELD_CLASSVAR\n",
            "\n",
            "    # If the type is InitVar, or if it's a matching string annotation,\n",
            "    # then it's an InitVar.\n",
            "    if f._field_type is _FIELD:\n",
            "        # The module we're checking against is the module we're\n",
            "        # currently in (dataclasses.py).\n",
            "        dataclasses = sys.modules[__name__]\n",
            "        if (_is_initvar(a_type, dataclasses)\n",
            "            or (isinstance(f.type, str)\n",
            "                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n",
            "                             _is_initvar))):\n",
            "            f._field_type = _FIELD_INITVAR\n",
            "\n",
            "    # Validations for individual fields.  This is delayed until now,\n",
            "    # instead of in the Field() constructor, since only here do we\n",
            "    # know the field name, which allows for better error reporting.\n",
            "\n",
            "    # Special restrictions for ClassVar and InitVar.\n",
            "    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n",
            "        if f.default_factory is not MISSING:\n",
            "            raise TypeError(f'field {f.name} cannot have a '\n",
            "                            'default factory')\n",
            "        # Should I check for other field settings? default_factory\n",
            "        # seems the most serious to check for.  Maybe add others.  For\n",
            "        # example, how about init=False (or really,\n",
            "        # init=<not-the-default-init-value>)?  It makes no sense for\n",
            "        # ClassVar and InitVar to specify init=<anything>.\n",
            "\n",
            "    # For real fields, disallow mutable defaults for known types.\n",
            "    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\n",
            "        raise ValueError(f'mutable default {type(f.default)} for field '\n",
            "                         f'{f.name} is not allowed: use default_factory')\n",
            "\n",
            "    return f\n",
            "\n",
            "\n",
            "def _set_new_attribute(cls, name, value):\n",
            "    # Never overwrites an existing attribute.  Returns True if the\n",
            "    # attribute already exists.\n",
            "    if name in cls.__dict__:\n",
            "        return True\n",
            "    setattr(cls, name, value)\n",
            "    return False\n",
            "\n",
            "\n",
            "# Decide if/how we're going to create a hash function.  Key is\n",
            "# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n",
            "# take.  The common case is to do nothing, so instead of providing a\n",
            "# function that is a no-op, use None to signify that.\n",
            "\n",
            "def _hash_set_none(cls, fields):\n",
            "    return None\n",
            "\n",
            "def _hash_add(cls, fields):\n",
            "    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n",
            "    return _hash_fn(flds)\n",
            "\n",
            "def _hash_exception(cls, fields):\n",
            "    # Raise an exception.\n",
            "    raise TypeError(f'Cannot overwrite attribute __hash__ '\n",
            "                    f'in class {cls.__name__}')\n",
            "\n",
            "#\n",
            "#                +-------------------------------------- unsafe_hash?\n",
            "#                |      +------------------------------- eq?\n",
            "#                |      |      +------------------------ frozen?\n",
            "#                |      |      |      +----------------  has-explicit-hash?\n",
            "#                |      |      |      |\n",
            "#                |      |      |      |        +-------  action\n",
            "#                |      |      |      |        |\n",
            "#                v      v      v      v        v\n",
            "_hash_action = {(False, False, False, False): None,\n",
            "                (False, False, False, True ): None,\n",
            "                (False, False, True,  False): None,\n",
            "                (False, False, True,  True ): None,\n",
            "                (False, True,  False, False): _hash_set_none,\n",
            "                (False, True,  False, True ): None,\n",
            "                (False, True,  True,  False): _hash_add,\n",
            "                (False, True,  True,  True ): None,\n",
            "                (True,  False, False, False): _hash_add,\n",
            "                (True,  False, False, True ): _hash_exception,\n",
            "                (True,  False, True,  False): _hash_add,\n",
            "                (True,  False, True,  True ): _hash_exception,\n",
            "                (True,  True,  False, False): _hash_add,\n",
            "                (True,  True,  False, True ): _hash_exception,\n",
            "                (True,  True,  True,  False): _hash_add,\n",
            "                (True,  True,  True,  True ): _hash_exception,\n",
            "                }\n",
            "# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n",
            "# version of this table.\n",
            "\n",
            "\n",
            "def _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\n",
            "    # Now that dicts retain insertion order, there's no reason to use\n",
            "    # an ordered dict.  I am leveraging that ordering here, because\n",
            "    # derived class fields overwrite base class fields, but the order\n",
            "    # is defined by the base class, which is found first.\n",
            "    fields = {}\n",
            "\n",
            "    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n",
            "                                           unsafe_hash, frozen))\n",
            "\n",
            "    # Find our base classes in reverse MRO order, and exclude\n",
            "    # ourselves.  In reversed order so that more derived classes\n",
            "    # override earlier field definitions in base classes.  As long as\n",
            "    # we're iterating over them, see if any are frozen.\n",
            "    any_frozen_base = False\n",
            "    has_dataclass_bases = False\n",
            "    for b in cls.__mro__[-1:0:-1]:\n",
            "        # Only process classes that have been processed by our\n",
            "        # decorator.  That is, they have a _FIELDS attribute.\n",
            "        base_fields = getattr(b, _FIELDS, None)\n",
            "        if base_fields:\n",
            "            has_dataclass_bases = True\n",
            "            for f in base_fields.values():\n",
            "                fields[f.name] = f\n",
            "            if getattr(b, _PARAMS).frozen:\n",
            "                any_frozen_base = True\n",
            "\n",
            "    # Annotations that are defined in this class (not in base\n",
            "    # classes).  If __annotations__ isn't present, then this class\n",
            "    # adds no new annotations.  We use this to compute fields that are\n",
            "    # added by this class.\n",
            "    #\n",
            "    # Fields are found from cls_annotations, which is guaranteed to be\n",
            "    # ordered.  Default values are from class attributes, if a field\n",
            "    # has a default.  If the default value is a Field(), then it\n",
            "    # contains additional info beyond (and possibly including) the\n",
            "    # actual default value.  Pseudo-fields ClassVars and InitVars are\n",
            "    # included, despite the fact that they're not real fields.  That's\n",
            "    # dealt with later.\n",
            "    cls_annotations = cls.__dict__.get('__annotations__', {})\n",
            "\n",
            "    # Now find fields in our class.  While doing so, validate some\n",
            "    # things, and set the default values (as class attributes) where\n",
            "    # we can.\n",
            "    cls_fields = [_get_field(cls, name, type)\n",
            "                  for name, type in cls_annotations.items()]\n",
            "    for f in cls_fields:\n",
            "        fields[f.name] = f\n",
            "\n",
            "        # If the class attribute (which is the default value for this\n",
            "        # field) exists and is of type 'Field', replace it with the\n",
            "        # real default.  This is so that normal class introspection\n",
            "        # sees a real default value, not a Field.\n",
            "        if isinstance(getattr(cls, f.name, None), Field):\n",
            "            if f.default is MISSING:\n",
            "                # If there's no default, delete the class attribute.\n",
            "                # This happens if we specify field(repr=False), for\n",
            "                # example (that is, we specified a field object, but\n",
            "                # no default value).  Also if we're using a default\n",
            "                # factory.  The class attribute should not be set at\n",
            "                # all in the post-processed class.\n",
            "                delattr(cls, f.name)\n",
            "            else:\n",
            "                setattr(cls, f.name, f.default)\n",
            "\n",
            "    # Do we have any Field members that don't also have annotations?\n",
            "    for name, value in cls.__dict__.items():\n",
            "        if isinstance(value, Field) and not name in cls_annotations:\n",
            "            raise TypeError(f'{name!r} is a field but has no type annotation')\n",
            "\n",
            "    # Check rules that apply if we are derived from any dataclasses.\n",
            "    if has_dataclass_bases:\n",
            "        # Raise an exception if any of our bases are frozen, but we're not.\n",
            "        if any_frozen_base and not frozen:\n",
            "            raise TypeError('cannot inherit non-frozen dataclass from a '\n",
            "                            'frozen one')\n",
            "\n",
            "        # Raise an exception if we're frozen, but none of our bases are.\n",
            "        if not any_frozen_base and frozen:\n",
            "            raise TypeError('cannot inherit frozen dataclass from a '\n",
            "                            'non-frozen one')\n",
            "\n",
            "    # Remember all of the fields on our class (including bases).  This\n",
            "    # also marks this class as being a dataclass.\n",
            "    setattr(cls, _FIELDS, fields)\n",
            "\n",
            "    # Was this class defined with an explicit __hash__?  Note that if\n",
            "    # __eq__ is defined in this class, then python will automatically\n",
            "    # set __hash__ to None.  This is a heuristic, as it's possible\n",
            "    # that such a __hash__ == None was not auto-generated, but it\n",
            "    # close enough.\n",
            "    class_hash = cls.__dict__.get('__hash__', MISSING)\n",
            "    has_explicit_hash = not (class_hash is MISSING or\n",
            "                             (class_hash is None and '__eq__' in cls.__dict__))\n",
            "\n",
            "    # If we're generating ordering methods, we must be generating the\n",
            "    # eq methods.\n",
            "    if order and not eq:\n",
            "        raise ValueError('eq must be true if order is true')\n",
            "\n",
            "    if init:\n",
            "        # Does this class have a post-init function?\n",
            "        has_post_init = hasattr(cls, _POST_INIT_NAME)\n",
            "\n",
            "        # Include InitVars and regular fields (so, not ClassVars).\n",
            "        flds = [f for f in fields.values()\n",
            "                if f._field_type in (_FIELD, _FIELD_INITVAR)]\n",
            "        _set_new_attribute(cls, '__init__',\n",
            "                           _init_fn(flds,\n",
            "                                    frozen,\n",
            "                                    has_post_init,\n",
            "                                    # The name to use for the \"self\"\n",
            "                                    # param in __init__.  Use \"self\"\n",
            "                                    # if possible.\n",
            "                                    '__dataclass_self__' if 'self' in fields\n",
            "                                            else 'self',\n",
            "                          ))\n",
            "\n",
            "    # Get the fields as a list, and include only real fields.  This is\n",
            "    # used in all of the following methods.\n",
            "    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n",
            "\n",
            "    if repr:\n",
            "        flds = [f for f in field_list if f.repr]\n",
            "        _set_new_attribute(cls, '__repr__', _repr_fn(flds))\n",
            "\n",
            "    if eq:\n",
            "        # Create _eq__ method.  There's no need for a __ne__ method,\n",
            "        # since python will call __eq__ and negate it.\n",
            "        flds = [f for f in field_list if f.compare]\n",
            "        self_tuple = _tuple_str('self', flds)\n",
            "        other_tuple = _tuple_str('other', flds)\n",
            "        _set_new_attribute(cls, '__eq__',\n",
            "                           _cmp_fn('__eq__', '==',\n",
            "                                   self_tuple, other_tuple))\n",
            "\n",
            "    if order:\n",
            "        # Create and set the ordering methods.\n",
            "        flds = [f for f in field_list if f.compare]\n",
            "        self_tuple = _tuple_str('self', flds)\n",
            "        other_tuple = _tuple_str('other', flds)\n",
            "        for name, op in [('__lt__', '<'),\n",
            "                         ('__le__', '<='),\n",
            "                         ('__gt__', '>'),\n",
            "                         ('__ge__', '>='),\n",
            "                         ]:\n",
            "            if _set_new_attribute(cls, name,\n",
            "                                  _cmp_fn(name, op, self_tuple, other_tuple)):\n",
            "                raise TypeError(f'Cannot overwrite attribute {name} '\n",
            "                                f'in class {cls.__name__}. Consider using '\n",
            "                                'functools.total_ordering')\n",
            "\n",
            "    if frozen:\n",
            "        for fn in _frozen_get_del_attr(cls, field_list):\n",
            "            if _set_new_attribute(cls, fn.__name__, fn):\n",
            "                raise TypeError(f'Cannot overwrite attribute {fn.__name__} '\n",
            "                                f'in class {cls.__name__}')\n",
            "\n",
            "    # Decide if/how we're going to create a hash function.\n",
            "    hash_action = _hash_action[bool(unsafe_hash),\n",
            "                               bool(eq),\n",
            "                               bool(frozen),\n",
            "                               has_explicit_hash]\n",
            "    if hash_action:\n",
            "        # No need to call _set_new_attribute here, since by the time\n",
            "        # we're here the overwriting is unconditional.\n",
            "        cls.__hash__ = hash_action(cls, field_list)\n",
            "\n",
            "    if not getattr(cls, '__doc__'):\n",
            "        # Create a class doc-string.\n",
            "        cls.__doc__ = (cls.__name__ +\n",
            "                       str(inspect.signature(cls)).replace(' -> None', ''))\n",
            "\n",
            "    return cls\n",
            "\n",
            "\n",
            "# _cls should never be specified by keyword, so start it with an\n",
            "# underscore.  The presence of _cls is used to detect if this\n",
            "# decorator is being called with parameters or not.\n",
            "def dataclass(_cls=None, *, init=True, repr=True, eq=True, order=False,\n",
            "              unsafe_hash=False, frozen=False):\n",
            "    \"\"\"Returns the same class as was passed in, with dunder methods\n",
            "    added based on the fields defined in the class.\n",
            "\n",
            "    Examines PEP 526 __annotations__ to determine fields.\n",
            "\n",
            "    If init is true, an __init__() method is added to the class. If\n",
            "    repr is true, a __repr__() method is added. If order is true, rich\n",
            "    comparison dunder methods are added. If unsafe_hash is true, a\n",
            "    __hash__() method function is added. If frozen is true, fields may\n",
            "    not be assigned to after instance creation.\n",
            "    \"\"\"\n",
            "\n",
            "    def wrap(cls):\n",
            "        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n",
            "\n",
            "    # See if we're being called as @dataclass or @dataclass().\n",
            "    if _cls is None:\n",
            "        # We're called with parens.\n",
            "        return wrap\n",
            "\n",
            "    # We're called as @dataclass without parens.\n",
            "    return wrap(_cls)\n",
            "\n",
            "\n",
            "def fields(class_or_instance):\n",
            "    \"\"\"Return a tuple describing the fields of this dataclass.\n",
            "\n",
            "    Accepts a dataclass or an instance of one. Tuple elements are of\n",
            "    type Field.\n",
            "    \"\"\"\n",
            "\n",
            "    # Might it be worth caching this, per class?\n",
            "    try:\n",
            "        fields = getattr(class_or_instance, _FIELDS)\n",
            "    except AttributeError:\n",
            "        raise TypeError('must be called with a dataclass type or instance')\n",
            "\n",
            "    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n",
            "    # order, so the order of the tuple is as the fields were defined.\n",
            "    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n",
            "\n",
            "\n",
            "def _is_dataclass_instance(obj):\n",
            "    \"\"\"Returns True if obj is an instance of a dataclass.\"\"\"\n",
            "    return not isinstance(obj, type) and hasattr(obj, _FIELDS)\n",
            "\n",
            "\n",
            "def is_dataclass(obj):\n",
            "    \"\"\"Returns True if obj is a dataclass or an instance of a\n",
            "    dataclass.\"\"\"\n",
            "    return hasattr(obj, _FIELDS)\n",
            "\n",
            "\n",
            "def asdict(obj, *, dict_factory=dict):\n",
            "    \"\"\"Return the fields of a dataclass instance as a new dictionary mapping\n",
            "    field names to field values.\n",
            "\n",
            "    Example usage:\n",
            "\n",
            "      @dataclass\n",
            "      class C:\n",
            "          x: int\n",
            "          y: int\n",
            "\n",
            "      c = C(1, 2)\n",
            "      assert asdict(c) == {'x': 1, 'y': 2}\n",
            "\n",
            "    If given, 'dict_factory' will be used instead of built-in dict.\n",
            "    The function applies recursively to field values that are\n",
            "    dataclass instances. This will also look into built-in containers:\n",
            "    tuples, lists, and dicts.\n",
            "    \"\"\"\n",
            "    if not _is_dataclass_instance(obj):\n",
            "        raise TypeError(\"asdict() should be called on dataclass instances\")\n",
            "    return _asdict_inner(obj, dict_factory)\n",
            "\n",
            "\n",
            "def _asdict_inner(obj, dict_factory):\n",
            "    if _is_dataclass_instance(obj):\n",
            "        result = []\n",
            "        for f in fields(obj):\n",
            "            value = _asdict_inner(getattr(obj, f.name), dict_factory)\n",
            "            result.append((f.name, value))\n",
            "        return dict_factory(result)\n",
            "    elif isinstance(obj, (list, tuple)):\n",
            "        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)\n",
            "    elif isinstance(obj, dict):\n",
            "        return type(obj)((_asdict_inner(k, dict_factory), _asdict_inner(v, dict_factory))\n",
            "                          for k, v in obj.items())\n",
            "    else:\n",
            "        return copy.deepcopy(obj)\n",
            "\n",
            "\n",
            "def astuple(obj, *, tuple_factory=tuple):\n",
            "    \"\"\"Return the fields of a dataclass instance as a new tuple of field values.\n",
            "\n",
            "    Example usage::\n",
            "\n",
            "      @dataclass\n",
            "      class C:\n",
            "          x: int\n",
            "          y: int\n",
            "\n",
            "    c = C(1, 2)\n",
            "    assert astuple(c) == (1, 2)\n",
            "\n",
            "    If given, 'tuple_factory' will be used instead of built-in tuple.\n",
            "    The function applies recursively to field values that are\n",
            "    dataclass instances. This will also look into built-in containers:\n",
            "    tuples, lists, and dicts.\n",
            "    \"\"\"\n",
            "\n",
            "    if not _is_dataclass_instance(obj):\n",
            "        raise TypeError(\"astuple() should be called on dataclass instances\")\n",
            "    return _astuple_inner(obj, tuple_factory)\n",
            "\n",
            "\n",
            "def _astuple_inner(obj, tuple_factory):\n",
            "    if _is_dataclass_instance(obj):\n",
            "        result = []\n",
            "        for f in fields(obj):\n",
            "            value = _astuple_inner(getattr(obj, f.name), tuple_factory)\n",
            "            result.append(value)\n",
            "        return tuple_factory(result)\n",
            "    elif isinstance(obj, (list, tuple)):\n",
            "        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n",
            "    elif isinstance(obj, dict):\n",
            "        return type(obj)((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n",
            "                          for k, v in obj.items())\n",
            "    else:\n",
            "        return copy.deepcopy(obj)\n",
            "\n",
            "\n",
            "def make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n",
            "                   repr=True, eq=True, order=False, unsafe_hash=False,\n",
            "                   frozen=False):\n",
            "    \"\"\"Return a new dynamically created dataclass.\n",
            "\n",
            "    The dataclass name will be 'cls_name'.  'fields' is an iterable\n",
            "    of either (name), (name, type) or (name, type, Field) objects. If type is\n",
            "    omitted, use the string 'typing.Any'.  Field objects are created by\n",
            "    the equivalent of calling 'field(name, type [, Field-info])'.\n",
            "\n",
            "      C = make_dataclass('C', ['x', ('y', int), ('z', int, field(init=False))], bases=(Base,))\n",
            "\n",
            "    is equivalent to:\n",
            "\n",
            "      @dataclass\n",
            "      class C(Base):\n",
            "          x: 'typing.Any'\n",
            "          y: int\n",
            "          z: int = field(init=False)\n",
            "\n",
            "    For the bases and namespace parameters, see the builtin type() function.\n",
            "\n",
            "    The parameters init, repr, eq, order, unsafe_hash, and frozen are passed to\n",
            "    dataclass().\n",
            "    \"\"\"\n",
            "\n",
            "    if namespace is None:\n",
            "        namespace = {}\n",
            "    else:\n",
            "        # Copy namespace since we're going to mutate it.\n",
            "        namespace = namespace.copy()\n",
            "\n",
            "    # While we're looking through the field names, validate that they\n",
            "    # are identifiers, are not keywords, and not duplicates.\n",
            "    seen = set()\n",
            "    anns = {}\n",
            "    for item in fields:\n",
            "        if isinstance(item, str):\n",
            "            name = item\n",
            "            tp = 'typing.Any'\n",
            "        elif len(item) == 2:\n",
            "            name, tp, = item\n",
            "        elif len(item) == 3:\n",
            "            name, tp, spec = item\n",
            "            namespace[name] = spec\n",
            "        else:\n",
            "            raise TypeError(f'Invalid field: {item!r}')\n",
            "\n",
            "        if not isinstance(name, str) or not name.isidentifier():\n",
            "            raise TypeError(f'Field names must be valid identifers: {name!r}')\n",
            "        if keyword.iskeyword(name):\n",
            "            raise TypeError(f'Field names must not be keywords: {name!r}')\n",
            "        if name in seen:\n",
            "            raise TypeError(f'Field name duplicated: {name!r}')\n",
            "\n",
            "        seen.add(name)\n",
            "        anns[name] = tp\n",
            "\n",
            "    namespace['__annotations__'] = anns\n",
            "    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n",
            "    # of generic dataclassses.\n",
            "    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\n",
            "    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n",
            "                     unsafe_hash=unsafe_hash, frozen=frozen)\n",
            "\n",
            "\n",
            "def replace(obj, **changes):\n",
            "    \"\"\"Return a new object replacing specified fields with new values.\n",
            "\n",
            "    This is especially useful for frozen classes.  Example usage:\n",
            "\n",
            "      @dataclass(frozen=True)\n",
            "      class C:\n",
            "          x: int\n",
            "          y: int\n",
            "\n",
            "      c = C(1, 2)\n",
            "      c1 = replace(c, x=3)\n",
            "      assert c1.x == 3 and c1.y == 2\n",
            "      \"\"\"\n",
            "\n",
            "    # We're going to mutate 'changes', but that's okay because it's a\n",
            "    # new dict, even if called with 'replace(obj, **my_changes)'.\n",
            "\n",
            "    if not _is_dataclass_instance(obj):\n",
            "        raise TypeError(\"replace() should be called on dataclass instances\")\n",
            "\n",
            "    # It's an error to have init=False fields in 'changes'.\n",
            "    # If a field is not in 'changes', read its value from the provided obj.\n",
            "\n",
            "    for f in getattr(obj, _FIELDS).values():\n",
            "        if not f.init:\n",
            "            # Error if this field is specified in changes.\n",
            "            if f.name in changes:\n",
            "                raise ValueError(f'field {f.name} is declared with '\n",
            "                                 'init=False, it cannot be specified with '\n",
            "                                 'replace()')\n",
            "            continue\n",
            "\n",
            "        if f.name not in changes:\n",
            "            changes[f.name] = getattr(obj, f.name)\n",
            "\n",
            "    # Create the new object, which calls __init__() and\n",
            "    # __post_init__() (if defined), using all of the init fields we've\n",
            "    # added and/or left in 'changes'.  If there are values supplied in\n",
            "    # changes that aren't fields, this will correctly raise a\n",
            "    # TypeError.\n",
            "    return obj.__class__(**changes)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"An interface to GraphViz.\"\"\"\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "import copy\n",
            "import io\n",
            "import errno\n",
            "import os\n",
            "import re\n",
            "import subprocess\n",
            "import sys\n",
            "import tempfile\n",
            "import warnings\n",
            "\n",
            "try:\n",
            "    import dot_parser\n",
            "except Exception as e:\n",
            "    warnings.warn(\n",
            "        \"Couldn't import dot_parser, \"\n",
            "        \"loading of dot files will not be possible.\")\n",
            "\n",
            "\n",
            "__author__ = 'Ero Carrera'\n",
            "__version__ = '1.3.0'\n",
            "__license__ = 'MIT'\n",
            "\n",
            "\n",
            "PY3 = sys.version_info >= (3, 0, 0)\n",
            "if PY3:\n",
            "    str_type = str\n",
            "else:\n",
            "    str_type = basestring\n",
            "\n",
            "\n",
            "GRAPH_ATTRIBUTES = { 'Damping', 'K', 'URL', 'aspect', 'bb', 'bgcolor',\n",
            "    'center', 'charset', 'clusterrank', 'colorscheme', 'comment', 'compound',\n",
            "    'concentrate', 'defaultdist', 'dim', 'dimen', 'diredgeconstraints',\n",
            "    'dpi', 'epsilon', 'esep', 'fontcolor', 'fontname', 'fontnames',\n",
            "    'fontpath', 'fontsize', 'id', 'label', 'labeljust', 'labelloc',\n",
            "    'landscape', 'layers', 'layersep', 'layout', 'levels', 'levelsgap',\n",
            "    'lheight', 'lp', 'lwidth', 'margin', 'maxiter', 'mclimit', 'mindist',\n",
            "    'mode', 'model', 'mosek', 'nodesep', 'nojustify', 'normalize', 'nslimit',\n",
            "    'nslimit1', 'ordering', 'orientation', 'outputorder', 'overlap',\n",
            "    'overlap_scaling', 'pack', 'packmode', 'pad', 'page', 'pagedir',\n",
            "    'quadtree', 'quantum', 'rankdir', 'ranksep', 'ratio', 'remincross',\n",
            "    'repulsiveforce', 'resolution', 'root', 'rotate', 'searchsize', 'sep',\n",
            "    'showboxes', 'size', 'smoothing', 'sortv', 'splines', 'start',\n",
            "    'stylesheet', 'target', 'truecolor', 'viewport', 'voro_margin',\n",
            "    # for subgraphs\n",
            "    'rank'  }\n",
            "\n",
            "\n",
            "EDGE_ATTRIBUTES = { 'URL', 'arrowhead', 'arrowsize', 'arrowtail',\n",
            "    'color', 'colorscheme', 'comment', 'constraint', 'decorate', 'dir',\n",
            "    'edgeURL', 'edgehref', 'edgetarget', 'edgetooltip', 'fontcolor',\n",
            "    'fontname', 'fontsize', 'headURL', 'headclip', 'headhref', 'headlabel',\n",
            "    'headport', 'headtarget', 'headtooltip', 'href', 'id', 'label',\n",
            "    'labelURL', 'labelangle', 'labeldistance', 'labelfloat', 'labelfontcolor',\n",
            "    'labelfontname', 'labelfontsize', 'labelhref', 'labeltarget',\n",
            "    'labeltooltip', 'layer', 'len', 'lhead', 'lp', 'ltail', 'minlen',\n",
            "    'nojustify', 'penwidth', 'pos', 'samehead', 'sametail', 'showboxes',\n",
            "    'style', 'tailURL', 'tailclip', 'tailhref', 'taillabel', 'tailport',\n",
            "    'tailtarget', 'tailtooltip', 'target', 'tooltip', 'weight',\n",
            "    'rank'  }\n",
            "\n",
            "\n",
            "NODE_ATTRIBUTES = { 'URL', 'color', 'colorscheme', 'comment',\n",
            "    'distortion', 'fillcolor', 'fixedsize', 'fontcolor', 'fontname',\n",
            "    'fontsize', 'group', 'height', 'id', 'image', 'imagescale', 'label',\n",
            "    'labelloc', 'layer', 'margin', 'nojustify', 'orientation', 'penwidth',\n",
            "    'peripheries', 'pin', 'pos', 'rects', 'regular', 'root', 'samplepoints',\n",
            "    'shape', 'shapefile', 'showboxes', 'sides', 'skew', 'sortv', 'style',\n",
            "    'target', 'tooltip', 'vertices', 'width', 'z',\n",
            "    # The following are attributes dot2tex\n",
            "    'texlbl',  'texmode'  }\n",
            "\n",
            "\n",
            "CLUSTER_ATTRIBUTES = { 'K', 'URL', 'bgcolor', 'color', 'colorscheme',\n",
            "    'fillcolor', 'fontcolor', 'fontname', 'fontsize', 'label', 'labeljust',\n",
            "    'labelloc', 'lheight', 'lp', 'lwidth', 'nojustify', 'pencolor',\n",
            "    'penwidth', 'peripheries', 'sortv', 'style', 'target', 'tooltip' }\n",
            "\n",
            "\n",
            "#\n",
            "# Extended version of ASPN's Python Cookbook Recipe:\n",
            "# Frozen dictionaries.\n",
            "# https://code.activestate.com/recipes/414283/\n",
            "#\n",
            "# This version freezes dictionaries used as values within dictionaries.\n",
            "#\n",
            "class frozendict(dict):\n",
            "    def _blocked_attribute(obj):\n",
            "        raise AttributeError('A frozendict cannot be modified.')\n",
            "    _blocked_attribute = property(_blocked_attribute)\n",
            "\n",
            "    __delitem__ = __setitem__ = clear = _blocked_attribute\n",
            "    pop = popitem = setdefault = update = _blocked_attribute\n",
            "\n",
            "    def __new__(cls, *args, **kw):\n",
            "        new = dict.__new__(cls)\n",
            "\n",
            "        args_ = []\n",
            "        for arg in args:\n",
            "            if isinstance(arg, dict):\n",
            "                arg = copy.copy(arg)\n",
            "                for k in arg:\n",
            "                    v = arg[k]\n",
            "                    if isinstance(v, frozendict):\n",
            "                        arg[k] = v\n",
            "                    elif isinstance(v, dict):\n",
            "                        arg[k] = frozendict(v)\n",
            "                    elif isinstance(v, list):\n",
            "                        v_ = list()\n",
            "                        for elm in v:\n",
            "                            if isinstance(elm, dict):\n",
            "                                v_.append( frozendict(elm) )\n",
            "                            else:\n",
            "                                v_.append( elm )\n",
            "                        arg[k] = tuple(v_)\n",
            "                args_.append( arg )\n",
            "            else:\n",
            "                args_.append( arg )\n",
            "\n",
            "        dict.__init__(new, *args_, **kw)\n",
            "        return new\n",
            "\n",
            "    def __init__(self, *args, **kw):\n",
            "        pass\n",
            "\n",
            "    def __hash__(self):\n",
            "        try:\n",
            "            return self._cached_hash\n",
            "        except AttributeError:\n",
            "            h = self._cached_hash = hash(tuple(sorted(self.items())))\n",
            "            return h\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"frozendict(%s)\" % dict.__repr__(self)\n",
            "\n",
            "\n",
            "dot_keywords = ['graph', 'subgraph', 'digraph', 'node', 'edge', 'strict']\n",
            "\n",
            "id_re_alpha_nums = re.compile('^[_a-zA-Z][a-zA-Z0-9_,]*$', re.UNICODE)\n",
            "id_re_alpha_nums_with_ports = re.compile(\n",
            "    '^[_a-zA-Z][a-zA-Z0-9_,:\\\"]*[a-zA-Z0-9_,\\\"]+$', re.UNICODE)\n",
            "id_re_num = re.compile('^[0-9,]+$', re.UNICODE)\n",
            "id_re_with_port = re.compile('^([^:]*):([^:]*)$', re.UNICODE)\n",
            "id_re_dbl_quoted = re.compile('^\\\".*\\\"$', re.S|re.UNICODE)\n",
            "id_re_html = re.compile('^<.*>$', re.S|re.UNICODE)\n",
            "\n",
            "\n",
            "def needs_quotes( s ):\n",
            "    \"\"\"Checks whether a string is a dot language ID.\n",
            "\n",
            "    It will check whether the string is solely composed\n",
            "    by the characters allowed in an ID or not.\n",
            "    If the string is one of the reserved keywords it will\n",
            "    need quotes too but the user will need to add them\n",
            "    manually.\n",
            "    \"\"\"\n",
            "\n",
            "    # If the name is a reserved keyword it will need quotes but pydot\n",
            "    # can't tell when it's being used as a keyword or when it's simply\n",
            "    # a name. Hence the user needs to supply the quotes when an element\n",
            "    # would use a reserved keyword as name. This function will return\n",
            "    # false indicating that a keyword string, if provided as-is, won't\n",
            "    # need quotes.\n",
            "    if s in dot_keywords:\n",
            "        return False\n",
            "\n",
            "    chars = [ord(c) for c in s if ord(c)>0x7f or ord(c)==0]\n",
            "    if chars and not id_re_dbl_quoted.match(s) and not id_re_html.match(s):\n",
            "        return True\n",
            "\n",
            "    for test_re in [id_re_alpha_nums, id_re_num,\n",
            "                    id_re_dbl_quoted, id_re_html,\n",
            "                    id_re_alpha_nums_with_ports]:\n",
            "        if test_re.match(s):\n",
            "            return False\n",
            "\n",
            "    m = id_re_with_port.match(s)\n",
            "    if m:\n",
            "        return needs_quotes(m.group(1)) or needs_quotes(m.group(2))\n",
            "\n",
            "    return True\n",
            "\n",
            "\n",
            "def quote_if_necessary(s):\n",
            "    \"\"\"Enclode attribute value in quotes, if needed.\"\"\"\n",
            "    if isinstance(s, bool):\n",
            "        if s is True:\n",
            "            return 'True'\n",
            "        return 'False'\n",
            "\n",
            "    if not isinstance( s, str_type):\n",
            "        return s\n",
            "\n",
            "    if not s:\n",
            "        return s\n",
            "\n",
            "    if needs_quotes(s):\n",
            "        replace = {'\"'  : r'\\\"',\n",
            "                   \"\\n\" : r'\\n',\n",
            "                   \"\\r\" : r'\\r'}\n",
            "        for (a,b) in replace.items():\n",
            "            s = s.replace(a, b)\n",
            "\n",
            "        return '\"' + s + '\"'\n",
            "\n",
            "    return s\n",
            "\n",
            "\n",
            "\n",
            "def graph_from_dot_data(s):\n",
            "    \"\"\"Load graphs from DOT description in string `s`.\n",
            "\n",
            "    @param s: string in [DOT language](\n",
            "        https://en.wikipedia.org/wiki/DOT_(graph_description_language))\n",
            "\n",
            "    @return: Graphs that result from parsing.\n",
            "    @rtype: `list` of `pydot.Dot`\n",
            "    \"\"\"\n",
            "    return dot_parser.parse_dot_data(s)\n",
            "\n",
            "\n",
            "def graph_from_dot_file(path, encoding=None):\n",
            "    \"\"\"Load graphs from DOT file at `path`.\n",
            "\n",
            "    @param path: to DOT file\n",
            "    @param encoding: as passed to `io.open`.\n",
            "        For example, `'utf-8'`.\n",
            "\n",
            "    @return: Graphs that result from parsing.\n",
            "    @rtype: `list` of `pydot.Dot`\n",
            "    \"\"\"\n",
            "    with io.open(path, 'rt', encoding=encoding) as f:\n",
            "        s = f.read()\n",
            "    if not PY3:\n",
            "        s = unicode(s)\n",
            "    graphs = graph_from_dot_data(s)\n",
            "    return graphs\n",
            "\n",
            "\n",
            "\n",
            "def graph_from_edges(edge_list, node_prefix='', directed=False):\n",
            "    \"\"\"Creates a basic graph out of an edge list.\n",
            "\n",
            "    The edge list has to be a list of tuples representing\n",
            "    the nodes connected by the edge.\n",
            "    The values can be anything: bool, int, float, str.\n",
            "\n",
            "    If the graph is undirected by default, it is only\n",
            "    calculated from one of the symmetric halves of the matrix.\n",
            "    \"\"\"\n",
            "\n",
            "    if directed:\n",
            "        graph = Dot(graph_type='digraph')\n",
            "\n",
            "    else:\n",
            "        graph = Dot(graph_type='graph')\n",
            "\n",
            "    for edge in edge_list:\n",
            "\n",
            "        if isinstance(edge[0], str):\n",
            "            src = node_prefix + edge[0]\n",
            "        else:\n",
            "            src = node_prefix + str(edge[0])\n",
            "\n",
            "        if isinstance(edge[1], str):\n",
            "            dst = node_prefix + edge[1]\n",
            "        else:\n",
            "            dst = node_prefix + str(edge[1])\n",
            "\n",
            "        e = Edge( src, dst )\n",
            "        graph.add_edge(e)\n",
            "\n",
            "    return graph\n",
            "\n",
            "\n",
            "def graph_from_adjacency_matrix(matrix, node_prefix= u'', directed=False):\n",
            "    \"\"\"Creates a basic graph out of an adjacency matrix.\n",
            "\n",
            "    The matrix has to be a list of rows of values\n",
            "    representing an adjacency matrix.\n",
            "    The values can be anything: bool, int, float, as long\n",
            "    as they can evaluate to True or False.\n",
            "    \"\"\"\n",
            "\n",
            "    node_orig = 1\n",
            "\n",
            "    if directed:\n",
            "        graph = Dot(graph_type='digraph')\n",
            "    else:\n",
            "        graph = Dot(graph_type='graph')\n",
            "\n",
            "    for row in matrix:\n",
            "        if not directed:\n",
            "            skip = matrix.index(row)\n",
            "            r = row[skip:]\n",
            "        else:\n",
            "            skip = 0\n",
            "            r = row\n",
            "        node_dest = skip+1\n",
            "\n",
            "        for e in r:\n",
            "            if e:\n",
            "                graph.add_edge(\n",
            "                    Edge( node_prefix + node_orig,\n",
            "                        node_prefix + node_dest) )\n",
            "            node_dest += 1\n",
            "        node_orig += 1\n",
            "\n",
            "    return graph\n",
            "\n",
            "\n",
            "\n",
            "def graph_from_incidence_matrix(matrix, node_prefix='', directed=False):\n",
            "    \"\"\"Creates a basic graph out of an incidence matrix.\n",
            "\n",
            "    The matrix has to be a list of rows of values\n",
            "    representing an incidence matrix.\n",
            "    The values can be anything: bool, int, float, as long\n",
            "    as they can evaluate to True or False.\n",
            "    \"\"\"\n",
            "\n",
            "    node_orig = 1\n",
            "\n",
            "    if directed:\n",
            "        graph = Dot(graph_type='digraph')\n",
            "    else:\n",
            "        graph = Dot(graph_type='graph')\n",
            "\n",
            "    for row in matrix:\n",
            "        nodes = []\n",
            "        c = 1\n",
            "\n",
            "        for node in row:\n",
            "            if node:\n",
            "                nodes.append(c*node)\n",
            "            c += 1\n",
            "            nodes.sort()\n",
            "\n",
            "        if len(nodes) == 2:\n",
            "            graph.add_edge(\n",
            "                Edge( node_prefix + abs(nodes[0]),\n",
            "                    node_prefix + nodes[1] ))\n",
            "\n",
            "    if not directed:\n",
            "        graph.set_simplify(True)\n",
            "\n",
            "    return graph\n",
            "\n",
            "\n",
            "class Common(object):\n",
            "    \"\"\"Common information to several classes.\n",
            "\n",
            "    Should not be directly used, several classes are derived from\n",
            "    this one.\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "    def __getstate__(self):\n",
            "\n",
            "        dict = copy.copy(self.obj_dict)\n",
            "\n",
            "        return dict\n",
            "\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "\n",
            "        self.obj_dict = state\n",
            "\n",
            "\n",
            "    def __get_attribute__(self, attr):\n",
            "        \"\"\"Look for default attributes for this node\"\"\"\n",
            "\n",
            "        attr_val = self.obj_dict['attributes'].get(attr, None)\n",
            "\n",
            "        if attr_val is None:\n",
            "            # get the defaults for nodes/edges\n",
            "\n",
            "            default_node_name = self.obj_dict['type']\n",
            "\n",
            "            # The defaults for graphs are set on a node named 'graph'\n",
            "            if default_node_name in ('subgraph', 'digraph', 'cluster'):\n",
            "                default_node_name = 'graph'\n",
            "\n",
            "            g = self.get_parent_graph()\n",
            "            if g is not None:\n",
            "                defaults = g.get_node( default_node_name )\n",
            "            else:\n",
            "                return None\n",
            "\n",
            "            # Multiple defaults could be set by having repeated 'graph [...]'\n",
            "            # 'node [...]', 'edge [...]' statements. In such case, if the\n",
            "            # same attribute is set in different statements, only the first\n",
            "            # will be returned. In order to get all, one would call the\n",
            "            # get_*_defaults() methods and handle those. Or go node by node\n",
            "            # (of the ones specifying defaults) and modify the attributes\n",
            "            # individually.\n",
            "            #\n",
            "            if not isinstance(defaults, (list, tuple)):\n",
            "                defaults = [defaults]\n",
            "\n",
            "            for default in defaults:\n",
            "                attr_val = default.obj_dict['attributes'].get(attr, None)\n",
            "                if attr_val:\n",
            "                    return attr_val\n",
            "        else:\n",
            "            return attr_val\n",
            "\n",
            "        return None\n",
            "\n",
            "\n",
            "    def set_parent_graph(self, parent_graph):\n",
            "\n",
            "        self.obj_dict['parent_graph'] = parent_graph\n",
            "\n",
            "\n",
            "    def get_parent_graph(self):\n",
            "\n",
            "        return self.obj_dict.get('parent_graph', None)\n",
            "\n",
            "\n",
            "    def set(self, name, value):\n",
            "        \"\"\"Set an attribute value by name.\n",
            "\n",
            "        Given an attribute 'name' it will set its value to 'value'.\n",
            "        There's always the possibility of using the methods:\n",
            "\n",
            "            set_'name'(value)\n",
            "\n",
            "        which are defined for all the existing attributes.\n",
            "        \"\"\"\n",
            "\n",
            "        self.obj_dict['attributes'][name] = value\n",
            "\n",
            "\n",
            "    def get(self, name):\n",
            "        \"\"\"Get an attribute value by name.\n",
            "\n",
            "        Given an attribute 'name' it will get its value.\n",
            "        There's always the possibility of using the methods:\n",
            "\n",
            "            get_'name'()\n",
            "\n",
            "        which are defined for all the existing attributes.\n",
            "        \"\"\"\n",
            "\n",
            "        return self.obj_dict['attributes'].get(name, None)\n",
            "\n",
            "\n",
            "    def get_attributes(self):\n",
            "        \"\"\"\"\"\"\n",
            "\n",
            "        return self.obj_dict['attributes']\n",
            "\n",
            "\n",
            "    def set_sequence(self, seq):\n",
            "\n",
            "        self.obj_dict['sequence'] = seq\n",
            "\n",
            "\n",
            "    def get_sequence(self):\n",
            "\n",
            "        return self.obj_dict['sequence']\n",
            "\n",
            "\n",
            "    def create_attribute_methods(self, obj_attributes):\n",
            "\n",
            "        #for attr in self.obj_dict['attributes']:\n",
            "        for attr in obj_attributes:\n",
            "\n",
            "            # Generate all the Setter methods.\n",
            "            #\n",
            "            self.__setattr__(\n",
            "                'set_'+attr,\n",
            "                lambda x, a=attr :\n",
            "                    self.obj_dict['attributes'].__setitem__(a, x) )\n",
            "\n",
            "            # Generate all the Getter methods.\n",
            "            #\n",
            "            self.__setattr__(\n",
            "                'get_'+attr, lambda a=attr : self.__get_attribute__(a))\n",
            "\n",
            "\n",
            "\n",
            "class Error(Exception):\n",
            "    \"\"\"General error handling class.\n",
            "    \"\"\"\n",
            "    def __init__(self, value):\n",
            "        self.value = value\n",
            "    def __str__(self):\n",
            "        return self.value\n",
            "\n",
            "\n",
            "class InvocationException(Exception):\n",
            "    \"\"\"Indicate ploblem while running any GraphViz executable.\n",
            "    \"\"\"\n",
            "    def __init__(self, value):\n",
            "        self.value = value\n",
            "    def __str__(self):\n",
            "        return self.value\n",
            "\n",
            "\n",
            "\n",
            "class Node(Common):\n",
            "    \"\"\"A graph node.\n",
            "\n",
            "    This class represents a graph's node with all its attributes.\n",
            "\n",
            "    node(name, attribute=value, ...)\n",
            "\n",
            "    name: node's name\n",
            "\n",
            "    All the attributes defined in the Graphviz dot language should\n",
            "    be supported.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, name = '', obj_dict = None, **attrs):\n",
            "\n",
            "        #\n",
            "        # Nodes will take attributes of\n",
            "        # all other types because the defaults\n",
            "        # for any GraphViz object are dealt with\n",
            "        # as if they were Node definitions\n",
            "        #\n",
            "\n",
            "        if obj_dict is not None:\n",
            "\n",
            "            self.obj_dict = obj_dict\n",
            "\n",
            "        else:\n",
            "\n",
            "            self.obj_dict = dict()\n",
            "\n",
            "            # Copy the attributes\n",
            "            #\n",
            "            self.obj_dict[ 'attributes' ] = dict( attrs )\n",
            "            self.obj_dict[ 'type' ] = 'node'\n",
            "            self.obj_dict[ 'parent_graph' ] = None\n",
            "            self.obj_dict[ 'parent_node_list' ] = None\n",
            "            self.obj_dict[ 'sequence' ] = None\n",
            "\n",
            "            # Remove the compass point\n",
            "            #\n",
            "            port = None\n",
            "            if isinstance(name, str_type) and not name.startswith('\"'):\n",
            "                idx = name.find(':')\n",
            "                if idx > 0 and idx+1 < len(name):\n",
            "                    name, port = name[:idx], name[idx:]\n",
            "\n",
            "            if isinstance(name, int):\n",
            "                name = str(name)\n",
            "\n",
            "            self.obj_dict['name'] = quote_if_necessary(name)\n",
            "            self.obj_dict['port'] = port\n",
            "\n",
            "        self.create_attribute_methods(NODE_ATTRIBUTES)\n",
            "\n",
            "    def __str__(self):\n",
            "        return self.to_string()\n",
            "\n",
            "\n",
            "    def set_name(self, node_name):\n",
            "        \"\"\"Set the node's name.\"\"\"\n",
            "\n",
            "        self.obj_dict['name'] = node_name\n",
            "\n",
            "\n",
            "    def get_name(self):\n",
            "        \"\"\"Get the node's name.\"\"\"\n",
            "\n",
            "        return self.obj_dict['name']\n",
            "\n",
            "\n",
            "    def get_port(self):\n",
            "        \"\"\"Get the node's port.\"\"\"\n",
            "\n",
            "        return self.obj_dict['port']\n",
            "\n",
            "\n",
            "    def add_style(self, style):\n",
            "\n",
            "        styles = self.obj_dict['attributes'].get('style', None)\n",
            "        if not styles and style:\n",
            "            styles = [ style ]\n",
            "        else:\n",
            "            styles = styles.split(',')\n",
            "            styles.append( style )\n",
            "\n",
            "        self.obj_dict['attributes']['style'] = ','.join( styles )\n",
            "\n",
            "\n",
            "    def to_string(self):\n",
            "        \"\"\"Return string representation of node in DOT language.\"\"\"\n",
            "\n",
            "\n",
            "        # RMF: special case defaults for node, edge and graph properties.\n",
            "        #\n",
            "        node = quote_if_necessary(self.obj_dict['name'])\n",
            "\n",
            "        node_attr = list()\n",
            "\n",
            "        for attr in self.obj_dict['attributes']:\n",
            "            value = self.obj_dict['attributes'][attr]\n",
            "            if value == '':\n",
            "                value = '\"\"'\n",
            "            if value is not None:\n",
            "                node_attr.append(\n",
            "                    '%s=%s' % (attr, quote_if_necessary(value) ) )\n",
            "            else:\n",
            "                node_attr.append( attr )\n",
            "\n",
            "\n",
            "        # No point in having nodes setting any defaults if the don't set\n",
            "        # any attributes...\n",
            "        #\n",
            "        if node in ('graph', 'node', 'edge') and len(node_attr) == 0:\n",
            "            return ''\n",
            "\n",
            "        node_attr = ', '.join(node_attr)\n",
            "\n",
            "        if node_attr:\n",
            "            node += ' [' + node_attr + ']'\n",
            "\n",
            "        return node + ';'\n",
            "\n",
            "\n",
            "\n",
            "class Edge(Common):\n",
            "    \"\"\"A graph edge.\n",
            "\n",
            "    This class represents a graph's edge with all its attributes.\n",
            "\n",
            "    edge(src, dst, attribute=value, ...)\n",
            "\n",
            "    src: source node\n",
            "    dst: destination node\n",
            "\n",
            "    `src` and `dst` can be specified as a `Node` object,\n",
            "    or as the node's name string.\n",
            "\n",
            "    All the attributes defined in the Graphviz dot language should\n",
            "    be supported.\n",
            "\n",
            " \tAttributes can be set through the dynamically generated methods:\n",
            "\n",
            "     set_[attribute name], i.e. set_label, set_fontname\n",
            "\n",
            "    or directly by using the instance's special dictionary:\n",
            "\n",
            "     Edge.obj_dict['attributes'][attribute name], i.e.\n",
            "\n",
            "        edge_instance.obj_dict['attributes']['label']\n",
            "        edge_instance.obj_dict['attributes']['fontname']\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, src='', dst='', obj_dict=None, **attrs):\n",
            "        self.obj_dict = dict()\n",
            "        if isinstance(src, Node):\n",
            "            src = src.get_name()\n",
            "        if isinstance(dst, Node):\n",
            "            dst = dst.get_name()\n",
            "        points = (quote_if_necessary(src),\n",
            "                  quote_if_necessary(dst))\n",
            "        self.obj_dict['points'] = points\n",
            "        if obj_dict is None:\n",
            "            # Copy the attributes\n",
            "            self.obj_dict[ 'attributes' ] = dict( attrs )\n",
            "            self.obj_dict[ 'type' ] = 'edge'\n",
            "            self.obj_dict[ 'parent_graph' ] = None\n",
            "            self.obj_dict[ 'parent_edge_list' ] = None\n",
            "            self.obj_dict[ 'sequence' ] = None\n",
            "        else:\n",
            "            self.obj_dict = obj_dict\n",
            "        self.create_attribute_methods(EDGE_ATTRIBUTES)\n",
            "\n",
            "    def __str__(self):\n",
            "        return self.to_string()\n",
            "\n",
            "\n",
            "    def get_source(self):\n",
            "        \"\"\"Get the edges source node name.\"\"\"\n",
            "\n",
            "        return self.obj_dict['points'][0]\n",
            "\n",
            "\n",
            "    def get_destination(self):\n",
            "        \"\"\"Get the edge's destination node name.\"\"\"\n",
            "\n",
            "        return self.obj_dict['points'][1]\n",
            "\n",
            "\n",
            "    def __hash__(self):\n",
            "\n",
            "         return hash( hash(self.get_source()) +\n",
            "                     hash(self.get_destination()) )\n",
            "\n",
            "\n",
            "    def __eq__(self, edge):\n",
            "        \"\"\"Compare two edges.\n",
            "\n",
            "        If the parent graph is directed, arcs linking\n",
            "        node A to B are considered equal and A->B != B->A\n",
            "\n",
            "        If the parent graph is undirected, any edge\n",
            "        connecting two nodes is equal to any other\n",
            "        edge connecting the same nodes, A->B == B->A\n",
            "        \"\"\"\n",
            "\n",
            "        if not isinstance(edge, Edge):\n",
            "            raise Error('Can not compare and '\n",
            "                        'edge to a non-edge object.')\n",
            "\n",
            "        if self.get_parent_graph().get_top_graph_type() == 'graph':\n",
            "\n",
            "            # If the graph is undirected, the edge has neither\n",
            "            # source nor destination.\n",
            "            #\n",
            "            if\t( ( self.get_source() == edge.get_source() and\n",
            "                  self.get_destination() == edge.get_destination() ) or\n",
            "                ( edge.get_source() == self.get_destination() and\n",
            "                 edge.get_destination() == self.get_source() ) ):\n",
            "                return True\n",
            "\n",
            "        else:\n",
            "\n",
            "            if (self.get_source()==edge.get_source() and\n",
            "                    self.get_destination()==edge.get_destination()):\n",
            "                return True\n",
            "\n",
            "        return False\n",
            "\n",
            "\n",
            "\n",
            "    def parse_node_ref(self, node_str):\n",
            "\n",
            "        if not isinstance(node_str, str):\n",
            "            return node_str\n",
            "\n",
            "        if node_str.startswith('\"') and node_str.endswith('\"'):\n",
            "\n",
            "            return node_str\n",
            "\n",
            "        node_port_idx = node_str.rfind(':')\n",
            "\n",
            "        if (node_port_idx>0 and node_str[0]=='\"' and\n",
            "            node_str[node_port_idx-1]=='\"'):\n",
            "\n",
            "            return node_str\n",
            "\n",
            "        if node_port_idx>0:\n",
            "\n",
            "            a = node_str[:node_port_idx]\n",
            "            b = node_str[node_port_idx+1:]\n",
            "\n",
            "            node = quote_if_necessary(a)\n",
            "\n",
            "            node += ':'+quote_if_necessary(b)\n",
            "\n",
            "            return node\n",
            "\n",
            "        return node_str\n",
            "\n",
            "\n",
            "    def to_string(self):\n",
            "        \"\"\"Return string representation of edge in DOT language.\"\"\"\n",
            "\n",
            "        src = self.parse_node_ref( self.get_source() )\n",
            "        dst = self.parse_node_ref( self.get_destination() )\n",
            "\n",
            "        if isinstance(src, frozendict):\n",
            "            edge = [ Subgraph(obj_dict=src).to_string() ]\n",
            "        elif isinstance(src, int):\n",
            "            edge = [ str(src) ]\n",
            "        else:\n",
            "            edge = [ src ]\n",
            "\n",
            "        if\t(self.get_parent_graph() and\n",
            "            self.get_parent_graph().get_top_graph_type() and\n",
            "            self.get_parent_graph().get_top_graph_type() == 'digraph' ):\n",
            "\n",
            "            edge.append( '->' )\n",
            "\n",
            "        else:\n",
            "            edge.append( '--' )\n",
            "\n",
            "        if isinstance(dst, frozendict):\n",
            "            edge.append( Subgraph(obj_dict=dst).to_string() )\n",
            "        elif isinstance(dst, int):\n",
            "            edge.append( str(dst) )\n",
            "        else:\n",
            "            edge.append( dst )\n",
            "\n",
            "\n",
            "        edge_attr = list()\n",
            "\n",
            "        for attr in self.obj_dict['attributes']:\n",
            "            value = self.obj_dict['attributes'][attr]\n",
            "            if value == '':\n",
            "                value = '\"\"'\n",
            "            if value is not None:\n",
            "                edge_attr.append(\n",
            "                    '%s=%s' % (attr, quote_if_necessary(value) ) )\n",
            "            else:\n",
            "                edge_attr.append( attr )\n",
            "\n",
            "        edge_attr = ', '.join(edge_attr)\n",
            "\n",
            "        if edge_attr:\n",
            "            edge.append( ' [' + edge_attr + ']' )\n",
            "\n",
            "        return ' '.join(edge) + ';'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class Graph(Common):\n",
            "    \"\"\"Class representing a graph in Graphviz's dot language.\n",
            "\n",
            "    This class implements the methods to work on a representation\n",
            "    of a graph in Graphviz's dot language.\n",
            "\n",
            "    graph(  graph_name='G', graph_type='digraph',\n",
            "        strict=False, suppress_disconnected=False, attribute=value, ...)\n",
            "\n",
            "    graph_name:\n",
            "        the graph's name\n",
            "    graph_type:\n",
            "        can be 'graph' or 'digraph'\n",
            "    suppress_disconnected:\n",
            "        defaults to False, which will remove from the\n",
            "        graph any disconnected nodes.\n",
            "    simplify:\n",
            "        if True it will avoid displaying equal edges, i.e.\n",
            "        only one edge between two nodes. removing the\n",
            "        duplicated ones.\n",
            "\n",
            "    All the attributes defined in the Graphviz dot language should\n",
            "    be supported.\n",
            "\n",
            "    Attributes can be set through the dynamically generated methods:\n",
            "\n",
            "     set_[attribute name], i.e. set_size, set_fontname\n",
            "\n",
            "    or using the instance's attributes:\n",
            "\n",
            "     Graph.obj_dict['attributes'][attribute name], i.e.\n",
            "\n",
            "        graph_instance.obj_dict['attributes']['label']\n",
            "        graph_instance.obj_dict['attributes']['fontname']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "    def __init__(self, graph_name='G', obj_dict=None,\n",
            "                 graph_type='digraph', strict=False,\n",
            "                 suppress_disconnected=False, simplify=False, **attrs):\n",
            "\n",
            "        if obj_dict is not None:\n",
            "            self.obj_dict = obj_dict\n",
            "\n",
            "        else:\n",
            "\n",
            "            self.obj_dict = dict()\n",
            "\n",
            "            self.obj_dict['attributes'] = dict(attrs)\n",
            "\n",
            "            if graph_type not in ['graph', 'digraph']:\n",
            "                raise Error((\n",
            "                    'Invalid type \"{t}\". '\n",
            "                    'Accepted graph types are: '\n",
            "                    'graph, digraph').format(t=graph_type))\n",
            "\n",
            "\n",
            "            self.obj_dict['name'] = quote_if_necessary(graph_name)\n",
            "            self.obj_dict['type'] = graph_type\n",
            "\n",
            "            self.obj_dict['strict'] = strict\n",
            "            self.obj_dict['suppress_disconnected'] = suppress_disconnected\n",
            "            self.obj_dict['simplify'] = simplify\n",
            "\n",
            "            self.obj_dict['current_child_sequence'] = 1\n",
            "            self.obj_dict['nodes'] = dict()\n",
            "            self.obj_dict['edges'] = dict()\n",
            "            self.obj_dict['subgraphs'] = dict()\n",
            "\n",
            "            self.set_parent_graph(self)\n",
            "\n",
            "\n",
            "        self.create_attribute_methods(GRAPH_ATTRIBUTES)\n",
            "\n",
            "    def __str__(self):\n",
            "        return self.to_string()\n",
            "\n",
            "\n",
            "    def get_graph_type(self):\n",
            "\n",
            "        return self.obj_dict['type']\n",
            "\n",
            "\n",
            "    def get_top_graph_type(self):\n",
            "\n",
            "        parent = self\n",
            "        while True:\n",
            "            parent_ = parent.get_parent_graph()\n",
            "            if parent_ == parent:\n",
            "                break\n",
            "            parent = parent_\n",
            "\n",
            "        return parent.obj_dict['type']\n",
            "\n",
            "\n",
            "    def set_graph_defaults(self, **attrs):\n",
            "\n",
            "        self.add_node( Node('graph', **attrs) )\n",
            "\n",
            "\n",
            "    def get_graph_defaults(self, **attrs):\n",
            "\n",
            "        graph_nodes = self.get_node('graph')\n",
            "\n",
            "        if isinstance( graph_nodes, (list, tuple)):\n",
            "            return [ node.get_attributes() for node in graph_nodes ]\n",
            "\n",
            "        return graph_nodes.get_attributes()\n",
            "\n",
            "\n",
            "\n",
            "    def set_node_defaults(self, **attrs):\n",
            "\n",
            "        self.add_node( Node('node', **attrs) )\n",
            "\n",
            "\n",
            "    def get_node_defaults(self, **attrs):\n",
            "\n",
            "\n",
            "        graph_nodes = self.get_node('node')\n",
            "\n",
            "        if isinstance( graph_nodes, (list, tuple)):\n",
            "            return [ node.get_attributes() for node in graph_nodes ]\n",
            "\n",
            "        return graph_nodes.get_attributes()\n",
            "\n",
            "\n",
            "    def set_edge_defaults(self, **attrs):\n",
            "\n",
            "        self.add_node( Node('edge', **attrs) )\n",
            "\n",
            "\n",
            "\n",
            "    def get_edge_defaults(self, **attrs):\n",
            "\n",
            "        graph_nodes = self.get_node('edge')\n",
            "\n",
            "        if isinstance( graph_nodes, (list, tuple)):\n",
            "            return [ node.get_attributes() for node in graph_nodes ]\n",
            "\n",
            "        return graph_nodes.get_attributes()\n",
            "\n",
            "\n",
            "\n",
            "    def set_simplify(self, simplify):\n",
            "        \"\"\"Set whether to simplify or not.\n",
            "\n",
            "        If True it will avoid displaying equal edges, i.e.\n",
            "        only one edge between two nodes. removing the\n",
            "        duplicated ones.\n",
            "        \"\"\"\n",
            "\n",
            "        self.obj_dict['simplify'] = simplify\n",
            "\n",
            "\n",
            "\n",
            "    def get_simplify(self):\n",
            "        \"\"\"Get whether to simplify or not.\n",
            "\n",
            "        Refer to set_simplify for more information.\n",
            "        \"\"\"\n",
            "\n",
            "        return self.obj_dict['simplify']\n",
            "\n",
            "\n",
            "    def set_type(self, graph_type):\n",
            "        \"\"\"Set the graph's type, 'graph' or 'digraph'.\"\"\"\n",
            "\n",
            "        self.obj_dict['type'] = graph_type\n",
            "\n",
            "\n",
            "\n",
            "    def get_type(self):\n",
            "        \"\"\"Get the graph's type, 'graph' or 'digraph'.\"\"\"\n",
            "\n",
            "        return self.obj_dict['type']\n",
            "\n",
            "\n",
            "\n",
            "    def set_name(self, graph_name):\n",
            "        \"\"\"Set the graph's name.\"\"\"\n",
            "\n",
            "        self.obj_dict['name'] = graph_name\n",
            "\n",
            "\n",
            "\n",
            "    def get_name(self):\n",
            "        \"\"\"Get the graph's name.\"\"\"\n",
            "\n",
            "        return self.obj_dict['name']\n",
            "\n",
            "\n",
            "\n",
            "    def set_strict(self, val):\n",
            "        \"\"\"Set graph to 'strict' mode.\n",
            "\n",
            "        This option is only valid for top level graphs.\n",
            "        \"\"\"\n",
            "\n",
            "        self.obj_dict['strict'] = val\n",
            "\n",
            "\n",
            "\n",
            "    def get_strict(self, val):\n",
            "        \"\"\"Get graph's 'strict' mode (True, False).\n",
            "\n",
            "        This option is only valid for top level graphs.\n",
            "        \"\"\"\n",
            "\n",
            "        return self.obj_dict['strict']\n",
            "\n",
            "\n",
            "\n",
            "    def set_suppress_disconnected(self, val):\n",
            "        \"\"\"Suppress disconnected nodes in the output graph.\n",
            "\n",
            "        This option will skip nodes in\n",
            "        the graph with no incoming or outgoing\n",
            "        edges. This option works also\n",
            "        for subgraphs and has effect only in the\n",
            "        current graph/subgraph.\n",
            "        \"\"\"\n",
            "\n",
            "        self.obj_dict['suppress_disconnected'] = val\n",
            "\n",
            "\n",
            "\n",
            "    def get_suppress_disconnected(self, val):\n",
            "        \"\"\"Get if suppress disconnected is set.\n",
            "\n",
            "        Refer to set_suppress_disconnected for more information.\n",
            "        \"\"\"\n",
            "\n",
            "        return self.obj_dict['suppress_disconnected']\n",
            "\n",
            "\n",
            "    def get_next_sequence_number(self):\n",
            "\n",
            "        seq = self.obj_dict['current_child_sequence']\n",
            "\n",
            "        self.obj_dict['current_child_sequence'] += 1\n",
            "\n",
            "        return seq\n",
            "\n",
            "\n",
            "\n",
            "    def add_node(self, graph_node):\n",
            "        \"\"\"Adds a node object to the graph.\n",
            "\n",
            "        It takes a node object as its only argument and returns\n",
            "        None.\n",
            "        \"\"\"\n",
            "\n",
            "        if not isinstance(graph_node, Node):\n",
            "            raise TypeError(\n",
            "                'add_node() received ' +\n",
            "                'a non node class object: ' + str(graph_node))\n",
            "\n",
            "\n",
            "        node = self.get_node(graph_node.get_name())\n",
            "\n",
            "        if not node:\n",
            "\n",
            "            self.obj_dict['nodes'][graph_node.get_name()] = [\n",
            "                graph_node.obj_dict ]\n",
            "\n",
            "            #self.node_dict[graph_node.get_name()] = graph_node.attributes\n",
            "            graph_node.set_parent_graph(self.get_parent_graph())\n",
            "\n",
            "        else:\n",
            "\n",
            "            self.obj_dict['nodes'][graph_node.get_name()].append(\n",
            "                graph_node.obj_dict )\n",
            "\n",
            "        graph_node.set_sequence(self.get_next_sequence_number())\n",
            "\n",
            "\n",
            "\n",
            "    def del_node(self, name, index=None):\n",
            "        \"\"\"Delete a node from the graph.\n",
            "\n",
            "        Given a node's name all node(s) with that same name\n",
            "        will be deleted if 'index' is not specified or set\n",
            "        to None.\n",
            "        If there are several nodes with that same name and\n",
            "        'index' is given, only the node in that position\n",
            "        will be deleted.\n",
            "\n",
            "        'index' should be an integer specifying the position\n",
            "        of the node to delete. If index is larger than the\n",
            "        number of nodes with that name, no action is taken.\n",
            "\n",
            "        If nodes are deleted it returns True. If no action\n",
            "        is taken it returns False.\n",
            "        \"\"\"\n",
            "\n",
            "        if isinstance(name, Node):\n",
            "            name = name.get_name()\n",
            "\n",
            "        if name in self.obj_dict['nodes']:\n",
            "\n",
            "            if (index is not None and\n",
            "                index < len(self.obj_dict['nodes'][name])):\n",
            "                del self.obj_dict['nodes'][name][index]\n",
            "                return True\n",
            "            else:\n",
            "                del self.obj_dict['nodes'][name]\n",
            "                return True\n",
            "\n",
            "        return False\n",
            "\n",
            "\n",
            "    def get_node(self, name):\n",
            "        \"\"\"Retrieve a node from the graph.\n",
            "\n",
            "        Given a node's name the corresponding Node\n",
            "        instance will be returned.\n",
            "\n",
            "        If one or more nodes exist with that name a list of\n",
            "        Node instances is returned.\n",
            "        An empty list is returned otherwise.\n",
            "        \"\"\"\n",
            "\n",
            "        match = list()\n",
            "\n",
            "        if name in self.obj_dict['nodes']:\n",
            "\n",
            "            match.extend(\n",
            "                [Node(obj_dict=obj_dict)\n",
            "                 for obj_dict in self.obj_dict['nodes'][name]])\n",
            "\n",
            "        return match\n",
            "\n",
            "\n",
            "    def get_nodes(self):\n",
            "        \"\"\"Get the list of Node instances.\"\"\"\n",
            "\n",
            "        return self.get_node_list()\n",
            "\n",
            "\n",
            "    def get_node_list(self):\n",
            "        \"\"\"Get the list of Node instances.\n",
            "\n",
            "        This method returns the list of Node instances\n",
            "        composing the graph.\n",
            "        \"\"\"\n",
            "\n",
            "        node_objs = list()\n",
            "\n",
            "        for node in self.obj_dict['nodes']:\n",
            "                obj_dict_list = self.obj_dict['nodes'][node]\n",
            "                node_objs.extend( [ Node( obj_dict = obj_d )\n",
            "                                   for obj_d in obj_dict_list ] )\n",
            "\n",
            "        return node_objs\n",
            "\n",
            "\n",
            "\n",
            "    def add_edge(self, graph_edge):\n",
            "        \"\"\"Adds an edge object to the graph.\n",
            "\n",
            "        It takes a edge object as its only argument and returns\n",
            "        None.\n",
            "        \"\"\"\n",
            "\n",
            "        if not isinstance(graph_edge, Edge):\n",
            "            raise TypeError(\n",
            "                'add_edge() received a non edge class object: ' +\n",
            "                str(graph_edge))\n",
            "\n",
            "        edge_points = ( graph_edge.get_source(),\n",
            "                       graph_edge.get_destination() )\n",
            "\n",
            "        if edge_points in self.obj_dict['edges']:\n",
            "\n",
            "            edge_list = self.obj_dict['edges'][edge_points]\n",
            "            edge_list.append(graph_edge.obj_dict)\n",
            "\n",
            "        else:\n",
            "\n",
            "            self.obj_dict['edges'][edge_points] = [ graph_edge.obj_dict ]\n",
            "\n",
            "\n",
            "        graph_edge.set_sequence( self.get_next_sequence_number() )\n",
            "\n",
            "        graph_edge.set_parent_graph( self.get_parent_graph() )\n",
            "\n",
            "\n",
            "\n",
            "    def del_edge(self, src_or_list, dst=None, index=None):\n",
            "        \"\"\"Delete an edge from the graph.\n",
            "\n",
            "        Given an edge's (source, destination) node names all\n",
            "        matching edges(s) will be deleted if 'index' is not\n",
            "        specified or set to None.\n",
            "        If there are several matching edges and 'index' is\n",
            "        given, only the edge in that position will be deleted.\n",
            "\n",
            "        'index' should be an integer specifying the position\n",
            "        of the edge to delete. If index is larger than the\n",
            "        number of matching edges, no action is taken.\n",
            "\n",
            "        If edges are deleted it returns True. If no action\n",
            "        is taken it returns False.\n",
            "        \"\"\"\n",
            "\n",
            "        if isinstance( src_or_list, (list, tuple)):\n",
            "            if dst is not None and isinstance(dst, int):\n",
            "                index = dst\n",
            "            src, dst = src_or_list\n",
            "        else:\n",
            "            src, dst = src_or_list, dst\n",
            "\n",
            "        if isinstance(src, Node):\n",
            "            src = src.get_name()\n",
            "\n",
            "        if isinstance(dst, Node):\n",
            "            dst = dst.get_name()\n",
            "\n",
            "        if (src, dst) in self.obj_dict['edges']:\n",
            "\n",
            "            if (index is not None and\n",
            "                index < len(self.obj_dict['edges'][(src, dst)])):\n",
            "                del self.obj_dict['edges'][(src, dst)][index]\n",
            "                return True\n",
            "            else:\n",
            "                del self.obj_dict['edges'][(src, dst)]\n",
            "                return True\n",
            "\n",
            "        return False\n",
            "\n",
            "\n",
            "    def get_edge(self, src_or_list, dst=None):\n",
            "        \"\"\"Retrieved an edge from the graph.\n",
            "\n",
            "        Given an edge's source and destination the corresponding\n",
            "        Edge instance(s) will be returned.\n",
            "\n",
            "        If one or more edges exist with that source and destination\n",
            "        a list of Edge instances is returned.\n",
            "        An empty list is returned otherwise.\n",
            "        \"\"\"\n",
            "\n",
            "        if isinstance( src_or_list, (list, tuple)) and dst is None:\n",
            "            edge_points = tuple(src_or_list)\n",
            "            edge_points_reverse = (edge_points[1], edge_points[0])\n",
            "        else:\n",
            "            edge_points = (src_or_list, dst)\n",
            "            edge_points_reverse = (dst, src_or_list)\n",
            "\n",
            "        match = list()\n",
            "\n",
            "        if edge_points in self.obj_dict['edges'] or (\n",
            "            self.get_top_graph_type() == 'graph' and\n",
            "            edge_points_reverse in self.obj_dict['edges']):\n",
            "\n",
            "            edges_obj_dict = self.obj_dict['edges'].get(\n",
            "                edge_points,\n",
            "                self.obj_dict['edges'].get( edge_points_reverse, None ))\n",
            "\n",
            "            for edge_obj_dict in edges_obj_dict:\n",
            "                match.append(\n",
            "                    Edge(edge_points[0],\n",
            "                         edge_points[1],\n",
            "                         obj_dict=edge_obj_dict))\n",
            "\n",
            "        return match\n",
            "\n",
            "\n",
            "    def get_edges(self):\n",
            "        return self.get_edge_list()\n",
            "\n",
            "\n",
            "    def get_edge_list(self):\n",
            "        \"\"\"Get the list of Edge instances.\n",
            "\n",
            "        This method returns the list of Edge instances\n",
            "        composing the graph.\n",
            "        \"\"\"\n",
            "\n",
            "        edge_objs = list()\n",
            "\n",
            "        for edge in self.obj_dict['edges']:\n",
            "                obj_dict_list = self.obj_dict['edges'][edge]\n",
            "                edge_objs.extend(\n",
            "                    [Edge(obj_dict=obj_d)\n",
            "                     for obj_d in obj_dict_list])\n",
            "\n",
            "        return edge_objs\n",
            "\n",
            "\n",
            "\n",
            "    def add_subgraph(self, sgraph):\n",
            "        \"\"\"Adds an subgraph object to the graph.\n",
            "\n",
            "        It takes a subgraph object as its only argument and returns\n",
            "        None.\n",
            "        \"\"\"\n",
            "\n",
            "        if (not isinstance(sgraph, Subgraph) and\n",
            "            not isinstance(sgraph, Cluster)):\n",
            "            raise TypeError(\n",
            "                'add_subgraph() received a non subgraph class object:' +\n",
            "                str(sgraph))\n",
            "\n",
            "        if sgraph.get_name() in self.obj_dict['subgraphs']:\n",
            "\n",
            "            sgraph_list = self.obj_dict['subgraphs'][ sgraph.get_name() ]\n",
            "            sgraph_list.append( sgraph.obj_dict )\n",
            "\n",
            "        else:\n",
            "            self.obj_dict['subgraphs'][sgraph.get_name()] = [\n",
            "                sgraph.obj_dict]\n",
            "\n",
            "        sgraph.set_sequence( self.get_next_sequence_number() )\n",
            "\n",
            "        sgraph.set_parent_graph( self.get_parent_graph() )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    def get_subgraph(self, name):\n",
            "        \"\"\"Retrieved a subgraph from the graph.\n",
            "\n",
            "        Given a subgraph's name the corresponding\n",
            "        Subgraph instance will be returned.\n",
            "\n",
            "        If one or more subgraphs exist with the same name, a list of\n",
            "        Subgraph instances is returned.\n",
            "        An empty list is returned otherwise.\n",
            "        \"\"\"\n",
            "\n",
            "        match = list()\n",
            "\n",
            "        if name in self.obj_dict['subgraphs']:\n",
            "\n",
            "            sgraphs_obj_dict = self.obj_dict['subgraphs'].get( name )\n",
            "\n",
            "            for obj_dict_list in sgraphs_obj_dict:\n",
            "                #match.extend( Subgraph( obj_dict = obj_d )\n",
            "                #             for obj_d in obj_dict_list )\n",
            "                match.append( Subgraph( obj_dict = obj_dict_list ) )\n",
            "\n",
            "        return match\n",
            "\n",
            "\n",
            "    def get_subgraphs(self):\n",
            "\n",
            "        return self.get_subgraph_list()\n",
            "\n",
            "\n",
            "    def get_subgraph_list(self):\n",
            "        \"\"\"Get the list of Subgraph instances.\n",
            "\n",
            "        This method returns the list of Subgraph instances\n",
            "        in the graph.\n",
            "        \"\"\"\n",
            "\n",
            "        sgraph_objs = list()\n",
            "\n",
            "        for sgraph in self.obj_dict['subgraphs']:\n",
            "                obj_dict_list = self.obj_dict['subgraphs'][sgraph]\n",
            "                sgraph_objs.extend(\n",
            "                    [Subgraph(obj_dict=obj_d)\n",
            "                     for obj_d in obj_dict_list])\n",
            "\n",
            "        return sgraph_objs\n",
            "\n",
            "\n",
            "\n",
            "    def set_parent_graph(self, parent_graph):\n",
            "\n",
            "        self.obj_dict['parent_graph'] = parent_graph\n",
            "\n",
            "        for k in self.obj_dict['nodes']:\n",
            "            obj_list = self.obj_dict['nodes'][k]\n",
            "            for obj in obj_list:\n",
            "                obj['parent_graph'] = parent_graph\n",
            "\n",
            "        for k in self.obj_dict['edges']:\n",
            "            obj_list = self.obj_dict['edges'][k]\n",
            "            for obj in obj_list:\n",
            "                obj['parent_graph'] = parent_graph\n",
            "\n",
            "        for k in self.obj_dict['subgraphs']:\n",
            "            obj_list = self.obj_dict['subgraphs'][k]\n",
            "            for obj in obj_list:\n",
            "                Graph(obj_dict=obj).set_parent_graph(parent_graph)\n",
            "\n",
            "\n",
            "\n",
            "    def to_string(self):\n",
            "        \"\"\"Return string representation of graph in DOT language.\n",
            "\n",
            "        @return: graph and subelements\n",
            "        @rtype: `str`\n",
            "        \"\"\"\n",
            "\n",
            "\n",
            "        graph = list()\n",
            "\n",
            "        if self.obj_dict.get('strict', None) is not None:\n",
            "\n",
            "            if (self == self.get_parent_graph() and\n",
            "                    self.obj_dict['strict']):\n",
            "\n",
            "                graph.append('strict ')\n",
            "\n",
            "        graph_type = self.obj_dict['type']\n",
            "        if (graph_type == 'subgraph' and\n",
            "                not self.obj_dict.get('show_keyword', True)):\n",
            "            graph_type = ''\n",
            "        s = '{type} {name} {{\\n'.format(\n",
            "            type=graph_type,\n",
            "            name=self.obj_dict['name'])\n",
            "        graph.append(s)\n",
            "\n",
            "        for attr in self.obj_dict['attributes']:\n",
            "\n",
            "            if self.obj_dict['attributes'].get(attr, None) is not None:\n",
            "\n",
            "                val = self.obj_dict['attributes'].get(attr)\n",
            "                if val == '':\n",
            "                    val = '\"\"'\n",
            "                if val is not None:\n",
            "                    graph.append('%s=%s' %\n",
            "                                 (attr, quote_if_necessary(val)))\n",
            "                else:\n",
            "                    graph.append( attr )\n",
            "\n",
            "                graph.append( ';\\n' )\n",
            "\n",
            "\n",
            "        edges_done = set()\n",
            "\n",
            "        edge_obj_dicts = list()\n",
            "        for k in self.obj_dict['edges']:\n",
            "            edge_obj_dicts.extend(self.obj_dict['edges'][k])\n",
            "\n",
            "        if edge_obj_dicts:\n",
            "            edge_src_set, edge_dst_set = list(zip(\n",
            "                *[obj['points'] for obj in edge_obj_dicts]))\n",
            "            edge_src_set, edge_dst_set = set(edge_src_set), set(edge_dst_set)\n",
            "        else:\n",
            "            edge_src_set, edge_dst_set = set(), set()\n",
            "\n",
            "        node_obj_dicts = list()\n",
            "        for k in self.obj_dict['nodes']:\n",
            "            node_obj_dicts.extend(self.obj_dict['nodes'][k])\n",
            "\n",
            "        sgraph_obj_dicts = list()\n",
            "        for k in self.obj_dict['subgraphs']:\n",
            "            sgraph_obj_dicts.extend(self.obj_dict['subgraphs'][k])\n",
            "\n",
            "\n",
            "        obj_list = [(obj['sequence'], obj)\n",
            "                    for obj in (edge_obj_dicts +\n",
            "                                node_obj_dicts + sgraph_obj_dicts) ]\n",
            "        obj_list.sort(key=lambda x: x[0])\n",
            "\n",
            "        for idx, obj in obj_list:\n",
            "\n",
            "            if obj['type'] == 'node':\n",
            "\n",
            "                node = Node(obj_dict=obj)\n",
            "\n",
            "                if self.obj_dict.get('suppress_disconnected', False):\n",
            "\n",
            "                    if (node.get_name() not in edge_src_set and\n",
            "                        node.get_name() not in edge_dst_set):\n",
            "\n",
            "                        continue\n",
            "\n",
            "                graph.append( node.to_string()+'\\n' )\n",
            "\n",
            "            elif obj['type'] == 'edge':\n",
            "\n",
            "                edge = Edge(obj_dict=obj)\n",
            "\n",
            "                if (self.obj_dict.get('simplify', False) and\n",
            "                        edge in edges_done):\n",
            "                    continue\n",
            "\n",
            "                graph.append( edge.to_string() + '\\n' )\n",
            "                edges_done.add(edge)\n",
            "\n",
            "            else:\n",
            "\n",
            "                sgraph = Subgraph(obj_dict=obj)\n",
            "\n",
            "                graph.append( sgraph.to_string()+'\\n' )\n",
            "\n",
            "        graph.append( '}\\n' )\n",
            "\n",
            "        return ''.join(graph)\n",
            "\n",
            "\n",
            "\n",
            "class Subgraph(Graph):\n",
            "\n",
            "    \"\"\"Class representing a subgraph in Graphviz's dot language.\n",
            "\n",
            "    This class implements the methods to work on a representation\n",
            "    of a subgraph in Graphviz's dot language.\n",
            "\n",
            "    subgraph(graph_name='subG',\n",
            "             suppress_disconnected=False,\n",
            "             attribute=value,\n",
            "             ...)\n",
            "\n",
            "    graph_name:\n",
            "        the subgraph's name\n",
            "    suppress_disconnected:\n",
            "        defaults to false, which will remove from the\n",
            "        subgraph any disconnected nodes.\n",
            "    All the attributes defined in the Graphviz dot language should\n",
            "    be supported.\n",
            "\n",
            "    Attributes can be set through the dynamically generated methods:\n",
            "\n",
            "     set_[attribute name], i.e. set_size, set_fontname\n",
            "\n",
            "    or using the instance's attributes:\n",
            "\n",
            "     Subgraph.obj_dict['attributes'][attribute name], i.e.\n",
            "\n",
            "        subgraph_instance.obj_dict['attributes']['label']\n",
            "        subgraph_instance.obj_dict['attributes']['fontname']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "    # RMF: subgraph should have all the\n",
            "    # attributes of graph so it can be passed\n",
            "    # as a graph to all methods\n",
            "    #\n",
            "    def __init__(self, graph_name='',\n",
            "                 obj_dict=None, suppress_disconnected=False,\n",
            "                 simplify=False, **attrs):\n",
            "\n",
            "\n",
            "        Graph.__init__(\n",
            "            self, graph_name=graph_name, obj_dict=obj_dict,\n",
            "            suppress_disconnected=suppress_disconnected,\n",
            "            simplify=simplify, **attrs)\n",
            "\n",
            "        if obj_dict is None:\n",
            "\n",
            "            self.obj_dict['type'] = 'subgraph'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class Cluster(Graph):\n",
            "\n",
            "    \"\"\"Class representing a cluster in Graphviz's dot language.\n",
            "\n",
            "    This class implements the methods to work on a representation\n",
            "    of a cluster in Graphviz's dot language.\n",
            "\n",
            "    cluster(graph_name='subG',\n",
            "            suppress_disconnected=False,\n",
            "            attribute=value,\n",
            "            ...)\n",
            "\n",
            "    graph_name:\n",
            "        the cluster's name\n",
            "        (the string 'cluster' will be always prepended)\n",
            "    suppress_disconnected:\n",
            "        defaults to false, which will remove from the\n",
            "        cluster any disconnected nodes.\n",
            "    All the attributes defined in the Graphviz dot language should\n",
            "    be supported.\n",
            "\n",
            "    Attributes can be set through the dynamically generated methods:\n",
            "\n",
            "     set_[attribute name], i.e. set_color, set_fontname\n",
            "\n",
            "    or using the instance's attributes:\n",
            "\n",
            "     Cluster.obj_dict['attributes'][attribute name], i.e.\n",
            "\n",
            "        cluster_instance.obj_dict['attributes']['label']\n",
            "        cluster_instance.obj_dict['attributes']['fontname']\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "    def __init__(self, graph_name='subG',\n",
            "                 obj_dict=None, suppress_disconnected=False,\n",
            "                 simplify=False, **attrs):\n",
            "\n",
            "        Graph.__init__(\n",
            "            self, graph_name=graph_name, obj_dict=obj_dict,\n",
            "            suppress_disconnected=suppress_disconnected,\n",
            "            simplify=simplify, **attrs)\n",
            "\n",
            "        if obj_dict is None:\n",
            "\n",
            "            self.obj_dict['type'] = 'subgraph'\n",
            "            self.obj_dict['name'] = quote_if_necessary('cluster_'+graph_name)\n",
            "\n",
            "        self.create_attribute_methods(CLUSTER_ATTRIBUTES)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "class Dot(Graph):\n",
            "    \"\"\"A container for handling a dot language file.\n",
            "\n",
            "    This class implements methods to write and process\n",
            "    a dot language file. It is a derived class of\n",
            "    the base class 'Graph'.\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "\n",
            "    def __init__(self, *argsl, **argsd):\n",
            "        Graph.__init__(self, *argsl, **argsd)\n",
            "\n",
            "        self.shape_files = list()\n",
            "        self.formats = [\n",
            "            'canon', 'cmap', 'cmapx',\n",
            "            'cmapx_np', 'dia', 'dot',\n",
            "            'fig', 'gd', 'gd2', 'gif',\n",
            "            'hpgl', 'imap', 'imap_np', 'ismap',\n",
            "            'jpe', 'jpeg', 'jpg', 'mif',\n",
            "            'mp', 'pcl', 'pdf', 'pic', 'plain',\n",
            "            'plain-ext', 'png', 'ps', 'ps2',\n",
            "            'svg', 'svgz', 'vml', 'vmlz',\n",
            "            'vrml', 'vtx', 'wbmp', 'xdot', 'xlib']\n",
            "\n",
            "        self.prog = 'dot'\n",
            "\n",
            "        # Automatically creates all\n",
            "        # the methods enabling the creation\n",
            "        # of output in any of the supported formats.\n",
            "        for frmt in self.formats:\n",
            "            def new_method(\n",
            "                    f=frmt, prog=self.prog,\n",
            "                    encoding=None):\n",
            "                \"\"\"Refer to docstring of method `create`.\"\"\"\n",
            "                return self.create(\n",
            "                    format=f, prog=prog, encoding=encoding)\n",
            "            name = 'create_{fmt}'.format(fmt=frmt)\n",
            "            self.__setattr__(name, new_method)\n",
            "\n",
            "        for frmt in self.formats+['raw']:\n",
            "            def new_method(\n",
            "                    path, f=frmt, prog=self.prog,\n",
            "                    encoding=None):\n",
            "                \"\"\"Refer to docstring of method `write.`\"\"\"\n",
            "                self.write(\n",
            "                    path, format=f, prog=prog,\n",
            "                    encoding=encoding)\n",
            "            name = 'write_{fmt}'.format(fmt=frmt)\n",
            "            self.__setattr__(name, new_method)\n",
            "\n",
            "    def __getstate__(self):\n",
            "\n",
            "        dict = copy.copy(self.obj_dict)\n",
            "\n",
            "        return dict\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "\n",
            "        self.obj_dict = state\n",
            "\n",
            "\n",
            "    def set_shape_files(self, file_paths):\n",
            "        \"\"\"Add the paths of the required image files.\n",
            "\n",
            "        If the graph needs graphic objects to\n",
            "        be used as shapes or otherwise\n",
            "        those need to be in the same folder as\n",
            "        the graph is going to be rendered\n",
            "        from. Alternatively the absolute path to\n",
            "        the files can be specified when\n",
            "        including the graphics in the graph.\n",
            "\n",
            "        The files in the location pointed to by\n",
            "        the path(s) specified as arguments\n",
            "        to this method will be copied to\n",
            "        the same temporary location where the\n",
            "        graph is going to be rendered.\n",
            "        \"\"\"\n",
            "\n",
            "        if isinstance( file_paths, str_type):\n",
            "            self.shape_files.append( file_paths )\n",
            "\n",
            "        if isinstance( file_paths, (list, tuple) ):\n",
            "            self.shape_files.extend( file_paths )\n",
            "\n",
            "\n",
            "    def set_prog(self, prog):\n",
            "        \"\"\"Sets the default program.\n",
            "\n",
            "        Sets the default program in charge of processing\n",
            "        the dot file into a graph.\n",
            "        \"\"\"\n",
            "        self.prog = prog\n",
            "\n",
            "\n",
            "    def write(self, path, prog=None, format='raw', encoding=None):\n",
            "        \"\"\"Writes a graph to a file.\n",
            "\n",
            "        Given a filename 'path' it will open/create and truncate\n",
            "        such file and write on it a representation of the graph\n",
            "        defined by the dot object in the format specified by\n",
            "        'format' and using the encoding specified by `encoding` for text.\n",
            "        The format 'raw' is used to dump the string representation\n",
            "        of the Dot object, without further processing.\n",
            "        The output can be processed by any of graphviz tools, defined\n",
            "        in 'prog', which defaults to 'dot'\n",
            "        Returns True or False according to the success of the write\n",
            "        operation.\n",
            "\n",
            "        There's also the preferred possibility of using:\n",
            "\n",
            "            write_'format'(path, prog='program')\n",
            "\n",
            "        which are automatically defined for all the supported formats.\n",
            "        [write_ps(), write_gif(), write_dia(), ...]\n",
            "\n",
            "        The encoding is passed to `open` [1].\n",
            "\n",
            "        [1] https://docs.python.org/3/library/functions.html#open\n",
            "        \"\"\"\n",
            "        if prog is None:\n",
            "            prog = self.prog\n",
            "        if format == 'raw':\n",
            "            s = self.to_string()\n",
            "            if not PY3:\n",
            "                s = unicode(s)\n",
            "            with io.open(path, mode='wt', encoding=encoding) as f:\n",
            "                f.write(s)\n",
            "        else:\n",
            "            s = self.create(prog, format, encoding=encoding)\n",
            "            with io.open(path, mode='wb') as f:\n",
            "                f.write(s)\n",
            "        return True\n",
            "\n",
            "\n",
            "    def create(self, prog=None, format='ps', encoding=None):\n",
            "        \"\"\"Creates and returns a binary image for the graph.\n",
            "\n",
            "        create will write the graph to a temporary dot file in the\n",
            "        encoding specified by `encoding` and process it with the\n",
            "        program given by 'prog' (which defaults to 'twopi'), reading\n",
            "        the binary image output and return it as:\n",
            "\n",
            "        - `str` of bytes in Python 2\n",
            "        - `bytes` in Python 3\n",
            "\n",
            "        There's also the preferred possibility of using:\n",
            "\n",
            "            create_'format'(prog='program')\n",
            "\n",
            "        which are automatically defined for all the supported formats,\n",
            "        for example:\n",
            "\n",
            "          - `create_ps()`\n",
            "          - `create_gif()`\n",
            "          - `create_dia()`\n",
            "\n",
            "        If 'prog' is a list, instead of a string,\n",
            "        then the fist item is expected to be the program name,\n",
            "        followed by any optional command-line arguments for it:\n",
            "\n",
            "            [ 'twopi', '-Tdot', '-s10' ]\n",
            "\n",
            "\n",
            "        @param prog: either:\n",
            "\n",
            "          - name of GraphViz executable that\n",
            "            can be found in the `$PATH`, or\n",
            "\n",
            "          - absolute path to GraphViz executable.\n",
            "\n",
            "          If you have added GraphViz to the `$PATH` and\n",
            "          use its executables as installed\n",
            "          (without renaming any of them)\n",
            "          then their names are:\n",
            "\n",
            "            - `'dot'`\n",
            "            - `'twopi'`\n",
            "            - `'neato'`\n",
            "            - `'circo'`\n",
            "            - `'fdp'`\n",
            "            - `'sfdp'`\n",
            "\n",
            "          On Windows, these have the notorious \".exe\" extension that,\n",
            "          only for the above strings, will be added automatically.\n",
            "\n",
            "          The `$PATH` is inherited from `os.env['PATH']` and\n",
            "          passed to `subprocess.Popen` using the `env` argument.\n",
            "\n",
            "          If you haven't added GraphViz to your `$PATH` on Windows,\n",
            "          then you may want to give the absolute path to the\n",
            "          executable (for example, to `dot.exe`) in `prog`.\n",
            "        \"\"\"\n",
            "        default_names = {\n",
            "            'dot', 'twopi', 'neato',\n",
            "            'circo', 'fdp', 'sfdp'}\n",
            "        if prog is None:\n",
            "            prog = self.prog\n",
            "        assert prog is not None\n",
            "        if isinstance(prog, (list, tuple)):\n",
            "            prog, args = prog[0], prog[1:]\n",
            "        else:\n",
            "            args = []\n",
            "        if os.name == 'nt' and prog in default_names:\n",
            "            if not prog.endswith('.exe'):\n",
            "                prog += '.exe'\n",
            "        # temp file\n",
            "        tmp_fd, tmp_name = tempfile.mkstemp()\n",
            "        os.close(tmp_fd)\n",
            "        self.write(tmp_name, encoding=encoding)\n",
            "        tmp_dir = os.path.dirname(tmp_name)\n",
            "        # For each of the image files...\n",
            "        for img in self.shape_files:\n",
            "            # Get its data\n",
            "            f = open(img, 'rb')\n",
            "            f_data = f.read()\n",
            "            f.close()\n",
            "            # And copy it under a file with the same name in\n",
            "            # the temporary directory\n",
            "            f = open(os.path.join(tmp_dir, os.path.basename(img)), 'wb')\n",
            "            f.write(f_data)\n",
            "            f.close()\n",
            "        # explicitly inherit `$PATH`, on Windows too,\n",
            "        # with `shell=False`\n",
            "        env = dict()\n",
            "        env['PATH'] = os.environ.get('PATH', '')\n",
            "        env['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '')\n",
            "        cmdline = [prog, '-T' + format] + args + [tmp_name]\n",
            "        try:\n",
            "            p = subprocess.Popen(\n",
            "                cmdline,\n",
            "                env=env,\n",
            "                cwd=tmp_dir,\n",
            "                shell=False,\n",
            "                stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
            "        except OSError as e:\n",
            "            if e.errno == errno.ENOENT:\n",
            "                args = list(e.args)\n",
            "                args[1] = '\"{prog}\" not found in path.'.format(\n",
            "                    prog=prog)\n",
            "                raise OSError(*args)\n",
            "            else:\n",
            "                raise\n",
            "        stdout_data, stderr_data = p.communicate()\n",
            "        # clean file litter\n",
            "        for img in self.shape_files:\n",
            "            os.unlink(os.path.join(tmp_dir, os.path.basename(img)))\n",
            "        os.unlink(tmp_name)\n",
            "        # print(stdout_data)\n",
            "        if p.returncode != 0:\n",
            "            print(\n",
            "                ('{cmdline} return code: {c}\\n\\n'\n",
            "                 'stdout, stderr:\\n {out}\\n{err}\\n').format(\n",
            "                     cmdline=cmdline,\n",
            "                     c=p.returncode,\n",
            "                     out=stdout_data,\n",
            "                     err=stderr_data))\n",
            "        assert p.returncode == 0, p.returncode\n",
            "        return stdout_data\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "from base64 import b64encode\n",
            "try:\n",
            "    from collections.abc import Callable\n",
            "except ImportError:\n",
            "    from collections import Callable\n",
            "from errno import EOPNOTSUPP, EINVAL, EAGAIN\n",
            "import functools\n",
            "from io import BytesIO\n",
            "import logging\n",
            "import os\n",
            "from os import SEEK_CUR\n",
            "import socket\n",
            "import struct\n",
            "import sys\n",
            "\n",
            "__version__ = \"1.7.0\"\n",
            "\n",
            "\n",
            "if os.name == \"nt\" and sys.version_info < (3, 0):\n",
            "    try:\n",
            "        import win_inet_pton\n",
            "    except ImportError:\n",
            "        raise ImportError(\n",
            "            \"To run PySocks on Windows you must install win_inet_pton\")\n",
            "\n",
            "log = logging.getLogger(__name__)\n",
            "\n",
            "PROXY_TYPE_SOCKS4 = SOCKS4 = 1\n",
            "PROXY_TYPE_SOCKS5 = SOCKS5 = 2\n",
            "PROXY_TYPE_HTTP = HTTP = 3\n",
            "\n",
            "PROXY_TYPES = {\"SOCKS4\": SOCKS4, \"SOCKS5\": SOCKS5, \"HTTP\": HTTP}\n",
            "PRINTABLE_PROXY_TYPES = dict(zip(PROXY_TYPES.values(), PROXY_TYPES.keys()))\n",
            "\n",
            "_orgsocket = _orig_socket = socket.socket\n",
            "\n",
            "\n",
            "def set_self_blocking(function):\n",
            "\n",
            "    @functools.wraps(function)\n",
            "    def wrapper(*args, **kwargs):\n",
            "        self = args[0]\n",
            "        try:\n",
            "            _is_blocking = self.gettimeout()\n",
            "            if _is_blocking == 0:\n",
            "                self.setblocking(True)\n",
            "            return function(*args, **kwargs)\n",
            "        except Exception as e:\n",
            "            raise\n",
            "        finally:\n",
            "            # set orgin blocking\n",
            "            if _is_blocking == 0:\n",
            "                self.setblocking(False)\n",
            "    return wrapper\n",
            "\n",
            "\n",
            "class ProxyError(IOError):\n",
            "    \"\"\"Socket_err contains original socket.error exception.\"\"\"\n",
            "    def __init__(self, msg, socket_err=None):\n",
            "        self.msg = msg\n",
            "        self.socket_err = socket_err\n",
            "\n",
            "        if socket_err:\n",
            "            self.msg += \": {}\".format(socket_err)\n",
            "\n",
            "    def __str__(self):\n",
            "        return self.msg\n",
            "\n",
            "\n",
            "class GeneralProxyError(ProxyError):\n",
            "    pass\n",
            "\n",
            "\n",
            "class ProxyConnectionError(ProxyError):\n",
            "    pass\n",
            "\n",
            "\n",
            "class SOCKS5AuthError(ProxyError):\n",
            "    pass\n",
            "\n",
            "\n",
            "class SOCKS5Error(ProxyError):\n",
            "    pass\n",
            "\n",
            "\n",
            "class SOCKS4Error(ProxyError):\n",
            "    pass\n",
            "\n",
            "\n",
            "class HTTPError(ProxyError):\n",
            "    pass\n",
            "\n",
            "SOCKS4_ERRORS = {\n",
            "    0x5B: \"Request rejected or failed\",\n",
            "    0x5C: (\"Request rejected because SOCKS server cannot connect to identd on\"\n",
            "           \" the client\"),\n",
            "    0x5D: (\"Request rejected because the client program and identd report\"\n",
            "           \" different user-ids\")\n",
            "}\n",
            "\n",
            "SOCKS5_ERRORS = {\n",
            "    0x01: \"General SOCKS server failure\",\n",
            "    0x02: \"Connection not allowed by ruleset\",\n",
            "    0x03: \"Network unreachable\",\n",
            "    0x04: \"Host unreachable\",\n",
            "    0x05: \"Connection refused\",\n",
            "    0x06: \"TTL expired\",\n",
            "    0x07: \"Command not supported, or protocol error\",\n",
            "    0x08: \"Address type not supported\"\n",
            "}\n",
            "\n",
            "DEFAULT_PORTS = {SOCKS4: 1080, SOCKS5: 1080, HTTP: 8080}\n",
            "\n",
            "\n",
            "def set_default_proxy(proxy_type=None, addr=None, port=None, rdns=True,\n",
            "                      username=None, password=None):\n",
            "    \"\"\"Sets a default proxy.\n",
            "\n",
            "    All further socksocket objects will use the default unless explicitly\n",
            "    changed. All parameters are as for socket.set_proxy().\"\"\"\n",
            "    socksocket.default_proxy = (proxy_type, addr, port, rdns,\n",
            "                                username.encode() if username else None,\n",
            "                                password.encode() if password else None)\n",
            "\n",
            "\n",
            "def setdefaultproxy(*args, **kwargs):\n",
            "    if \"proxytype\" in kwargs:\n",
            "        kwargs[\"proxy_type\"] = kwargs.pop(\"proxytype\")\n",
            "    return set_default_proxy(*args, **kwargs)\n",
            "\n",
            "\n",
            "def get_default_proxy():\n",
            "    \"\"\"Returns the default proxy, set by set_default_proxy.\"\"\"\n",
            "    return socksocket.default_proxy\n",
            "\n",
            "getdefaultproxy = get_default_proxy\n",
            "\n",
            "\n",
            "def wrap_module(module):\n",
            "    \"\"\"Attempts to replace a module's socket library with a SOCKS socket.\n",
            "\n",
            "    Must set a default proxy using set_default_proxy(...) first. This will\n",
            "    only work on modules that import socket directly into the namespace;\n",
            "    most of the Python Standard Library falls into this category.\"\"\"\n",
            "    if socksocket.default_proxy:\n",
            "        module.socket.socket = socksocket\n",
            "    else:\n",
            "        raise GeneralProxyError(\"No default proxy specified\")\n",
            "\n",
            "wrapmodule = wrap_module\n",
            "\n",
            "\n",
            "def create_connection(dest_pair,\n",
            "                      timeout=None, source_address=None,\n",
            "                      proxy_type=None, proxy_addr=None,\n",
            "                      proxy_port=None, proxy_rdns=True,\n",
            "                      proxy_username=None, proxy_password=None,\n",
            "                      socket_options=None):\n",
            "    \"\"\"create_connection(dest_pair, *[, timeout], **proxy_args) -> socket object\n",
            "\n",
            "    Like socket.create_connection(), but connects to proxy\n",
            "    before returning the socket object.\n",
            "\n",
            "    dest_pair - 2-tuple of (IP/hostname, port).\n",
            "    **proxy_args - Same args passed to socksocket.set_proxy() if present.\n",
            "    timeout - Optional socket timeout value, in seconds.\n",
            "    source_address - tuple (host, port) for the socket to bind to as its source\n",
            "    address before connecting (only for compatibility)\n",
            "    \"\"\"\n",
            "    # Remove IPv6 brackets on the remote address and proxy address.\n",
            "    remote_host, remote_port = dest_pair\n",
            "    if remote_host.startswith(\"[\"):\n",
            "        remote_host = remote_host.strip(\"[]\")\n",
            "    if proxy_addr and proxy_addr.startswith(\"[\"):\n",
            "        proxy_addr = proxy_addr.strip(\"[]\")\n",
            "\n",
            "    err = None\n",
            "\n",
            "    # Allow the SOCKS proxy to be on IPv4 or IPv6 addresses.\n",
            "    for r in socket.getaddrinfo(proxy_addr, proxy_port, 0, socket.SOCK_STREAM):\n",
            "        family, socket_type, proto, canonname, sa = r\n",
            "        sock = None\n",
            "        try:\n",
            "            sock = socksocket(family, socket_type, proto)\n",
            "\n",
            "            if socket_options:\n",
            "                for opt in socket_options:\n",
            "                    sock.setsockopt(*opt)\n",
            "\n",
            "            if isinstance(timeout, (int, float)):\n",
            "                sock.settimeout(timeout)\n",
            "\n",
            "            if proxy_type:\n",
            "                sock.set_proxy(proxy_type, proxy_addr, proxy_port, proxy_rdns,\n",
            "                               proxy_username, proxy_password)\n",
            "            if source_address:\n",
            "                sock.bind(source_address)\n",
            "\n",
            "            sock.connect((remote_host, remote_port))\n",
            "            return sock\n",
            "\n",
            "        except (socket.error, ProxyConnectionError) as e:\n",
            "            err = e\n",
            "            if sock:\n",
            "                sock.close()\n",
            "                sock = None\n",
            "\n",
            "    if err:\n",
            "        raise err\n",
            "\n",
            "    raise socket.error(\"gai returned empty list.\")\n",
            "\n",
            "\n",
            "class _BaseSocket(socket.socket):\n",
            "    \"\"\"Allows Python 2 delegated methods such as send() to be overridden.\"\"\"\n",
            "    def __init__(self, *pos, **kw):\n",
            "        _orig_socket.__init__(self, *pos, **kw)\n",
            "\n",
            "        self._savedmethods = dict()\n",
            "        for name in self._savenames:\n",
            "            self._savedmethods[name] = getattr(self, name)\n",
            "            delattr(self, name)  # Allows normal overriding mechanism to work\n",
            "\n",
            "    _savenames = list()\n",
            "\n",
            "\n",
            "def _makemethod(name):\n",
            "    return lambda self, *pos, **kw: self._savedmethods[name](*pos, **kw)\n",
            "for name in (\"sendto\", \"send\", \"recvfrom\", \"recv\"):\n",
            "    method = getattr(_BaseSocket, name, None)\n",
            "\n",
            "    # Determine if the method is not defined the usual way\n",
            "    # as a function in the class.\n",
            "    # Python 2 uses __slots__, so there are descriptors for each method,\n",
            "    # but they are not functions.\n",
            "    if not isinstance(method, Callable):\n",
            "        _BaseSocket._savenames.append(name)\n",
            "        setattr(_BaseSocket, name, _makemethod(name))\n",
            "\n",
            "\n",
            "class socksocket(_BaseSocket):\n",
            "    \"\"\"socksocket([family[, type[, proto]]]) -> socket object\n",
            "\n",
            "    Open a SOCKS enabled socket. The parameters are the same as\n",
            "    those of the standard socket init. In order for SOCKS to work,\n",
            "    you must specify family=AF_INET and proto=0.\n",
            "    The \"type\" argument must be either SOCK_STREAM or SOCK_DGRAM.\n",
            "    \"\"\"\n",
            "\n",
            "    default_proxy = None\n",
            "\n",
            "    def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM,\n",
            "                 proto=0, *args, **kwargs):\n",
            "        if type not in (socket.SOCK_STREAM, socket.SOCK_DGRAM):\n",
            "            msg = \"Socket type must be stream or datagram, not {!r}\"\n",
            "            raise ValueError(msg.format(type))\n",
            "\n",
            "        super(socksocket, self).__init__(family, type, proto, *args, **kwargs)\n",
            "        self._proxyconn = None  # TCP connection to keep UDP relay alive\n",
            "\n",
            "        if self.default_proxy:\n",
            "            self.proxy = self.default_proxy\n",
            "        else:\n",
            "            self.proxy = (None, None, None, None, None, None)\n",
            "        self.proxy_sockname = None\n",
            "        self.proxy_peername = None\n",
            "\n",
            "        self._timeout = None\n",
            "\n",
            "    def _readall(self, file, count):\n",
            "        \"\"\"Receive EXACTLY the number of bytes requested from the file object.\n",
            "\n",
            "        Blocks until the required number of bytes have been received.\"\"\"\n",
            "        data = b\"\"\n",
            "        while len(data) < count:\n",
            "            d = file.read(count - len(data))\n",
            "            if not d:\n",
            "                raise GeneralProxyError(\"Connection closed unexpectedly\")\n",
            "            data += d\n",
            "        return data\n",
            "\n",
            "    def settimeout(self, timeout):\n",
            "        self._timeout = timeout\n",
            "        try:\n",
            "            # test if we're connected, if so apply timeout\n",
            "            peer = self.get_proxy_peername()\n",
            "            super(socksocket, self).settimeout(self._timeout)\n",
            "        except socket.error:\n",
            "            pass\n",
            "\n",
            "    def gettimeout(self):\n",
            "        return self._timeout\n",
            "\n",
            "    def setblocking(self, v):\n",
            "        if v:\n",
            "            self.settimeout(None)\n",
            "        else:\n",
            "            self.settimeout(0.0)\n",
            "\n",
            "    def set_proxy(self, proxy_type=None, addr=None, port=None, rdns=True,\n",
            "                  username=None, password=None):\n",
            "        \"\"\" Sets the proxy to be used.\n",
            "\n",
            "        proxy_type -  The type of the proxy to be used. Three types\n",
            "                        are supported: PROXY_TYPE_SOCKS4 (including socks4a),\n",
            "                        PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP\n",
            "        addr -        The address of the server (IP or DNS).\n",
            "        port -        The port of the server. Defaults to 1080 for SOCKS\n",
            "                        servers and 8080 for HTTP proxy servers.\n",
            "        rdns -        Should DNS queries be performed on the remote side\n",
            "                       (rather than the local side). The default is True.\n",
            "                       Note: This has no effect with SOCKS4 servers.\n",
            "        username -    Username to authenticate with to the server.\n",
            "                       The default is no authentication.\n",
            "        password -    Password to authenticate with to the server.\n",
            "                       Only relevant when username is also provided.\"\"\"\n",
            "        self.proxy = (proxy_type, addr, port, rdns,\n",
            "                      username.encode() if username else None,\n",
            "                      password.encode() if password else None)\n",
            "\n",
            "    def setproxy(self, *args, **kwargs):\n",
            "        if \"proxytype\" in kwargs:\n",
            "            kwargs[\"proxy_type\"] = kwargs.pop(\"proxytype\")\n",
            "        return self.set_proxy(*args, **kwargs)\n",
            "\n",
            "    def bind(self, *pos, **kw):\n",
            "        \"\"\"Implements proxy connection for UDP sockets.\n",
            "\n",
            "        Happens during the bind() phase.\"\"\"\n",
            "        (proxy_type, proxy_addr, proxy_port, rdns, username,\n",
            "         password) = self.proxy\n",
            "        if not proxy_type or self.type != socket.SOCK_DGRAM:\n",
            "            return _orig_socket.bind(self, *pos, **kw)\n",
            "\n",
            "        if self._proxyconn:\n",
            "            raise socket.error(EINVAL, \"Socket already bound to an address\")\n",
            "        if proxy_type != SOCKS5:\n",
            "            msg = \"UDP only supported by SOCKS5 proxy type\"\n",
            "            raise socket.error(EOPNOTSUPP, msg)\n",
            "        super(socksocket, self).bind(*pos, **kw)\n",
            "\n",
            "        # Need to specify actual local port because\n",
            "        # some relays drop packets if a port of zero is specified.\n",
            "        # Avoid specifying host address in case of NAT though.\n",
            "        _, port = self.getsockname()\n",
            "        dst = (\"0\", port)\n",
            "\n",
            "        self._proxyconn = _orig_socket()\n",
            "        proxy = self._proxy_addr()\n",
            "        self._proxyconn.connect(proxy)\n",
            "\n",
            "        UDP_ASSOCIATE = b\"\\x03\"\n",
            "        _, relay = self._SOCKS5_request(self._proxyconn, UDP_ASSOCIATE, dst)\n",
            "\n",
            "        # The relay is most likely on the same host as the SOCKS proxy,\n",
            "        # but some proxies return a private IP address (10.x.y.z)\n",
            "        host, _ = proxy\n",
            "        _, port = relay\n",
            "        super(socksocket, self).connect((host, port))\n",
            "        super(socksocket, self).settimeout(self._timeout)\n",
            "        self.proxy_sockname = (\"0.0.0.0\", 0)  # Unknown\n",
            "\n",
            "    def sendto(self, bytes, *args, **kwargs):\n",
            "        if self.type != socket.SOCK_DGRAM:\n",
            "            return super(socksocket, self).sendto(bytes, *args, **kwargs)\n",
            "        if not self._proxyconn:\n",
            "            self.bind((\"\", 0))\n",
            "\n",
            "        address = args[-1]\n",
            "        flags = args[:-1]\n",
            "\n",
            "        header = BytesIO()\n",
            "        RSV = b\"\\x00\\x00\"\n",
            "        header.write(RSV)\n",
            "        STANDALONE = b\"\\x00\"\n",
            "        header.write(STANDALONE)\n",
            "        self._write_SOCKS5_address(address, header)\n",
            "\n",
            "        sent = super(socksocket, self).send(header.getvalue() + bytes, *flags,\n",
            "                                            **kwargs)\n",
            "        return sent - header.tell()\n",
            "\n",
            "    def send(self, bytes, flags=0, **kwargs):\n",
            "        if self.type == socket.SOCK_DGRAM:\n",
            "            return self.sendto(bytes, flags, self.proxy_peername, **kwargs)\n",
            "        else:\n",
            "            return super(socksocket, self).send(bytes, flags, **kwargs)\n",
            "\n",
            "    def recvfrom(self, bufsize, flags=0):\n",
            "        if self.type != socket.SOCK_DGRAM:\n",
            "            return super(socksocket, self).recvfrom(bufsize, flags)\n",
            "        if not self._proxyconn:\n",
            "            self.bind((\"\", 0))\n",
            "\n",
            "        buf = BytesIO(super(socksocket, self).recv(bufsize + 1024, flags))\n",
            "        buf.seek(2, SEEK_CUR)\n",
            "        frag = buf.read(1)\n",
            "        if ord(frag):\n",
            "            raise NotImplementedError(\"Received UDP packet fragment\")\n",
            "        fromhost, fromport = self._read_SOCKS5_address(buf)\n",
            "\n",
            "        if self.proxy_peername:\n",
            "            peerhost, peerport = self.proxy_peername\n",
            "            if fromhost != peerhost or peerport not in (0, fromport):\n",
            "                raise socket.error(EAGAIN, \"Packet filtered\")\n",
            "\n",
            "        return (buf.read(bufsize), (fromhost, fromport))\n",
            "\n",
            "    def recv(self, *pos, **kw):\n",
            "        bytes, _ = self.recvfrom(*pos, **kw)\n",
            "        return bytes\n",
            "\n",
            "    def close(self):\n",
            "        if self._proxyconn:\n",
            "            self._proxyconn.close()\n",
            "        return super(socksocket, self).close()\n",
            "\n",
            "    def get_proxy_sockname(self):\n",
            "        \"\"\"Returns the bound IP address and port number at the proxy.\"\"\"\n",
            "        return self.proxy_sockname\n",
            "\n",
            "    getproxysockname = get_proxy_sockname\n",
            "\n",
            "    def get_proxy_peername(self):\n",
            "        \"\"\"\n",
            "        Returns the IP and port number of the proxy.\n",
            "        \"\"\"\n",
            "        return self.getpeername()\n",
            "\n",
            "    getproxypeername = get_proxy_peername\n",
            "\n",
            "    def get_peername(self):\n",
            "        \"\"\"Returns the IP address and port number of the destination machine.\n",
            "\n",
            "        Note: get_proxy_peername returns the proxy.\"\"\"\n",
            "        return self.proxy_peername\n",
            "\n",
            "    getpeername = get_peername\n",
            "\n",
            "    def _negotiate_SOCKS5(self, *dest_addr):\n",
            "        \"\"\"Negotiates a stream connection through a SOCKS5 server.\"\"\"\n",
            "        CONNECT = b\"\\x01\"\n",
            "        self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(\n",
            "            self, CONNECT, dest_addr)\n",
            "\n",
            "    def _SOCKS5_request(self, conn, cmd, dst):\n",
            "        \"\"\"\n",
            "        Send SOCKS5 request with given command (CMD field) and\n",
            "        address (DST field). Returns resolved DST address that was used.\n",
            "        \"\"\"\n",
            "        proxy_type, addr, port, rdns, username, password = self.proxy\n",
            "\n",
            "        writer = conn.makefile(\"wb\")\n",
            "        reader = conn.makefile(\"rb\", 0)  # buffering=0 renamed in Python 3\n",
            "        try:\n",
            "            # First we'll send the authentication packages we support.\n",
            "            if username and password:\n",
            "                # The username/password details were supplied to the\n",
            "                # set_proxy method so we support the USERNAME/PASSWORD\n",
            "                # authentication (in addition to the standard none).\n",
            "                writer.write(b\"\\x05\\x02\\x00\\x02\")\n",
            "            else:\n",
            "                # No username/password were entered, therefore we\n",
            "                # only support connections with no authentication.\n",
            "                writer.write(b\"\\x05\\x01\\x00\")\n",
            "\n",
            "            # We'll receive the server's response to determine which\n",
            "            # method was selected\n",
            "            writer.flush()\n",
            "            chosen_auth = self._readall(reader, 2)\n",
            "\n",
            "            if chosen_auth[0:1] != b\"\\x05\":\n",
            "                # Note: string[i:i+1] is used because indexing of a bytestring\n",
            "                # via bytestring[i] yields an integer in Python 3\n",
            "                raise GeneralProxyError(\n",
            "                    \"SOCKS5 proxy server sent invalid data\")\n",
            "\n",
            "            # Check the chosen authentication method\n",
            "\n",
            "            if chosen_auth[1:2] == b\"\\x02\":\n",
            "                # Okay, we need to perform a basic username/password\n",
            "                # authentication.\n",
            "                if not (username and password):\n",
            "                    # Although we said we don't support authentication, the\n",
            "                    # server may still request basic username/password\n",
            "                    # authentication\n",
            "                    raise SOCKS5AuthError(\"No username/password supplied. \"\n",
            "                                          \"Server requested username/password\"\n",
            "                                          \" authentication\")\n",
            "\n",
            "                writer.write(b\"\\x01\" + chr(len(username)).encode()\n",
            "                             + username\n",
            "                             + chr(len(password)).encode()\n",
            "                             + password)\n",
            "                writer.flush()\n",
            "                auth_status = self._readall(reader, 2)\n",
            "                if auth_status[0:1] != b\"\\x01\":\n",
            "                    # Bad response\n",
            "                    raise GeneralProxyError(\n",
            "                        \"SOCKS5 proxy server sent invalid data\")\n",
            "                if auth_status[1:2] != b\"\\x00\":\n",
            "                    # Authentication failed\n",
            "                    raise SOCKS5AuthError(\"SOCKS5 authentication failed\")\n",
            "\n",
            "                # Otherwise, authentication succeeded\n",
            "\n",
            "            # No authentication is required if 0x00\n",
            "            elif chosen_auth[1:2] != b\"\\x00\":\n",
            "                # Reaching here is always bad\n",
            "                if chosen_auth[1:2] == b\"\\xFF\":\n",
            "                    raise SOCKS5AuthError(\n",
            "                        \"All offered SOCKS5 authentication methods were\"\n",
            "                        \" rejected\")\n",
            "                else:\n",
            "                    raise GeneralProxyError(\n",
            "                        \"SOCKS5 proxy server sent invalid data\")\n",
            "\n",
            "            # Now we can request the actual connection\n",
            "            writer.write(b\"\\x05\" + cmd + b\"\\x00\")\n",
            "            resolved = self._write_SOCKS5_address(dst, writer)\n",
            "            writer.flush()\n",
            "\n",
            "            # Get the response\n",
            "            resp = self._readall(reader, 3)\n",
            "            if resp[0:1] != b\"\\x05\":\n",
            "                raise GeneralProxyError(\n",
            "                    \"SOCKS5 proxy server sent invalid data\")\n",
            "\n",
            "            status = ord(resp[1:2])\n",
            "            if status != 0x00:\n",
            "                # Connection failed: server returned an error\n",
            "                error = SOCKS5_ERRORS.get(status, \"Unknown error\")\n",
            "                raise SOCKS5Error(\"{:#04x}: {}\".format(status, error))\n",
            "\n",
            "            # Get the bound address/port\n",
            "            bnd = self._read_SOCKS5_address(reader)\n",
            "\n",
            "            super(socksocket, self).settimeout(self._timeout)\n",
            "            return (resolved, bnd)\n",
            "        finally:\n",
            "            reader.close()\n",
            "            writer.close()\n",
            "\n",
            "    def _write_SOCKS5_address(self, addr, file):\n",
            "        \"\"\"\n",
            "        Return the host and port packed for the SOCKS5 protocol,\n",
            "        and the resolved address as a tuple object.\n",
            "        \"\"\"\n",
            "        host, port = addr\n",
            "        proxy_type, _, _, rdns, username, password = self.proxy\n",
            "        family_to_byte = {socket.AF_INET: b\"\\x01\", socket.AF_INET6: b\"\\x04\"}\n",
            "\n",
            "        # If the given destination address is an IP address, we'll\n",
            "        # use the IP address request even if remote resolving was specified.\n",
            "        # Detect whether the address is IPv4/6 directly.\n",
            "        for family in (socket.AF_INET, socket.AF_INET6):\n",
            "            try:\n",
            "                addr_bytes = socket.inet_pton(family, host)\n",
            "                file.write(family_to_byte[family] + addr_bytes)\n",
            "                host = socket.inet_ntop(family, addr_bytes)\n",
            "                file.write(struct.pack(\">H\", port))\n",
            "                return host, port\n",
            "            except socket.error:\n",
            "                continue\n",
            "\n",
            "        # Well it's not an IP number, so it's probably a DNS name.\n",
            "        if rdns:\n",
            "            # Resolve remotely\n",
            "            host_bytes = host.encode(\"idna\")\n",
            "            file.write(b\"\\x03\" + chr(len(host_bytes)).encode() + host_bytes)\n",
            "        else:\n",
            "            # Resolve locally\n",
            "            addresses = socket.getaddrinfo(host, port, socket.AF_UNSPEC,\n",
            "                                           socket.SOCK_STREAM,\n",
            "                                           socket.IPPROTO_TCP,\n",
            "                                           socket.AI_ADDRCONFIG)\n",
            "            # We can't really work out what IP is reachable, so just pick the\n",
            "            # first.\n",
            "            target_addr = addresses[0]\n",
            "            family = target_addr[0]\n",
            "            host = target_addr[4][0]\n",
            "\n",
            "            addr_bytes = socket.inet_pton(family, host)\n",
            "            file.write(family_to_byte[family] + addr_bytes)\n",
            "            host = socket.inet_ntop(family, addr_bytes)\n",
            "        file.write(struct.pack(\">H\", port))\n",
            "        return host, port\n",
            "\n",
            "    def _read_SOCKS5_address(self, file):\n",
            "        atyp = self._readall(file, 1)\n",
            "        if atyp == b\"\\x01\":\n",
            "            addr = socket.inet_ntoa(self._readall(file, 4))\n",
            "        elif atyp == b\"\\x03\":\n",
            "            length = self._readall(file, 1)\n",
            "            addr = self._readall(file, ord(length))\n",
            "        elif atyp == b\"\\x04\":\n",
            "            addr = socket.inet_ntop(socket.AF_INET6, self._readall(file, 16))\n",
            "        else:\n",
            "            raise GeneralProxyError(\"SOCKS5 proxy server sent invalid data\")\n",
            "\n",
            "        port = struct.unpack(\">H\", self._readall(file, 2))[0]\n",
            "        return addr, port\n",
            "\n",
            "    def _negotiate_SOCKS4(self, dest_addr, dest_port):\n",
            "        \"\"\"Negotiates a connection through a SOCKS4 server.\"\"\"\n",
            "        proxy_type, addr, port, rdns, username, password = self.proxy\n",
            "\n",
            "        writer = self.makefile(\"wb\")\n",
            "        reader = self.makefile(\"rb\", 0)  # buffering=0 renamed in Python 3\n",
            "        try:\n",
            "            # Check if the destination address provided is an IP address\n",
            "            remote_resolve = False\n",
            "            try:\n",
            "                addr_bytes = socket.inet_aton(dest_addr)\n",
            "            except socket.error:\n",
            "                # It's a DNS name. Check where it should be resolved.\n",
            "                if rdns:\n",
            "                    addr_bytes = b\"\\x00\\x00\\x00\\x01\"\n",
            "                    remote_resolve = True\n",
            "                else:\n",
            "                    addr_bytes = socket.inet_aton(\n",
            "                        socket.gethostbyname(dest_addr))\n",
            "\n",
            "            # Construct the request packet\n",
            "            writer.write(struct.pack(\">BBH\", 0x04, 0x01, dest_port))\n",
            "            writer.write(addr_bytes)\n",
            "\n",
            "            # The username parameter is considered userid for SOCKS4\n",
            "            if username:\n",
            "                writer.write(username)\n",
            "            writer.write(b\"\\x00\")\n",
            "\n",
            "            # DNS name if remote resolving is required\n",
            "            # NOTE: This is actually an extension to the SOCKS4 protocol\n",
            "            # called SOCKS4A and may not be supported in all cases.\n",
            "            if remote_resolve:\n",
            "                writer.write(dest_addr.encode(\"idna\") + b\"\\x00\")\n",
            "            writer.flush()\n",
            "\n",
            "            # Get the response from the server\n",
            "            resp = self._readall(reader, 8)\n",
            "            if resp[0:1] != b\"\\x00\":\n",
            "                # Bad data\n",
            "                raise GeneralProxyError(\n",
            "                    \"SOCKS4 proxy server sent invalid data\")\n",
            "\n",
            "            status = ord(resp[1:2])\n",
            "            if status != 0x5A:\n",
            "                # Connection failed: server returned an error\n",
            "                error = SOCKS4_ERRORS.get(status, \"Unknown error\")\n",
            "                raise SOCKS4Error(\"{:#04x}: {}\".format(status, error))\n",
            "\n",
            "            # Get the bound address/port\n",
            "            self.proxy_sockname = (socket.inet_ntoa(resp[4:]),\n",
            "                                   struct.unpack(\">H\", resp[2:4])[0])\n",
            "            if remote_resolve:\n",
            "                self.proxy_peername = socket.inet_ntoa(addr_bytes), dest_port\n",
            "            else:\n",
            "                self.proxy_peername = dest_addr, dest_port\n",
            "        finally:\n",
            "            reader.close()\n",
            "            writer.close()\n",
            "\n",
            "    def _negotiate_HTTP(self, dest_addr, dest_port):\n",
            "        \"\"\"Negotiates a connection through an HTTP server.\n",
            "\n",
            "        NOTE: This currently only supports HTTP CONNECT-style proxies.\"\"\"\n",
            "        proxy_type, addr, port, rdns, username, password = self.proxy\n",
            "\n",
            "        # If we need to resolve locally, we do this now\n",
            "        addr = dest_addr if rdns else socket.gethostbyname(dest_addr)\n",
            "\n",
            "        http_headers = [\n",
            "            (b\"CONNECT \" + addr.encode(\"idna\") + b\":\"\n",
            "             + str(dest_port).encode() + b\" HTTP/1.1\"),\n",
            "            b\"Host: \" + dest_addr.encode(\"idna\")\n",
            "        ]\n",
            "\n",
            "        if username and password:\n",
            "            http_headers.append(b\"Proxy-Authorization: basic \"\n",
            "                                + b64encode(username + b\":\" + password))\n",
            "\n",
            "        http_headers.append(b\"\\r\\n\")\n",
            "\n",
            "        self.sendall(b\"\\r\\n\".join(http_headers))\n",
            "\n",
            "        # We just need the first line to check if the connection was successful\n",
            "        fobj = self.makefile()\n",
            "        status_line = fobj.readline()\n",
            "        fobj.close()\n",
            "\n",
            "        if not status_line:\n",
            "            raise GeneralProxyError(\"Connection closed unexpectedly\")\n",
            "\n",
            "        try:\n",
            "            proto, status_code, status_msg = status_line.split(\" \", 2)\n",
            "        except ValueError:\n",
            "            raise GeneralProxyError(\"HTTP proxy server sent invalid response\")\n",
            "\n",
            "        if not proto.startswith(\"HTTP/\"):\n",
            "            raise GeneralProxyError(\n",
            "                \"Proxy server does not appear to be an HTTP proxy\")\n",
            "\n",
            "        try:\n",
            "            status_code = int(status_code)\n",
            "        except ValueError:\n",
            "            raise HTTPError(\n",
            "                \"HTTP proxy server did not return a valid HTTP status\")\n",
            "\n",
            "        if status_code != 200:\n",
            "            error = \"{}: {}\".format(status_code, status_msg)\n",
            "            if status_code in (400, 403, 405):\n",
            "                # It's likely that the HTTP proxy server does not support the\n",
            "                # CONNECT tunneling method\n",
            "                error += (\"\\n[*] Note: The HTTP proxy server may not be\"\n",
            "                          \" supported by PySocks (must be a CONNECT tunnel\"\n",
            "                          \" proxy)\")\n",
            "            raise HTTPError(error)\n",
            "\n",
            "        self.proxy_sockname = (b\"0.0.0.0\", 0)\n",
            "        self.proxy_peername = addr, dest_port\n",
            "\n",
            "    _proxy_negotiators = {\n",
            "                           SOCKS4: _negotiate_SOCKS4,\n",
            "                           SOCKS5: _negotiate_SOCKS5,\n",
            "                           HTTP: _negotiate_HTTP\n",
            "                         }\n",
            "\n",
            "    @set_self_blocking\n",
            "    def connect(self, dest_pair, catch_errors=None):\n",
            "        \"\"\"\n",
            "        Connects to the specified destination through a proxy.\n",
            "        Uses the same API as socket's connect().\n",
            "        To select the proxy server, use set_proxy().\n",
            "\n",
            "        dest_pair - 2-tuple of (IP/hostname, port).\n",
            "        \"\"\"\n",
            "        if len(dest_pair) != 2 or dest_pair[0].startswith(\"[\"):\n",
            "            # Probably IPv6, not supported -- raise an error, and hope\n",
            "            # Happy Eyeballs (RFC6555) makes sure at least the IPv4\n",
            "            # connection works...\n",
            "            raise socket.error(\"PySocks doesn't support IPv6: %s\"\n",
            "                               % str(dest_pair))\n",
            "\n",
            "        dest_addr, dest_port = dest_pair\n",
            "\n",
            "        if self.type == socket.SOCK_DGRAM:\n",
            "            if not self._proxyconn:\n",
            "                self.bind((\"\", 0))\n",
            "            dest_addr = socket.gethostbyname(dest_addr)\n",
            "\n",
            "            # If the host address is INADDR_ANY or similar, reset the peer\n",
            "            # address so that packets are received from any peer\n",
            "            if dest_addr == \"0.0.0.0\" and not dest_port:\n",
            "                self.proxy_peername = None\n",
            "            else:\n",
            "                self.proxy_peername = (dest_addr, dest_port)\n",
            "            return\n",
            "\n",
            "        (proxy_type, proxy_addr, proxy_port, rdns, username,\n",
            "         password) = self.proxy\n",
            "\n",
            "        # Do a minimal input check first\n",
            "        if (not isinstance(dest_pair, (list, tuple))\n",
            "                or len(dest_pair) != 2\n",
            "                or not dest_addr\n",
            "                or not isinstance(dest_port, int)):\n",
            "            # Inputs failed, raise an error\n",
            "            raise GeneralProxyError(\n",
            "                \"Invalid destination-connection (host, port) pair\")\n",
            "\n",
            "        # We set the timeout here so that we don't hang in connection or during\n",
            "        # negotiation.\n",
            "        super(socksocket, self).settimeout(self._timeout)\n",
            "\n",
            "        if proxy_type is None:\n",
            "            # Treat like regular socket object\n",
            "            self.proxy_peername = dest_pair\n",
            "            super(socksocket, self).settimeout(self._timeout)\n",
            "            super(socksocket, self).connect((dest_addr, dest_port))\n",
            "            return\n",
            "\n",
            "        proxy_addr = self._proxy_addr()\n",
            "\n",
            "        try:\n",
            "            # Initial connection to proxy server.\n",
            "            super(socksocket, self).connect(proxy_addr)\n",
            "\n",
            "        except socket.error as error:\n",
            "            # Error while connecting to proxy\n",
            "            self.close()\n",
            "            if not catch_errors:\n",
            "                proxy_addr, proxy_port = proxy_addr\n",
            "                proxy_server = \"{}:{}\".format(proxy_addr, proxy_port)\n",
            "                printable_type = PRINTABLE_PROXY_TYPES[proxy_type]\n",
            "\n",
            "                msg = \"Error connecting to {} proxy {}\".format(printable_type,\n",
            "                                                                    proxy_server)\n",
            "                log.debug(\"%s due to: %s\", msg, error)\n",
            "                raise ProxyConnectionError(msg, error)\n",
            "            else:\n",
            "                raise error\n",
            "\n",
            "        else:\n",
            "            # Connected to proxy server, now negotiate\n",
            "            try:\n",
            "                # Calls negotiate_{SOCKS4, SOCKS5, HTTP}\n",
            "                negotiate = self._proxy_negotiators[proxy_type]\n",
            "                negotiate(self, dest_addr, dest_port)\n",
            "            except socket.error as error:\n",
            "                if not catch_errors:\n",
            "                    # Wrap socket errors\n",
            "                    self.close()\n",
            "                    raise GeneralProxyError(\"Socket error\", error)\n",
            "                else:\n",
            "                    raise error\n",
            "            except ProxyError:\n",
            "                # Protocol error while negotiating with proxy\n",
            "                self.close()\n",
            "                raise\n",
            "                \n",
            "    @set_self_blocking\n",
            "    def connect_ex(self, dest_pair):\n",
            "        \"\"\" https://docs.python.org/3/library/socket.html#socket.socket.connect_ex\n",
            "        Like connect(address), but return an error indicator instead of raising an exception for errors returned by the C-level connect() call (other problems, such as \"host not found\" can still raise exceptions).\n",
            "        \"\"\"\n",
            "        try:\n",
            "            self.connect(dest_pair, catch_errors=True)\n",
            "            return 0\n",
            "        except OSError as e:\n",
            "            # If the error is numeric (socket errors are numeric), then return number as \n",
            "            # connect_ex expects. Otherwise raise the error again (socket timeout for example)\n",
            "            if e.errno:\n",
            "                return e.errno\n",
            "            else:\n",
            "                raise\n",
            "\n",
            "    def _proxy_addr(self):\n",
            "        \"\"\"\n",
            "        Return proxy address to connect to as tuple object\n",
            "        \"\"\"\n",
            "        (proxy_type, proxy_addr, proxy_port, rdns, username,\n",
            "         password) = self.proxy\n",
            "        proxy_port = proxy_port or DEFAULT_PORTS.get(proxy_type)\n",
            "        if not proxy_port:\n",
            "            raise GeneralProxyError(\"Invalid proxy type\")\n",
            "        return proxy_addr, proxy_port\n",
            "Found files.\n",
            "In try\n",
            "#!/usr/bin/env python\n",
            "\n",
            "#\n",
            "#   Cython -- Main Program, generic\n",
            "#\n",
            "\n",
            "if __name__ == '__main__':\n",
            "\n",
            "    import os\n",
            "    import sys\n",
            "\n",
            "    # Make sure we import the right Cython\n",
            "    cythonpath, _ = os.path.split(os.path.realpath(__file__))\n",
            "    sys.path.insert(0, cythonpath)\n",
            "\n",
            "    from Cython.Compiler.Main import main\n",
            "    main(command_line = 1)\n",
            "\n",
            "else:\n",
            "    # Void cython.* directives.\n",
            "    from Cython.Shadow import *\n",
            "    ## and bring in the __version__\n",
            "    from Cython import __version__\n",
            "    from Cython import load_ipython_extension\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Author: John MacFarlane <jgm@berkeley.edu>\n",
            "# Copyright: (C) 2013 John MacFarlane\n",
            "# License: BSD3\n",
            "\n",
            "\"\"\"\n",
            "Functions to aid writing python scripts that process the pandoc\n",
            "AST serialized as JSON.\n",
            "\"\"\"\n",
            "\n",
            "import codecs\n",
            "import hashlib\n",
            "import io\n",
            "import json\n",
            "import os\n",
            "import sys\n",
            "\n",
            "\n",
            "# some utility-functions: make it easier to create your own filters\n",
            "\n",
            "\n",
            "def get_filename4code(module, content, ext=None):\n",
            "    \"\"\"Generate filename based on content\n",
            "\n",
            "    The function ensures that the (temporary) directory exists, so that the\n",
            "    file can be written.\n",
            "\n",
            "    Example:\n",
            "        filename = get_filename4code(\"myfilter\", code)\n",
            "    \"\"\"\n",
            "    imagedir = module + \"-images\"\n",
            "    fn = hashlib.sha1(content.encode(sys.getfilesystemencoding())).hexdigest()\n",
            "    try:\n",
            "        os.mkdir(imagedir)\n",
            "        sys.stderr.write('Created directory ' + imagedir + '\\n')\n",
            "    except OSError:\n",
            "        pass\n",
            "    if ext:\n",
            "        fn += \".\" + ext\n",
            "    return os.path.join(imagedir, fn)\n",
            "\n",
            "def get_value(kv, key, value = None):\n",
            "    \"\"\"get value from the keyvalues (options)\"\"\"\n",
            "    res = []\n",
            "    for k, v in kv:\n",
            "        if k == key:\n",
            "            value = v\n",
            "        else:\n",
            "            res.append([k, v])\n",
            "    return value, res\n",
            "\n",
            "def get_caption(kv):\n",
            "    \"\"\"get caption from the keyvalues (options)\n",
            "\n",
            "    Example:\n",
            "      if key == 'CodeBlock':\n",
            "        [[ident, classes, keyvals], code] = value\n",
            "        caption, typef, keyvals = get_caption(keyvals)\n",
            "        ...\n",
            "        return Para([Image([ident, [], keyvals], caption, [filename, typef])])\n",
            "    \"\"\"\n",
            "    caption = []\n",
            "    typef = \"\"\n",
            "    value, res = get_value(kv, u\"caption\")\n",
            "    if value is not None:\n",
            "        caption = [Str(value)]\n",
            "        typef = \"fig:\"\n",
            "\n",
            "    return caption, typef, res\n",
            "\n",
            "\n",
            "def get_extension(format, default, **alternates):\n",
            "    \"\"\"get the extension for the result, needs a default and some specialisations\n",
            "\n",
            "    Example:\n",
            "      filetype = get_extension(format, \"png\", html=\"svg\", latex=\"eps\")\n",
            "    \"\"\"\n",
            "    try:\n",
            "        return alternates[format]\n",
            "    except KeyError:\n",
            "        return default\n",
            "\n",
            "# end of utilities\n",
            "\n",
            "\n",
            "def walk(x, action, format, meta):\n",
            "    \"\"\"Walk a tree, applying an action to every object.\n",
            "    Returns a modified tree.  An action is a function of the form\n",
            "    `action(key, value, format, meta)`, where:\n",
            "\n",
            "    * `key` is the type of the pandoc object (e.g. 'Str', 'Para') `value` is\n",
            "    * the contents of the object (e.g. a string for 'Str', a list of\n",
            "      inline elements for 'Para')\n",
            "    * `format` is the target output format (as supplied by the\n",
            "      `format` argument of `walk`)\n",
            "    * `meta` is the document's metadata\n",
            "\n",
            "    The return of an action is either:\n",
            "\n",
            "    * `None`: this means that the object should remain unchanged\n",
            "    * a pandoc object: this will replace the original object\n",
            "    * a list of pandoc objects: these will replace the original object; the\n",
            "      list is merged with the neighbors of the orignal objects (spliced into\n",
            "      the list the original object belongs to); returning an empty list deletes\n",
            "      the object\n",
            "    \"\"\"\n",
            "    if isinstance(x, list):\n",
            "        array = []\n",
            "        for item in x:\n",
            "            if isinstance(item, dict) and 't' in item:\n",
            "                res = action(item['t'],\n",
            "                             item['c'] if 'c' in item else None, format, meta)\n",
            "                if res is None:\n",
            "                    array.append(walk(item, action, format, meta))\n",
            "                elif isinstance(res, list):\n",
            "                    for z in res:\n",
            "                        array.append(walk(z, action, format, meta))\n",
            "                else:\n",
            "                    array.append(walk(res, action, format, meta))\n",
            "            else:\n",
            "                array.append(walk(item, action, format, meta))\n",
            "        return array\n",
            "    elif isinstance(x, dict):\n",
            "        for k in x:\n",
            "            x[k] = walk(x[k], action, format, meta)\n",
            "        return x\n",
            "    else:\n",
            "        return x\n",
            "\n",
            "def toJSONFilter(action):\n",
            "    \"\"\"Like `toJSONFilters`, but takes a single action as argument.\n",
            "    \"\"\"\n",
            "    toJSONFilters([action])\n",
            "\n",
            "\n",
            "def toJSONFilters(actions):\n",
            "    \"\"\"Generate a JSON-to-JSON filter from stdin to stdout\n",
            "\n",
            "    The filter:\n",
            "\n",
            "    * reads a JSON-formatted pandoc document from stdin\n",
            "    * transforms it by walking the tree and performing the actions\n",
            "    * returns a new JSON-formatted pandoc document to stdout\n",
            "\n",
            "    The argument `actions` is a list of functions of the form\n",
            "    `action(key, value, format, meta)`, as described in more\n",
            "    detail under `walk`.\n",
            "\n",
            "    This function calls `applyJSONFilters`, with the `format`\n",
            "    argument provided by the first command-line argument,\n",
            "    if present.  (Pandoc sets this by default when calling\n",
            "    filters.)\n",
            "    \"\"\"\n",
            "    try:\n",
            "        input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')\n",
            "    except AttributeError:\n",
            "        # Python 2 does not have sys.stdin.buffer.\n",
            "        # REF: https://stackoverflow.com/questions/2467928/python-unicodeencode\n",
            "        input_stream = codecs.getreader(\"utf-8\")(sys.stdin)\n",
            "\n",
            "    source = input_stream.read()\n",
            "    if len(sys.argv) > 1:\n",
            "        format = sys.argv[1]\n",
            "    else:\n",
            "        format = \"\"\n",
            "\n",
            "    sys.stdout.write(applyJSONFilters(actions, source, format))\n",
            "\n",
            "def applyJSONFilters(actions, source, format=\"\"):\n",
            "    \"\"\"Walk through JSON structure and apply filters\n",
            "\n",
            "    This:\n",
            "\n",
            "    * reads a JSON-formatted pandoc document from a source string\n",
            "    * transforms it by walking the tree and performing the actions\n",
            "    * returns a new JSON-formatted pandoc document as a string\n",
            "\n",
            "    The `actions` argument is a list of functions (see `walk`\n",
            "    for a full description).\n",
            "\n",
            "    The argument `source` is a string encoded JSON object.\n",
            "\n",
            "    The argument `format` is a string describing the output format.\n",
            "\n",
            "    Returns a the new JSON-formatted pandoc document.\n",
            "    \"\"\"\n",
            "\n",
            "    doc = json.loads(source)\n",
            "\n",
            "    if 'meta' in doc:\n",
            "        meta = doc['meta']\n",
            "    elif doc[0]:  # old API\n",
            "        meta = doc[0]['unMeta']\n",
            "    else:\n",
            "        meta = {}\n",
            "    altered = doc\n",
            "    for action in actions:\n",
            "        altered = walk(altered, action, format, meta)\n",
            "\n",
            "    return json.dumps(altered)\n",
            "\n",
            "\n",
            "def stringify(x):\n",
            "    \"\"\"Walks the tree x and returns concatenated string content,\n",
            "    leaving out all formatting.\n",
            "    \"\"\"\n",
            "    result = []\n",
            "\n",
            "    def go(key, val, format, meta):\n",
            "        if key in ['Str', 'MetaString']:\n",
            "            result.append(val)\n",
            "        elif key == 'Code':\n",
            "            result.append(val[1])\n",
            "        elif key == 'Math':\n",
            "            result.append(val[1])\n",
            "        elif key == 'LineBreak':\n",
            "            result.append(\" \")\n",
            "        elif key == 'SoftBreak':\n",
            "            result.append(\" \")\n",
            "        elif key == 'Space':\n",
            "            result.append(\" \")\n",
            "\n",
            "    walk(x, go, \"\", {})\n",
            "    return ''.join(result)\n",
            "\n",
            "\n",
            "def attributes(attrs):\n",
            "    \"\"\"Returns an attribute list, constructed from the\n",
            "    dictionary attrs.\n",
            "    \"\"\"\n",
            "    attrs = attrs or {}\n",
            "    ident = attrs.get(\"id\", \"\")\n",
            "    classes = attrs.get(\"classes\", [])\n",
            "    keyvals = [[x, attrs[x]] for x in attrs if (x != \"classes\" and x != \"id\")]\n",
            "    return [ident, classes, keyvals]\n",
            "\n",
            "\n",
            "def elt(eltType, numargs):\n",
            "    def fun(*args):\n",
            "        lenargs = len(args)\n",
            "        if lenargs != numargs:\n",
            "            raise ValueError(eltType + ' expects ' + str(numargs) +\n",
            "                             ' arguments, but given ' + str(lenargs))\n",
            "        if numargs == 0:\n",
            "            xs = []\n",
            "        elif len(args) == 1:\n",
            "            xs = args[0]\n",
            "        else:\n",
            "            xs = list(args)\n",
            "        return {'t': eltType, 'c': xs}\n",
            "    return fun\n",
            "\n",
            "# Constructors for block elements\n",
            "\n",
            "Plain = elt('Plain', 1)\n",
            "Para = elt('Para', 1)\n",
            "CodeBlock = elt('CodeBlock', 2)\n",
            "RawBlock = elt('RawBlock', 2)\n",
            "BlockQuote = elt('BlockQuote', 1)\n",
            "OrderedList = elt('OrderedList', 2)\n",
            "BulletList = elt('BulletList', 1)\n",
            "DefinitionList = elt('DefinitionList', 1)\n",
            "Header = elt('Header', 3)\n",
            "HorizontalRule = elt('HorizontalRule', 0)\n",
            "Table = elt('Table', 5)\n",
            "Div = elt('Div', 2)\n",
            "Null = elt('Null', 0)\n",
            "\n",
            "# Constructors for inline elements\n",
            "\n",
            "Str = elt('Str', 1)\n",
            "Emph = elt('Emph', 1)\n",
            "Strong = elt('Strong', 1)\n",
            "Strikeout = elt('Strikeout', 1)\n",
            "Superscript = elt('Superscript', 1)\n",
            "Subscript = elt('Subscript', 1)\n",
            "SmallCaps = elt('SmallCaps', 1)\n",
            "Quoted = elt('Quoted', 2)\n",
            "Cite = elt('Cite', 2)\n",
            "Code = elt('Code', 2)\n",
            "Space = elt('Space', 0)\n",
            "LineBreak = elt('LineBreak', 0)\n",
            "Math = elt('Math', 2)\n",
            "RawInline = elt('RawInline', 2)\n",
            "Link = elt('Link', 3)\n",
            "Image = elt('Image', 3)\n",
            "Note = elt('Note', 1)\n",
            "SoftBreak = elt('SoftBreak', 0)\n",
            "Span = elt('Span', 2)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "\"\"\"\n",
            "gspread_dataframe\n",
            "~~~~~~~~~~~~~~~~~\n",
            "\n",
            "This module contains functions to retrieve a gspread worksheet as a\n",
            "`pandas.DataFrame`, and to set the contents of a worksheet\n",
            "using a `pandas.DataFrame`. To use these functions, have\n",
            "Pandas 0.14.0 or greater installed.\n",
            "\"\"\"\n",
            "from gspread.utils import fill_gaps\n",
            "from gspread.models import Cell\n",
            "from collections import defaultdict\n",
            "import logging\n",
            "import re\n",
            "\n",
            "try:\n",
            "    from itertools import chain, zip_longest\n",
            "except ImportError:\n",
            "    from itertools import chain, izip_longest as zip_longest\n",
            "\n",
            "logger = logging.getLogger(__name__)\n",
            "\n",
            "# pandas import and version check\n",
            "\n",
            "import pandas as pd\n",
            "major, minor = tuple([int(i) for i in\n",
            "    re.search(r'^(\\d+)\\.(\\d+)\\..+$', pd.__version__).groups()\n",
            "    ])\n",
            "if (major, minor) < (0, 14):\n",
            "    raise ImportError(\"pandas version too old (<0.14.0) to support gspread_dataframe\")\n",
            "logger.debug(\n",
            "    \"Imported satisfactory (>=0.14.0) Pandas module: %s\",\n",
            "    pd.__version__)\n",
            "from pandas.io.parsers import TextParser\n",
            "\n",
            "__all__ = ('set_with_dataframe', 'get_as_dataframe')\n",
            "\n",
            "def _cellrepr(value, allow_formulas):\n",
            "    \"\"\"\n",
            "    Get a string representation of dataframe value.\n",
            "\n",
            "    :param :value: the value to represent\n",
            "    :param :allow_formulas: if True, allow values starting with '='\n",
            "            to be interpreted as formulas; otherwise, escape\n",
            "            them with an apostrophe to avoid formula interpretation.\n",
            "    \"\"\"\n",
            "    if pd.isnull(value) is True:\n",
            "        return \"\"\n",
            "    if isinstance(value, float):\n",
            "        value = repr(value)\n",
            "    else:\n",
            "        value = str(value)\n",
            "    if value.startswith(\"'\") or ((not allow_formulas) and value.startswith('=')):\n",
            "        value = \"'%s\" % value\n",
            "    return value\n",
            "\n",
            "def _resize_to_minimum(worksheet, rows=None, cols=None):\n",
            "    \"\"\"\n",
            "    Resize the worksheet to guarantee a minimum size, either in rows,\n",
            "    or columns, or both.\n",
            "\n",
            "    Both rows and cols are optional.\n",
            "    \"\"\"\n",
            "    # get the current size\n",
            "    current_cols, current_rows = (\n",
            "        worksheet.col_count,\n",
            "        worksheet.row_count\n",
            "        )\n",
            "    if rows is not None and rows <= current_rows:\n",
            "        rows = None\n",
            "    if cols is not None and cols <= current_cols:\n",
            "        cols = None\n",
            "\n",
            "    if cols is not None or rows is not None:\n",
            "        worksheet.resize(rows, cols)\n",
            "\n",
            "def _get_all_values(worksheet, evaluate_formulas):\n",
            "    data = worksheet.spreadsheet.values_get(\n",
            "        worksheet.title, \n",
            "        params={\n",
            "            'valueRenderOption': ('UNFORMATTED_VALUE' if evaluate_formulas else 'FORMULA'),\n",
            "            'dateTimeRenderOption': 'FORMATTED_STRING'\n",
            "        }\n",
            "    )\n",
            "    (row_offset, column_offset) = (1, 1)\n",
            "    (last_row, last_column) = (worksheet.row_count, worksheet.col_count)\n",
            "    values = data.get('values', [])\n",
            "\n",
            "    rect_values = fill_gaps(\n",
            "        values,\n",
            "        rows=last_row - row_offset + 1,\n",
            "        cols=last_column - column_offset + 1\n",
            "    )\n",
            "\n",
            "    cells = [\n",
            "        Cell(row=i + row_offset, col=j + column_offset, value=value)\n",
            "        for i, row in enumerate(rect_values)\n",
            "        for j, value in enumerate(row)\n",
            "    ]\n",
            "\n",
            "    # defaultdicts fill in gaps for empty rows/cells not returned by gdocs\n",
            "    rows = defaultdict(lambda: defaultdict(str))\n",
            "    for cell in cells:\n",
            "        row = rows.setdefault(int(cell.row), defaultdict(str))\n",
            "        row[cell.col] = cell.value \n",
            "\n",
            "    if not rows:\n",
            "        return []\n",
            "\n",
            "    all_row_keys = chain.from_iterable(row.keys() for row in rows.values())\n",
            "    rect_cols = range(1, max(all_row_keys) + 1)\n",
            "    rect_rows = range(1, max(rows.keys()) + 1)\n",
            "\n",
            "    return [[rows[i][j] for j in rect_cols] for i in rect_rows]\n",
            "\n",
            "def get_as_dataframe(worksheet,\n",
            "                     evaluate_formulas=False,\n",
            "                     **options):\n",
            "    \"\"\"\n",
            "    Returns the worksheet contents as a DataFrame.\n",
            "\n",
            "    :param worksheet: the worksheet.\n",
            "    :param evaluate_formulas: if True, get the value of a cell after\n",
            "            formula evaluation; otherwise get the formula itself if present.\n",
            "            Defaults to False.\n",
            "    :param \\*\\*options: all the options for pandas.io.parsers.TextParser,\n",
            "            according to the version of pandas that is installed.\n",
            "            (Note: TextParser supports only the default 'python' parser engine,\n",
            "            not the C engine.)\n",
            "    :returns: pandas.DataFrame\n",
            "    \"\"\"\n",
            "    all_values = _get_all_values(worksheet, evaluate_formulas)\n",
            "    return TextParser(all_values, **options).read()\n",
            "\n",
            "def set_with_dataframe(worksheet,\n",
            "                       dataframe,\n",
            "                       row=1,\n",
            "                       col=1,\n",
            "                       include_index=False,\n",
            "                       include_column_header=True,\n",
            "                       resize=False,\n",
            "                       allow_formulas=True):\n",
            "    \"\"\"\n",
            "    Sets the values of a given DataFrame, anchoring its upper-left corner\n",
            "    at (row, col). (Default is row 1, column 1.)\n",
            "\n",
            "    :param worksheet: the gspread worksheet to set with content of DataFrame.\n",
            "    :param dataframe: the DataFrame.\n",
            "    :param include_index: if True, include the DataFrame's index as an\n",
            "            additional column. Defaults to False.\n",
            "    :param include_column_header: if True, add a header row before data with\n",
            "            column names. (If include_index is True, the index's name will be\n",
            "            used as its column's header.) Defaults to True.\n",
            "    :param resize: if True, changes the worksheet's size to match the shape\n",
            "            of the provided DataFrame. If False, worksheet will only be\n",
            "            resized as necessary to contain the DataFrame contents.\n",
            "            Defaults to False.\n",
            "    :param allow_formulas: if True, interprets `=foo` as a formula in\n",
            "            cell values; otherwise all text beginning with `=` is escaped\n",
            "            to avoid its interpretation as a formula. Defaults to True.\n",
            "    \"\"\"\n",
            "    # x_pos, y_pos refers to the position of data rows only,\n",
            "    # excluding any header rows in the google sheet.\n",
            "    # If header-related params are True, the values are adjusted\n",
            "    # to allow space for the headers.\n",
            "    y, x = dataframe.shape\n",
            "    if include_index:\n",
            "        x += 1\n",
            "    if include_column_header:\n",
            "        y += 1\n",
            "    if resize:\n",
            "        worksheet.resize(y, x)\n",
            "    else:\n",
            "        _resize_to_minimum(worksheet, y, x)\n",
            "\n",
            "    updates = []\n",
            "\n",
            "    if include_column_header:\n",
            "        elts = list(dataframe.columns)\n",
            "        if include_index:\n",
            "            elts = [ dataframe.index.name ] + elts\n",
            "        for idx, val in enumerate(elts):\n",
            "            updates.append(\n",
            "                (row,\n",
            "                 col+idx,\n",
            "                 _cellrepr(val, allow_formulas))\n",
            "            )\n",
            "        row += 1\n",
            "\n",
            "    values = []\n",
            "    for value_row, index_value in zip_longest(dataframe.values, dataframe.index):\n",
            "        if include_index:\n",
            "            value_row = [index_value] + list(value_row)\n",
            "        values.append(value_row)\n",
            "    for y_idx, value_row in enumerate(values):\n",
            "        for x_idx, cell_value in enumerate(value_row):\n",
            "            updates.append(\n",
            "                (y_idx+row,\n",
            "                 x_idx+col,\n",
            "                 _cellrepr(cell_value, allow_formulas))\n",
            "            )\n",
            "\n",
            "    if not updates:\n",
            "        logger.debug(\"No updates to perform on worksheet.\")\n",
            "        return\n",
            "\n",
            "    cells_to_update = [ Cell(row, col, value) for row, col, value in updates ]\n",
            "    logger.debug(\"%d cell updates to send\", len(cells_to_update))\n",
            "\n",
            "    resp = worksheet.update_cells(cells_to_update, value_input_option='USER_ENTERED')\n",
            "    logger.debug(\"Cell update response: %s\", resp)\n",
            "\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "import fnmatch\n",
            "import functools\n",
            "import io\n",
            "import ntpath\n",
            "import os\n",
            "import posixpath\n",
            "import re\n",
            "import sys\n",
            "import time\n",
            "from collections import Sequence\n",
            "from contextlib import contextmanager\n",
            "from errno import EINVAL, ENOENT\n",
            "from operator import attrgetter\n",
            "from stat import S_ISDIR, S_ISLNK, S_ISREG, S_ISSOCK, S_ISBLK, S_ISCHR, S_ISFIFO\n",
            "try:\n",
            "    from urllib import quote as urlquote, quote as urlquote_from_bytes\n",
            "except ImportError:\n",
            "    from urllib.parse import quote as urlquote, quote_from_bytes as urlquote_from_bytes\n",
            "\n",
            "\n",
            "try:\n",
            "    intern = intern\n",
            "except NameError:\n",
            "    intern = sys.intern\n",
            "try:\n",
            "    basestring = basestring\n",
            "except NameError:\n",
            "    basestring = str\n",
            "\n",
            "supports_symlinks = True\n",
            "try:\n",
            "    import nt\n",
            "except ImportError:\n",
            "    nt = None\n",
            "else:\n",
            "    if sys.getwindowsversion()[:2] >= (6, 0) and sys.version_info >= (3, 2):\n",
            "        from nt import _getfinalpathname\n",
            "    else:\n",
            "        supports_symlinks = False\n",
            "        _getfinalpathname = None\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    \"PurePath\", \"PurePosixPath\", \"PureWindowsPath\",\n",
            "    \"Path\", \"PosixPath\", \"WindowsPath\",\n",
            "    ]\n",
            "\n",
            "#\n",
            "# Internals\n",
            "#\n",
            "\n",
            "_py2 = sys.version_info < (3,)\n",
            "_py2_fs_encoding = 'ascii'\n",
            "\n",
            "def _py2_fsencode(parts):\n",
            "    # py2 => minimal unicode support\n",
            "    return [part.encode(_py2_fs_encoding) if isinstance(part, unicode)\n",
            "            else part for part in parts]\n",
            "\n",
            "def _is_wildcard_pattern(pat):\n",
            "    # Whether this pattern needs actual matching using fnmatch, or can\n",
            "    # be looked up directly as a file.\n",
            "    return \"*\" in pat or \"?\" in pat or \"[\" in pat\n",
            "\n",
            "\n",
            "class _Flavour(object):\n",
            "    \"\"\"A flavour implements a particular (platform-specific) set of path\n",
            "    semantics.\"\"\"\n",
            "\n",
            "    def __init__(self):\n",
            "        self.join = self.sep.join\n",
            "\n",
            "    def parse_parts(self, parts):\n",
            "        if _py2:\n",
            "            parts = _py2_fsencode(parts)\n",
            "        parsed = []\n",
            "        sep = self.sep\n",
            "        altsep = self.altsep\n",
            "        drv = root = ''\n",
            "        it = reversed(parts)\n",
            "        for part in it:\n",
            "            if not part:\n",
            "                continue\n",
            "            if altsep:\n",
            "                part = part.replace(altsep, sep)\n",
            "            drv, root, rel = self.splitroot(part)\n",
            "            if sep in rel:\n",
            "                for x in reversed(rel.split(sep)):\n",
            "                    if x and x != '.':\n",
            "                        parsed.append(intern(x))\n",
            "            else:\n",
            "                if rel and rel != '.':\n",
            "                    parsed.append(intern(rel))\n",
            "            if drv or root:\n",
            "                if not drv:\n",
            "                    # If no drive is present, try to find one in the previous\n",
            "                    # parts. This makes the result of parsing e.g.\n",
            "                    # (\"C:\", \"/\", \"a\") reasonably intuitive.\n",
            "                    for part in it:\n",
            "                        drv = self.splitroot(part)[0]\n",
            "                        if drv:\n",
            "                            break\n",
            "                break\n",
            "        if drv or root:\n",
            "            parsed.append(drv + root)\n",
            "        parsed.reverse()\n",
            "        return drv, root, parsed\n",
            "\n",
            "    def join_parsed_parts(self, drv, root, parts, drv2, root2, parts2):\n",
            "        \"\"\"\n",
            "        Join the two paths represented by the respective\n",
            "        (drive, root, parts) tuples.  Return a new (drive, root, parts) tuple.\n",
            "        \"\"\"\n",
            "        if root2:\n",
            "            if not drv2 and drv:\n",
            "                return drv, root2, [drv + root2] + parts2[1:]\n",
            "        elif drv2:\n",
            "            if drv2 == drv or self.casefold(drv2) == self.casefold(drv):\n",
            "                # Same drive => second path is relative to the first\n",
            "                return drv, root, parts + parts2[1:]\n",
            "        else:\n",
            "            # Second path is non-anchored (common case)\n",
            "            return drv, root, parts + parts2\n",
            "        return drv2, root2, parts2\n",
            "\n",
            "\n",
            "class _WindowsFlavour(_Flavour):\n",
            "    # Reference for Windows paths can be found at\n",
            "    # http://msdn.microsoft.com/en-us/library/aa365247%28v=vs.85%29.aspx\n",
            "\n",
            "    sep = '\\\\'\n",
            "    altsep = '/'\n",
            "    has_drv = True\n",
            "    pathmod = ntpath\n",
            "\n",
            "    is_supported = (nt is not None)\n",
            "\n",
            "    drive_letters = (\n",
            "        set(chr(x) for x in range(ord('a'), ord('z') + 1)) |\n",
            "        set(chr(x) for x in range(ord('A'), ord('Z') + 1))\n",
            "    )\n",
            "    ext_namespace_prefix = '\\\\\\\\?\\\\'\n",
            "\n",
            "    reserved_names = (\n",
            "        set(['CON', 'PRN', 'AUX', 'NUL']) |\n",
            "        set(['COM%d' % i for i in range(1, 10)]) |\n",
            "        set(['LPT%d' % i for i in range(1, 10)])\n",
            "        )\n",
            "\n",
            "    # Interesting findings about extended paths:\n",
            "    # - '\\\\?\\c:\\a', '//?/c:\\a' and '//?/c:/a' are all supported\n",
            "    #   but '\\\\?\\c:/a' is not\n",
            "    # - extended paths are always absolute; \"relative\" extended paths will\n",
            "    #   fail.\n",
            "\n",
            "    def splitroot(self, part, sep=sep):\n",
            "        first = part[0:1]\n",
            "        second = part[1:2]\n",
            "        if (second == sep and first == sep):\n",
            "            # XXX extended paths should also disable the collapsing of \".\"\n",
            "            # components (according to MSDN docs).\n",
            "            prefix, part = self._split_extended_path(part)\n",
            "            first = part[0:1]\n",
            "            second = part[1:2]\n",
            "        else:\n",
            "            prefix = ''\n",
            "        third = part[2:3]\n",
            "        if (second == sep and first == sep and third != sep):\n",
            "            # is a UNC path:\n",
            "            # vvvvvvvvvvvvvvvvvvvvv root\n",
            "            # \\\\machine\\mountpoint\\directory\\etc\\...\n",
            "            #            directory ^^^^^^^^^^^^^^\n",
            "            index = part.find(sep, 2)\n",
            "            if index != -1:\n",
            "                index2 = part.find(sep, index + 1)\n",
            "                # a UNC path can't have two slashes in a row\n",
            "                # (after the initial two)\n",
            "                if index2 != index + 1:\n",
            "                    if index2 == -1:\n",
            "                        index2 = len(part)\n",
            "                    if prefix:\n",
            "                        return prefix + part[1:index2], sep, part[index2+1:]\n",
            "                    else:\n",
            "                        return part[:index2], sep, part[index2+1:]\n",
            "        drv = root = ''\n",
            "        if second == ':' and first in self.drive_letters:\n",
            "            drv = part[:2]\n",
            "            part = part[2:]\n",
            "            first = third\n",
            "        if first == sep:\n",
            "            root = first\n",
            "            part = part.lstrip(sep)\n",
            "        return prefix + drv, root, part\n",
            "\n",
            "    def casefold(self, s):\n",
            "        return s.lower()\n",
            "\n",
            "    def casefold_parts(self, parts):\n",
            "        return [p.lower() for p in parts]\n",
            "\n",
            "    def resolve(self, path):\n",
            "        s = str(path)\n",
            "        if not s:\n",
            "            return os.getcwd()\n",
            "        if _getfinalpathname is not None:\n",
            "            return self._ext_to_normal(_getfinalpathname(s))\n",
            "        # Means fallback on absolute\n",
            "        return None\n",
            "\n",
            "    def _split_extended_path(self, s, ext_prefix=ext_namespace_prefix):\n",
            "        prefix = ''\n",
            "        if s.startswith(ext_prefix):\n",
            "            prefix = s[:4]\n",
            "            s = s[4:]\n",
            "            if s.startswith('UNC\\\\'):\n",
            "                prefix += s[:3]\n",
            "                s = '\\\\' + s[3:]\n",
            "        return prefix, s\n",
            "\n",
            "    def _ext_to_normal(self, s):\n",
            "        # Turn back an extended path into a normal DOS-like path\n",
            "        return self._split_extended_path(s)[1]\n",
            "\n",
            "    def is_reserved(self, parts):\n",
            "        # NOTE: the rules for reserved names seem somewhat complicated\n",
            "        # (e.g. r\"..\\NUL\" is reserved but not r\"foo\\NUL\").\n",
            "        # We err on the side of caution and return True for paths which are\n",
            "        # not considered reserved by Windows.\n",
            "        if not parts:\n",
            "            return False\n",
            "        if parts[0].startswith('\\\\\\\\'):\n",
            "            # UNC paths are never reserved\n",
            "            return False\n",
            "        return parts[-1].partition('.')[0].upper() in self.reserved_names\n",
            "\n",
            "    def make_uri(self, path):\n",
            "        # Under Windows, file URIs use the UTF-8 encoding.\n",
            "        drive = path.drive\n",
            "        if len(drive) == 2 and drive[1] == ':':\n",
            "            # It's a path on a local drive => 'file:///c:/a/b'\n",
            "            rest = path.as_posix()[2:].lstrip('/')\n",
            "            return 'file:///%s/%s' % (\n",
            "                drive, urlquote_from_bytes(rest.encode('utf-8')))\n",
            "        else:\n",
            "            # It's a path on a network drive => 'file://host/share/a/b'\n",
            "            return 'file:' + urlquote_from_bytes(path.as_posix().encode('utf-8'))\n",
            "\n",
            "\n",
            "class _PosixFlavour(_Flavour):\n",
            "    sep = '/'\n",
            "    altsep = ''\n",
            "    has_drv = False\n",
            "    pathmod = posixpath\n",
            "\n",
            "    is_supported = (os.name != 'nt')\n",
            "\n",
            "    def splitroot(self, part, sep=sep):\n",
            "        if part and part[0] == sep:\n",
            "            stripped_part = part.lstrip(sep)\n",
            "            # According to POSIX path resolution:\n",
            "            # http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap04.html#tag_04_11\n",
            "            # \"A pathname that begins with two successive slashes may be\n",
            "            # interpreted in an implementation-defined manner, although more\n",
            "            # than two leading slashes shall be treated as a single slash\".\n",
            "            if len(part) - len(stripped_part) == 2:\n",
            "                return '', sep * 2, stripped_part\n",
            "            else:\n",
            "                return '', sep, stripped_part\n",
            "        else:\n",
            "            return '', '', part\n",
            "\n",
            "    def casefold(self, s):\n",
            "        return s\n",
            "\n",
            "    def casefold_parts(self, parts):\n",
            "        return parts\n",
            "\n",
            "    def resolve(self, path):\n",
            "        sep = self.sep\n",
            "        accessor = path._accessor\n",
            "        seen = {}\n",
            "        def _resolve(path, rest):\n",
            "            if rest.startswith(sep):\n",
            "                path = ''\n",
            "\n",
            "            for name in rest.split(sep):\n",
            "                if not name or name == '.':\n",
            "                    # current dir\n",
            "                    continue\n",
            "                if name == '..':\n",
            "                    # parent dir\n",
            "                    path, _, _ = path.rpartition(sep)\n",
            "                    continue\n",
            "                newpath = path + sep + name\n",
            "                if newpath in seen:\n",
            "                    # Already seen this path\n",
            "                    path = seen[newpath]\n",
            "                    if path is not None:\n",
            "                        # use cached value\n",
            "                        continue\n",
            "                    # The symlink is not resolved, so we must have a symlink loop.\n",
            "                    raise RuntimeError(\"Symlink loop from %r\" % newpath)\n",
            "                # Resolve the symbolic link\n",
            "                try:\n",
            "                    target = accessor.readlink(newpath)\n",
            "                except OSError as e:\n",
            "                    if e.errno != EINVAL:\n",
            "                        raise\n",
            "                    # Not a symlink\n",
            "                    path = newpath\n",
            "                else:\n",
            "                    seen[newpath] = None # not resolved symlink\n",
            "                    path = _resolve(path, target)\n",
            "                    seen[newpath] = path # resolved symlink\n",
            "\n",
            "            return path\n",
            "        # NOTE: according to POSIX, getcwd() cannot contain path components\n",
            "        # which are symlinks.\n",
            "        base = '' if path.is_absolute() else os.getcwd()\n",
            "        return _resolve(base, str(path)) or sep\n",
            "\n",
            "    def is_reserved(self, parts):\n",
            "        return False\n",
            "\n",
            "    def make_uri(self, path):\n",
            "        # We represent the path using the local filesystem encoding,\n",
            "        # for portability to other applications.\n",
            "        bpath = bytes(path)\n",
            "        return 'file://' + urlquote_from_bytes(bpath)\n",
            "\n",
            "\n",
            "_windows_flavour = _WindowsFlavour()\n",
            "_posix_flavour = _PosixFlavour()\n",
            "\n",
            "\n",
            "class _Accessor:\n",
            "    \"\"\"An accessor implements a particular (system-specific or not) way of\n",
            "    accessing paths on the filesystem.\"\"\"\n",
            "\n",
            "\n",
            "class _NormalAccessor(_Accessor):\n",
            "\n",
            "    def _wrap_strfunc(strfunc):\n",
            "        @functools.wraps(strfunc)\n",
            "        def wrapped(pathobj, *args):\n",
            "            return strfunc(str(pathobj), *args)\n",
            "        return staticmethod(wrapped)\n",
            "\n",
            "    def _wrap_binary_strfunc(strfunc):\n",
            "        @functools.wraps(strfunc)\n",
            "        def wrapped(pathobjA, pathobjB, *args):\n",
            "            return strfunc(str(pathobjA), str(pathobjB), *args)\n",
            "        return staticmethod(wrapped)\n",
            "\n",
            "    stat = _wrap_strfunc(os.stat)\n",
            "\n",
            "    lstat = _wrap_strfunc(os.lstat)\n",
            "\n",
            "    open = _wrap_strfunc(os.open)\n",
            "\n",
            "    listdir = _wrap_strfunc(os.listdir)\n",
            "\n",
            "    chmod = _wrap_strfunc(os.chmod)\n",
            "\n",
            "    if hasattr(os, \"lchmod\"):\n",
            "        lchmod = _wrap_strfunc(os.lchmod)\n",
            "    else:\n",
            "        def lchmod(self, pathobj, mode):\n",
            "            raise NotImplementedError(\"lchmod() not available on this system\")\n",
            "\n",
            "    mkdir = _wrap_strfunc(os.mkdir)\n",
            "\n",
            "    unlink = _wrap_strfunc(os.unlink)\n",
            "\n",
            "    rmdir = _wrap_strfunc(os.rmdir)\n",
            "\n",
            "    rename = _wrap_binary_strfunc(os.rename)\n",
            "\n",
            "    if sys.version_info >= (3, 3):\n",
            "        replace = _wrap_binary_strfunc(os.replace)\n",
            "\n",
            "    if nt:\n",
            "        if supports_symlinks:\n",
            "            symlink = _wrap_binary_strfunc(os.symlink)\n",
            "        else:\n",
            "            def symlink(a, b, target_is_directory):\n",
            "                raise NotImplementedError(\"symlink() not available on this system\")\n",
            "    else:\n",
            "        # Under POSIX, os.symlink() takes two args\n",
            "        @staticmethod\n",
            "        def symlink(a, b, target_is_directory):\n",
            "            return os.symlink(str(a), str(b))\n",
            "\n",
            "    utime = _wrap_strfunc(os.utime)\n",
            "\n",
            "    # Helper for resolve()\n",
            "    def readlink(self, path):\n",
            "        return os.readlink(path)\n",
            "\n",
            "\n",
            "_normal_accessor = _NormalAccessor()\n",
            "\n",
            "\n",
            "#\n",
            "# Globbing helpers\n",
            "#\n",
            "\n",
            "@contextmanager\n",
            "def _cached(func):\n",
            "    try:\n",
            "        func.__cached__\n",
            "        yield func\n",
            "    except AttributeError:\n",
            "        cache = {}\n",
            "        def wrapper(*args):\n",
            "            try:\n",
            "                return cache[args]\n",
            "            except KeyError:\n",
            "                value = cache[args] = func(*args)\n",
            "                return value\n",
            "        wrapper.__cached__ = True\n",
            "        try:\n",
            "            yield wrapper\n",
            "        finally:\n",
            "            cache.clear()\n",
            "\n",
            "def _make_selector(pattern_parts):\n",
            "    pat = pattern_parts[0]\n",
            "    child_parts = pattern_parts[1:]\n",
            "    if pat == '**':\n",
            "        cls = _RecursiveWildcardSelector\n",
            "    elif '**' in pat:\n",
            "        raise ValueError(\"Invalid pattern: '**' can only be an entire path component\")\n",
            "    elif _is_wildcard_pattern(pat):\n",
            "        cls = _WildcardSelector\n",
            "    else:\n",
            "        cls = _PreciseSelector\n",
            "    return cls(pat, child_parts)\n",
            "\n",
            "if hasattr(functools, \"lru_cache\"):\n",
            "    _make_selector = functools.lru_cache()(_make_selector)\n",
            "\n",
            "\n",
            "class _Selector:\n",
            "    \"\"\"A selector matches a specific glob pattern part against the children\n",
            "    of a given path.\"\"\"\n",
            "\n",
            "    def __init__(self, child_parts):\n",
            "        self.child_parts = child_parts\n",
            "        if child_parts:\n",
            "            self.successor = _make_selector(child_parts)\n",
            "        else:\n",
            "            self.successor = _TerminatingSelector()\n",
            "\n",
            "    def select_from(self, parent_path):\n",
            "        \"\"\"Iterate over all child paths of `parent_path` matched by this\n",
            "        selector.  This can contain parent_path itself.\"\"\"\n",
            "        path_cls = type(parent_path)\n",
            "        is_dir = path_cls.is_dir\n",
            "        exists = path_cls.exists\n",
            "        listdir = parent_path._accessor.listdir\n",
            "        return self._select_from(parent_path, is_dir, exists, listdir)\n",
            "\n",
            "\n",
            "class _TerminatingSelector:\n",
            "\n",
            "    def _select_from(self, parent_path, is_dir, exists, listdir):\n",
            "        yield parent_path\n",
            "\n",
            "\n",
            "class _PreciseSelector(_Selector):\n",
            "\n",
            "    def __init__(self, name, child_parts):\n",
            "        self.name = name\n",
            "        _Selector.__init__(self, child_parts)\n",
            "\n",
            "    def _select_from(self, parent_path, is_dir, exists, listdir):\n",
            "        if not is_dir(parent_path):\n",
            "            return\n",
            "        path = parent_path._make_child_relpath(self.name)\n",
            "        if exists(path):\n",
            "            for p in self.successor._select_from(path, is_dir, exists, listdir):\n",
            "                yield p\n",
            "\n",
            "\n",
            "class _WildcardSelector(_Selector):\n",
            "\n",
            "    def __init__(self, pat, child_parts):\n",
            "        self.pat = re.compile(fnmatch.translate(pat))\n",
            "        _Selector.__init__(self, child_parts)\n",
            "\n",
            "    def _select_from(self, parent_path, is_dir, exists, listdir):\n",
            "        if not is_dir(parent_path):\n",
            "            return\n",
            "        cf = parent_path._flavour.casefold\n",
            "        for name in listdir(parent_path):\n",
            "            casefolded = cf(name)\n",
            "            if self.pat.match(casefolded):\n",
            "                path = parent_path._make_child_relpath(name)\n",
            "                for p in self.successor._select_from(path, is_dir, exists, listdir):\n",
            "                    yield p\n",
            "\n",
            "\n",
            "class _RecursiveWildcardSelector(_Selector):\n",
            "\n",
            "    def __init__(self, pat, child_parts):\n",
            "        _Selector.__init__(self, child_parts)\n",
            "\n",
            "    def _iterate_directories(self, parent_path, is_dir, listdir):\n",
            "        yield parent_path\n",
            "        for name in listdir(parent_path):\n",
            "            path = parent_path._make_child_relpath(name)\n",
            "            if is_dir(path):\n",
            "                for p in self._iterate_directories(path, is_dir, listdir):\n",
            "                    yield p\n",
            "\n",
            "    def _select_from(self, parent_path, is_dir, exists, listdir):\n",
            "        if not is_dir(parent_path):\n",
            "            return\n",
            "        with _cached(listdir) as listdir:\n",
            "            yielded = set()\n",
            "            try:\n",
            "                successor_select = self.successor._select_from\n",
            "                for starting_point in self._iterate_directories(parent_path, is_dir, listdir):\n",
            "                    for p in successor_select(starting_point, is_dir, exists, listdir):\n",
            "                        if p not in yielded:\n",
            "                            yield p\n",
            "                            yielded.add(p)\n",
            "            finally:\n",
            "                yielded.clear()\n",
            "\n",
            "\n",
            "#\n",
            "# Public API\n",
            "#\n",
            "\n",
            "class _PathParents(Sequence):\n",
            "    \"\"\"This object provides sequence-like access to the logical ancestors\n",
            "    of a path.  Don't try to construct it yourself.\"\"\"\n",
            "    __slots__ = ('_pathcls', '_drv', '_root', '_parts')\n",
            "\n",
            "    def __init__(self, path):\n",
            "        # We don't store the instance to avoid reference cycles\n",
            "        self._pathcls = type(path)\n",
            "        self._drv = path._drv\n",
            "        self._root = path._root\n",
            "        self._parts = path._parts\n",
            "\n",
            "    def __len__(self):\n",
            "        if self._drv or self._root:\n",
            "            return len(self._parts) - 1\n",
            "        else:\n",
            "            return len(self._parts)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        if idx < 0 or idx >= len(self):\n",
            "            raise IndexError(idx)\n",
            "        return self._pathcls._from_parsed_parts(self._drv, self._root,\n",
            "                                                self._parts[:-idx - 1])\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"<{0}.parents>\".format(self._pathcls.__name__)\n",
            "\n",
            "\n",
            "class PurePath(object):\n",
            "    \"\"\"PurePath represents a filesystem path and offers operations which\n",
            "    don't imply any actual filesystem I/O.  Depending on your system,\n",
            "    instantiating a PurePath will return either a PurePosixPath or a\n",
            "    PureWindowsPath object.  You can also instantiate either of these classes\n",
            "    directly, regardless of your system.\n",
            "    \"\"\"\n",
            "    __slots__ = (\n",
            "        '_drv', '_root', '_parts',\n",
            "        '_str', '_hash', '_pparts', '_cached_cparts',\n",
            "    )\n",
            "\n",
            "    def __new__(cls, *args):\n",
            "        \"\"\"Construct a PurePath from one or several strings and or existing\n",
            "        PurePath objects.  The strings and path objects are combined so as\n",
            "        to yield a canonicalized path, which is incorporated into the\n",
            "        new PurePath object.\n",
            "        \"\"\"\n",
            "        if cls is PurePath:\n",
            "            cls = PureWindowsPath if os.name == 'nt' else PurePosixPath\n",
            "        return cls._from_parts(args)\n",
            "\n",
            "    def __reduce__(self):\n",
            "        # Using the parts tuple helps share interned path parts\n",
            "        # when pickling related paths.\n",
            "        return (self.__class__, tuple(self._parts))\n",
            "\n",
            "    @classmethod\n",
            "    def _parse_args(cls, args):\n",
            "        # This is useful when you don't want to create an instance, just\n",
            "        # canonicalize some constructor arguments.\n",
            "        parts = []\n",
            "        for a in args:\n",
            "            if isinstance(a, PurePath):\n",
            "                parts += a._parts\n",
            "            elif isinstance(a, basestring):\n",
            "                parts.append(a)\n",
            "            else:\n",
            "                raise TypeError(\n",
            "                    \"argument should be a path or str object, not %r\"\n",
            "                    % type(a))\n",
            "        return cls._flavour.parse_parts(parts)\n",
            "\n",
            "    @classmethod\n",
            "    def _from_parts(cls, args, init=True):\n",
            "        # We need to call _parse_args on the instance, so as to get the\n",
            "        # right flavour.\n",
            "        self = object.__new__(cls)\n",
            "        drv, root, parts = self._parse_args(args)\n",
            "        self._drv = drv\n",
            "        self._root = root\n",
            "        self._parts = parts\n",
            "        if init:\n",
            "            self._init()\n",
            "        return self\n",
            "\n",
            "    @classmethod\n",
            "    def _from_parsed_parts(cls, drv, root, parts, init=True):\n",
            "        self = object.__new__(cls)\n",
            "        self._drv = drv\n",
            "        self._root = root\n",
            "        self._parts = parts\n",
            "        if init:\n",
            "            self._init()\n",
            "        return self\n",
            "\n",
            "    @classmethod\n",
            "    def _format_parsed_parts(cls, drv, root, parts):\n",
            "        if drv or root:\n",
            "            return drv + root + cls._flavour.join(parts[1:])\n",
            "        else:\n",
            "            return cls._flavour.join(parts)\n",
            "\n",
            "    def _init(self):\n",
            "        # Overriden in concrete Path\n",
            "        pass\n",
            "\n",
            "    def _make_child(self, args):\n",
            "        drv, root, parts = self._parse_args(args)\n",
            "        drv, root, parts = self._flavour.join_parsed_parts(\n",
            "            self._drv, self._root, self._parts, drv, root, parts)\n",
            "        return self._from_parsed_parts(drv, root, parts)\n",
            "\n",
            "    def __str__(self):\n",
            "        \"\"\"Return the string representation of the path, suitable for\n",
            "        passing to system calls.\"\"\"\n",
            "        try:\n",
            "            return self._str\n",
            "        except AttributeError:\n",
            "            self._str = self._format_parsed_parts(self._drv, self._root,\n",
            "                                                  self._parts) or '.'\n",
            "            return self._str\n",
            "\n",
            "    def as_posix(self):\n",
            "        \"\"\"Return the string representation of the path with forward (/)\n",
            "        slashes.\"\"\"\n",
            "        f = self._flavour\n",
            "        return str(self).replace(f.sep, '/')\n",
            "\n",
            "    def __bytes__(self):\n",
            "        \"\"\"Return the bytes representation of the path.  This is only\n",
            "        recommended to use under Unix.\"\"\"\n",
            "        if sys.version_info < (3, 2):\n",
            "            raise NotImplementedError(\"needs Python 3.2 or later\")\n",
            "        return os.fsencode(str(self))\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"{0}({1!r})\".format(self.__class__.__name__, self.as_posix())\n",
            "\n",
            "    def as_uri(self):\n",
            "        \"\"\"Return the path as a 'file' URI.\"\"\"\n",
            "        if not self.is_absolute():\n",
            "            raise ValueError(\"relative path can't be expressed as a file URI\")\n",
            "        return self._flavour.make_uri(self)\n",
            "\n",
            "    @property\n",
            "    def _cparts(self):\n",
            "        # Cached casefolded parts, for hashing and comparison\n",
            "        try:\n",
            "            return self._cached_cparts\n",
            "        except AttributeError:\n",
            "            self._cached_cparts = self._flavour.casefold_parts(self._parts)\n",
            "            return self._cached_cparts\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if not isinstance(other, PurePath):\n",
            "            return NotImplemented\n",
            "        return self._cparts == other._cparts and self._flavour is other._flavour\n",
            "\n",
            "    def __ne__(self, other):\n",
            "        return not self == other\n",
            "\n",
            "    def __hash__(self):\n",
            "        try:\n",
            "            return self._hash\n",
            "        except AttributeError:\n",
            "            self._hash = hash(tuple(self._cparts))\n",
            "            return self._hash\n",
            "\n",
            "    def __lt__(self, other):\n",
            "        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n",
            "            return NotImplemented\n",
            "        return self._cparts < other._cparts\n",
            "\n",
            "    def __le__(self, other):\n",
            "        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n",
            "            return NotImplemented\n",
            "        return self._cparts <= other._cparts\n",
            "\n",
            "    def __gt__(self, other):\n",
            "        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n",
            "            return NotImplemented\n",
            "        return self._cparts > other._cparts\n",
            "\n",
            "    def __ge__(self, other):\n",
            "        if not isinstance(other, PurePath) or self._flavour is not other._flavour:\n",
            "            return NotImplemented\n",
            "        return self._cparts >= other._cparts\n",
            "\n",
            "    drive = property(attrgetter('_drv'),\n",
            "                     doc=\"\"\"The drive prefix (letter or UNC path), if any.\"\"\")\n",
            "\n",
            "    root = property(attrgetter('_root'),\n",
            "                    doc=\"\"\"The root of the path, if any.\"\"\")\n",
            "\n",
            "    @property\n",
            "    def anchor(self):\n",
            "        \"\"\"The concatenation of the drive and root, or ''.\"\"\"\n",
            "        anchor = self._drv + self._root\n",
            "        return anchor\n",
            "\n",
            "    @property\n",
            "    def name(self):\n",
            "        \"\"\"The final path component, if any.\"\"\"\n",
            "        parts = self._parts\n",
            "        if len(parts) == (1 if (self._drv or self._root) else 0):\n",
            "            return ''\n",
            "        return parts[-1]\n",
            "\n",
            "    @property\n",
            "    def suffix(self):\n",
            "        \"\"\"The final component's last suffix, if any.\"\"\"\n",
            "        name = self.name\n",
            "        i = name.rfind('.')\n",
            "        if 0 < i < len(name) - 1:\n",
            "            return name[i:]\n",
            "        else:\n",
            "            return ''\n",
            "\n",
            "    @property\n",
            "    def suffixes(self):\n",
            "        \"\"\"A list of the final component's suffixes, if any.\"\"\"\n",
            "        name = self.name\n",
            "        if name.endswith('.'):\n",
            "            return []\n",
            "        name = name.lstrip('.')\n",
            "        return ['.' + suffix for suffix in name.split('.')[1:]]\n",
            "\n",
            "    @property\n",
            "    def stem(self):\n",
            "        \"\"\"The final path component, minus its last suffix.\"\"\"\n",
            "        name = self.name\n",
            "        i = name.rfind('.')\n",
            "        if 0 < i < len(name) - 1:\n",
            "            return name[:i]\n",
            "        else:\n",
            "            return name\n",
            "\n",
            "    def with_name(self, name):\n",
            "        \"\"\"Return a new path with the file name changed.\"\"\"\n",
            "        if not self.name:\n",
            "            raise ValueError(\"%r has an empty name\" % (self,))\n",
            "        return self._from_parsed_parts(self._drv, self._root,\n",
            "                                       self._parts[:-1] + [name])\n",
            "\n",
            "    def with_suffix(self, suffix):\n",
            "        \"\"\"Return a new path with the file suffix changed (or added, if none).\"\"\"\n",
            "        # XXX if suffix is None, should the current suffix be removed?\n",
            "        drv, root, parts = self._flavour.parse_parts((suffix,))\n",
            "        if drv or root or len(parts) != 1:\n",
            "            raise ValueError(\"Invalid suffix %r\" % (suffix))\n",
            "        suffix = parts[0]\n",
            "        if not suffix.startswith('.'):\n",
            "            raise ValueError(\"Invalid suffix %r\" % (suffix))\n",
            "        name = self.name\n",
            "        if not name:\n",
            "            raise ValueError(\"%r has an empty name\" % (self,))\n",
            "        old_suffix = self.suffix\n",
            "        if not old_suffix:\n",
            "            name = name + suffix\n",
            "        else:\n",
            "            name = name[:-len(old_suffix)] + suffix\n",
            "        return self._from_parsed_parts(self._drv, self._root,\n",
            "                                       self._parts[:-1] + [name])\n",
            "\n",
            "    def relative_to(self, *other):\n",
            "        \"\"\"Return the relative path to another path identified by the passed\n",
            "        arguments.  If the operation is not possible (because this is not\n",
            "        a subpath of the other path), raise ValueError.\n",
            "        \"\"\"\n",
            "        # For the purpose of this method, drive and root are considered\n",
            "        # separate parts, i.e.:\n",
            "        #   Path('c:/').relative_to('c:')  gives Path('/')\n",
            "        #   Path('c:/').relative_to('/')   raise ValueError\n",
            "        if not other:\n",
            "            raise TypeError(\"need at least one argument\")\n",
            "        parts = self._parts\n",
            "        drv = self._drv\n",
            "        root = self._root\n",
            "        if root:\n",
            "            abs_parts = [drv, root] + parts[1:]\n",
            "        else:\n",
            "            abs_parts = parts\n",
            "        to_drv, to_root, to_parts = self._parse_args(other)\n",
            "        if to_root:\n",
            "            to_abs_parts = [to_drv, to_root] + to_parts[1:]\n",
            "        else:\n",
            "            to_abs_parts = to_parts\n",
            "        n = len(to_abs_parts)\n",
            "        cf = self._flavour.casefold_parts\n",
            "        if (root or drv) if n == 0 else cf(abs_parts[:n]) != cf(to_abs_parts):\n",
            "            formatted = self._format_parsed_parts(to_drv, to_root, to_parts)\n",
            "            raise ValueError(\"{!r} does not start with {!r}\"\n",
            "                             .format(str(self), str(formatted)))\n",
            "        return self._from_parsed_parts('', root if n == 1 else '',\n",
            "                                       abs_parts[n:])\n",
            "\n",
            "    @property\n",
            "    def parts(self):\n",
            "        \"\"\"An object providing sequence-like access to the\n",
            "        components in the filesystem path.\"\"\"\n",
            "        # We cache the tuple to avoid building a new one each time .parts\n",
            "        # is accessed.  XXX is this necessary?\n",
            "        try:\n",
            "            return self._pparts\n",
            "        except AttributeError:\n",
            "            self._pparts = tuple(self._parts)\n",
            "            return self._pparts\n",
            "\n",
            "    def joinpath(self, *args):\n",
            "        \"\"\"Combine this path with one or several arguments, and return a\n",
            "        new path representing either a subpath (if all arguments are relative\n",
            "        paths) or a totally different path (if one of the arguments is\n",
            "        anchored).\n",
            "        \"\"\"\n",
            "        return self._make_child(args)\n",
            "\n",
            "    def __truediv__(self, key):\n",
            "        return self._make_child((key,))\n",
            "\n",
            "    def __rtruediv__(self, key):\n",
            "        return self._from_parts([key] + self._parts)\n",
            "\n",
            "    if sys.version_info < (3,):\n",
            "        __div__ = __truediv__\n",
            "        __rdiv__ = __rtruediv__\n",
            "\n",
            "    @property\n",
            "    def parent(self):\n",
            "        \"\"\"The logical parent of the path.\"\"\"\n",
            "        drv = self._drv\n",
            "        root = self._root\n",
            "        parts = self._parts\n",
            "        if len(parts) == 1 and (drv or root):\n",
            "            return self\n",
            "        return self._from_parsed_parts(drv, root, parts[:-1])\n",
            "\n",
            "    @property\n",
            "    def parents(self):\n",
            "        \"\"\"A sequence of this path's logical parents.\"\"\"\n",
            "        return _PathParents(self)\n",
            "\n",
            "    def is_absolute(self):\n",
            "        \"\"\"True if the path is absolute (has both a root and, if applicable,\n",
            "        a drive).\"\"\"\n",
            "        if not self._root:\n",
            "            return False\n",
            "        return not self._flavour.has_drv or bool(self._drv)\n",
            "\n",
            "    def is_reserved(self):\n",
            "        \"\"\"Return True if the path contains one of the special names reserved\n",
            "        by the system, if any.\"\"\"\n",
            "        return self._flavour.is_reserved(self._parts)\n",
            "\n",
            "    def match(self, path_pattern):\n",
            "        \"\"\"\n",
            "        Return True if this path matches the given pattern.\n",
            "        \"\"\"\n",
            "        cf = self._flavour.casefold\n",
            "        path_pattern = cf(path_pattern)\n",
            "        drv, root, pat_parts = self._flavour.parse_parts((path_pattern,))\n",
            "        if not pat_parts:\n",
            "            raise ValueError(\"empty pattern\")\n",
            "        if drv and drv != cf(self._drv):\n",
            "            return False\n",
            "        if root and root != cf(self._root):\n",
            "            return False\n",
            "        parts = self._cparts\n",
            "        if drv or root:\n",
            "            if len(pat_parts) != len(parts):\n",
            "                return False\n",
            "            pat_parts = pat_parts[1:]\n",
            "        elif len(pat_parts) > len(parts):\n",
            "            return False\n",
            "        for part, pat in zip(reversed(parts), reversed(pat_parts)):\n",
            "            if not fnmatch.fnmatchcase(part, pat):\n",
            "                return False\n",
            "        return True\n",
            "\n",
            "\n",
            "class PurePosixPath(PurePath):\n",
            "    _flavour = _posix_flavour\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class PureWindowsPath(PurePath):\n",
            "    _flavour = _windows_flavour\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "# Filesystem-accessing classes\n",
            "\n",
            "\n",
            "class Path(PurePath):\n",
            "    __slots__ = (\n",
            "        '_accessor',\n",
            "    )\n",
            "\n",
            "    def __new__(cls, *args, **kwargs):\n",
            "        if cls is Path:\n",
            "            cls = WindowsPath if os.name == 'nt' else PosixPath\n",
            "        self = cls._from_parts(args, init=False)\n",
            "        if not self._flavour.is_supported:\n",
            "            raise NotImplementedError(\"cannot instantiate %r on your system\"\n",
            "                                      % (cls.__name__,))\n",
            "        self._init()\n",
            "        return self\n",
            "\n",
            "    def _init(self,\n",
            "              # Private non-constructor arguments\n",
            "              template=None,\n",
            "              ):\n",
            "        if template is not None:\n",
            "            self._accessor = template._accessor\n",
            "        else:\n",
            "            self._accessor = _normal_accessor\n",
            "\n",
            "    def _make_child_relpath(self, part):\n",
            "        # This is an optimization used for dir walking.  `part` must be\n",
            "        # a single part relative to this path.\n",
            "        parts = self._parts + [part]\n",
            "        return self._from_parsed_parts(self._drv, self._root, parts)\n",
            "\n",
            "    def _opener(self, name, flags, mode=0o666):\n",
            "        # A stub for the opener argument to built-in open()\n",
            "        return self._accessor.open(self, flags, mode)\n",
            "\n",
            "    def _raw_open(self, flags, mode=0o777):\n",
            "        \"\"\"\n",
            "        Open the file pointed by this path and return a file descriptor,\n",
            "        as os.open() does.\n",
            "        \"\"\"\n",
            "        return self._accessor.open(self, flags, mode)\n",
            "\n",
            "    # Public API\n",
            "\n",
            "    @classmethod\n",
            "    def cwd(cls):\n",
            "        \"\"\"Return a new path pointing to the current working directory\n",
            "        (as returned by os.getcwd()).\n",
            "        \"\"\"\n",
            "        return cls(os.getcwd())\n",
            "\n",
            "    def iterdir(self):\n",
            "        \"\"\"Iterate over the files in this directory.  Does not yield any\n",
            "        result for the special paths '.' and '..'.\n",
            "        \"\"\"\n",
            "        for name in self._accessor.listdir(self):\n",
            "            if name in ('.', '..'):\n",
            "                # Yielding a path object for these makes little sense\n",
            "                continue\n",
            "            yield self._make_child_relpath(name)\n",
            "\n",
            "    def glob(self, pattern):\n",
            "        \"\"\"Iterate over this subtree and yield all existing files (of any\n",
            "        kind, including directories) matching the given pattern.\n",
            "        \"\"\"\n",
            "        pattern = self._flavour.casefold(pattern)\n",
            "        drv, root, pattern_parts = self._flavour.parse_parts((pattern,))\n",
            "        if drv or root:\n",
            "            raise NotImplementedError(\"Non-relative patterns are unsupported\")\n",
            "        selector = _make_selector(tuple(pattern_parts))\n",
            "        for p in selector.select_from(self):\n",
            "            yield p\n",
            "\n",
            "    def rglob(self, pattern):\n",
            "        \"\"\"Recursively yield all existing files (of any kind, including\n",
            "        directories) matching the given pattern, anywhere in this subtree.\n",
            "        \"\"\"\n",
            "        pattern = self._flavour.casefold(pattern)\n",
            "        drv, root, pattern_parts = self._flavour.parse_parts((pattern,))\n",
            "        if drv or root:\n",
            "            raise NotImplementedError(\"Non-relative patterns are unsupported\")\n",
            "        selector = _make_selector((\"**\",) + tuple(pattern_parts))\n",
            "        for p in selector.select_from(self):\n",
            "            yield p\n",
            "\n",
            "    def absolute(self):\n",
            "        \"\"\"Return an absolute version of this path.  This function works\n",
            "        even if the path doesn't point to anything.\n",
            "\n",
            "        No normalization is done, i.e. all '.' and '..' will be kept along.\n",
            "        Use resolve() to get the canonical path to a file.\n",
            "        \"\"\"\n",
            "        # XXX untested yet!\n",
            "        if self.is_absolute():\n",
            "            return self\n",
            "        # FIXME this must defer to the specific flavour (and, under Windows,\n",
            "        # use nt._getfullpathname())\n",
            "        obj = self._from_parts([os.getcwd()] + self._parts, init=False)\n",
            "        obj._init(template=self)\n",
            "        return obj\n",
            "\n",
            "    def resolve(self):\n",
            "        \"\"\"\n",
            "        Make the path absolute, resolving all symlinks on the way and also\n",
            "        normalizing it (for example turning slashes into backslashes under\n",
            "        Windows).\n",
            "        \"\"\"\n",
            "        s = self._flavour.resolve(self)\n",
            "        if s is None:\n",
            "            # No symlink resolution => for consistency, raise an error if\n",
            "            # the path doesn't exist or is forbidden\n",
            "            self.stat()\n",
            "            s = str(self.absolute())\n",
            "        # Now we have no symlinks in the path, it's safe to normalize it.\n",
            "        normed = self._flavour.pathmod.normpath(s)\n",
            "        obj = self._from_parts((normed,), init=False)\n",
            "        obj._init(template=self)\n",
            "        return obj\n",
            "\n",
            "    def stat(self):\n",
            "        \"\"\"\n",
            "        Return the result of the stat() system call on this path, like\n",
            "        os.stat() does.\n",
            "        \"\"\"\n",
            "        return self._accessor.stat(self)\n",
            "\n",
            "    def owner(self):\n",
            "        \"\"\"\n",
            "        Return the login name of the file owner.\n",
            "        \"\"\"\n",
            "        import pwd\n",
            "        return pwd.getpwuid(self.stat().st_uid).pw_name\n",
            "\n",
            "    def group(self):\n",
            "        \"\"\"\n",
            "        Return the group name of the file gid.\n",
            "        \"\"\"\n",
            "        import grp\n",
            "        return grp.getgrgid(self.stat().st_gid).gr_name\n",
            "\n",
            "    def open(self, mode='r', buffering=-1, encoding=None,\n",
            "             errors=None, newline=None):\n",
            "        \"\"\"\n",
            "        Open the file pointed by this path and return a file object, as\n",
            "        the built-in open() function does.\n",
            "        \"\"\"\n",
            "        if sys.version_info >= (3, 3):\n",
            "            return io.open(str(self), mode, buffering, encoding, errors, newline,\n",
            "                           opener=self._opener)\n",
            "        else:\n",
            "            return io.open(str(self), mode, buffering, encoding, errors, newline)\n",
            "\n",
            "    def touch(self, mode=0o666, exist_ok=True):\n",
            "        \"\"\"\n",
            "        Create this file with the given access mode, if it doesn't exist.\n",
            "        \"\"\"\n",
            "        if exist_ok:\n",
            "            # First try to bump modification time\n",
            "            # Implementation note: GNU touch uses the UTIME_NOW option of\n",
            "            # the utimensat() / futimens() functions.\n",
            "            t = time.time()\n",
            "            try:\n",
            "                self._accessor.utime(self, (t, t))\n",
            "            except OSError:\n",
            "                # Avoid exception chaining\n",
            "                pass\n",
            "            else:\n",
            "                return\n",
            "        flags = os.O_CREAT | os.O_WRONLY\n",
            "        if not exist_ok:\n",
            "            flags |= os.O_EXCL\n",
            "        fd = self._raw_open(flags, mode)\n",
            "        os.close(fd)\n",
            "\n",
            "    def mkdir(self, mode=0o777, parents=False):\n",
            "        if not parents:\n",
            "            self._accessor.mkdir(self, mode)\n",
            "        else:\n",
            "            try:\n",
            "                self._accessor.mkdir(self, mode)\n",
            "            except OSError as e:\n",
            "                if e.errno != ENOENT:\n",
            "                    raise\n",
            "                self.parent.mkdir(parents=True)\n",
            "                self._accessor.mkdir(self, mode)\n",
            "\n",
            "    def chmod(self, mode):\n",
            "        \"\"\"\n",
            "        Change the permissions of the path, like os.chmod().\n",
            "        \"\"\"\n",
            "        self._accessor.chmod(self, mode)\n",
            "\n",
            "    def lchmod(self, mode):\n",
            "        \"\"\"\n",
            "        Like chmod(), except if the path points to a symlink, the symlink's\n",
            "        permissions are changed, rather than its target's.\n",
            "        \"\"\"\n",
            "        self._accessor.lchmod(self, mode)\n",
            "\n",
            "    def unlink(self):\n",
            "        \"\"\"\n",
            "        Remove this file or link.\n",
            "        If the path is a directory, use rmdir() instead.\n",
            "        \"\"\"\n",
            "        self._accessor.unlink(self)\n",
            "\n",
            "    def rmdir(self):\n",
            "        \"\"\"\n",
            "        Remove this directory.  The directory must be empty.\n",
            "        \"\"\"\n",
            "        self._accessor.rmdir(self)\n",
            "\n",
            "    def lstat(self):\n",
            "        \"\"\"\n",
            "        Like stat(), except if the path points to a symlink, the symlink's\n",
            "        status information is returned, rather than its target's.\n",
            "        \"\"\"\n",
            "        return self._accessor.lstat(self)\n",
            "\n",
            "    def rename(self, target):\n",
            "        \"\"\"\n",
            "        Rename this path to the given path.\n",
            "        \"\"\"\n",
            "        self._accessor.rename(self, target)\n",
            "\n",
            "    def replace(self, target):\n",
            "        \"\"\"\n",
            "        Rename this path to the given path, clobbering the existing\n",
            "        destination if it exists.\n",
            "        \"\"\"\n",
            "        if sys.version_info < (3, 3):\n",
            "            raise NotImplementedError(\"replace() is only available \"\n",
            "                                      \"with Python 3.3 and later\")\n",
            "        self._accessor.replace(self, target)\n",
            "\n",
            "    def symlink_to(self, target, target_is_directory=False):\n",
            "        \"\"\"\n",
            "        Make this path a symlink pointing to the given path.\n",
            "        Note the order of arguments (self, target) is the reverse of os.symlink's.\n",
            "        \"\"\"\n",
            "        self._accessor.symlink(target, self, target_is_directory)\n",
            "\n",
            "    # Convenience functions for querying the stat results\n",
            "\n",
            "    def exists(self):\n",
            "        \"\"\"\n",
            "        Whether this path exists.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            self.stat()\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            return False\n",
            "        return True\n",
            "\n",
            "    def is_dir(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a directory.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISDIR(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "    def is_file(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a regular file (also True for symlinks pointing\n",
            "        to regular files).\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISREG(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "    def is_symlink(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a symbolic link.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISLNK(self.lstat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist\n",
            "            return False\n",
            "\n",
            "    def is_block_device(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a block device.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISBLK(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "    def is_char_device(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a character device.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISCHR(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "    def is_fifo(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a FIFO.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISFIFO(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "    def is_socket(self):\n",
            "        \"\"\"\n",
            "        Whether this path is a socket.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return S_ISSOCK(self.stat().st_mode)\n",
            "        except OSError as e:\n",
            "            if e.errno != ENOENT:\n",
            "                raise\n",
            "            # Path doesn't exist or is a broken symlink\n",
            "            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n",
            "            return False\n",
            "\n",
            "\n",
            "class PosixPath(Path, PurePosixPath):\n",
            "    __slots__ = ()\n",
            "\n",
            "class WindowsPath(Path, PureWindowsPath):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#!/usr/bin/env python\n",
            "#\n",
            "# Copyright (c) 2009-2013, Luke Maurits <luke@maurits.id.au>\n",
            "# All rights reserved.\n",
            "# With contributions from:\n",
            "#  * Chris Clark\n",
            "#  * Klein Stephane\n",
            "#\n",
            "# Redistribution and use in source and binary forms, with or without\n",
            "# modification, are permitted provided that the following conditions are met:\n",
            "#\n",
            "# * Redistributions of source code must retain the above copyright notice,\n",
            "#   this list of conditions and the following disclaimer.\n",
            "# * Redistributions in binary form must reproduce the above copyright notice,\n",
            "#   this list of conditions and the following disclaimer in the documentation\n",
            "#   and/or other materials provided with the distribution.\n",
            "# * The name of the author may not be used to endorse or promote products\n",
            "#   derived from this software without specific prior written permission.\n",
            "#\n",
            "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
            "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n",
            "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
            "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
            "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
            "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
            "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
            "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
            "# POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            "__version__ = \"0.7.2\"\n",
            "\n",
            "import copy\n",
            "import csv\n",
            "import random\n",
            "import re\n",
            "import sys\n",
            "import textwrap\n",
            "import itertools\n",
            "import unicodedata\n",
            "\n",
            "py3k = sys.version_info[0] >= 3\n",
            "if py3k:\n",
            "    unicode = str\n",
            "    basestring = str\n",
            "    itermap = map\n",
            "    iterzip = zip\n",
            "    uni_chr = chr\n",
            "    from html.parser import HTMLParser\n",
            "else: \n",
            "    itermap = itertools.imap\n",
            "    iterzip = itertools.izip\n",
            "    uni_chr = unichr\n",
            "    from HTMLParser import HTMLParser\n",
            "\n",
            "if py3k and sys.version_info[1] >= 2:\n",
            "    from html import escape\n",
            "else:\n",
            "    from cgi import escape\n",
            "\n",
            "# hrule styles\n",
            "FRAME = 0\n",
            "ALL   = 1\n",
            "NONE  = 2\n",
            "HEADER = 3\n",
            "\n",
            "# Table styles\n",
            "DEFAULT = 10\n",
            "MSWORD_FRIENDLY = 11\n",
            "PLAIN_COLUMNS = 12\n",
            "RANDOM = 20\n",
            "\n",
            "_re = re.compile(\"\\033\\[[0-9;]*m\")\n",
            "\n",
            "def _get_size(text):\n",
            "    lines = text.split(\"\\n\")\n",
            "    height = len(lines)\n",
            "    width = max([_str_block_width(line) for line in lines])\n",
            "    return (width, height)\n",
            "        \n",
            "class PrettyTable(object):\n",
            "\n",
            "    def __init__(self, field_names=None, **kwargs):\n",
            "\n",
            "        \"\"\"Return a new PrettyTable instance\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        encoding - Unicode encoding scheme used to decode any encoded input\n",
            "        field_names - list or tuple of field names\n",
            "        fields - list or tuple of field names to include in displays\n",
            "        start - index of first data row to include in output\n",
            "        end - index of last data row to include in output PLUS ONE (list slice style)\n",
            "        header - print a header showing field names (True or False)\n",
            "        header_style - stylisation to apply to field names in header (\"cap\", \"title\", \"upper\", \"lower\" or None)\n",
            "        border - print a border around the table (True or False)\n",
            "        hrules - controls printing of horizontal rules after rows.  Allowed values: FRAME, HEADER, ALL, NONE\n",
            "        vrules - controls printing of vertical rules between columns.  Allowed values: FRAME, ALL, NONE\n",
            "        int_format - controls formatting of integer data\n",
            "        float_format - controls formatting of floating point data\n",
            "        padding_width - number of spaces on either side of column data (only used if left and right paddings are None)\n",
            "        left_padding_width - number of spaces on left hand side of column data\n",
            "        right_padding_width - number of spaces on right hand side of column data\n",
            "        vertical_char - single character string used to draw vertical lines\n",
            "        horizontal_char - single character string used to draw horizontal lines\n",
            "        junction_char - single character string used to draw line junctions\n",
            "        sortby - name of field to sort rows by\n",
            "        sort_key - sorting key function, applied to data points before sorting\n",
            "        valign - default valign for each row (None, \"t\", \"m\" or \"b\")\n",
            "        reversesort - True or False to sort in descending or ascending order\"\"\"\n",
            "\n",
            "        self.encoding = kwargs.get(\"encoding\", \"UTF-8\")\n",
            "\n",
            "        # Data\n",
            "        self._field_names = []\n",
            "        self._align = {}\n",
            "        self._valign = {}\n",
            "        self._max_width = {}\n",
            "        self._rows = []\n",
            "        if field_names:\n",
            "            self.field_names = field_names\n",
            "        else:\n",
            "            self._widths = []\n",
            "\n",
            "        # Options\n",
            "        self._options = \"start end fields header border sortby reversesort sort_key attributes format hrules vrules\".split()\n",
            "        self._options.extend(\"int_format float_format padding_width left_padding_width right_padding_width\".split())\n",
            "        self._options.extend(\"vertical_char horizontal_char junction_char header_style valign xhtml print_empty\".split())\n",
            "        for option in self._options:\n",
            "            if option in kwargs:\n",
            "                self._validate_option(option, kwargs[option])\n",
            "            else:\n",
            "                kwargs[option] = None\n",
            "\n",
            "        self._start = kwargs[\"start\"] or 0\n",
            "        self._end = kwargs[\"end\"] or None\n",
            "        self._fields = kwargs[\"fields\"] or None\n",
            "\n",
            "        if kwargs[\"header\"] in (True, False):\n",
            "            self._header = kwargs[\"header\"]\n",
            "        else:\n",
            "            self._header = True\n",
            "        self._header_style = kwargs[\"header_style\"] or None\n",
            "        if kwargs[\"border\"] in (True, False):\n",
            "            self._border = kwargs[\"border\"]\n",
            "        else:\n",
            "            self._border = True\n",
            "        self._hrules = kwargs[\"hrules\"] or FRAME\n",
            "        self._vrules = kwargs[\"vrules\"] or ALL\n",
            "\n",
            "        self._sortby = kwargs[\"sortby\"] or None\n",
            "        if kwargs[\"reversesort\"] in (True, False):\n",
            "            self._reversesort = kwargs[\"reversesort\"]\n",
            "        else:\n",
            "            self._reversesort = False\n",
            "        self._sort_key = kwargs[\"sort_key\"] or (lambda x: x)\n",
            "\n",
            "        self._int_format = kwargs[\"int_format\"] or {}\n",
            "        self._float_format = kwargs[\"float_format\"] or {}\n",
            "        self._padding_width = kwargs[\"padding_width\"] or 1\n",
            "        self._left_padding_width = kwargs[\"left_padding_width\"] or None\n",
            "        self._right_padding_width = kwargs[\"right_padding_width\"] or None\n",
            "\n",
            "        self._vertical_char = kwargs[\"vertical_char\"] or self._unicode(\"|\")\n",
            "        self._horizontal_char = kwargs[\"horizontal_char\"] or self._unicode(\"-\")\n",
            "        self._junction_char = kwargs[\"junction_char\"] or self._unicode(\"+\")\n",
            "        \n",
            "        if kwargs[\"print_empty\"] in (True, False):\n",
            "            self._print_empty = kwargs[\"print_empty\"]\n",
            "        else:\n",
            "            self._print_empty = True\n",
            "        self._format = kwargs[\"format\"] or False\n",
            "        self._xhtml = kwargs[\"xhtml\"] or False\n",
            "        self._attributes = kwargs[\"attributes\"] or {}\n",
            "   \n",
            "    def _unicode(self, value):\n",
            "        if not isinstance(value, basestring):\n",
            "            value = str(value)\n",
            "        if not isinstance(value, unicode):\n",
            "            value = unicode(value, self.encoding, \"strict\")\n",
            "        return value\n",
            "\n",
            "    def _justify(self, text, width, align):\n",
            "        excess = width - _str_block_width(text)\n",
            "        if align == \"l\":\n",
            "            return text + excess * \" \"\n",
            "        elif align == \"r\":\n",
            "            return excess * \" \" + text\n",
            "        else:\n",
            "            if excess % 2:\n",
            "                # Uneven padding\n",
            "                # Put more space on right if text is of odd length...\n",
            "                if _str_block_width(text) % 2:\n",
            "                    return (excess//2)*\" \" + text + (excess//2 + 1)*\" \"\n",
            "                # and more space on left if text is of even length\n",
            "                else:\n",
            "                    return (excess//2 + 1)*\" \" + text + (excess//2)*\" \"\n",
            "                # Why distribute extra space this way?  To match the behaviour of\n",
            "                # the inbuilt str.center() method.\n",
            "            else:\n",
            "                # Equal padding on either side\n",
            "                return (excess//2)*\" \" + text + (excess//2)*\" \"\n",
            "\n",
            "    def __getattr__(self, name):\n",
            "\n",
            "        if name == \"rowcount\":\n",
            "            return len(self._rows)\n",
            "        elif name == \"colcount\":\n",
            "            if self._field_names:\n",
            "                return len(self._field_names)\n",
            "            elif self._rows:\n",
            "                return len(self._rows[0])\n",
            "            else:\n",
            "                return 0\n",
            "        else:\n",
            "            raise AttributeError(name)\n",
            " \n",
            "    def __getitem__(self, index):\n",
            "\n",
            "        new = PrettyTable()\n",
            "        new.field_names = self.field_names\n",
            "        for attr in self._options:\n",
            "            setattr(new, \"_\"+attr, getattr(self, \"_\"+attr))\n",
            "        setattr(new, \"_align\", getattr(self, \"_align\"))\n",
            "        if isinstance(index, slice):\n",
            "            for row in self._rows[index]:\n",
            "                new.add_row(row)\n",
            "        elif isinstance(index, int):\n",
            "            new.add_row(self._rows[index])\n",
            "        else:\n",
            "            raise Exception(\"Index %s is invalid, must be an integer or slice\" % str(index))\n",
            "        return new\n",
            "\n",
            "    if py3k:\n",
            "        def __str__(self):\n",
            "           return self.__unicode__()\n",
            "    else:\n",
            "        def __str__(self):\n",
            "           return self.__unicode__().encode(self.encoding)\n",
            "\n",
            "    def __unicode__(self):\n",
            "        return self.get_string()\n",
            "\n",
            "    ##############################\n",
            "    # ATTRIBUTE VALIDATORS       #\n",
            "    ##############################\n",
            "\n",
            "    # The method _validate_option is all that should be used elsewhere in the code base to validate options.\n",
            "    # It will call the appropriate validation method for that option.  The individual validation methods should\n",
            "    # never need to be called directly (although nothing bad will happen if they *are*).\n",
            "    # Validation happens in TWO places.\n",
            "    # Firstly, in the property setters defined in the ATTRIBUTE MANAGMENT section.\n",
            "    # Secondly, in the _get_options method, where keyword arguments are mixed with persistent settings\n",
            "\n",
            "    def _validate_option(self, option, val):\n",
            "        if option in (\"field_names\"):\n",
            "            self._validate_field_names(val)\n",
            "        elif option in (\"start\", \"end\", \"max_width\", \"padding_width\", \"left_padding_width\", \"right_padding_width\", \"format\"):\n",
            "            self._validate_nonnegative_int(option, val)\n",
            "        elif option in (\"sortby\"):\n",
            "            self._validate_field_name(option, val)\n",
            "        elif option in (\"sort_key\"):\n",
            "            self._validate_function(option, val)\n",
            "        elif option in (\"hrules\"):\n",
            "            self._validate_hrules(option, val)\n",
            "        elif option in (\"vrules\"):\n",
            "            self._validate_vrules(option, val)\n",
            "        elif option in (\"fields\"):\n",
            "            self._validate_all_field_names(option, val)\n",
            "        elif option in (\"header\", \"border\", \"reversesort\", \"xhtml\", \"print_empty\"):\n",
            "            self._validate_true_or_false(option, val)\n",
            "        elif option in (\"header_style\"):\n",
            "            self._validate_header_style(val)\n",
            "        elif option in (\"int_format\"):\n",
            "            self._validate_int_format(option, val)\n",
            "        elif option in (\"float_format\"):\n",
            "            self._validate_float_format(option, val)\n",
            "        elif option in (\"vertical_char\", \"horizontal_char\", \"junction_char\"):\n",
            "            self._validate_single_char(option, val)\n",
            "        elif option in (\"attributes\"):\n",
            "            self._validate_attributes(option, val)\n",
            "        else:\n",
            "            raise Exception(\"Unrecognised option: %s!\" % option)\n",
            "\n",
            "    def _validate_field_names(self, val):\n",
            "        # Check for appropriate length\n",
            "        if self._field_names:\n",
            "            try:\n",
            "               assert len(val) == len(self._field_names)\n",
            "            except AssertionError:\n",
            "               raise Exception(\"Field name list has incorrect number of values, (actual) %d!=%d (expected)\" % (len(val), len(self._field_names)))\n",
            "        if self._rows:\n",
            "            try:\n",
            "               assert len(val) == len(self._rows[0])\n",
            "            except AssertionError:\n",
            "               raise Exception(\"Field name list has incorrect number of values, (actual) %d!=%d (expected)\" % (len(val), len(self._rows[0])))\n",
            "        # Check for uniqueness\n",
            "        try:\n",
            "            assert len(val) == len(set(val))\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Field names must be unique!\")\n",
            "\n",
            "    def _validate_header_style(self, val):\n",
            "        try:\n",
            "            assert val in (\"cap\", \"title\", \"upper\", \"lower\", None)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid header style, use cap, title, upper, lower or None!\")\n",
            "\n",
            "    def _validate_align(self, val):\n",
            "        try:\n",
            "            assert val in [\"l\",\"c\",\"r\"]\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Alignment %s is invalid, use l, c or r!\" % val)\n",
            "\n",
            "    def _validate_valign(self, val):\n",
            "        try:\n",
            "            assert val in [\"t\",\"m\",\"b\",None]\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Alignment %s is invalid, use t, m, b or None!\" % val)\n",
            "\n",
            "    def _validate_nonnegative_int(self, name, val):\n",
            "        try:\n",
            "            assert int(val) >= 0\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s: %s!\" % (name, self._unicode(val)))\n",
            "\n",
            "    def _validate_true_or_false(self, name, val):\n",
            "        try:\n",
            "            assert val in (True, False)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be True or False.\" % name)\n",
            "\n",
            "    def _validate_int_format(self, name, val):\n",
            "        if val == \"\":\n",
            "            return\n",
            "        try:\n",
            "            assert type(val) in (str, unicode)\n",
            "            assert val.isdigit()\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be an integer format string.\" % name)\n",
            "\n",
            "    def _validate_float_format(self, name, val):\n",
            "        if val == \"\":\n",
            "            return\n",
            "        try:\n",
            "            assert type(val) in (str, unicode)\n",
            "            assert \".\" in val\n",
            "            bits = val.split(\".\")\n",
            "            assert len(bits) <= 2\n",
            "            assert bits[0] == \"\" or bits[0].isdigit()\n",
            "            assert bits[1] == \"\" or bits[1].isdigit()\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be a float format string.\" % name)\n",
            "\n",
            "    def _validate_function(self, name, val):\n",
            "        try:\n",
            "            assert hasattr(val, \"__call__\")\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be a function.\" % name)\n",
            "\n",
            "    def _validate_hrules(self, name, val):\n",
            "        try:\n",
            "            assert val in (ALL, FRAME, HEADER, NONE)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be ALL, FRAME, HEADER or NONE.\" % name)\n",
            "\n",
            "    def _validate_vrules(self, name, val):\n",
            "        try:\n",
            "            assert val in (ALL, FRAME, NONE)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be ALL, FRAME, or NONE.\" % name)\n",
            "\n",
            "    def _validate_field_name(self, name, val):\n",
            "        try:\n",
            "            assert (val in self._field_names) or (val is None)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid field name: %s!\" % val)\n",
            "\n",
            "    def _validate_all_field_names(self, name, val):\n",
            "        try:\n",
            "            for x in val:\n",
            "                self._validate_field_name(name, x)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"fields must be a sequence of field names!\")\n",
            "\n",
            "    def _validate_single_char(self, name, val):\n",
            "        try:\n",
            "            assert _str_block_width(val) == 1\n",
            "        except AssertionError:\n",
            "            raise Exception(\"Invalid value for %s!  Must be a string of length 1.\" % name)\n",
            "\n",
            "    def _validate_attributes(self, name, val):\n",
            "        try:\n",
            "            assert isinstance(val, dict)\n",
            "        except AssertionError:\n",
            "            raise Exception(\"attributes must be a dictionary of name/value pairs!\")\n",
            "\n",
            "    ##############################\n",
            "    # ATTRIBUTE MANAGEMENT       #\n",
            "    ##############################\n",
            "\n",
            "    def _get_field_names(self):\n",
            "        return self._field_names\n",
            "        \"\"\"The names of the fields\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        fields - list or tuple of field names\"\"\"\n",
            "    def _set_field_names(self, val):\n",
            "        val = [self._unicode(x) for x in val]\n",
            "        self._validate_option(\"field_names\", val)\n",
            "        if self._field_names:\n",
            "            old_names = self._field_names[:]\n",
            "        self._field_names = val\n",
            "        if self._align and old_names:\n",
            "            for old_name, new_name in zip(old_names, val):\n",
            "                self._align[new_name] = self._align[old_name]\n",
            "            for old_name in old_names:\n",
            "                if old_name not in self._align:\n",
            "                    self._align.pop(old_name)\n",
            "        else:\n",
            "            for field in self._field_names:\n",
            "                self._align[field] = \"c\"\n",
            "        if self._valign and old_names:\n",
            "            for old_name, new_name in zip(old_names, val):\n",
            "                self._valign[new_name] = self._valign[old_name]\n",
            "            for old_name in old_names:\n",
            "                if old_name not in self._valign:\n",
            "                    self._valign.pop(old_name)\n",
            "        else:\n",
            "            for field in self._field_names:\n",
            "                self._valign[field] = \"t\"\n",
            "    field_names = property(_get_field_names, _set_field_names)\n",
            "\n",
            "    def _get_align(self):\n",
            "        return self._align\n",
            "    def _set_align(self, val):\n",
            "        self._validate_align(val)\n",
            "        for field in self._field_names:\n",
            "            self._align[field] = val\n",
            "    align = property(_get_align, _set_align)\n",
            "\n",
            "    def _get_valign(self):\n",
            "        return self._valign\n",
            "    def _set_valign(self, val):\n",
            "        self._validate_valign(val)\n",
            "        for field in self._field_names:\n",
            "            self._valign[field] = val\n",
            "    valign = property(_get_valign, _set_valign)\n",
            "\n",
            "    def _get_max_width(self):\n",
            "        return self._max_width\n",
            "    def _set_max_width(self, val):\n",
            "        self._validate_option(\"max_width\", val)\n",
            "        for field in self._field_names:\n",
            "            self._max_width[field] = val\n",
            "    max_width = property(_get_max_width, _set_max_width)\n",
            "    \n",
            "    def _get_fields(self):\n",
            "        \"\"\"List or tuple of field names to include in displays\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        fields - list or tuple of field names to include in displays\"\"\"\n",
            "        return self._fields\n",
            "    def _set_fields(self, val):\n",
            "        self._validate_option(\"fields\", val)\n",
            "        self._fields = val\n",
            "    fields = property(_get_fields, _set_fields)\n",
            "\n",
            "    def _get_start(self):\n",
            "        \"\"\"Start index of the range of rows to print\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        start - index of first data row to include in output\"\"\"\n",
            "        return self._start\n",
            "\n",
            "    def _set_start(self, val):\n",
            "        self._validate_option(\"start\", val)\n",
            "        self._start = val\n",
            "    start = property(_get_start, _set_start)\n",
            "\n",
            "    def _get_end(self):\n",
            "        \"\"\"End index of the range of rows to print\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        end - index of last data row to include in output PLUS ONE (list slice style)\"\"\"\n",
            "        return self._end\n",
            "    def _set_end(self, val):\n",
            "        self._validate_option(\"end\", val)\n",
            "        self._end = val\n",
            "    end = property(_get_end, _set_end)\n",
            "\n",
            "    def _get_sortby(self):\n",
            "        \"\"\"Name of field by which to sort rows\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        sortby - field name to sort by\"\"\"\n",
            "        return self._sortby\n",
            "    def _set_sortby(self, val):\n",
            "        self._validate_option(\"sortby\", val)\n",
            "        self._sortby = val\n",
            "    sortby = property(_get_sortby, _set_sortby)\n",
            "\n",
            "    def _get_reversesort(self):\n",
            "        \"\"\"Controls direction of sorting (ascending vs descending)\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        reveresort - set to True to sort by descending order, or False to sort by ascending order\"\"\"\n",
            "        return self._reversesort\n",
            "    def _set_reversesort(self, val):\n",
            "        self._validate_option(\"reversesort\", val)\n",
            "        self._reversesort = val\n",
            "    reversesort = property(_get_reversesort, _set_reversesort)\n",
            "\n",
            "    def _get_sort_key(self):\n",
            "        \"\"\"Sorting key function, applied to data points before sorting\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        sort_key - a function which takes one argument and returns something to be sorted\"\"\"\n",
            "        return self._sort_key\n",
            "    def _set_sort_key(self, val):\n",
            "        self._validate_option(\"sort_key\", val)\n",
            "        self._sort_key = val\n",
            "    sort_key = property(_get_sort_key, _set_sort_key)\n",
            " \n",
            "    def _get_header(self):\n",
            "        \"\"\"Controls printing of table header with field names\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        header - print a header showing field names (True or False)\"\"\"\n",
            "        return self._header\n",
            "    def _set_header(self, val):\n",
            "        self._validate_option(\"header\", val)\n",
            "        self._header = val\n",
            "    header = property(_get_header, _set_header)\n",
            "\n",
            "    def _get_header_style(self):\n",
            "        \"\"\"Controls stylisation applied to field names in header\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        header_style - stylisation to apply to field names in header (\"cap\", \"title\", \"upper\", \"lower\" or None)\"\"\"\n",
            "        return self._header_style\n",
            "    def _set_header_style(self, val):\n",
            "        self._validate_header_style(val)\n",
            "        self._header_style = val\n",
            "    header_style = property(_get_header_style, _set_header_style)\n",
            "\n",
            "    def _get_border(self):\n",
            "        \"\"\"Controls printing of border around table\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        border - print a border around the table (True or False)\"\"\"\n",
            "        return self._border\n",
            "    def _set_border(self, val):\n",
            "        self._validate_option(\"border\", val)\n",
            "        self._border = val\n",
            "    border = property(_get_border, _set_border)\n",
            "\n",
            "    def _get_hrules(self):\n",
            "        \"\"\"Controls printing of horizontal rules after rows\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        hrules - horizontal rules style.  Allowed values: FRAME, ALL, HEADER, NONE\"\"\"\n",
            "        return self._hrules\n",
            "    def _set_hrules(self, val):\n",
            "        self._validate_option(\"hrules\", val)\n",
            "        self._hrules = val\n",
            "    hrules = property(_get_hrules, _set_hrules)\n",
            "\n",
            "    def _get_vrules(self):\n",
            "        \"\"\"Controls printing of vertical rules between columns\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        vrules - vertical rules style.  Allowed values: FRAME, ALL, NONE\"\"\"\n",
            "        return self._vrules\n",
            "    def _set_vrules(self, val):\n",
            "        self._validate_option(\"vrules\", val)\n",
            "        self._vrules = val\n",
            "    vrules = property(_get_vrules, _set_vrules)\n",
            "\n",
            "    def _get_int_format(self):\n",
            "        \"\"\"Controls formatting of integer data\n",
            "        Arguments:\n",
            "\n",
            "        int_format - integer format string\"\"\"\n",
            "        return self._int_format\n",
            "    def _set_int_format(self, val):\n",
            "#        self._validate_option(\"int_format\", val)\n",
            "        for field in self._field_names:\n",
            "            self._int_format[field] = val\n",
            "    int_format = property(_get_int_format, _set_int_format)\n",
            "\n",
            "    def _get_float_format(self):\n",
            "        \"\"\"Controls formatting of floating point data\n",
            "        Arguments:\n",
            "\n",
            "        float_format - floating point format string\"\"\"\n",
            "        return self._float_format\n",
            "    def _set_float_format(self, val):\n",
            "#        self._validate_option(\"float_format\", val)\n",
            "        for field in self._field_names:\n",
            "            self._float_format[field] = val\n",
            "    float_format = property(_get_float_format, _set_float_format)\n",
            "\n",
            "    def _get_padding_width(self):\n",
            "        \"\"\"The number of empty spaces between a column's edge and its content\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        padding_width - number of spaces, must be a positive integer\"\"\"\n",
            "        return self._padding_width\n",
            "    def _set_padding_width(self, val):\n",
            "        self._validate_option(\"padding_width\", val)\n",
            "        self._padding_width = val\n",
            "    padding_width = property(_get_padding_width, _set_padding_width)\n",
            "\n",
            "    def _get_left_padding_width(self):\n",
            "        \"\"\"The number of empty spaces between a column's left edge and its content\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        left_padding - number of spaces, must be a positive integer\"\"\"\n",
            "        return self._left_padding_width\n",
            "    def _set_left_padding_width(self, val):\n",
            "        self._validate_option(\"left_padding_width\", val)\n",
            "        self._left_padding_width = val\n",
            "    left_padding_width = property(_get_left_padding_width, _set_left_padding_width)\n",
            "\n",
            "    def _get_right_padding_width(self):\n",
            "        \"\"\"The number of empty spaces between a column's right edge and its content\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        right_padding - number of spaces, must be a positive integer\"\"\"\n",
            "        return self._right_padding_width\n",
            "    def _set_right_padding_width(self, val):\n",
            "        self._validate_option(\"right_padding_width\", val)\n",
            "        self._right_padding_width = val\n",
            "    right_padding_width = property(_get_right_padding_width, _set_right_padding_width)\n",
            "\n",
            "    def _get_vertical_char(self):\n",
            "        \"\"\"The charcter used when printing table borders to draw vertical lines\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        vertical_char - single character string used to draw vertical lines\"\"\"\n",
            "        return self._vertical_char\n",
            "    def _set_vertical_char(self, val):\n",
            "        val = self._unicode(val)\n",
            "        self._validate_option(\"vertical_char\", val)\n",
            "        self._vertical_char = val\n",
            "    vertical_char = property(_get_vertical_char, _set_vertical_char)\n",
            "\n",
            "    def _get_horizontal_char(self):\n",
            "        \"\"\"The charcter used when printing table borders to draw horizontal lines\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        horizontal_char - single character string used to draw horizontal lines\"\"\"\n",
            "        return self._horizontal_char\n",
            "    def _set_horizontal_char(self, val):\n",
            "        val = self._unicode(val)\n",
            "        self._validate_option(\"horizontal_char\", val)\n",
            "        self._horizontal_char = val\n",
            "    horizontal_char = property(_get_horizontal_char, _set_horizontal_char)\n",
            "\n",
            "    def _get_junction_char(self):\n",
            "        \"\"\"The charcter used when printing table borders to draw line junctions\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        junction_char - single character string used to draw line junctions\"\"\"\n",
            "        return self._junction_char\n",
            "    def _set_junction_char(self, val):\n",
            "        val = self._unicode(val)\n",
            "        self._validate_option(\"vertical_char\", val)\n",
            "        self._junction_char = val\n",
            "    junction_char = property(_get_junction_char, _set_junction_char)\n",
            "\n",
            "    def _get_format(self):\n",
            "        \"\"\"Controls whether or not HTML tables are formatted to match styling options\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        format - True or False\"\"\"\n",
            "        return self._format\n",
            "    def _set_format(self, val):\n",
            "        self._validate_option(\"format\", val)\n",
            "        self._format = val\n",
            "    format = property(_get_format, _set_format)\n",
            "\n",
            "    def _get_print_empty(self):\n",
            "        \"\"\"Controls whether or not empty tables produce a header and frame or just an empty string\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        print_empty - True or False\"\"\"\n",
            "        return self._print_empty\n",
            "    def _set_print_empty(self, val):\n",
            "        self._validate_option(\"print_empty\", val)\n",
            "        self._print_empty = val\n",
            "    print_empty = property(_get_print_empty, _set_print_empty)\n",
            "\n",
            "    def _get_attributes(self):\n",
            "        \"\"\"A dictionary of HTML attribute name/value pairs to be included in the <table> tag when printing HTML\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        attributes - dictionary of attributes\"\"\"\n",
            "        return self._attributes\n",
            "    def _set_attributes(self, val):\n",
            "        self._validate_option(\"attributes\", val)\n",
            "        self._attributes = val\n",
            "    attributes = property(_get_attributes, _set_attributes)\n",
            "\n",
            "    ##############################\n",
            "    # OPTION MIXER               #\n",
            "    ##############################\n",
            "\n",
            "    def _get_options(self, kwargs):\n",
            "\n",
            "        options = {}\n",
            "        for option in self._options:\n",
            "            if option in kwargs:\n",
            "                self._validate_option(option, kwargs[option])\n",
            "                options[option] = kwargs[option]\n",
            "            else:\n",
            "                options[option] = getattr(self, \"_\"+option)\n",
            "        return options\n",
            "\n",
            "    ##############################\n",
            "    # PRESET STYLE LOGIC         #\n",
            "    ##############################\n",
            "\n",
            "    def set_style(self, style):\n",
            "\n",
            "        if style == DEFAULT:\n",
            "            self._set_default_style()\n",
            "        elif style == MSWORD_FRIENDLY:\n",
            "            self._set_msword_style()\n",
            "        elif style == PLAIN_COLUMNS:\n",
            "            self._set_columns_style()\n",
            "        elif style == RANDOM:\n",
            "            self._set_random_style()\n",
            "        else:\n",
            "            raise Exception(\"Invalid pre-set style!\")\n",
            "\n",
            "    def _set_default_style(self):\n",
            "\n",
            "        self.header = True\n",
            "        self.border = True\n",
            "        self._hrules = FRAME\n",
            "        self._vrules = ALL\n",
            "        self.padding_width = 1\n",
            "        self.left_padding_width = 1\n",
            "        self.right_padding_width = 1\n",
            "        self.vertical_char = \"|\"\n",
            "        self.horizontal_char = \"-\"\n",
            "        self.junction_char = \"+\"\n",
            "\n",
            "    def _set_msword_style(self):\n",
            "\n",
            "        self.header = True\n",
            "        self.border = True\n",
            "        self._hrules = NONE\n",
            "        self.padding_width = 1\n",
            "        self.left_padding_width = 1\n",
            "        self.right_padding_width = 1\n",
            "        self.vertical_char = \"|\"\n",
            "\n",
            "    def _set_columns_style(self):\n",
            "\n",
            "        self.header = True\n",
            "        self.border = False\n",
            "        self.padding_width = 1\n",
            "        self.left_padding_width = 0\n",
            "        self.right_padding_width = 8\n",
            "\n",
            "    def _set_random_style(self):\n",
            "\n",
            "        # Just for fun!\n",
            "        self.header = random.choice((True, False))\n",
            "        self.border = random.choice((True, False))\n",
            "        self._hrules = random.choice((ALL, FRAME, HEADER, NONE))\n",
            "        self._vrules = random.choice((ALL, FRAME, NONE))\n",
            "        self.left_padding_width = random.randint(0,5)\n",
            "        self.right_padding_width = random.randint(0,5)\n",
            "        self.vertical_char = random.choice(\"~!@#$%^&*()_+|-=\\{}[];':\\\",./;<>?\")\n",
            "        self.horizontal_char = random.choice(\"~!@#$%^&*()_+|-=\\{}[];':\\\",./;<>?\")\n",
            "        self.junction_char = random.choice(\"~!@#$%^&*()_+|-=\\{}[];':\\\",./;<>?\")\n",
            "\n",
            "    ##############################\n",
            "    # DATA INPUT METHODS         #\n",
            "    ##############################\n",
            "\n",
            "    def add_row(self, row):\n",
            "\n",
            "        \"\"\"Add a row to the table\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        row - row of data, should be a list with as many elements as the table\n",
            "        has fields\"\"\"\n",
            "\n",
            "        if self._field_names and len(row) != len(self._field_names):\n",
            "            raise Exception(\"Row has incorrect number of values, (actual) %d!=%d (expected)\" %(len(row),len(self._field_names)))\n",
            "        if not self._field_names:\n",
            "            self.field_names = [(\"Field %d\" % (n+1)) for n in range(0,len(row))]\n",
            "        self._rows.append(list(row))\n",
            "\n",
            "    def del_row(self, row_index):\n",
            "\n",
            "        \"\"\"Delete a row to the table\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        row_index - The index of the row you want to delete.  Indexing starts at 0.\"\"\"\n",
            "\n",
            "        if row_index > len(self._rows)-1:\n",
            "            raise Exception(\"Cant delete row at index %d, table only has %d rows!\" % (row_index, len(self._rows)))\n",
            "        del self._rows[row_index]\n",
            "\n",
            "    def add_column(self, fieldname, column, align=\"c\", valign=\"t\"):\n",
            "\n",
            "        \"\"\"Add a column to the table.\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        fieldname - name of the field to contain the new column of data\n",
            "        column - column of data, should be a list with as many elements as the\n",
            "        table has rows\n",
            "        align - desired alignment for this column - \"l\" for left, \"c\" for centre and \"r\" for right\n",
            "        valign - desired vertical alignment for new columns - \"t\" for top, \"m\" for middle and \"b\" for bottom\"\"\"\n",
            "\n",
            "        if len(self._rows) in (0, len(column)):\n",
            "            self._validate_align(align)\n",
            "            self._validate_valign(valign)\n",
            "            self._field_names.append(fieldname)\n",
            "            self._align[fieldname] = align\n",
            "            self._valign[fieldname] = valign\n",
            "            for i in range(0, len(column)):\n",
            "                if len(self._rows) < i+1:\n",
            "                    self._rows.append([])\n",
            "                self._rows[i].append(column[i])\n",
            "        else:\n",
            "            raise Exception(\"Column length %d does not match number of rows %d!\" % (len(column), len(self._rows)))\n",
            "\n",
            "    def clear_rows(self):\n",
            "\n",
            "        \"\"\"Delete all rows from the table but keep the current field names\"\"\"\n",
            "\n",
            "        self._rows = []\n",
            "\n",
            "    def clear(self):\n",
            "\n",
            "        \"\"\"Delete all rows and field names from the table, maintaining nothing but styling options\"\"\"\n",
            "\n",
            "        self._rows = []\n",
            "        self._field_names = []\n",
            "        self._widths = []\n",
            "\n",
            "    ##############################\n",
            "    # MISC PUBLIC METHODS        #\n",
            "    ##############################\n",
            "\n",
            "    def copy(self):\n",
            "        return copy.deepcopy(self)\n",
            "\n",
            "    ##############################\n",
            "    # MISC PRIVATE METHODS       #\n",
            "    ##############################\n",
            "\n",
            "    def _format_value(self, field, value):\n",
            "        if isinstance(value, int) and field in self._int_format:\n",
            "            value = self._unicode((\"%%%sd\" % self._int_format[field]) % value)\n",
            "        elif isinstance(value, float) and field in self._float_format:\n",
            "            value = self._unicode((\"%%%sf\" % self._float_format[field]) % value)\n",
            "        return self._unicode(value)\n",
            "\n",
            "    def _compute_widths(self, rows, options):\n",
            "        if options[\"header\"]:\n",
            "            widths = [_get_size(field)[0] for field in self._field_names]\n",
            "        else:\n",
            "            widths = len(self.field_names) * [0]\n",
            "        for row in rows:\n",
            "            for index, value in enumerate(row):\n",
            "                fieldname = self.field_names[index]\n",
            "                if fieldname in self.max_width:\n",
            "                    widths[index] = max(widths[index], min(_get_size(value)[0], self.max_width[fieldname]))\n",
            "                else:\n",
            "                    widths[index] = max(widths[index], _get_size(value)[0])\n",
            "        self._widths = widths\n",
            "\n",
            "    def _get_padding_widths(self, options):\n",
            "\n",
            "        if options[\"left_padding_width\"] is not None:\n",
            "            lpad = options[\"left_padding_width\"]\n",
            "        else:\n",
            "            lpad = options[\"padding_width\"]\n",
            "        if options[\"right_padding_width\"] is not None:\n",
            "            rpad = options[\"right_padding_width\"]\n",
            "        else:\n",
            "            rpad = options[\"padding_width\"]\n",
            "        return lpad, rpad\n",
            "\n",
            "    def _get_rows(self, options):\n",
            "        \"\"\"Return only those data rows that should be printed, based on slicing and sorting.\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        options - dictionary of option settings.\"\"\"\n",
            "       \n",
            "        # Make a copy of only those rows in the slice range \n",
            "        rows = copy.deepcopy(self._rows[options[\"start\"]:options[\"end\"]])\n",
            "        # Sort if necessary\n",
            "        if options[\"sortby\"]:\n",
            "            sortindex = self._field_names.index(options[\"sortby\"])\n",
            "            # Decorate\n",
            "            rows = [[row[sortindex]]+row for row in rows]\n",
            "            # Sort\n",
            "            rows.sort(reverse=options[\"reversesort\"], key=options[\"sort_key\"])\n",
            "            # Undecorate\n",
            "            rows = [row[1:] for row in rows]\n",
            "        return rows\n",
            "        \n",
            "    def _format_row(self, row, options):\n",
            "        return [self._format_value(field, value) for (field, value) in zip(self._field_names, row)]\n",
            "\n",
            "    def _format_rows(self, rows, options):\n",
            "        return [self._format_row(row, options) for row in rows]\n",
            " \n",
            "    ##############################\n",
            "    # PLAIN TEXT STRING METHODS  #\n",
            "    ##############################\n",
            "\n",
            "    def get_string(self, **kwargs):\n",
            "\n",
            "        \"\"\"Return string representation of table in current state.\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        start - index of first data row to include in output\n",
            "        end - index of last data row to include in output PLUS ONE (list slice style)\n",
            "        fields - names of fields (columns) to include\n",
            "        header - print a header showing field names (True or False)\n",
            "        border - print a border around the table (True or False)\n",
            "        hrules - controls printing of horizontal rules after rows.  Allowed values: ALL, FRAME, HEADER, NONE\n",
            "        vrules - controls printing of vertical rules between columns.  Allowed values: FRAME, ALL, NONE\n",
            "        int_format - controls formatting of integer data\n",
            "        float_format - controls formatting of floating point data\n",
            "        padding_width - number of spaces on either side of column data (only used if left and right paddings are None)\n",
            "        left_padding_width - number of spaces on left hand side of column data\n",
            "        right_padding_width - number of spaces on right hand side of column data\n",
            "        vertical_char - single character string used to draw vertical lines\n",
            "        horizontal_char - single character string used to draw horizontal lines\n",
            "        junction_char - single character string used to draw line junctions\n",
            "        sortby - name of field to sort rows by\n",
            "        sort_key - sorting key function, applied to data points before sorting\n",
            "        reversesort - True or False to sort in descending or ascending order\n",
            "        print empty - if True, stringify just the header for an empty table, if False return an empty string \"\"\"\n",
            "\n",
            "        options = self._get_options(kwargs)\n",
            "\n",
            "        lines = []\n",
            "\n",
            "        # Don't think too hard about an empty table\n",
            "        # Is this the desired behaviour?  Maybe we should still print the header?\n",
            "        if self.rowcount == 0 and (not options[\"print_empty\"] or not options[\"border\"]):\n",
            "            return \"\"\n",
            "\n",
            "        # Get the rows we need to print, taking into account slicing, sorting, etc.\n",
            "        rows = self._get_rows(options)\n",
            "\n",
            "        # Turn all data in all rows into Unicode, formatted as desired\n",
            "        formatted_rows = self._format_rows(rows, options)\n",
            "\n",
            "        # Compute column widths\n",
            "        self._compute_widths(formatted_rows, options)\n",
            "\n",
            "        # Add header or top of border\n",
            "        self._hrule = self._stringify_hrule(options)\n",
            "        if options[\"header\"]:\n",
            "            lines.append(self._stringify_header(options))\n",
            "        elif options[\"border\"] and options[\"hrules\"] in (ALL, FRAME):\n",
            "            lines.append(self._hrule)\n",
            "\n",
            "        # Add rows\n",
            "        for row in formatted_rows:\n",
            "            lines.append(self._stringify_row(row, options))\n",
            "\n",
            "        # Add bottom of border\n",
            "        if options[\"border\"] and options[\"hrules\"] == FRAME:\n",
            "            lines.append(self._hrule)\n",
            "        \n",
            "        return self._unicode(\"\\n\").join(lines)\n",
            "\n",
            "    def _stringify_hrule(self, options):\n",
            "\n",
            "        if not options[\"border\"]:\n",
            "            return \"\"\n",
            "        lpad, rpad = self._get_padding_widths(options)\n",
            "        if options['vrules'] in (ALL, FRAME):\n",
            "            bits = [options[\"junction_char\"]]\n",
            "        else:\n",
            "            bits = [options[\"horizontal_char\"]]\n",
            "        # For tables with no data or fieldnames\n",
            "        if not self._field_names:\n",
            "                bits.append(options[\"junction_char\"])\n",
            "                return \"\".join(bits)\n",
            "        for field, width in zip(self._field_names, self._widths):\n",
            "            if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                continue\n",
            "            bits.append((width+lpad+rpad)*options[\"horizontal_char\"])\n",
            "            if options['vrules'] == ALL:\n",
            "                bits.append(options[\"junction_char\"])\n",
            "            else:\n",
            "                bits.append(options[\"horizontal_char\"])\n",
            "        if options[\"vrules\"] == FRAME:\n",
            "            bits.pop()\n",
            "            bits.append(options[\"junction_char\"])\n",
            "        return \"\".join(bits)\n",
            "\n",
            "    def _stringify_header(self, options):\n",
            "\n",
            "        bits = []\n",
            "        lpad, rpad = self._get_padding_widths(options)\n",
            "        if options[\"border\"]:\n",
            "            if options[\"hrules\"] in (ALL, FRAME):\n",
            "                bits.append(self._hrule)\n",
            "                bits.append(\"\\n\")\n",
            "            if options[\"vrules\"] in (ALL, FRAME):\n",
            "                bits.append(options[\"vertical_char\"])\n",
            "            else:\n",
            "                bits.append(\" \")\n",
            "        # For tables with no data or field names\n",
            "        if not self._field_names:\n",
            "            if options[\"vrules\"] in (ALL, FRAME):\n",
            "                bits.append(options[\"vertical_char\"])\n",
            "            else:\n",
            "                bits.append(\" \")\n",
            "        for field, width, in zip(self._field_names, self._widths):\n",
            "            if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                continue\n",
            "            if self._header_style == \"cap\":\n",
            "                fieldname = field.capitalize()\n",
            "            elif self._header_style == \"title\":\n",
            "                fieldname = field.title()\n",
            "            elif self._header_style == \"upper\":\n",
            "                fieldname = field.upper()\n",
            "            elif self._header_style == \"lower\":\n",
            "                fieldname = field.lower()\n",
            "            else:\n",
            "                fieldname = field\n",
            "            bits.append(\" \" * lpad + self._justify(fieldname, width, self._align[field]) + \" \" * rpad)\n",
            "            if options[\"border\"]:\n",
            "                if options[\"vrules\"] == ALL:\n",
            "                    bits.append(options[\"vertical_char\"])\n",
            "                else:\n",
            "                    bits.append(\" \")\n",
            "        # If vrules is FRAME, then we just appended a space at the end\n",
            "        # of the last field, when we really want a vertical character\n",
            "        if options[\"border\"] and options[\"vrules\"] == FRAME:\n",
            "            bits.pop()\n",
            "            bits.append(options[\"vertical_char\"])\n",
            "        if options[\"border\"] and options[\"hrules\"] != NONE:\n",
            "            bits.append(\"\\n\")\n",
            "            bits.append(self._hrule)\n",
            "        return \"\".join(bits)\n",
            "\n",
            "    def _stringify_row(self, row, options):\n",
            "       \n",
            "        for index, field, value, width, in zip(range(0,len(row)), self._field_names, row, self._widths):\n",
            "            # Enforce max widths\n",
            "            lines = value.split(\"\\n\")\n",
            "            new_lines = []\n",
            "            for line in lines: \n",
            "                if _str_block_width(line) > width:\n",
            "                    line = textwrap.fill(line, width)\n",
            "                new_lines.append(line)\n",
            "            lines = new_lines\n",
            "            value = \"\\n\".join(lines)\n",
            "            row[index] = value\n",
            "\n",
            "        row_height = 0\n",
            "        for c in row:\n",
            "            h = _get_size(c)[1]\n",
            "            if h > row_height:\n",
            "                row_height = h\n",
            "\n",
            "        bits = []\n",
            "        lpad, rpad = self._get_padding_widths(options)\n",
            "        for y in range(0, row_height):\n",
            "            bits.append([])\n",
            "            if options[\"border\"]:\n",
            "                if options[\"vrules\"] in (ALL, FRAME):\n",
            "                    bits[y].append(self.vertical_char)\n",
            "                else:\n",
            "                    bits[y].append(\" \")\n",
            "\n",
            "        for field, value, width, in zip(self._field_names, row, self._widths):\n",
            "\n",
            "            valign = self._valign[field]\n",
            "            lines = value.split(\"\\n\")\n",
            "            dHeight = row_height - len(lines)\n",
            "            if dHeight:\n",
            "                if valign == \"m\":\n",
            "                  lines = [\"\"] * int(dHeight / 2) + lines + [\"\"] * (dHeight - int(dHeight / 2))\n",
            "                elif valign == \"b\":\n",
            "                  lines = [\"\"] * dHeight + lines\n",
            "                else:\n",
            "                  lines = lines + [\"\"] * dHeight\n",
            "\n",
            "            y = 0\n",
            "            for l in lines:\n",
            "                if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                    continue\n",
            "\n",
            "                bits[y].append(\" \" * lpad + self._justify(l, width, self._align[field]) + \" \" * rpad)\n",
            "                if options[\"border\"]:\n",
            "                    if options[\"vrules\"] == ALL:\n",
            "                        bits[y].append(self.vertical_char)\n",
            "                    else:\n",
            "                        bits[y].append(\" \")\n",
            "                y += 1\n",
            "\n",
            "        # If vrules is FRAME, then we just appended a space at the end\n",
            "        # of the last field, when we really want a vertical character\n",
            "        for y in range(0, row_height):\n",
            "            if options[\"border\"] and options[\"vrules\"] == FRAME:\n",
            "                bits[y].pop()\n",
            "                bits[y].append(options[\"vertical_char\"])\n",
            "        \n",
            "        if options[\"border\"] and options[\"hrules\"]== ALL:\n",
            "            bits[row_height-1].append(\"\\n\")\n",
            "            bits[row_height-1].append(self._hrule)\n",
            "\n",
            "        for y in range(0, row_height):\n",
            "            bits[y] = \"\".join(bits[y])\n",
            "\n",
            "        return \"\\n\".join(bits)\n",
            "\n",
            "    ##############################\n",
            "    # HTML STRING METHODS        #\n",
            "    ##############################\n",
            "\n",
            "    def get_html_string(self, **kwargs):\n",
            "\n",
            "        \"\"\"Return string representation of HTML formatted version of table in current state.\n",
            "\n",
            "        Arguments:\n",
            "\n",
            "        start - index of first data row to include in output\n",
            "        end - index of last data row to include in output PLUS ONE (list slice style)\n",
            "        fields - names of fields (columns) to include\n",
            "        header - print a header showing field names (True or False)\n",
            "        border - print a border around the table (True or False)\n",
            "        hrules - controls printing of horizontal rules after rows.  Allowed values: ALL, FRAME, HEADER, NONE\n",
            "        vrules - controls printing of vertical rules between columns.  Allowed values: FRAME, ALL, NONE\n",
            "        int_format - controls formatting of integer data\n",
            "        float_format - controls formatting of floating point data\n",
            "        padding_width - number of spaces on either side of column data (only used if left and right paddings are None)\n",
            "        left_padding_width - number of spaces on left hand side of column data\n",
            "        right_padding_width - number of spaces on right hand side of column data\n",
            "        sortby - name of field to sort rows by\n",
            "        sort_key - sorting key function, applied to data points before sorting\n",
            "        attributes - dictionary of name/value pairs to include as HTML attributes in the <table> tag\n",
            "        xhtml - print <br/> tags if True, <br> tags if false\"\"\"\n",
            "\n",
            "        options = self._get_options(kwargs)\n",
            "\n",
            "        if options[\"format\"]:\n",
            "            string = self._get_formatted_html_string(options)\n",
            "        else:\n",
            "            string = self._get_simple_html_string(options)\n",
            "\n",
            "        return string\n",
            "\n",
            "    def _get_simple_html_string(self, options):\n",
            "\n",
            "        lines = []\n",
            "        if options[\"xhtml\"]:\n",
            "            linebreak = \"<br/>\"\n",
            "        else:\n",
            "            linebreak = \"<br>\"\n",
            "\n",
            "        open_tag = []\n",
            "        open_tag.append(\"<table\")\n",
            "        if options[\"attributes\"]:\n",
            "            for attr_name in options[\"attributes\"]:\n",
            "                open_tag.append(\" %s=\\\"%s\\\"\" % (attr_name, options[\"attributes\"][attr_name]))\n",
            "        open_tag.append(\">\")\n",
            "        lines.append(\"\".join(open_tag))\n",
            "\n",
            "        # Headers\n",
            "        if options[\"header\"]:\n",
            "            lines.append(\"    <tr>\")\n",
            "            for field in self._field_names:\n",
            "                if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                    continue\n",
            "                lines.append(\"        <th>%s</th>\" % escape(field).replace(\"\\n\", linebreak))\n",
            "            lines.append(\"    </tr>\")\n",
            "\n",
            "        # Data\n",
            "        rows = self._get_rows(options)\n",
            "        formatted_rows = self._format_rows(rows, options)\n",
            "        for row in formatted_rows:\n",
            "            lines.append(\"    <tr>\")\n",
            "            for field, datum in zip(self._field_names, row):\n",
            "                if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                    continue\n",
            "                lines.append(\"        <td>%s</td>\" % escape(datum).replace(\"\\n\", linebreak))\n",
            "            lines.append(\"    </tr>\")\n",
            "\n",
            "        lines.append(\"</table>\")\n",
            "\n",
            "        return self._unicode(\"\\n\").join(lines)\n",
            "\n",
            "    def _get_formatted_html_string(self, options):\n",
            "\n",
            "        lines = []\n",
            "        lpad, rpad = self._get_padding_widths(options)\n",
            "        if options[\"xhtml\"]:\n",
            "            linebreak = \"<br/>\"\n",
            "        else:\n",
            "            linebreak = \"<br>\"\n",
            "\n",
            "        open_tag = []\n",
            "        open_tag.append(\"<table\")\n",
            "        if options[\"border\"]:\n",
            "            if options[\"hrules\"] == ALL and options[\"vrules\"] == ALL:\n",
            "                open_tag.append(\" frame=\\\"box\\\" rules=\\\"all\\\"\")\n",
            "            elif options[\"hrules\"] == FRAME and options[\"vrules\"] == FRAME:\n",
            "                open_tag.append(\" frame=\\\"box\\\"\")\n",
            "            elif options[\"hrules\"] == FRAME and options[\"vrules\"] == ALL:\n",
            "                open_tag.append(\" frame=\\\"box\\\" rules=\\\"cols\\\"\")\n",
            "            elif options[\"hrules\"] == FRAME:\n",
            "                open_tag.append(\" frame=\\\"hsides\\\"\")\n",
            "            elif options[\"hrules\"] == ALL:\n",
            "                open_tag.append(\" frame=\\\"hsides\\\" rules=\\\"rows\\\"\")\n",
            "            elif options[\"vrules\"] == FRAME:\n",
            "                open_tag.append(\" frame=\\\"vsides\\\"\")\n",
            "            elif options[\"vrules\"] == ALL:\n",
            "                open_tag.append(\" frame=\\\"vsides\\\" rules=\\\"cols\\\"\")\n",
            "        if options[\"attributes\"]:\n",
            "            for attr_name in options[\"attributes\"]:\n",
            "                open_tag.append(\" %s=\\\"%s\\\"\" % (attr_name, options[\"attributes\"][attr_name]))\n",
            "        open_tag.append(\">\")\n",
            "        lines.append(\"\".join(open_tag))\n",
            "\n",
            "        # Headers\n",
            "        if options[\"header\"]:\n",
            "            lines.append(\"    <tr>\")\n",
            "            for field in self._field_names:\n",
            "                if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                    continue\n",
            "                lines.append(\"        <th style=\\\"padding-left: %dem; padding-right: %dem; text-align: center\\\">%s</th>\" % (lpad, rpad, escape(field).replace(\"\\n\", linebreak)))\n",
            "            lines.append(\"    </tr>\")\n",
            "\n",
            "        # Data\n",
            "        rows = self._get_rows(options)\n",
            "        formatted_rows = self._format_rows(rows, options)\n",
            "        aligns = []\n",
            "        valigns = []\n",
            "        for field in self._field_names:\n",
            "            aligns.append({ \"l\" : \"left\", \"r\" : \"right\", \"c\" : \"center\" }[self._align[field]])\n",
            "            valigns.append({\"t\" : \"top\", \"m\" : \"middle\", \"b\" : \"bottom\"}[self._valign[field]])\n",
            "        for row in formatted_rows:\n",
            "            lines.append(\"    <tr>\")\n",
            "            for field, datum, align, valign in zip(self._field_names, row, aligns, valigns):\n",
            "                if options[\"fields\"] and field not in options[\"fields\"]:\n",
            "                    continue\n",
            "                lines.append(\"        <td style=\\\"padding-left: %dem; padding-right: %dem; text-align: %s; vertical-align: %s\\\">%s</td>\" % (lpad, rpad, align, valign, escape(datum).replace(\"\\n\", linebreak)))\n",
            "            lines.append(\"    </tr>\")\n",
            "        lines.append(\"</table>\")\n",
            "\n",
            "        return self._unicode(\"\\n\").join(lines)\n",
            "\n",
            "##############################\n",
            "# UNICODE WIDTH FUNCTIONS    #\n",
            "##############################\n",
            "\n",
            "def _char_block_width(char):\n",
            "    # Basic Latin, which is probably the most common case\n",
            "    #if char in xrange(0x0021, 0x007e):\n",
            "    #if char >= 0x0021 and char <= 0x007e:\n",
            "    if 0x0021 <= char <= 0x007e:\n",
            "        return 1\n",
            "    # Chinese, Japanese, Korean (common)\n",
            "    if 0x4e00 <= char <= 0x9fff:\n",
            "        return 2\n",
            "    # Hangul\n",
            "    if 0xac00 <= char <= 0xd7af:\n",
            "        return 2\n",
            "    # Combining?\n",
            "    if unicodedata.combining(uni_chr(char)):\n",
            "        return 0\n",
            "    # Hiragana and Katakana\n",
            "    if 0x3040 <= char <= 0x309f or 0x30a0 <= char <= 0x30ff:\n",
            "        return 2\n",
            "    # Full-width Latin characters\n",
            "    if 0xff01 <= char <= 0xff60:\n",
            "        return 2\n",
            "    # CJK punctuation\n",
            "    if 0x3000 <= char <= 0x303e:\n",
            "        return 2\n",
            "    # Backspace and delete\n",
            "    if char in (0x0008, 0x007f):\n",
            "        return -1\n",
            "    # Other control characters\n",
            "    elif char in (0x0000, 0x001f):\n",
            "        return 0\n",
            "    # Take a guess\n",
            "    return 1\n",
            "\n",
            "def _str_block_width(val):\n",
            "\n",
            "    return sum(itermap(_char_block_width, itermap(ord, _re.sub(\"\", val))))\n",
            "\n",
            "##############################\n",
            "# TABLE FACTORIES            #\n",
            "##############################\n",
            "\n",
            "def from_csv(fp, field_names = None, **kwargs):\n",
            "\n",
            "    dialect = csv.Sniffer().sniff(fp.read(1024))\n",
            "    fp.seek(0)\n",
            "    reader = csv.reader(fp, dialect)\n",
            "\n",
            "    table = PrettyTable(**kwargs)\n",
            "    if field_names:\n",
            "        table.field_names = field_names\n",
            "    else:\n",
            "        if py3k:\n",
            "            table.field_names = [x.strip() for x in next(reader)]\n",
            "        else:\n",
            "            table.field_names = [x.strip() for x in reader.next()]\n",
            "\n",
            "    for row in reader:\n",
            "        table.add_row([x.strip() for x in row])\n",
            "\n",
            "    return table\n",
            "\n",
            "def from_db_cursor(cursor, **kwargs):\n",
            "\n",
            "    if cursor.description:\n",
            "        table = PrettyTable(**kwargs)\n",
            "        table.field_names = [col[0] for col in cursor.description]\n",
            "        for row in cursor.fetchall():\n",
            "            table.add_row(row)\n",
            "        return table\n",
            "\n",
            "class TableHandler(HTMLParser):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        HTMLParser.__init__(self)\n",
            "        self.kwargs = kwargs\n",
            "        self.tables = []\n",
            "        self.last_row = []\n",
            "        self.rows = []\n",
            "        self.max_row_width = 0\n",
            "        self.active = None\n",
            "        self.last_content = \"\"\n",
            "        self.is_last_row_header = False\n",
            "\n",
            "    def handle_starttag(self,tag, attrs):\n",
            "        self.active = tag\n",
            "        if tag == \"th\":\n",
            "            self.is_last_row_header = True\n",
            "\n",
            "    def handle_endtag(self,tag):\n",
            "        if tag in [\"th\", \"td\"]:\n",
            "            stripped_content = self.last_content.strip()\n",
            "            self.last_row.append(stripped_content)\n",
            "        if tag == \"tr\":\n",
            "            self.rows.append(\n",
            "                (self.last_row, self.is_last_row_header))\n",
            "            self.max_row_width = max(self.max_row_width, len(self.last_row))\n",
            "            self.last_row = []\n",
            "            self.is_last_row_header = False\n",
            "        if tag == \"table\":\n",
            "            table = self.generate_table(self.rows)\n",
            "            self.tables.append(table)\n",
            "            self.rows = []\n",
            "        self.last_content = \" \"\n",
            "        self.active = None\n",
            "\n",
            "\n",
            "    def handle_data(self, data):\n",
            "        self.last_content += data\n",
            "\n",
            "    def generate_table(self, rows):\n",
            "        \"\"\"\n",
            "        Generates from a list of rows a PrettyTable object.\n",
            "        \"\"\"\n",
            "        table = PrettyTable(**self.kwargs)\n",
            "        for row in self.rows:\n",
            "            if len(row[0]) < self.max_row_width:\n",
            "                appends = self.max_row_width - len(row[0])\n",
            "                for i in range(1,appends):\n",
            "                    row[0].append(\"-\")\n",
            "\n",
            "            if row[1] == True:\n",
            "                self.make_fields_unique(row[0])\n",
            "                table.field_names = row[0]\n",
            "            else:\n",
            "                table.add_row(row[0])\n",
            "        return table\n",
            "\n",
            "    def make_fields_unique(self, fields):\n",
            "        \"\"\"\n",
            "        iterates over the row and make each field unique\n",
            "        \"\"\"\n",
            "        for i in range(0, len(fields)):\n",
            "            for j in range(i+1, len(fields)):\n",
            "                if fields[i] == fields[j]:\n",
            "                    fields[j] += \"'\"\n",
            "\n",
            "def from_html(html_code, **kwargs):\n",
            "    \"\"\"\n",
            "    Generates a list of PrettyTables from a string of HTML code. Each <table> in\n",
            "    the HTML becomes one PrettyTable object.\n",
            "    \"\"\"\n",
            "\n",
            "    parser = TableHandler(**kwargs)\n",
            "    parser.feed(html_code)\n",
            "    return parser.tables\n",
            "\n",
            "def from_html_one(html_code, **kwargs):\n",
            "    \"\"\"\n",
            "    Generates a PrettyTables from a string of HTML code which contains only a\n",
            "    single <table>\n",
            "    \"\"\"\n",
            "\n",
            "    tables = from_html(html_code, **kwargs)\n",
            "    try:\n",
            "        assert len(tables) == 1\n",
            "    except AssertionError:\n",
            "        raise Exception(\"More than one <table> in provided HTML code!  Use from_html instead.\")\n",
            "    return tables[0]\n",
            "\n",
            "##############################\n",
            "# MAIN (TEST FUNCTION)       #\n",
            "##############################\n",
            "\n",
            "def main():\n",
            "\n",
            "    x = PrettyTable([\"City name\", \"Area\", \"Population\", \"Annual Rainfall\"])\n",
            "    x.sortby = \"Population\"\n",
            "    x.reversesort = True\n",
            "    x.int_format[\"Area\"] = \"04d\"\n",
            "    x.float_format = \"6.1f\"\n",
            "    x.align[\"City name\"] = \"l\" # Left align city names\n",
            "    x.add_row([\"Adelaide\", 1295, 1158259, 600.5])\n",
            "    x.add_row([\"Brisbane\", 5905, 1857594, 1146.4])\n",
            "    x.add_row([\"Darwin\", 112, 120900, 1714.7])\n",
            "    x.add_row([\"Hobart\", 1357, 205556, 619.5])\n",
            "    x.add_row([\"Sydney\", 2058, 4336374, 1214.8])\n",
            "    x.add_row([\"Melbourne\", 1566, 3806092, 646.9])\n",
            "    x.add_row([\"Perth\", 5386, 1554769, 869.4])\n",
            "    print(x)\n",
            "    \n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "## Copyright 2013-2014 Ray Holder\n",
            "##\n",
            "## Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "## you may not use this file except in compliance with the License.\n",
            "## You may obtain a copy of the License at\n",
            "##\n",
            "## http://www.apache.org/licenses/LICENSE-2.0\n",
            "##\n",
            "## Unless required by applicable law or agreed to in writing, software\n",
            "## distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "## See the License for the specific language governing permissions and\n",
            "## limitations under the License.\n",
            "\n",
            "import random\n",
            "import six\n",
            "import sys\n",
            "import time\n",
            "import traceback\n",
            "\n",
            "\n",
            "# sys.maxint / 2, since Python 3.2 doesn't have a sys.maxint...\n",
            "MAX_WAIT = 1073741823\n",
            "\n",
            "\n",
            "def retry(*dargs, **dkw):\n",
            "    \"\"\"\n",
            "    Decorator function that instantiates the Retrying object\n",
            "    @param *dargs: positional arguments passed to Retrying object\n",
            "    @param **dkw: keyword arguments passed to the Retrying object\n",
            "    \"\"\"\n",
            "    # support both @retry and @retry() as valid syntax\n",
            "    if len(dargs) == 1 and callable(dargs[0]):\n",
            "        def wrap_simple(f):\n",
            "\n",
            "            @six.wraps(f)\n",
            "            def wrapped_f(*args, **kw):\n",
            "                return Retrying().call(f, *args, **kw)\n",
            "\n",
            "            return wrapped_f\n",
            "\n",
            "        return wrap_simple(dargs[0])\n",
            "\n",
            "    else:\n",
            "        def wrap(f):\n",
            "\n",
            "            @six.wraps(f)\n",
            "            def wrapped_f(*args, **kw):\n",
            "                return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
            "\n",
            "            return wrapped_f\n",
            "\n",
            "        return wrap\n",
            "\n",
            "\n",
            "class Retrying(object):\n",
            "\n",
            "    def __init__(self,\n",
            "                 stop=None, wait=None,\n",
            "                 stop_max_attempt_number=None,\n",
            "                 stop_max_delay=None,\n",
            "                 wait_fixed=None,\n",
            "                 wait_random_min=None, wait_random_max=None,\n",
            "                 wait_incrementing_start=None, wait_incrementing_increment=None,\n",
            "                 wait_exponential_multiplier=None, wait_exponential_max=None,\n",
            "                 retry_on_exception=None,\n",
            "                 retry_on_result=None,\n",
            "                 wrap_exception=False,\n",
            "                 stop_func=None,\n",
            "                 wait_func=None,\n",
            "                 wait_jitter_max=None):\n",
            "\n",
            "        self._stop_max_attempt_number = 5 if stop_max_attempt_number is None else stop_max_attempt_number\n",
            "        self._stop_max_delay = 100 if stop_max_delay is None else stop_max_delay\n",
            "        self._wait_fixed = 1000 if wait_fixed is None else wait_fixed\n",
            "        self._wait_random_min = 0 if wait_random_min is None else wait_random_min\n",
            "        self._wait_random_max = 1000 if wait_random_max is None else wait_random_max\n",
            "        self._wait_incrementing_start = 0 if wait_incrementing_start is None else wait_incrementing_start\n",
            "        self._wait_incrementing_increment = 100 if wait_incrementing_increment is None else wait_incrementing_increment\n",
            "        self._wait_exponential_multiplier = 1 if wait_exponential_multiplier is None else wait_exponential_multiplier\n",
            "        self._wait_exponential_max = MAX_WAIT if wait_exponential_max is None else wait_exponential_max\n",
            "        self._wait_jitter_max = 0 if wait_jitter_max is None else wait_jitter_max\n",
            "\n",
            "        # TODO add chaining of stop behaviors\n",
            "        # stop behavior\n",
            "        stop_funcs = []\n",
            "        if stop_max_attempt_number is not None:\n",
            "            stop_funcs.append(self.stop_after_attempt)\n",
            "\n",
            "        if stop_max_delay is not None:\n",
            "            stop_funcs.append(self.stop_after_delay)\n",
            "\n",
            "        if stop_func is not None:\n",
            "            self.stop = stop_func\n",
            "\n",
            "        elif stop is None:\n",
            "            self.stop = lambda attempts, delay: any(f(attempts, delay) for f in stop_funcs)\n",
            "\n",
            "        else:\n",
            "            self.stop = getattr(self, stop)\n",
            "\n",
            "        # TODO add chaining of wait behaviors\n",
            "        # wait behavior\n",
            "        wait_funcs = [lambda *args, **kwargs: 0]\n",
            "        if wait_fixed is not None:\n",
            "            wait_funcs.append(self.fixed_sleep)\n",
            "\n",
            "        if wait_random_min is not None or wait_random_max is not None:\n",
            "            wait_funcs.append(self.random_sleep)\n",
            "\n",
            "        if wait_incrementing_start is not None or wait_incrementing_increment is not None:\n",
            "            wait_funcs.append(self.incrementing_sleep)\n",
            "\n",
            "        if wait_exponential_multiplier is not None or wait_exponential_max is not None:\n",
            "            wait_funcs.append(self.exponential_sleep)\n",
            "\n",
            "        if wait_func is not None:\n",
            "            self.wait = wait_func\n",
            "\n",
            "        elif wait is None:\n",
            "            self.wait = lambda attempts, delay: max(f(attempts, delay) for f in wait_funcs)\n",
            "\n",
            "        else:\n",
            "            self.wait = getattr(self, wait)\n",
            "\n",
            "        # retry on exception filter\n",
            "        if retry_on_exception is None:\n",
            "            self._retry_on_exception = self.always_reject\n",
            "        else:\n",
            "            self._retry_on_exception = retry_on_exception\n",
            "\n",
            "        # TODO simplify retrying by Exception types\n",
            "        # retry on result filter\n",
            "        if retry_on_result is None:\n",
            "            self._retry_on_result = self.never_reject\n",
            "        else:\n",
            "            self._retry_on_result = retry_on_result\n",
            "\n",
            "        self._wrap_exception = wrap_exception\n",
            "\n",
            "    def stop_after_attempt(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"Stop after the previous attempt >= stop_max_attempt_number.\"\"\"\n",
            "        return previous_attempt_number >= self._stop_max_attempt_number\n",
            "\n",
            "    def stop_after_delay(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"Stop after the time from the first attempt >= stop_max_delay.\"\"\"\n",
            "        return delay_since_first_attempt_ms >= self._stop_max_delay\n",
            "\n",
            "    def no_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"Don't sleep at all before retrying.\"\"\"\n",
            "        return 0\n",
            "\n",
            "    def fixed_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"Sleep a fixed amount of time between each retry.\"\"\"\n",
            "        return self._wait_fixed\n",
            "\n",
            "    def random_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"Sleep a random amount of time between wait_random_min and wait_random_max\"\"\"\n",
            "        return random.randint(self._wait_random_min, self._wait_random_max)\n",
            "\n",
            "    def incrementing_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        \"\"\"\n",
            "        Sleep an incremental amount of time after each attempt, starting at\n",
            "        wait_incrementing_start and incrementing by wait_incrementing_increment\n",
            "        \"\"\"\n",
            "        result = self._wait_incrementing_start + (self._wait_incrementing_increment * (previous_attempt_number - 1))\n",
            "        if result < 0:\n",
            "            result = 0\n",
            "        return result\n",
            "\n",
            "    def exponential_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n",
            "        exp = 2 ** previous_attempt_number\n",
            "        result = self._wait_exponential_multiplier * exp\n",
            "        if result > self._wait_exponential_max:\n",
            "            result = self._wait_exponential_max\n",
            "        if result < 0:\n",
            "            result = 0\n",
            "        return result\n",
            "\n",
            "    def never_reject(self, result):\n",
            "        return False\n",
            "\n",
            "    def always_reject(self, result):\n",
            "        return True\n",
            "\n",
            "    def should_reject(self, attempt):\n",
            "        reject = False\n",
            "        if attempt.has_exception:\n",
            "            reject |= self._retry_on_exception(attempt.value[1])\n",
            "        else:\n",
            "            reject |= self._retry_on_result(attempt.value)\n",
            "\n",
            "        return reject\n",
            "\n",
            "    def call(self, fn, *args, **kwargs):\n",
            "        start_time = int(round(time.time() * 1000))\n",
            "        attempt_number = 1\n",
            "        while True:\n",
            "            try:\n",
            "                attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
            "            except:\n",
            "                tb = sys.exc_info()\n",
            "                attempt = Attempt(tb, attempt_number, True)\n",
            "\n",
            "            if not self.should_reject(attempt):\n",
            "                return attempt.get(self._wrap_exception)\n",
            "\n",
            "            delay_since_first_attempt_ms = int(round(time.time() * 1000)) - start_time\n",
            "            if self.stop(attempt_number, delay_since_first_attempt_ms):\n",
            "                if not self._wrap_exception and attempt.has_exception:\n",
            "                    # get() on an attempt with an exception should cause it to be raised, but raise just in case\n",
            "                    raise attempt.get()\n",
            "                else:\n",
            "                    raise RetryError(attempt)\n",
            "            else:\n",
            "                sleep = self.wait(attempt_number, delay_since_first_attempt_ms)\n",
            "                if self._wait_jitter_max:\n",
            "                    jitter = random.random() * self._wait_jitter_max\n",
            "                    sleep = sleep + max(0, jitter)\n",
            "                time.sleep(sleep / 1000.0)\n",
            "\n",
            "            attempt_number += 1\n",
            "\n",
            "\n",
            "class Attempt(object):\n",
            "    \"\"\"\n",
            "    An Attempt encapsulates a call to a target function that may end as a\n",
            "    normal return value from the function or an Exception depending on what\n",
            "    occurred during the execution.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, value, attempt_number, has_exception):\n",
            "        self.value = value\n",
            "        self.attempt_number = attempt_number\n",
            "        self.has_exception = has_exception\n",
            "\n",
            "    def get(self, wrap_exception=False):\n",
            "        \"\"\"\n",
            "        Return the return value of this Attempt instance or raise an Exception.\n",
            "        If wrap_exception is true, this Attempt is wrapped inside of a\n",
            "        RetryError before being raised.\n",
            "        \"\"\"\n",
            "        if self.has_exception:\n",
            "            if wrap_exception:\n",
            "                raise RetryError(self)\n",
            "            else:\n",
            "                six.reraise(self.value[0], self.value[1], self.value[2])\n",
            "        else:\n",
            "            return self.value\n",
            "\n",
            "    def __repr__(self):\n",
            "        if self.has_exception:\n",
            "            return \"Attempts: {0}, Error:\\n{1}\".format(self.attempt_number, \"\".join(traceback.format_tb(self.value[2])))\n",
            "        else:\n",
            "            return \"Attempts: {0}, Value: {1}\".format(self.attempt_number, self.value)\n",
            "\n",
            "\n",
            "class RetryError(Exception):\n",
            "    \"\"\"\n",
            "    A RetryError encapsulates the last Attempt instance right before giving up.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, last_attempt):\n",
            "        self.last_attempt = last_attempt\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"RetryError[{0}]\".format(self.last_attempt)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 Google Inc.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#      http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "\"\"\"Transport adapter for httplib2.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "\n",
            "import logging\n",
            "\n",
            "from google.auth import exceptions\n",
            "from google.auth import transport\n",
            "import httplib2\n",
            "\n",
            "\n",
            "_LOGGER = logging.getLogger(__name__)\n",
            "# Properties present in file-like streams / buffers.\n",
            "_STREAM_PROPERTIES = ('read', 'seek', 'tell')\n",
            "\n",
            "\n",
            "class _Response(transport.Response):\n",
            "    \"\"\"httplib2 transport response adapter.\n",
            "\n",
            "    Args:\n",
            "        response (httplib2.Response): The raw httplib2 response.\n",
            "        data (bytes): The response body.\n",
            "    \"\"\"\n",
            "    def __init__(self, response, data):\n",
            "        self._response = response\n",
            "        self._data = data\n",
            "\n",
            "    @property\n",
            "    def status(self):\n",
            "        \"\"\"int: The HTTP status code.\"\"\"\n",
            "        return self._response.status\n",
            "\n",
            "    @property\n",
            "    def headers(self):\n",
            "        \"\"\"Mapping[str, str]: The HTTP response headers.\"\"\"\n",
            "        return dict(self._response)\n",
            "\n",
            "    @property\n",
            "    def data(self):\n",
            "        \"\"\"bytes: The response body.\"\"\"\n",
            "        return self._data\n",
            "\n",
            "\n",
            "class Request(transport.Request):\n",
            "    \"\"\"httplib2 request adapter.\n",
            "\n",
            "    This class is used internally for making requests using various transports\n",
            "    in a consistent way. If you use :class:`AuthorizedHttp` you do not need\n",
            "    to construct or use this class directly.\n",
            "\n",
            "    This class can be useful if you want to manually refresh a\n",
            "    :class:`~google.auth.credentials.Credentials` instance::\n",
            "\n",
            "        import google_auth_httplib2\n",
            "        import httplib2\n",
            "\n",
            "        http = httplib2.Http()\n",
            "        request = google_auth_httplib2.Request(http)\n",
            "\n",
            "        credentials.refresh(request)\n",
            "\n",
            "    Args:\n",
            "        http (httplib2.Http): The underlying http object to use to make\n",
            "            requests.\n",
            "\n",
            "    .. automethod:: __call__\n",
            "    \"\"\"\n",
            "    def __init__(self, http):\n",
            "        self.http = http\n",
            "\n",
            "    def __call__(self, url, method='GET', body=None, headers=None,\n",
            "                 timeout=None, **kwargs):\n",
            "        \"\"\"Make an HTTP request using httplib2.\n",
            "\n",
            "        Args:\n",
            "            url (str): The URI to be requested.\n",
            "            method (str): The HTTP method to use for the request. Defaults\n",
            "                to 'GET'.\n",
            "            body (bytes): The payload / body in HTTP request.\n",
            "            headers (Mapping[str, str]): Request headers.\n",
            "            timeout (Optional[int]): The number of seconds to wait for a\n",
            "                response from the server. This is ignored by httplib2 and will\n",
            "                issue a warning.\n",
            "            kwargs: Additional arguments passed throught to the underlying\n",
            "                :meth:`httplib2.Http.request` method.\n",
            "\n",
            "        Returns:\n",
            "            google.auth.transport.Response: The HTTP response.\n",
            "\n",
            "        Raises:\n",
            "            google.auth.exceptions.TransportError: If any exception occurred.\n",
            "        \"\"\"\n",
            "        if timeout is not None:\n",
            "            _LOGGER.warning(\n",
            "                'httplib2 transport does not support per-request timeout. '\n",
            "                'Set the timeout when constructing the httplib2.Http instance.'\n",
            "            )\n",
            "\n",
            "        try:\n",
            "            _LOGGER.debug('Making request: %s %s', method, url)\n",
            "            response, data = self.http.request(\n",
            "                url, method=method, body=body, headers=headers, **kwargs)\n",
            "            return _Response(response, data)\n",
            "        except httplib2.HttpLib2Error as exc:\n",
            "            raise exceptions.TransportError(exc)\n",
            "\n",
            "\n",
            "def _make_default_http():\n",
            "    \"\"\"Returns a default httplib2.Http instance.\"\"\"\n",
            "    return httplib2.Http()\n",
            "\n",
            "\n",
            "class AuthorizedHttp(object):\n",
            "    \"\"\"A httplib2 HTTP class with credentials.\n",
            "\n",
            "    This class is used to perform requests to API endpoints that require\n",
            "    authorization::\n",
            "\n",
            "        from google.auth.transport._httplib2 import AuthorizedHttp\n",
            "\n",
            "        authed_http = AuthorizedHttp(credentials)\n",
            "\n",
            "        response = authed_http.request(\n",
            "            'https://www.googleapis.com/storage/v1/b')\n",
            "\n",
            "    This class implements :meth:`request` in the same way as\n",
            "    :class:`httplib2.Http` and can usually be used just like any other\n",
            "    instance of :class:``httplib2.Http`.\n",
            "\n",
            "    The underlying :meth:`request` implementation handles adding the\n",
            "    credentials' headers to the request and refreshing credentials as needed.\n",
            "    \"\"\"\n",
            "    def __init__(self, credentials, http=None,\n",
            "                 refresh_status_codes=transport.DEFAULT_REFRESH_STATUS_CODES,\n",
            "                 max_refresh_attempts=transport.DEFAULT_MAX_REFRESH_ATTEMPTS):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            credentials (google.auth.credentials.Credentials): The credentials\n",
            "                to add to the request.\n",
            "            http (httplib2.Http): The underlying HTTP object to\n",
            "                use to make requests. If not specified, a\n",
            "                :class:`httplib2.Http` instance will be constructed.\n",
            "            refresh_status_codes (Sequence[int]): Which HTTP status codes\n",
            "                indicate that credentials should be refreshed and the request\n",
            "                should be retried.\n",
            "            max_refresh_attempts (int): The maximum number of times to attempt\n",
            "                to refresh the credentials and retry the request.\n",
            "        \"\"\"\n",
            "\n",
            "        if http is None:\n",
            "            http = _make_default_http()\n",
            "\n",
            "        self.http = http\n",
            "        self.credentials = credentials\n",
            "        self._refresh_status_codes = refresh_status_codes\n",
            "        self._max_refresh_attempts = max_refresh_attempts\n",
            "        # Request instance used by internal methods (for example,\n",
            "        # credentials.refresh).\n",
            "        self._request = Request(self.http)\n",
            "\n",
            "    def request(self, uri, method='GET', body=None, headers=None,\n",
            "                **kwargs):\n",
            "        \"\"\"Implementation of httplib2's Http.request.\"\"\"\n",
            "\n",
            "        _credential_refresh_attempt = kwargs.pop(\n",
            "            '_credential_refresh_attempt', 0)\n",
            "\n",
            "        # Make a copy of the headers. They will be modified by the credentials\n",
            "        # and we want to pass the original headers if we recurse.\n",
            "        request_headers = headers.copy() if headers is not None else {}\n",
            "\n",
            "        self.credentials.before_request(\n",
            "            self._request, method, uri, request_headers)\n",
            "\n",
            "        # Check if the body is a file-like stream, and if so, save the body\n",
            "        # stream position so that it can be restored in case of refresh.\n",
            "        body_stream_position = None\n",
            "        if all(getattr(body, stream_prop, None) for stream_prop in\n",
            "               _STREAM_PROPERTIES):\n",
            "            body_stream_position = body.tell()\n",
            "\n",
            "        # Make the request.\n",
            "        response, content = self.http.request(\n",
            "            uri, method, body=body, headers=request_headers, **kwargs)\n",
            "\n",
            "        # If the response indicated that the credentials needed to be\n",
            "        # refreshed, then refresh the credentials and re-attempt the\n",
            "        # request.\n",
            "        # A stored token may expire between the time it is retrieved and\n",
            "        # the time the request is made, so we may need to try twice.\n",
            "        if (response.status in self._refresh_status_codes\n",
            "                and _credential_refresh_attempt < self._max_refresh_attempts):\n",
            "\n",
            "            _LOGGER.info(\n",
            "                'Refreshing credentials due to a %s response. Attempt %s/%s.',\n",
            "                response.status, _credential_refresh_attempt + 1,\n",
            "                self._max_refresh_attempts)\n",
            "\n",
            "            self.credentials.refresh(self._request)\n",
            "\n",
            "            # Restore the body's stream position if needed.\n",
            "            if body_stream_position is not None:\n",
            "                body.seek(body_stream_position)\n",
            "\n",
            "            # Recurse. Pass in the original headers, not our modified set.\n",
            "            return self.request(\n",
            "                uri, method, body=body, headers=headers,\n",
            "                _credential_refresh_attempt=_credential_refresh_attempt + 1,\n",
            "                **kwargs)\n",
            "\n",
            "        return response, content\n",
            "\n",
            "    @property\n",
            "    def connections(self):\n",
            "        \"\"\"Proxy to httplib2.Http.connections.\"\"\"\n",
            "        return self.http.connections\n",
            "\n",
            "    @connections.setter\n",
            "    def connections(self, value):\n",
            "        \"\"\"Proxy to httplib2.Http.connections.\"\"\"\n",
            "        self.http.connections = value\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding: utf-8 -*-\n",
            "# module pyparsing.py\n",
            "#\n",
            "# Copyright (c) 2003-2019  Paul T. McGuire\n",
            "#\n",
            "# Permission is hereby granted, free of charge, to any person obtaining\n",
            "# a copy of this software and associated documentation files (the\n",
            "# \"Software\"), to deal in the Software without restriction, including\n",
            "# without limitation the rights to use, copy, modify, merge, publish,\n",
            "# distribute, sublicense, and/or sell copies of the Software, and to\n",
            "# permit persons to whom the Software is furnished to do so, subject to\n",
            "# the following conditions:\n",
            "#\n",
            "# The above copyright notice and this permission notice shall be\n",
            "# included in all copies or substantial portions of the Software.\n",
            "#\n",
            "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
            "# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
            "# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
            "# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
            "# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
            "# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
            "# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
            "#\n",
            "\n",
            "__doc__ = \\\n",
            "\"\"\"\n",
            "pyparsing module - Classes and methods to define and execute parsing grammars\n",
            "=============================================================================\n",
            "\n",
            "The pyparsing module is an alternative approach to creating and\n",
            "executing simple grammars, vs. the traditional lex/yacc approach, or the\n",
            "use of regular expressions.  With pyparsing, you don't need to learn\n",
            "a new syntax for defining grammars or matching expressions - the parsing\n",
            "module provides a library of classes that you use to construct the\n",
            "grammar directly in Python.\n",
            "\n",
            "Here is a program to parse \"Hello, World!\" (or any greeting of the form\n",
            "``\"<salutation>, <addressee>!\"``), built up using :class:`Word`,\n",
            ":class:`Literal`, and :class:`And` elements\n",
            "(the :class:`'+'<ParserElement.__add__>` operators create :class:`And` expressions,\n",
            "and the strings are auto-converted to :class:`Literal` expressions)::\n",
            "\n",
            "    from pyparsing import Word, alphas\n",
            "\n",
            "    # define grammar of a greeting\n",
            "    greet = Word(alphas) + \",\" + Word(alphas) + \"!\"\n",
            "\n",
            "    hello = \"Hello, World!\"\n",
            "    print (hello, \"->\", greet.parseString(hello))\n",
            "\n",
            "The program outputs the following::\n",
            "\n",
            "    Hello, World! -> ['Hello', ',', 'World', '!']\n",
            "\n",
            "The Python representation of the grammar is quite readable, owing to the\n",
            "self-explanatory class names, and the use of '+', '|' and '^' operators.\n",
            "\n",
            "The :class:`ParseResults` object returned from\n",
            ":class:`ParserElement.parseString` can be\n",
            "accessed as a nested list, a dictionary, or an object with named\n",
            "attributes.\n",
            "\n",
            "The pyparsing module handles some of the problems that are typically\n",
            "vexing when writing text parsers:\n",
            "\n",
            "  - extra or missing whitespace (the above program will also handle\n",
            "    \"Hello,World!\", \"Hello  ,  World  !\", etc.)\n",
            "  - quoted strings\n",
            "  - embedded comments\n",
            "\n",
            "\n",
            "Getting Started -\n",
            "-----------------\n",
            "Visit the classes :class:`ParserElement` and :class:`ParseResults` to\n",
            "see the base classes that most other pyparsing\n",
            "classes inherit from. Use the docstrings for examples of how to:\n",
            "\n",
            " - construct literal match expressions from :class:`Literal` and\n",
            "   :class:`CaselessLiteral` classes\n",
            " - construct character word-group expressions using the :class:`Word`\n",
            "   class\n",
            " - see how to create repetitive expressions using :class:`ZeroOrMore`\n",
            "   and :class:`OneOrMore` classes\n",
            " - use :class:`'+'<And>`, :class:`'|'<MatchFirst>`, :class:`'^'<Or>`,\n",
            "   and :class:`'&'<Each>` operators to combine simple expressions into\n",
            "   more complex ones\n",
            " - associate names with your parsed results using\n",
            "   :class:`ParserElement.setResultsName`\n",
            " - access the parsed data, which is returned as a :class:`ParseResults`\n",
            "   object\n",
            " - find some helpful expression short-cuts like :class:`delimitedList`\n",
            "   and :class:`oneOf`\n",
            " - find more useful common expressions in the :class:`pyparsing_common`\n",
            "   namespace class\n",
            "\"\"\"\n",
            "\n",
            "__version__ = \"2.4.2\"\n",
            "__versionTime__ = \"29 Jul 2019 02:58 UTC\"\n",
            "__author__ = \"Paul McGuire <ptmcg@users.sourceforge.net>\"\n",
            "\n",
            "import string\n",
            "from weakref import ref as wkref\n",
            "import copy\n",
            "import sys\n",
            "import warnings\n",
            "import re\n",
            "import sre_constants\n",
            "import collections\n",
            "import pprint\n",
            "import traceback\n",
            "import types\n",
            "from datetime import datetime\n",
            "from operator import itemgetter\n",
            "import itertools\n",
            "from functools import wraps\n",
            "\n",
            "try:\n",
            "    # Python 3\n",
            "    from itertools import filterfalse\n",
            "except ImportError:\n",
            "    from itertools import ifilterfalse as filterfalse\n",
            "\n",
            "try:\n",
            "    from _thread import RLock\n",
            "except ImportError:\n",
            "    from threading import RLock\n",
            "\n",
            "try:\n",
            "    # Python 3\n",
            "    from collections.abc import Iterable\n",
            "    from collections.abc import MutableMapping, Mapping\n",
            "except ImportError:\n",
            "    # Python 2.7\n",
            "    from collections import Iterable\n",
            "    from collections import MutableMapping, Mapping\n",
            "\n",
            "try:\n",
            "    from collections import OrderedDict as _OrderedDict\n",
            "except ImportError:\n",
            "    try:\n",
            "        from ordereddict import OrderedDict as _OrderedDict\n",
            "    except ImportError:\n",
            "        _OrderedDict = None\n",
            "\n",
            "try:\n",
            "    from types import SimpleNamespace\n",
            "except ImportError:\n",
            "    class SimpleNamespace: pass\n",
            "\n",
            "# version compatibility configuration\n",
            "__compat__ = SimpleNamespace()\n",
            "__compat__.__doc__ = \"\"\"\n",
            "    A cross-version compatibility configuration for pyparsing features that will be\n",
            "    released in a future version. By setting values in this configuration to True,\n",
            "    those features can be enabled in prior versions for compatibility development\n",
            "    and testing.\n",
            "\n",
            "     - collect_all_And_tokens - flag to enable fix for Issue #63 that fixes erroneous grouping\n",
            "       of results names when an And expression is nested within an Or or MatchFirst; set to\n",
            "       True to enable bugfix released in pyparsing 2.3.0, or False to preserve\n",
            "       pre-2.3.0 handling of named results\n",
            "\"\"\"\n",
            "__compat__.collect_all_And_tokens = True\n",
            "\n",
            "__diag__ = SimpleNamespace()\n",
            "__diag__.__doc__ = \"\"\"\n",
            "Diagnostic configuration (all default to False)\n",
            "     - warn_multiple_tokens_in_named_alternation - flag to enable warnings when a results\n",
            "       name is defined on a MatchFirst or Or expression with one or more And subexpressions\n",
            "       (only warns if __compat__.collect_all_And_tokens is False)\n",
            "     - warn_ungrouped_named_tokens_in_collection - flag to enable warnings when a results\n",
            "       name is defined on a containing expression with ungrouped subexpressions that also\n",
            "       have results names\n",
            "     - warn_name_set_on_empty_Forward - flag to enable warnings whan a Forward is defined\n",
            "       with a results name, but has no contents defined\n",
            "     - warn_on_multiple_string_args_to_oneof - flag to enable warnings whan oneOf is\n",
            "       incorrectly called with multiple str arguments\n",
            "     - enable_debug_on_named_expressions - flag to auto-enable debug on all subsequent\n",
            "       calls to ParserElement.setName()\n",
            "\"\"\"\n",
            "__diag__.warn_multiple_tokens_in_named_alternation = False\n",
            "__diag__.warn_ungrouped_named_tokens_in_collection = False\n",
            "__diag__.warn_name_set_on_empty_Forward = False\n",
            "__diag__.warn_on_multiple_string_args_to_oneof = False\n",
            "__diag__.enable_debug_on_named_expressions = False\n",
            "\n",
            "# ~ sys.stderr.write(\"testing pyparsing module, version %s, %s\\n\" % (__version__, __versionTime__))\n",
            "\n",
            "__all__ = ['__version__', '__versionTime__', '__author__', '__compat__', '__diag__',\n",
            "           'And', 'CaselessKeyword', 'CaselessLiteral', 'CharsNotIn', 'Combine', 'Dict', 'Each', 'Empty',\n",
            "           'FollowedBy', 'Forward', 'GoToColumn', 'Group', 'Keyword', 'LineEnd', 'LineStart', 'Literal',\n",
            "           'PrecededBy', 'MatchFirst', 'NoMatch', 'NotAny', 'OneOrMore', 'OnlyOnce', 'Optional', 'Or',\n",
            "           'ParseBaseException', 'ParseElementEnhance', 'ParseException', 'ParseExpression', 'ParseFatalException',\n",
            "           'ParseResults', 'ParseSyntaxException', 'ParserElement', 'QuotedString', 'RecursiveGrammarException',\n",
            "           'Regex', 'SkipTo', 'StringEnd', 'StringStart', 'Suppress', 'Token', 'TokenConverter',\n",
            "           'White', 'Word', 'WordEnd', 'WordStart', 'ZeroOrMore', 'Char',\n",
            "           'alphanums', 'alphas', 'alphas8bit', 'anyCloseTag', 'anyOpenTag', 'cStyleComment', 'col',\n",
            "           'commaSeparatedList', 'commonHTMLEntity', 'countedArray', 'cppStyleComment', 'dblQuotedString',\n",
            "           'dblSlashComment', 'delimitedList', 'dictOf', 'downcaseTokens', 'empty', 'hexnums',\n",
            "           'htmlComment', 'javaStyleComment', 'line', 'lineEnd', 'lineStart', 'lineno',\n",
            "           'makeHTMLTags', 'makeXMLTags', 'matchOnlyAtCol', 'matchPreviousExpr', 'matchPreviousLiteral',\n",
            "           'nestedExpr', 'nullDebugAction', 'nums', 'oneOf', 'opAssoc', 'operatorPrecedence', 'printables',\n",
            "           'punc8bit', 'pythonStyleComment', 'quotedString', 'removeQuotes', 'replaceHTMLEntity',\n",
            "           'replaceWith', 'restOfLine', 'sglQuotedString', 'srange', 'stringEnd',\n",
            "           'stringStart', 'traceParseAction', 'unicodeString', 'upcaseTokens', 'withAttribute',\n",
            "           'indentedBlock', 'originalTextFor', 'ungroup', 'infixNotation', 'locatedExpr', 'withClass',\n",
            "           'CloseMatch', 'tokenMap', 'pyparsing_common', 'pyparsing_unicode', 'unicode_set',\n",
            "           'conditionAsParseAction',\n",
            "           ]\n",
            "\n",
            "system_version = tuple(sys.version_info)[:3]\n",
            "PY_3 = system_version[0] == 3\n",
            "if PY_3:\n",
            "    _MAX_INT = sys.maxsize\n",
            "    basestring = str\n",
            "    unichr = chr\n",
            "    unicode = str\n",
            "    _ustr = str\n",
            "\n",
            "    # build list of single arg builtins, that can be used as parse actions\n",
            "    singleArgBuiltins = [sum, len, sorted, reversed, list, tuple, set, any, all, min, max]\n",
            "\n",
            "else:\n",
            "    _MAX_INT = sys.maxint\n",
            "    range = xrange\n",
            "\n",
            "    def _ustr(obj):\n",
            "        \"\"\"Drop-in replacement for str(obj) that tries to be Unicode\n",
            "        friendly. It first tries str(obj). If that fails with\n",
            "        a UnicodeEncodeError, then it tries unicode(obj). It then\n",
            "        < returns the unicode object | encodes it with the default\n",
            "        encoding | ... >.\n",
            "        \"\"\"\n",
            "        if isinstance(obj, unicode):\n",
            "            return obj\n",
            "\n",
            "        try:\n",
            "            # If this works, then _ustr(obj) has the same behaviour as str(obj), so\n",
            "            # it won't break any existing code.\n",
            "            return str(obj)\n",
            "\n",
            "        except UnicodeEncodeError:\n",
            "            # Else encode it\n",
            "            ret = unicode(obj).encode(sys.getdefaultencoding(), 'xmlcharrefreplace')\n",
            "            xmlcharref = Regex(r'&#\\d+;')\n",
            "            xmlcharref.setParseAction(lambda t: '\\\\u' + hex(int(t[0][2:-1]))[2:])\n",
            "            return xmlcharref.transformString(ret)\n",
            "\n",
            "    # build list of single arg builtins, tolerant of Python version, that can be used as parse actions\n",
            "    singleArgBuiltins = []\n",
            "    import __builtin__\n",
            "\n",
            "    for fname in \"sum len sorted reversed list tuple set any all min max\".split():\n",
            "        try:\n",
            "            singleArgBuiltins.append(getattr(__builtin__, fname))\n",
            "        except AttributeError:\n",
            "            continue\n",
            "\n",
            "_generatorType = type((y for y in range(1)))\n",
            "\n",
            "def _xml_escape(data):\n",
            "    \"\"\"Escape &, <, >, \", ', etc. in a string of data.\"\"\"\n",
            "\n",
            "    # ampersand must be replaced first\n",
            "    from_symbols = '&><\"\\''\n",
            "    to_symbols = ('&' + s + ';' for s in \"amp gt lt quot apos\".split())\n",
            "    for from_, to_ in zip(from_symbols, to_symbols):\n",
            "        data = data.replace(from_, to_)\n",
            "    return data\n",
            "\n",
            "alphas = string.ascii_uppercase + string.ascii_lowercase\n",
            "nums = \"0123456789\"\n",
            "hexnums = nums + \"ABCDEFabcdef\"\n",
            "alphanums = alphas + nums\n",
            "_bslash = chr(92)\n",
            "printables = \"\".join(c for c in string.printable if c not in string.whitespace)\n",
            "\n",
            "\n",
            "def conditionAsParseAction(fn, message=None, fatal=False):\n",
            "    msg = message if message is not None else \"failed user-defined condition\"\n",
            "    exc_type = ParseFatalException if fatal else ParseException\n",
            "    fn = _trim_arity(fn)\n",
            "\n",
            "    @wraps(fn)\n",
            "    def pa(s, l, t):\n",
            "        if not bool(fn(s, l, t)):\n",
            "            raise exc_type(s, l, msg)\n",
            "\n",
            "    return pa\n",
            "\n",
            "class ParseBaseException(Exception):\n",
            "    \"\"\"base exception class for all parsing runtime exceptions\"\"\"\n",
            "    # Performance tuning: we construct a *lot* of these, so keep this\n",
            "    # constructor as small and fast as possible\n",
            "    def __init__(self, pstr, loc=0, msg=None, elem=None):\n",
            "        self.loc = loc\n",
            "        if msg is None:\n",
            "            self.msg = pstr\n",
            "            self.pstr = \"\"\n",
            "        else:\n",
            "            self.msg = msg\n",
            "            self.pstr = pstr\n",
            "        self.parserElement = elem\n",
            "        self.args = (pstr, loc, msg)\n",
            "\n",
            "    @classmethod\n",
            "    def _from_exception(cls, pe):\n",
            "        \"\"\"\n",
            "        internal factory method to simplify creating one type of ParseException\n",
            "        from another - avoids having __init__ signature conflicts among subclasses\n",
            "        \"\"\"\n",
            "        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)\n",
            "\n",
            "    def __getattr__(self, aname):\n",
            "        \"\"\"supported attributes by name are:\n",
            "           - lineno - returns the line number of the exception text\n",
            "           - col - returns the column number of the exception text\n",
            "           - line - returns the line containing the exception text\n",
            "        \"\"\"\n",
            "        if aname == \"lineno\":\n",
            "            return lineno(self.loc, self.pstr)\n",
            "        elif aname in (\"col\", \"column\"):\n",
            "            return col(self.loc, self.pstr)\n",
            "        elif aname == \"line\":\n",
            "            return line(self.loc, self.pstr)\n",
            "        else:\n",
            "            raise AttributeError(aname)\n",
            "\n",
            "    def __str__(self):\n",
            "        if self.pstr:\n",
            "            if self.loc >= len(self.pstr):\n",
            "                foundstr = ', found end of text'\n",
            "            else:\n",
            "                foundstr = (', found %r' % self.pstr[self.loc:self.loc + 1]).replace(r'\\\\', '\\\\')\n",
            "        else:\n",
            "            foundstr = ''\n",
            "        return (\"%s%s  (at char %d), (line:%d, col:%d)\" %\n",
            "                   (self.msg, foundstr, self.loc, self.lineno, self.column))\n",
            "    def __repr__(self):\n",
            "        return _ustr(self)\n",
            "    def markInputline(self, markerString=\">!<\"):\n",
            "        \"\"\"Extracts the exception line from the input string, and marks\n",
            "           the location of the exception with a special symbol.\n",
            "        \"\"\"\n",
            "        line_str = self.line\n",
            "        line_column = self.column - 1\n",
            "        if markerString:\n",
            "            line_str = \"\".join((line_str[:line_column],\n",
            "                                markerString, line_str[line_column:]))\n",
            "        return line_str.strip()\n",
            "    def __dir__(self):\n",
            "        return \"lineno col line\".split() + dir(type(self))\n",
            "\n",
            "class ParseException(ParseBaseException):\n",
            "    \"\"\"\n",
            "    Exception thrown when parse expressions don't match class;\n",
            "    supported attributes by name are:\n",
            "    - lineno - returns the line number of the exception text\n",
            "    - col - returns the column number of the exception text\n",
            "    - line - returns the line containing the exception text\n",
            "\n",
            "    Example::\n",
            "\n",
            "        try:\n",
            "            Word(nums).setName(\"integer\").parseString(\"ABC\")\n",
            "        except ParseException as pe:\n",
            "            print(pe)\n",
            "            print(\"column: {}\".format(pe.col))\n",
            "\n",
            "    prints::\n",
            "\n",
            "       Expected integer (at char 0), (line:1, col:1)\n",
            "        column: 1\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    @staticmethod\n",
            "    def explain(exc, depth=16):\n",
            "        \"\"\"\n",
            "        Method to take an exception and translate the Python internal traceback into a list\n",
            "        of the pyparsing expressions that caused the exception to be raised.\n",
            "\n",
            "        Parameters:\n",
            "\n",
            "         - exc - exception raised during parsing (need not be a ParseException, in support\n",
            "           of Python exceptions that might be raised in a parse action)\n",
            "         - depth (default=16) - number of levels back in the stack trace to list expression\n",
            "           and function names; if None, the full stack trace names will be listed; if 0, only\n",
            "           the failing input line, marker, and exception string will be shown\n",
            "\n",
            "        Returns a multi-line string listing the ParserElements and/or function names in the\n",
            "        exception's stack trace.\n",
            "\n",
            "        Note: the diagnostic output will include string representations of the expressions\n",
            "        that failed to parse. These representations will be more helpful if you use `setName` to\n",
            "        give identifiable names to your expressions. Otherwise they will use the default string\n",
            "        forms, which may be cryptic to read.\n",
            "\n",
            "        explain() is only supported under Python 3.\n",
            "        \"\"\"\n",
            "        import inspect\n",
            "\n",
            "        if depth is None:\n",
            "            depth = sys.getrecursionlimit()\n",
            "        ret = []\n",
            "        if isinstance(exc, ParseBaseException):\n",
            "            ret.append(exc.line)\n",
            "            ret.append(' ' * (exc.col - 1) + '^')\n",
            "        ret.append(\"{0}: {1}\".format(type(exc).__name__, exc))\n",
            "\n",
            "        if depth > 0:\n",
            "            callers = inspect.getinnerframes(exc.__traceback__, context=depth)\n",
            "            seen = set()\n",
            "            for i, ff in enumerate(callers[-depth:]):\n",
            "                frm = ff[0]\n",
            "\n",
            "                f_self = frm.f_locals.get('self', None)\n",
            "                if isinstance(f_self, ParserElement):\n",
            "                    if frm.f_code.co_name not in ('parseImpl', '_parseNoCache'):\n",
            "                        continue\n",
            "                    if f_self in seen:\n",
            "                        continue\n",
            "                    seen.add(f_self)\n",
            "\n",
            "                    self_type = type(f_self)\n",
            "                    ret.append(\"{0}.{1} - {2}\".format(self_type.__module__,\n",
            "                                                      self_type.__name__,\n",
            "                                                      f_self))\n",
            "                elif f_self is not None:\n",
            "                    self_type = type(f_self)\n",
            "                    ret.append(\"{0}.{1}\".format(self_type.__module__,\n",
            "                                                self_type.__name__))\n",
            "                else:\n",
            "                    code = frm.f_code\n",
            "                    if code.co_name in ('wrapper', '<module>'):\n",
            "                        continue\n",
            "\n",
            "                    ret.append(\"{0}\".format(code.co_name))\n",
            "\n",
            "                depth -= 1\n",
            "                if not depth:\n",
            "                    break\n",
            "\n",
            "        return '\\n'.join(ret)\n",
            "\n",
            "\n",
            "class ParseFatalException(ParseBaseException):\n",
            "    \"\"\"user-throwable exception thrown when inconsistent parse content\n",
            "       is found; stops all parsing immediately\"\"\"\n",
            "    pass\n",
            "\n",
            "class ParseSyntaxException(ParseFatalException):\n",
            "    \"\"\"just like :class:`ParseFatalException`, but thrown internally\n",
            "    when an :class:`ErrorStop<And._ErrorStop>` ('-' operator) indicates\n",
            "    that parsing is to stop immediately because an unbacktrackable\n",
            "    syntax error has been found.\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "#~ class ReparseException(ParseBaseException):\n",
            "    #~ \"\"\"Experimental class - parse actions can raise this exception to cause\n",
            "       #~ pyparsing to reparse the input string:\n",
            "        #~ - with a modified input string, and/or\n",
            "        #~ - with a modified start location\n",
            "       #~ Set the values of the ReparseException in the constructor, and raise the\n",
            "       #~ exception in a parse action to cause pyparsing to use the new string/location.\n",
            "       #~ Setting the values as None causes no change to be made.\n",
            "       #~ \"\"\"\n",
            "    #~ def __init_( self, newstring, restartLoc ):\n",
            "        #~ self.newParseText = newstring\n",
            "        #~ self.reparseLoc = restartLoc\n",
            "\n",
            "class RecursiveGrammarException(Exception):\n",
            "    \"\"\"exception thrown by :class:`ParserElement.validate` if the\n",
            "    grammar could be improperly recursive\n",
            "    \"\"\"\n",
            "    def __init__(self, parseElementList):\n",
            "        self.parseElementTrace = parseElementList\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"RecursiveGrammarException: %s\" % self.parseElementTrace\n",
            "\n",
            "class _ParseResultsWithOffset(object):\n",
            "    def __init__(self, p1, p2):\n",
            "        self.tup = (p1, p2)\n",
            "    def __getitem__(self, i):\n",
            "        return self.tup[i]\n",
            "    def __repr__(self):\n",
            "        return repr(self.tup[0])\n",
            "    def setOffset(self, i):\n",
            "        self.tup = (self.tup[0], i)\n",
            "\n",
            "class ParseResults(object):\n",
            "    \"\"\"Structured parse results, to provide multiple means of access to\n",
            "    the parsed data:\n",
            "\n",
            "       - as a list (``len(results)``)\n",
            "       - by list index (``results[0], results[1]``, etc.)\n",
            "       - by attribute (``results.<resultsName>`` - see :class:`ParserElement.setResultsName`)\n",
            "\n",
            "    Example::\n",
            "\n",
            "        integer = Word(nums)\n",
            "        date_str = (integer.setResultsName(\"year\") + '/'\n",
            "                        + integer.setResultsName(\"month\") + '/'\n",
            "                        + integer.setResultsName(\"day\"))\n",
            "        # equivalent form:\n",
            "        # date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "        # parseString returns a ParseResults object\n",
            "        result = date_str.parseString(\"1999/12/31\")\n",
            "\n",
            "        def test(s, fn=repr):\n",
            "            print(\"%s -> %s\" % (s, fn(eval(s))))\n",
            "        test(\"list(result)\")\n",
            "        test(\"result[0]\")\n",
            "        test(\"result['month']\")\n",
            "        test(\"result.day\")\n",
            "        test(\"'month' in result\")\n",
            "        test(\"'minutes' in result\")\n",
            "        test(\"result.dump()\", str)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        list(result) -> ['1999', '/', '12', '/', '31']\n",
            "        result[0] -> '1999'\n",
            "        result['month'] -> '12'\n",
            "        result.day -> '31'\n",
            "        'month' in result -> True\n",
            "        'minutes' in result -> False\n",
            "        result.dump() -> ['1999', '/', '12', '/', '31']\n",
            "        - day: 31\n",
            "        - month: 12\n",
            "        - year: 1999\n",
            "    \"\"\"\n",
            "    def __new__(cls, toklist=None, name=None, asList=True, modal=True):\n",
            "        if isinstance(toklist, cls):\n",
            "            return toklist\n",
            "        retobj = object.__new__(cls)\n",
            "        retobj.__doinit = True\n",
            "        return retobj\n",
            "\n",
            "    # Performance tuning: we construct a *lot* of these, so keep this\n",
            "    # constructor as small and fast as possible\n",
            "    def __init__(self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance):\n",
            "        if self.__doinit:\n",
            "            self.__doinit = False\n",
            "            self.__name = None\n",
            "            self.__parent = None\n",
            "            self.__accumNames = {}\n",
            "            self.__asList = asList\n",
            "            self.__modal = modal\n",
            "            if toklist is None:\n",
            "                toklist = []\n",
            "            if isinstance(toklist, list):\n",
            "                self.__toklist = toklist[:]\n",
            "            elif isinstance(toklist, _generatorType):\n",
            "                self.__toklist = list(toklist)\n",
            "            else:\n",
            "                self.__toklist = [toklist]\n",
            "            self.__tokdict = dict()\n",
            "\n",
            "        if name is not None and name:\n",
            "            if not modal:\n",
            "                self.__accumNames[name] = 0\n",
            "            if isinstance(name, int):\n",
            "                name = _ustr(name)  # will always return a str, but use _ustr for consistency\n",
            "            self.__name = name\n",
            "            if not (isinstance(toklist, (type(None), basestring, list)) and toklist in (None, '', [])):\n",
            "                if isinstance(toklist, basestring):\n",
            "                    toklist = [toklist]\n",
            "                if asList:\n",
            "                    if isinstance(toklist, ParseResults):\n",
            "                        self[name] = _ParseResultsWithOffset(ParseResults(toklist.__toklist), 0)\n",
            "                    else:\n",
            "                        self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]), 0)\n",
            "                    self[name].__name = name\n",
            "                else:\n",
            "                    try:\n",
            "                        self[name] = toklist[0]\n",
            "                    except (KeyError, TypeError, IndexError):\n",
            "                        self[name] = toklist\n",
            "\n",
            "    def __getitem__(self, i):\n",
            "        if isinstance(i, (int, slice)):\n",
            "            return self.__toklist[i]\n",
            "        else:\n",
            "            if i not in self.__accumNames:\n",
            "                return self.__tokdict[i][-1][0]\n",
            "            else:\n",
            "                return ParseResults([v[0] for v in self.__tokdict[i]])\n",
            "\n",
            "    def __setitem__(self, k, v, isinstance=isinstance):\n",
            "        if isinstance(v, _ParseResultsWithOffset):\n",
            "            self.__tokdict[k] = self.__tokdict.get(k, list()) + [v]\n",
            "            sub = v[0]\n",
            "        elif isinstance(k, (int, slice)):\n",
            "            self.__toklist[k] = v\n",
            "            sub = v\n",
            "        else:\n",
            "            self.__tokdict[k] = self.__tokdict.get(k, list()) + [_ParseResultsWithOffset(v, 0)]\n",
            "            sub = v\n",
            "        if isinstance(sub, ParseResults):\n",
            "            sub.__parent = wkref(self)\n",
            "\n",
            "    def __delitem__(self, i):\n",
            "        if isinstance(i, (int, slice)):\n",
            "            mylen = len(self.__toklist)\n",
            "            del self.__toklist[i]\n",
            "\n",
            "            # convert int to slice\n",
            "            if isinstance(i, int):\n",
            "                if i < 0:\n",
            "                    i += mylen\n",
            "                i = slice(i, i + 1)\n",
            "            # get removed indices\n",
            "            removed = list(range(*i.indices(mylen)))\n",
            "            removed.reverse()\n",
            "            # fixup indices in token dictionary\n",
            "            for name, occurrences in self.__tokdict.items():\n",
            "                for j in removed:\n",
            "                    for k, (value, position) in enumerate(occurrences):\n",
            "                        occurrences[k] = _ParseResultsWithOffset(value, position - (position > j))\n",
            "        else:\n",
            "            del self.__tokdict[i]\n",
            "\n",
            "    def __contains__(self, k):\n",
            "        return k in self.__tokdict\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.__toklist)\n",
            "\n",
            "    def __bool__(self):\n",
            "        return (not not self.__toklist)\n",
            "    __nonzero__ = __bool__\n",
            "\n",
            "    def __iter__(self):\n",
            "        return iter(self.__toklist)\n",
            "\n",
            "    def __reversed__(self):\n",
            "        return iter(self.__toklist[::-1])\n",
            "\n",
            "    def _iterkeys(self):\n",
            "        if hasattr(self.__tokdict, \"iterkeys\"):\n",
            "            return self.__tokdict.iterkeys()\n",
            "        else:\n",
            "            return iter(self.__tokdict)\n",
            "\n",
            "    def _itervalues(self):\n",
            "        return (self[k] for k in self._iterkeys())\n",
            "\n",
            "    def _iteritems(self):\n",
            "        return ((k, self[k]) for k in self._iterkeys())\n",
            "\n",
            "    if PY_3:\n",
            "        keys = _iterkeys\n",
            "        \"\"\"Returns an iterator of all named result keys.\"\"\"\n",
            "\n",
            "        values = _itervalues\n",
            "        \"\"\"Returns an iterator of all named result values.\"\"\"\n",
            "\n",
            "        items = _iteritems\n",
            "        \"\"\"Returns an iterator of all named result key-value tuples.\"\"\"\n",
            "\n",
            "    else:\n",
            "        iterkeys = _iterkeys\n",
            "        \"\"\"Returns an iterator of all named result keys (Python 2.x only).\"\"\"\n",
            "\n",
            "        itervalues = _itervalues\n",
            "        \"\"\"Returns an iterator of all named result values (Python 2.x only).\"\"\"\n",
            "\n",
            "        iteritems = _iteritems\n",
            "        \"\"\"Returns an iterator of all named result key-value tuples (Python 2.x only).\"\"\"\n",
            "\n",
            "        def keys(self):\n",
            "            \"\"\"Returns all named result keys (as a list in Python 2.x, as an iterator in Python 3.x).\"\"\"\n",
            "            return list(self.iterkeys())\n",
            "\n",
            "        def values(self):\n",
            "            \"\"\"Returns all named result values (as a list in Python 2.x, as an iterator in Python 3.x).\"\"\"\n",
            "            return list(self.itervalues())\n",
            "\n",
            "        def items(self):\n",
            "            \"\"\"Returns all named result key-values (as a list of tuples in Python 2.x, as an iterator in Python 3.x).\"\"\"\n",
            "            return list(self.iteritems())\n",
            "\n",
            "    def haskeys(self):\n",
            "        \"\"\"Since keys() returns an iterator, this method is helpful in bypassing\n",
            "           code that looks for the existence of any defined results names.\"\"\"\n",
            "        return bool(self.__tokdict)\n",
            "\n",
            "    def pop(self, *args, **kwargs):\n",
            "        \"\"\"\n",
            "        Removes and returns item at specified index (default= ``last``).\n",
            "        Supports both ``list`` and ``dict`` semantics for ``pop()``. If\n",
            "        passed no argument or an integer argument, it will use ``list``\n",
            "        semantics and pop tokens from the list of parsed tokens. If passed\n",
            "        a non-integer argument (most likely a string), it will use ``dict``\n",
            "        semantics and pop the corresponding value from any defined results\n",
            "        names. A second default return value argument is supported, just as in\n",
            "        ``dict.pop()``.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            def remove_first(tokens):\n",
            "                tokens.pop(0)\n",
            "            print(OneOrMore(Word(nums)).parseString(\"0 123 321\")) # -> ['0', '123', '321']\n",
            "            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(\"0 123 321\")) # -> ['123', '321']\n",
            "\n",
            "            label = Word(alphas)\n",
            "            patt = label(\"LABEL\") + OneOrMore(Word(nums))\n",
            "            print(patt.parseString(\"AAB 123 321\").dump())\n",
            "\n",
            "            # Use pop() in a parse action to remove named result (note that corresponding value is not\n",
            "            # removed from list form of results)\n",
            "            def remove_LABEL(tokens):\n",
            "                tokens.pop(\"LABEL\")\n",
            "                return tokens\n",
            "            patt.addParseAction(remove_LABEL)\n",
            "            print(patt.parseString(\"AAB 123 321\").dump())\n",
            "\n",
            "        prints::\n",
            "\n",
            "            ['AAB', '123', '321']\n",
            "            - LABEL: AAB\n",
            "\n",
            "            ['AAB', '123', '321']\n",
            "        \"\"\"\n",
            "        if not args:\n",
            "            args = [-1]\n",
            "        for k, v in kwargs.items():\n",
            "            if k == 'default':\n",
            "                args = (args[0], v)\n",
            "            else:\n",
            "                raise TypeError(\"pop() got an unexpected keyword argument '%s'\" % k)\n",
            "        if (isinstance(args[0], int)\n",
            "                or len(args) == 1\n",
            "                or args[0] in self):\n",
            "            index = args[0]\n",
            "            ret = self[index]\n",
            "            del self[index]\n",
            "            return ret\n",
            "        else:\n",
            "            defaultvalue = args[1]\n",
            "            return defaultvalue\n",
            "\n",
            "    def get(self, key, defaultValue=None):\n",
            "        \"\"\"\n",
            "        Returns named result matching the given key, or if there is no\n",
            "        such name, then returns the given ``defaultValue`` or ``None`` if no\n",
            "        ``defaultValue`` is specified.\n",
            "\n",
            "        Similar to ``dict.get()``.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums)\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "            result = date_str.parseString(\"1999/12/31\")\n",
            "            print(result.get(\"year\")) # -> '1999'\n",
            "            print(result.get(\"hour\", \"not specified\")) # -> 'not specified'\n",
            "            print(result.get(\"hour\")) # -> None\n",
            "        \"\"\"\n",
            "        if key in self:\n",
            "            return self[key]\n",
            "        else:\n",
            "            return defaultValue\n",
            "\n",
            "    def insert(self, index, insStr):\n",
            "        \"\"\"\n",
            "        Inserts new element at location index in the list of parsed tokens.\n",
            "\n",
            "        Similar to ``list.insert()``.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            print(OneOrMore(Word(nums)).parseString(\"0 123 321\")) # -> ['0', '123', '321']\n",
            "\n",
            "            # use a parse action to insert the parse location in the front of the parsed results\n",
            "            def insert_locn(locn, tokens):\n",
            "                tokens.insert(0, locn)\n",
            "            print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString(\"0 123 321\")) # -> [0, '0', '123', '321']\n",
            "        \"\"\"\n",
            "        self.__toklist.insert(index, insStr)\n",
            "        # fixup indices in token dictionary\n",
            "        for name, occurrences in self.__tokdict.items():\n",
            "            for k, (value, position) in enumerate(occurrences):\n",
            "                occurrences[k] = _ParseResultsWithOffset(value, position + (position > index))\n",
            "\n",
            "    def append(self, item):\n",
            "        \"\"\"\n",
            "        Add single element to end of ParseResults list of elements.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            print(OneOrMore(Word(nums)).parseString(\"0 123 321\")) # -> ['0', '123', '321']\n",
            "\n",
            "            # use a parse action to compute the sum of the parsed integers, and add it to the end\n",
            "            def append_sum(tokens):\n",
            "                tokens.append(sum(map(int, tokens)))\n",
            "            print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString(\"0 123 321\")) # -> ['0', '123', '321', 444]\n",
            "        \"\"\"\n",
            "        self.__toklist.append(item)\n",
            "\n",
            "    def extend(self, itemseq):\n",
            "        \"\"\"\n",
            "        Add sequence of elements to end of ParseResults list of elements.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            patt = OneOrMore(Word(alphas))\n",
            "\n",
            "            # use a parse action to append the reverse of the matched strings, to make a palindrome\n",
            "            def make_palindrome(tokens):\n",
            "                tokens.extend(reversed([t[::-1] for t in tokens]))\n",
            "                return ''.join(tokens)\n",
            "            print(patt.addParseAction(make_palindrome).parseString(\"lskdj sdlkjf lksd\")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'\n",
            "        \"\"\"\n",
            "        if isinstance(itemseq, ParseResults):\n",
            "            self.__iadd__(itemseq)\n",
            "        else:\n",
            "            self.__toklist.extend(itemseq)\n",
            "\n",
            "    def clear(self):\n",
            "        \"\"\"\n",
            "        Clear all elements and results names.\n",
            "        \"\"\"\n",
            "        del self.__toklist[:]\n",
            "        self.__tokdict.clear()\n",
            "\n",
            "    def __getattr__(self, name):\n",
            "        try:\n",
            "            return self[name]\n",
            "        except KeyError:\n",
            "            return \"\"\n",
            "\n",
            "    def __add__(self, other):\n",
            "        ret = self.copy()\n",
            "        ret += other\n",
            "        return ret\n",
            "\n",
            "    def __iadd__(self, other):\n",
            "        if other.__tokdict:\n",
            "            offset = len(self.__toklist)\n",
            "            addoffset = lambda a: offset if a < 0 else a + offset\n",
            "            otheritems = other.__tokdict.items()\n",
            "            otherdictitems = [(k, _ParseResultsWithOffset(v[0], addoffset(v[1])))\n",
            "                              for k, vlist in otheritems for v in vlist]\n",
            "            for k, v in otherdictitems:\n",
            "                self[k] = v\n",
            "                if isinstance(v[0], ParseResults):\n",
            "                    v[0].__parent = wkref(self)\n",
            "\n",
            "        self.__toklist += other.__toklist\n",
            "        self.__accumNames.update(other.__accumNames)\n",
            "        return self\n",
            "\n",
            "    def __radd__(self, other):\n",
            "        if isinstance(other, int) and other == 0:\n",
            "            # useful for merging many ParseResults using sum() builtin\n",
            "            return self.copy()\n",
            "        else:\n",
            "            # this may raise a TypeError - so be it\n",
            "            return other + self\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"(%s, %s)\" % (repr(self.__toklist), repr(self.__tokdict))\n",
            "\n",
            "    def __str__(self):\n",
            "        return '[' + ', '.join(_ustr(i) if isinstance(i, ParseResults) else repr(i) for i in self.__toklist) + ']'\n",
            "\n",
            "    def _asStringList(self, sep=''):\n",
            "        out = []\n",
            "        for item in self.__toklist:\n",
            "            if out and sep:\n",
            "                out.append(sep)\n",
            "            if isinstance(item, ParseResults):\n",
            "                out += item._asStringList()\n",
            "            else:\n",
            "                out.append(_ustr(item))\n",
            "        return out\n",
            "\n",
            "    def asList(self):\n",
            "        \"\"\"\n",
            "        Returns the parse results as a nested list of matching tokens, all converted to strings.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            patt = OneOrMore(Word(alphas))\n",
            "            result = patt.parseString(\"sldkj lsdkj sldkj\")\n",
            "            # even though the result prints in string-like form, it is actually a pyparsing ParseResults\n",
            "            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']\n",
            "\n",
            "            # Use asList() to create an actual list\n",
            "            result_list = result.asList()\n",
            "            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']\n",
            "        \"\"\"\n",
            "        return [res.asList() if isinstance(res, ParseResults) else res for res in self.__toklist]\n",
            "\n",
            "    def asDict(self):\n",
            "        \"\"\"\n",
            "        Returns the named parse results as a nested dictionary.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums)\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "            result = date_str.parseString('12/31/1999')\n",
            "            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})\n",
            "\n",
            "            result_dict = result.asDict()\n",
            "            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}\n",
            "\n",
            "            # even though a ParseResults supports dict-like access, sometime you just need to have a dict\n",
            "            import json\n",
            "            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable\n",
            "            print(json.dumps(result.asDict())) # -> {\"month\": \"31\", \"day\": \"1999\", \"year\": \"12\"}\n",
            "        \"\"\"\n",
            "        if PY_3:\n",
            "            item_fn = self.items\n",
            "        else:\n",
            "            item_fn = self.iteritems\n",
            "\n",
            "        def toItem(obj):\n",
            "            if isinstance(obj, ParseResults):\n",
            "                if obj.haskeys():\n",
            "                    return obj.asDict()\n",
            "                else:\n",
            "                    return [toItem(v) for v in obj]\n",
            "            else:\n",
            "                return obj\n",
            "\n",
            "        return dict((k, toItem(v)) for k, v in item_fn())\n",
            "\n",
            "    def copy(self):\n",
            "        \"\"\"\n",
            "        Returns a new copy of a :class:`ParseResults` object.\n",
            "        \"\"\"\n",
            "        ret = ParseResults(self.__toklist)\n",
            "        ret.__tokdict = dict(self.__tokdict.items())\n",
            "        ret.__parent = self.__parent\n",
            "        ret.__accumNames.update(self.__accumNames)\n",
            "        ret.__name = self.__name\n",
            "        return ret\n",
            "\n",
            "    def asXML(self, doctag=None, namedItemsOnly=False, indent=\"\", formatted=True):\n",
            "        \"\"\"\n",
            "        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.\n",
            "        \"\"\"\n",
            "        nl = \"\\n\"\n",
            "        out = []\n",
            "        namedItems = dict((v[1], k) for (k, vlist) in self.__tokdict.items()\n",
            "                          for v in vlist)\n",
            "        nextLevelIndent = indent + \"  \"\n",
            "\n",
            "        # collapse out indents if formatting is not desired\n",
            "        if not formatted:\n",
            "            indent = \"\"\n",
            "            nextLevelIndent = \"\"\n",
            "            nl = \"\"\n",
            "\n",
            "        selfTag = None\n",
            "        if doctag is not None:\n",
            "            selfTag = doctag\n",
            "        else:\n",
            "            if self.__name:\n",
            "                selfTag = self.__name\n",
            "\n",
            "        if not selfTag:\n",
            "            if namedItemsOnly:\n",
            "                return \"\"\n",
            "            else:\n",
            "                selfTag = \"ITEM\"\n",
            "\n",
            "        out += [nl, indent, \"<\", selfTag, \">\"]\n",
            "\n",
            "        for i, res in enumerate(self.__toklist):\n",
            "            if isinstance(res, ParseResults):\n",
            "                if i in namedItems:\n",
            "                    out += [res.asXML(namedItems[i],\n",
            "                                      namedItemsOnly and doctag is None,\n",
            "                                      nextLevelIndent,\n",
            "                                      formatted)]\n",
            "                else:\n",
            "                    out += [res.asXML(None,\n",
            "                                      namedItemsOnly and doctag is None,\n",
            "                                      nextLevelIndent,\n",
            "                                      formatted)]\n",
            "            else:\n",
            "                # individual token, see if there is a name for it\n",
            "                resTag = None\n",
            "                if i in namedItems:\n",
            "                    resTag = namedItems[i]\n",
            "                if not resTag:\n",
            "                    if namedItemsOnly:\n",
            "                        continue\n",
            "                    else:\n",
            "                        resTag = \"ITEM\"\n",
            "                xmlBodyText = _xml_escape(_ustr(res))\n",
            "                out += [nl, nextLevelIndent, \"<\", resTag, \">\",\n",
            "                        xmlBodyText,\n",
            "                                                \"</\", resTag, \">\"]\n",
            "\n",
            "        out += [nl, indent, \"</\", selfTag, \">\"]\n",
            "        return \"\".join(out)\n",
            "\n",
            "    def __lookup(self, sub):\n",
            "        for k, vlist in self.__tokdict.items():\n",
            "            for v, loc in vlist:\n",
            "                if sub is v:\n",
            "                    return k\n",
            "        return None\n",
            "\n",
            "    def getName(self):\n",
            "        r\"\"\"\n",
            "        Returns the results name for this token expression. Useful when several\n",
            "        different expressions might match at a particular location.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums)\n",
            "            ssn_expr = Regex(r\"\\d\\d\\d-\\d\\d-\\d\\d\\d\\d\")\n",
            "            house_number_expr = Suppress('#') + Word(nums, alphanums)\n",
            "            user_data = (Group(house_number_expr)(\"house_number\")\n",
            "                        | Group(ssn_expr)(\"ssn\")\n",
            "                        | Group(integer)(\"age\"))\n",
            "            user_info = OneOrMore(user_data)\n",
            "\n",
            "            result = user_info.parseString(\"22 111-22-3333 #221B\")\n",
            "            for item in result:\n",
            "                print(item.getName(), ':', item[0])\n",
            "\n",
            "        prints::\n",
            "\n",
            "            age : 22\n",
            "            ssn : 111-22-3333\n",
            "            house_number : 221B\n",
            "        \"\"\"\n",
            "        if self.__name:\n",
            "            return self.__name\n",
            "        elif self.__parent:\n",
            "            par = self.__parent()\n",
            "            if par:\n",
            "                return par.__lookup(self)\n",
            "            else:\n",
            "                return None\n",
            "        elif (len(self) == 1\n",
            "              and len(self.__tokdict) == 1\n",
            "              and next(iter(self.__tokdict.values()))[0][1] in (0, -1)):\n",
            "            return next(iter(self.__tokdict.keys()))\n",
            "        else:\n",
            "            return None\n",
            "\n",
            "    def dump(self, indent='', full=True, include_list=True, _depth=0):\n",
            "        \"\"\"\n",
            "        Diagnostic method for listing out the contents of\n",
            "        a :class:`ParseResults`. Accepts an optional ``indent`` argument so\n",
            "        that this string can be embedded in a nested display of other data.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums)\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "            result = date_str.parseString('12/31/1999')\n",
            "            print(result.dump())\n",
            "\n",
            "        prints::\n",
            "\n",
            "            ['12', '/', '31', '/', '1999']\n",
            "            - day: 1999\n",
            "            - month: 31\n",
            "            - year: 12\n",
            "        \"\"\"\n",
            "        out = []\n",
            "        NL = '\\n'\n",
            "        if include_list:\n",
            "            out.append(indent + _ustr(self.asList()))\n",
            "        else:\n",
            "            out.append('')\n",
            "\n",
            "        if full:\n",
            "            if self.haskeys():\n",
            "                items = sorted((str(k), v) for k, v in self.items())\n",
            "                for k, v in items:\n",
            "                    if out:\n",
            "                        out.append(NL)\n",
            "                    out.append(\"%s%s- %s: \" % (indent, ('  ' * _depth), k))\n",
            "                    if isinstance(v, ParseResults):\n",
            "                        if v:\n",
            "                            out.append(v.dump(indent=indent, full=full, include_list=include_list, _depth=_depth + 1))\n",
            "                        else:\n",
            "                            out.append(_ustr(v))\n",
            "                    else:\n",
            "                        out.append(repr(v))\n",
            "            elif any(isinstance(vv, ParseResults) for vv in self):\n",
            "                v = self\n",
            "                for i, vv in enumerate(v):\n",
            "                    if isinstance(vv, ParseResults):\n",
            "                        out.append(\"\\n%s%s[%d]:\\n%s%s%s\" % (indent,\n",
            "                                                            ('  ' * (_depth)),\n",
            "                                                            i,\n",
            "                                                            indent,\n",
            "                                                            ('  ' * (_depth + 1)),\n",
            "                                                            vv.dump(indent=indent,\n",
            "                                                                    full=full,\n",
            "                                                                    include_list=include_list,\n",
            "                                                                    _depth=_depth + 1)))\n",
            "                    else:\n",
            "                        out.append(\"\\n%s%s[%d]:\\n%s%s%s\" % (indent,\n",
            "                                                            ('  ' * (_depth)),\n",
            "                                                            i,\n",
            "                                                            indent,\n",
            "                                                            ('  ' * (_depth + 1)),\n",
            "                                                            _ustr(vv)))\n",
            "\n",
            "        return \"\".join(out)\n",
            "\n",
            "    def pprint(self, *args, **kwargs):\n",
            "        \"\"\"\n",
            "        Pretty-printer for parsed results as a list, using the\n",
            "        `pprint <https://docs.python.org/3/library/pprint.html>`_ module.\n",
            "        Accepts additional positional or keyword args as defined for\n",
            "        `pprint.pprint <https://docs.python.org/3/library/pprint.html#pprint.pprint>`_ .\n",
            "\n",
            "        Example::\n",
            "\n",
            "            ident = Word(alphas, alphanums)\n",
            "            num = Word(nums)\n",
            "            func = Forward()\n",
            "            term = ident | num | Group('(' + func + ')')\n",
            "            func <<= ident + Group(Optional(delimitedList(term)))\n",
            "            result = func.parseString(\"fna a,b,(fnb c,d,200),100\")\n",
            "            result.pprint(width=40)\n",
            "\n",
            "        prints::\n",
            "\n",
            "            ['fna',\n",
            "             ['a',\n",
            "              'b',\n",
            "              ['(', 'fnb', ['c', 'd', '200'], ')'],\n",
            "              '100']]\n",
            "        \"\"\"\n",
            "        pprint.pprint(self.asList(), *args, **kwargs)\n",
            "\n",
            "    # add support for pickle protocol\n",
            "    def __getstate__(self):\n",
            "        return (self.__toklist,\n",
            "                (self.__tokdict.copy(),\n",
            "                 self.__parent is not None and self.__parent() or None,\n",
            "                 self.__accumNames,\n",
            "                 self.__name))\n",
            "\n",
            "    def __setstate__(self, state):\n",
            "        self.__toklist = state[0]\n",
            "        self.__tokdict, par, inAccumNames, self.__name = state[1]\n",
            "        self.__accumNames = {}\n",
            "        self.__accumNames.update(inAccumNames)\n",
            "        if par is not None:\n",
            "            self.__parent = wkref(par)\n",
            "        else:\n",
            "            self.__parent = None\n",
            "\n",
            "    def __getnewargs__(self):\n",
            "        return self.__toklist, self.__name, self.__asList, self.__modal\n",
            "\n",
            "    def __dir__(self):\n",
            "        return dir(type(self)) + list(self.keys())\n",
            "\n",
            "    @classmethod\n",
            "    def from_dict(cls, other, name=None):\n",
            "        \"\"\"\n",
            "        Helper classmethod to construct a ParseResults from a dict, preserving the\n",
            "        name-value relations as results names. If an optional 'name' argument is\n",
            "        given, a nested ParseResults will be returned\n",
            "        \"\"\"\n",
            "        def is_iterable(obj):\n",
            "            try:\n",
            "                iter(obj)\n",
            "            except Exception:\n",
            "                return False\n",
            "            else:\n",
            "                if PY_3:\n",
            "                    return not isinstance(obj, (str, bytes))\n",
            "                else:\n",
            "                    return not isinstance(obj, basestring)\n",
            "\n",
            "        ret = cls([])\n",
            "        for k, v in other.items():\n",
            "            if isinstance(v, Mapping):\n",
            "                ret += cls.from_dict(v, name=k)\n",
            "            else:\n",
            "                ret += cls([v], name=k, asList=is_iterable(v))\n",
            "        if name is not None:\n",
            "            ret = cls([ret], name=name)\n",
            "        return ret\n",
            "\n",
            "MutableMapping.register(ParseResults)\n",
            "\n",
            "def col (loc, strg):\n",
            "    \"\"\"Returns current column within a string, counting newlines as line separators.\n",
            "   The first column is number 1.\n",
            "\n",
            "   Note: the default parsing behavior is to expand tabs in the input string\n",
            "   before starting the parsing process.  See\n",
            "   :class:`ParserElement.parseString` for more\n",
            "   information on parsing strings containing ``<TAB>`` s, and suggested\n",
            "   methods to maintain a consistent view of the parsed string, the parse\n",
            "   location, and line and column positions within the parsed string.\n",
            "   \"\"\"\n",
            "    s = strg\n",
            "    return 1 if 0 < loc < len(s) and s[loc-1] == '\\n' else loc - s.rfind(\"\\n\", 0, loc)\n",
            "\n",
            "def lineno(loc, strg):\n",
            "    \"\"\"Returns current line number within a string, counting newlines as line separators.\n",
            "    The first line is number 1.\n",
            "\n",
            "    Note - the default parsing behavior is to expand tabs in the input string\n",
            "    before starting the parsing process.  See :class:`ParserElement.parseString`\n",
            "    for more information on parsing strings containing ``<TAB>`` s, and\n",
            "    suggested methods to maintain a consistent view of the parsed string, the\n",
            "    parse location, and line and column positions within the parsed string.\n",
            "    \"\"\"\n",
            "    return strg.count(\"\\n\", 0, loc) + 1\n",
            "\n",
            "def line(loc, strg):\n",
            "    \"\"\"Returns the line of text containing loc within a string, counting newlines as line separators.\n",
            "       \"\"\"\n",
            "    lastCR = strg.rfind(\"\\n\", 0, loc)\n",
            "    nextCR = strg.find(\"\\n\", loc)\n",
            "    if nextCR >= 0:\n",
            "        return strg[lastCR + 1:nextCR]\n",
            "    else:\n",
            "        return strg[lastCR + 1:]\n",
            "\n",
            "def _defaultStartDebugAction(instring, loc, expr):\n",
            "    print((\"Match \" + _ustr(expr) + \" at loc \" + _ustr(loc) + \"(%d,%d)\" % (lineno(loc, instring), col(loc, instring))))\n",
            "\n",
            "def _defaultSuccessDebugAction(instring, startloc, endloc, expr, toks):\n",
            "    print(\"Matched \" + _ustr(expr) + \" -> \" + str(toks.asList()))\n",
            "\n",
            "def _defaultExceptionDebugAction(instring, loc, expr, exc):\n",
            "    print(\"Exception raised:\" + _ustr(exc))\n",
            "\n",
            "def nullDebugAction(*args):\n",
            "    \"\"\"'Do-nothing' debug action, to suppress debugging output during parsing.\"\"\"\n",
            "    pass\n",
            "\n",
            "# Only works on Python 3.x - nonlocal is toxic to Python 2 installs\n",
            "#~ 'decorator to trim function calls to match the arity of the target'\n",
            "#~ def _trim_arity(func, maxargs=3):\n",
            "    #~ if func in singleArgBuiltins:\n",
            "        #~ return lambda s,l,t: func(t)\n",
            "    #~ limit = 0\n",
            "    #~ foundArity = False\n",
            "    #~ def wrapper(*args):\n",
            "        #~ nonlocal limit,foundArity\n",
            "        #~ while 1:\n",
            "            #~ try:\n",
            "                #~ ret = func(*args[limit:])\n",
            "                #~ foundArity = True\n",
            "                #~ return ret\n",
            "            #~ except TypeError:\n",
            "                #~ if limit == maxargs or foundArity:\n",
            "                    #~ raise\n",
            "                #~ limit += 1\n",
            "                #~ continue\n",
            "    #~ return wrapper\n",
            "\n",
            "# this version is Python 2.x-3.x cross-compatible\n",
            "'decorator to trim function calls to match the arity of the target'\n",
            "def _trim_arity(func, maxargs=2):\n",
            "    if func in singleArgBuiltins:\n",
            "        return lambda s, l, t: func(t)\n",
            "    limit = [0]\n",
            "    foundArity = [False]\n",
            "\n",
            "    # traceback return data structure changed in Py3.5 - normalize back to plain tuples\n",
            "    if system_version[:2] >= (3, 5):\n",
            "        def extract_stack(limit=0):\n",
            "            # special handling for Python 3.5.0 - extra deep call stack by 1\n",
            "            offset = -3 if system_version == (3, 5, 0) else -2\n",
            "            frame_summary = traceback.extract_stack(limit=-offset + limit - 1)[offset]\n",
            "            return [frame_summary[:2]]\n",
            "        def extract_tb(tb, limit=0):\n",
            "            frames = traceback.extract_tb(tb, limit=limit)\n",
            "            frame_summary = frames[-1]\n",
            "            return [frame_summary[:2]]\n",
            "    else:\n",
            "        extract_stack = traceback.extract_stack\n",
            "        extract_tb = traceback.extract_tb\n",
            "\n",
            "    # synthesize what would be returned by traceback.extract_stack at the call to\n",
            "    # user's parse action 'func', so that we don't incur call penalty at parse time\n",
            "\n",
            "    LINE_DIFF = 6\n",
            "    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND\n",
            "    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!\n",
            "    this_line = extract_stack(limit=2)[-1]\n",
            "    pa_call_line_synth = (this_line[0], this_line[1] + LINE_DIFF)\n",
            "\n",
            "    def wrapper(*args):\n",
            "        while 1:\n",
            "            try:\n",
            "                ret = func(*args[limit[0]:])\n",
            "                foundArity[0] = True\n",
            "                return ret\n",
            "            except TypeError:\n",
            "                # re-raise TypeErrors if they did not come from our arity testing\n",
            "                if foundArity[0]:\n",
            "                    raise\n",
            "                else:\n",
            "                    try:\n",
            "                        tb = sys.exc_info()[-1]\n",
            "                        if not extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth:\n",
            "                            raise\n",
            "                    finally:\n",
            "                        try:\n",
            "                            del tb\n",
            "                        except NameError:\n",
            "                            pass\n",
            "\n",
            "                if limit[0] <= maxargs:\n",
            "                    limit[0] += 1\n",
            "                    continue\n",
            "                raise\n",
            "\n",
            "    # copy func name to wrapper for sensible debug output\n",
            "    func_name = \"<parse action>\"\n",
            "    try:\n",
            "        func_name = getattr(func, '__name__',\n",
            "                            getattr(func, '__class__').__name__)\n",
            "    except Exception:\n",
            "        func_name = str(func)\n",
            "    wrapper.__name__ = func_name\n",
            "\n",
            "    return wrapper\n",
            "\n",
            "\n",
            "class ParserElement(object):\n",
            "    \"\"\"Abstract base level parser element class.\"\"\"\n",
            "    DEFAULT_WHITE_CHARS = \" \\n\\t\\r\"\n",
            "    verbose_stacktrace = False\n",
            "\n",
            "    @staticmethod\n",
            "    def setDefaultWhitespaceChars(chars):\n",
            "        r\"\"\"\n",
            "        Overrides the default whitespace chars\n",
            "\n",
            "        Example::\n",
            "\n",
            "            # default whitespace chars are space, <TAB> and newline\n",
            "            OneOrMore(Word(alphas)).parseString(\"abc def\\nghi jkl\")  # -> ['abc', 'def', 'ghi', 'jkl']\n",
            "\n",
            "            # change to just treat newline as significant\n",
            "            ParserElement.setDefaultWhitespaceChars(\" \\t\")\n",
            "            OneOrMore(Word(alphas)).parseString(\"abc def\\nghi jkl\")  # -> ['abc', 'def']\n",
            "        \"\"\"\n",
            "        ParserElement.DEFAULT_WHITE_CHARS = chars\n",
            "\n",
            "    @staticmethod\n",
            "    def inlineLiteralsUsing(cls):\n",
            "        \"\"\"\n",
            "        Set class to be used for inclusion of string literals into a parser.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            # default literal class used is Literal\n",
            "            integer = Word(nums)\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "            date_str.parseString(\"1999/12/31\")  # -> ['1999', '/', '12', '/', '31']\n",
            "\n",
            "\n",
            "            # change to Suppress\n",
            "            ParserElement.inlineLiteralsUsing(Suppress)\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "\n",
            "            date_str.parseString(\"1999/12/31\")  # -> ['1999', '12', '31']\n",
            "        \"\"\"\n",
            "        ParserElement._literalStringClass = cls\n",
            "\n",
            "    def __init__(self, savelist=False):\n",
            "        self.parseAction = list()\n",
            "        self.failAction = None\n",
            "        # ~ self.name = \"<unknown>\"  # don't define self.name, let subclasses try/except upcall\n",
            "        self.strRepr = None\n",
            "        self.resultsName = None\n",
            "        self.saveAsList = savelist\n",
            "        self.skipWhitespace = True\n",
            "        self.whiteChars = set(ParserElement.DEFAULT_WHITE_CHARS)\n",
            "        self.copyDefaultWhiteChars = True\n",
            "        self.mayReturnEmpty = False # used when checking for left-recursion\n",
            "        self.keepTabs = False\n",
            "        self.ignoreExprs = list()\n",
            "        self.debug = False\n",
            "        self.streamlined = False\n",
            "        self.mayIndexError = True # used to optimize exception handling for subclasses that don't advance parse index\n",
            "        self.errmsg = \"\"\n",
            "        self.modalResults = True # used to mark results names as modal (report only last) or cumulative (list all)\n",
            "        self.debugActions = (None, None, None)  # custom debug actions\n",
            "        self.re = None\n",
            "        self.callPreparse = True # used to avoid redundant calls to preParse\n",
            "        self.callDuringTry = False\n",
            "\n",
            "    def copy(self):\n",
            "        \"\"\"\n",
            "        Make a copy of this :class:`ParserElement`.  Useful for defining\n",
            "        different parse actions for the same parsing pattern, using copies of\n",
            "        the original parse element.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n",
            "            integerK = integer.copy().addParseAction(lambda toks: toks[0] * 1024) + Suppress(\"K\")\n",
            "            integerM = integer.copy().addParseAction(lambda toks: toks[0] * 1024 * 1024) + Suppress(\"M\")\n",
            "\n",
            "            print(OneOrMore(integerK | integerM | integer).parseString(\"5K 100 640K 256M\"))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            [5120, 100, 655360, 268435456]\n",
            "\n",
            "        Equivalent form of ``expr.copy()`` is just ``expr()``::\n",
            "\n",
            "            integerM = integer().addParseAction(lambda toks: toks[0] * 1024 * 1024) + Suppress(\"M\")\n",
            "        \"\"\"\n",
            "        cpy = copy.copy(self)\n",
            "        cpy.parseAction = self.parseAction[:]\n",
            "        cpy.ignoreExprs = self.ignoreExprs[:]\n",
            "        if self.copyDefaultWhiteChars:\n",
            "            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS\n",
            "        return cpy\n",
            "\n",
            "    def setName(self, name):\n",
            "        \"\"\"\n",
            "        Define name for this expression, makes debugging and exception messages clearer.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            Word(nums).parseString(\"ABC\")  # -> Exception: Expected W:(0123...) (at char 0), (line:1, col:1)\n",
            "            Word(nums).setName(\"integer\").parseString(\"ABC\")  # -> Exception: Expected integer (at char 0), (line:1, col:1)\n",
            "        \"\"\"\n",
            "        self.name = name\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        if __diag__.enable_debug_on_named_expressions:\n",
            "            self.setDebug()\n",
            "        return self\n",
            "\n",
            "    def setResultsName(self, name, listAllMatches=False):\n",
            "        \"\"\"\n",
            "        Define name for referencing matching tokens as a nested attribute\n",
            "        of the returned parse results.\n",
            "        NOTE: this returns a *copy* of the original :class:`ParserElement` object;\n",
            "        this is so that the client can define a basic element, such as an\n",
            "        integer, and reference it in multiple places with different names.\n",
            "\n",
            "        You can also set results names using the abbreviated syntax,\n",
            "        ``expr(\"name\")`` in place of ``expr.setResultsName(\"name\")``\n",
            "        - see :class:`__call__`.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            date_str = (integer.setResultsName(\"year\") + '/'\n",
            "                        + integer.setResultsName(\"month\") + '/'\n",
            "                        + integer.setResultsName(\"day\"))\n",
            "\n",
            "            # equivalent form:\n",
            "            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n",
            "        \"\"\"\n",
            "        return self._setResultsName(name, listAllMatches)\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        newself = self.copy()\n",
            "        if name.endswith(\"*\"):\n",
            "            name = name[:-1]\n",
            "            listAllMatches = True\n",
            "        newself.resultsName = name\n",
            "        newself.modalResults = not listAllMatches\n",
            "        return newself\n",
            "\n",
            "    def setBreak(self, breakFlag=True):\n",
            "        \"\"\"Method to invoke the Python pdb debugger when this element is\n",
            "           about to be parsed. Set ``breakFlag`` to True to enable, False to\n",
            "           disable.\n",
            "        \"\"\"\n",
            "        if breakFlag:\n",
            "            _parseMethod = self._parse\n",
            "            def breaker(instring, loc, doActions=True, callPreParse=True):\n",
            "                import pdb\n",
            "                # this call to pdb.set_trace() is intentional, not a checkin error\n",
            "                pdb.set_trace()\n",
            "                return _parseMethod(instring, loc, doActions, callPreParse)\n",
            "            breaker._originalParseMethod = _parseMethod\n",
            "            self._parse = breaker\n",
            "        else:\n",
            "            if hasattr(self._parse, \"_originalParseMethod\"):\n",
            "                self._parse = self._parse._originalParseMethod\n",
            "        return self\n",
            "\n",
            "    def setParseAction(self, *fns, **kwargs):\n",
            "        \"\"\"\n",
            "        Define one or more actions to perform when successfully matching parse element definition.\n",
            "        Parse action fn is a callable method with 0-3 arguments, called as ``fn(s, loc, toks)`` ,\n",
            "        ``fn(loc, toks)`` , ``fn(toks)`` , or just ``fn()`` , where:\n",
            "\n",
            "        - s   = the original string being parsed (see note below)\n",
            "        - loc = the location of the matching substring\n",
            "        - toks = a list of the matched tokens, packaged as a :class:`ParseResults` object\n",
            "\n",
            "        If the functions in fns modify the tokens, they can return them as the return\n",
            "        value from fn, and the modified list of tokens will replace the original.\n",
            "        Otherwise, fn does not need to return any value.\n",
            "\n",
            "        If None is passed as the parse action, all previously added parse actions for this\n",
            "        expression are cleared.\n",
            "\n",
            "        Optional keyword arguments:\n",
            "        - callDuringTry = (default= ``False``) indicate if parse action should be run during lookaheads and alternate testing\n",
            "\n",
            "        Note: the default parsing behavior is to expand tabs in the input string\n",
            "        before starting the parsing process.  See :class:`parseString for more\n",
            "        information on parsing strings containing ``<TAB>`` s, and suggested\n",
            "        methods to maintain a consistent view of the parsed string, the parse\n",
            "        location, and line and column positions within the parsed string.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums)\n",
            "            date_str = integer + '/' + integer + '/' + integer\n",
            "\n",
            "            date_str.parseString(\"1999/12/31\")  # -> ['1999', '/', '12', '/', '31']\n",
            "\n",
            "            # use parse action to convert to ints at parse time\n",
            "            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n",
            "            date_str = integer + '/' + integer + '/' + integer\n",
            "\n",
            "            # note that integer fields are now ints, not strings\n",
            "            date_str.parseString(\"1999/12/31\")  # -> [1999, '/', 12, '/', 31]\n",
            "        \"\"\"\n",
            "        if list(fns) == [None,]:\n",
            "            self.parseAction = []\n",
            "        else:\n",
            "            if not all(callable(fn) for fn in fns):\n",
            "                raise TypeError(\"parse actions must be callable\")\n",
            "            self.parseAction = list(map(_trim_arity, list(fns)))\n",
            "            self.callDuringTry = kwargs.get(\"callDuringTry\", False)\n",
            "        return self\n",
            "\n",
            "    def addParseAction(self, *fns, **kwargs):\n",
            "        \"\"\"\n",
            "        Add one or more parse actions to expression's list of parse actions. See :class:`setParseAction`.\n",
            "\n",
            "        See examples in :class:`copy`.\n",
            "        \"\"\"\n",
            "        self.parseAction += list(map(_trim_arity, list(fns)))\n",
            "        self.callDuringTry = self.callDuringTry or kwargs.get(\"callDuringTry\", False)\n",
            "        return self\n",
            "\n",
            "    def addCondition(self, *fns, **kwargs):\n",
            "        \"\"\"Add a boolean predicate function to expression's list of parse actions. See\n",
            "        :class:`setParseAction` for function call signatures. Unlike ``setParseAction``,\n",
            "        functions passed to ``addCondition`` need to return boolean success/fail of the condition.\n",
            "\n",
            "        Optional keyword arguments:\n",
            "        - message = define a custom message to be used in the raised exception\n",
            "        - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException\n",
            "\n",
            "        Example::\n",
            "\n",
            "            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n",
            "            year_int = integer.copy()\n",
            "            year_int.addCondition(lambda toks: toks[0] >= 2000, message=\"Only support years 2000 and later\")\n",
            "            date_str = year_int + '/' + integer + '/' + integer\n",
            "\n",
            "            result = date_str.parseString(\"1999/12/31\")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)\n",
            "        \"\"\"\n",
            "        for fn in fns:\n",
            "            self.parseAction.append(conditionAsParseAction(fn, message=kwargs.get('message'),\n",
            "                                                           fatal=kwargs.get('fatal', False)))\n",
            "\n",
            "        self.callDuringTry = self.callDuringTry or kwargs.get(\"callDuringTry\", False)\n",
            "        return self\n",
            "\n",
            "    def setFailAction(self, fn):\n",
            "        \"\"\"Define action to perform if parsing fails at this expression.\n",
            "           Fail acton fn is a callable function that takes the arguments\n",
            "           ``fn(s, loc, expr, err)`` where:\n",
            "           - s = string being parsed\n",
            "           - loc = location where expression match was attempted and failed\n",
            "           - expr = the parse expression that failed\n",
            "           - err = the exception thrown\n",
            "           The function returns no value.  It may throw :class:`ParseFatalException`\n",
            "           if it is desired to stop parsing immediately.\"\"\"\n",
            "        self.failAction = fn\n",
            "        return self\n",
            "\n",
            "    def _skipIgnorables(self, instring, loc):\n",
            "        exprsFound = True\n",
            "        while exprsFound:\n",
            "            exprsFound = False\n",
            "            for e in self.ignoreExprs:\n",
            "                try:\n",
            "                    while 1:\n",
            "                        loc, dummy = e._parse(instring, loc)\n",
            "                        exprsFound = True\n",
            "                except ParseException:\n",
            "                    pass\n",
            "        return loc\n",
            "\n",
            "    def preParse(self, instring, loc):\n",
            "        if self.ignoreExprs:\n",
            "            loc = self._skipIgnorables(instring, loc)\n",
            "\n",
            "        if self.skipWhitespace:\n",
            "            wt = self.whiteChars\n",
            "            instrlen = len(instring)\n",
            "            while loc < instrlen and instring[loc] in wt:\n",
            "                loc += 1\n",
            "\n",
            "        return loc\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        return loc, []\n",
            "\n",
            "    def postParse(self, instring, loc, tokenlist):\n",
            "        return tokenlist\n",
            "\n",
            "    # ~ @profile\n",
            "    def _parseNoCache(self, instring, loc, doActions=True, callPreParse=True):\n",
            "        TRY, MATCH, FAIL = 0, 1, 2\n",
            "        debugging = (self.debug)  # and doActions)\n",
            "\n",
            "        if debugging or self.failAction:\n",
            "            # ~ print (\"Match\", self, \"at loc\", loc, \"(%d, %d)\" % (lineno(loc, instring), col(loc, instring)))\n",
            "            if self.debugActions[TRY]:\n",
            "                self.debugActions[TRY](instring, loc, self)\n",
            "            try:\n",
            "                if callPreParse and self.callPreparse:\n",
            "                    preloc = self.preParse(instring, loc)\n",
            "                else:\n",
            "                    preloc = loc\n",
            "                tokensStart = preloc\n",
            "                if self.mayIndexError or preloc >= len(instring):\n",
            "                    try:\n",
            "                        loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "                    except IndexError:\n",
            "                        raise ParseException(instring, len(instring), self.errmsg, self)\n",
            "                else:\n",
            "                    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "            except Exception as err:\n",
            "                # ~ print (\"Exception raised:\", err)\n",
            "                if self.debugActions[FAIL]:\n",
            "                    self.debugActions[FAIL](instring, tokensStart, self, err)\n",
            "                if self.failAction:\n",
            "                    self.failAction(instring, tokensStart, self, err)\n",
            "                raise\n",
            "        else:\n",
            "            if callPreParse and self.callPreparse:\n",
            "                preloc = self.preParse(instring, loc)\n",
            "            else:\n",
            "                preloc = loc\n",
            "            tokensStart = preloc\n",
            "            if self.mayIndexError or preloc >= len(instring):\n",
            "                try:\n",
            "                    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "                except IndexError:\n",
            "                    raise ParseException(instring, len(instring), self.errmsg, self)\n",
            "            else:\n",
            "                loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "\n",
            "        tokens = self.postParse(instring, loc, tokens)\n",
            "\n",
            "        retTokens = ParseResults(tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults)\n",
            "        if self.parseAction and (doActions or self.callDuringTry):\n",
            "            if debugging:\n",
            "                try:\n",
            "                    for fn in self.parseAction:\n",
            "                        try:\n",
            "                            tokens = fn(instring, tokensStart, retTokens)\n",
            "                        except IndexError as parse_action_exc:\n",
            "                            exc = ParseException(\"exception raised in parse action\")\n",
            "                            exc.__cause__ = parse_action_exc\n",
            "                            raise exc\n",
            "\n",
            "                        if tokens is not None and tokens is not retTokens:\n",
            "                            retTokens = ParseResults(tokens,\n",
            "                                                      self.resultsName,\n",
            "                                                      asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),\n",
            "                                                      modal=self.modalResults)\n",
            "                except Exception as err:\n",
            "                    # ~ print \"Exception raised in user parse action:\", err\n",
            "                    if self.debugActions[FAIL]:\n",
            "                        self.debugActions[FAIL](instring, tokensStart, self, err)\n",
            "                    raise\n",
            "            else:\n",
            "                for fn in self.parseAction:\n",
            "                    try:\n",
            "                        tokens = fn(instring, tokensStart, retTokens)\n",
            "                    except IndexError as parse_action_exc:\n",
            "                        exc = ParseException(\"exception raised in parse action\")\n",
            "                        exc.__cause__ = parse_action_exc\n",
            "                        raise exc\n",
            "\n",
            "                    if tokens is not None and tokens is not retTokens:\n",
            "                        retTokens = ParseResults(tokens,\n",
            "                                                  self.resultsName,\n",
            "                                                  asList=self.saveAsList and isinstance(tokens, (ParseResults, list)),\n",
            "                                                  modal=self.modalResults)\n",
            "        if debugging:\n",
            "            # ~ print (\"Matched\", self, \"->\", retTokens.asList())\n",
            "            if self.debugActions[MATCH]:\n",
            "                self.debugActions[MATCH](instring, tokensStart, loc, self, retTokens)\n",
            "\n",
            "        return loc, retTokens\n",
            "\n",
            "    def tryParse(self, instring, loc):\n",
            "        try:\n",
            "            return self._parse(instring, loc, doActions=False)[0]\n",
            "        except ParseFatalException:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "    def canParseNext(self, instring, loc):\n",
            "        try:\n",
            "            self.tryParse(instring, loc)\n",
            "        except (ParseException, IndexError):\n",
            "            return False\n",
            "        else:\n",
            "            return True\n",
            "\n",
            "    class _UnboundedCache(object):\n",
            "        def __init__(self):\n",
            "            cache = {}\n",
            "            self.not_in_cache = not_in_cache = object()\n",
            "\n",
            "            def get(self, key):\n",
            "                return cache.get(key, not_in_cache)\n",
            "\n",
            "            def set(self, key, value):\n",
            "                cache[key] = value\n",
            "\n",
            "            def clear(self):\n",
            "                cache.clear()\n",
            "\n",
            "            def cache_len(self):\n",
            "                return len(cache)\n",
            "\n",
            "            self.get = types.MethodType(get, self)\n",
            "            self.set = types.MethodType(set, self)\n",
            "            self.clear = types.MethodType(clear, self)\n",
            "            self.__len__ = types.MethodType(cache_len, self)\n",
            "\n",
            "    if _OrderedDict is not None:\n",
            "        class _FifoCache(object):\n",
            "            def __init__(self, size):\n",
            "                self.not_in_cache = not_in_cache = object()\n",
            "\n",
            "                cache = _OrderedDict()\n",
            "\n",
            "                def get(self, key):\n",
            "                    return cache.get(key, not_in_cache)\n",
            "\n",
            "                def set(self, key, value):\n",
            "                    cache[key] = value\n",
            "                    while len(cache) > size:\n",
            "                        try:\n",
            "                            cache.popitem(False)\n",
            "                        except KeyError:\n",
            "                            pass\n",
            "\n",
            "                def clear(self):\n",
            "                    cache.clear()\n",
            "\n",
            "                def cache_len(self):\n",
            "                    return len(cache)\n",
            "\n",
            "                self.get = types.MethodType(get, self)\n",
            "                self.set = types.MethodType(set, self)\n",
            "                self.clear = types.MethodType(clear, self)\n",
            "                self.__len__ = types.MethodType(cache_len, self)\n",
            "\n",
            "    else:\n",
            "        class _FifoCache(object):\n",
            "            def __init__(self, size):\n",
            "                self.not_in_cache = not_in_cache = object()\n",
            "\n",
            "                cache = {}\n",
            "                key_fifo = collections.deque([], size)\n",
            "\n",
            "                def get(self, key):\n",
            "                    return cache.get(key, not_in_cache)\n",
            "\n",
            "                def set(self, key, value):\n",
            "                    cache[key] = value\n",
            "                    while len(key_fifo) > size:\n",
            "                        cache.pop(key_fifo.popleft(), None)\n",
            "                    key_fifo.append(key)\n",
            "\n",
            "                def clear(self):\n",
            "                    cache.clear()\n",
            "                    key_fifo.clear()\n",
            "\n",
            "                def cache_len(self):\n",
            "                    return len(cache)\n",
            "\n",
            "                self.get = types.MethodType(get, self)\n",
            "                self.set = types.MethodType(set, self)\n",
            "                self.clear = types.MethodType(clear, self)\n",
            "                self.__len__ = types.MethodType(cache_len, self)\n",
            "\n",
            "    # argument cache for optimizing repeated calls when backtracking through recursive expressions\n",
            "    packrat_cache = {} # this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail\n",
            "    packrat_cache_lock = RLock()\n",
            "    packrat_cache_stats = [0, 0]\n",
            "\n",
            "    # this method gets repeatedly called during backtracking with the same arguments -\n",
            "    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression\n",
            "    def _parseCache(self, instring, loc, doActions=True, callPreParse=True):\n",
            "        HIT, MISS = 0, 1\n",
            "        lookup = (self, instring, loc, callPreParse, doActions)\n",
            "        with ParserElement.packrat_cache_lock:\n",
            "            cache = ParserElement.packrat_cache\n",
            "            value = cache.get(lookup)\n",
            "            if value is cache.not_in_cache:\n",
            "                ParserElement.packrat_cache_stats[MISS] += 1\n",
            "                try:\n",
            "                    value = self._parseNoCache(instring, loc, doActions, callPreParse)\n",
            "                except ParseBaseException as pe:\n",
            "                    # cache a copy of the exception, without the traceback\n",
            "                    cache.set(lookup, pe.__class__(*pe.args))\n",
            "                    raise\n",
            "                else:\n",
            "                    cache.set(lookup, (value[0], value[1].copy()))\n",
            "                    return value\n",
            "            else:\n",
            "                ParserElement.packrat_cache_stats[HIT] += 1\n",
            "                if isinstance(value, Exception):\n",
            "                    raise value\n",
            "                return value[0], value[1].copy()\n",
            "\n",
            "    _parse = _parseNoCache\n",
            "\n",
            "    @staticmethod\n",
            "    def resetCache():\n",
            "        ParserElement.packrat_cache.clear()\n",
            "        ParserElement.packrat_cache_stats[:] = [0] * len(ParserElement.packrat_cache_stats)\n",
            "\n",
            "    _packratEnabled = False\n",
            "    @staticmethod\n",
            "    def enablePackrat(cache_size_limit=128):\n",
            "        \"\"\"Enables \"packrat\" parsing, which adds memoizing to the parsing logic.\n",
            "           Repeated parse attempts at the same string location (which happens\n",
            "           often in many complex grammars) can immediately return a cached value,\n",
            "           instead of re-executing parsing/validating code.  Memoizing is done of\n",
            "           both valid results and parsing exceptions.\n",
            "\n",
            "           Parameters:\n",
            "\n",
            "           - cache_size_limit - (default= ``128``) - if an integer value is provided\n",
            "             will limit the size of the packrat cache; if None is passed, then\n",
            "             the cache size will be unbounded; if 0 is passed, the cache will\n",
            "             be effectively disabled.\n",
            "\n",
            "           This speedup may break existing programs that use parse actions that\n",
            "           have side-effects.  For this reason, packrat parsing is disabled when\n",
            "           you first import pyparsing.  To activate the packrat feature, your\n",
            "           program must call the class method :class:`ParserElement.enablePackrat`.\n",
            "           For best results, call ``enablePackrat()`` immediately after\n",
            "           importing pyparsing.\n",
            "\n",
            "           Example::\n",
            "\n",
            "               import pyparsing\n",
            "               pyparsing.ParserElement.enablePackrat()\n",
            "        \"\"\"\n",
            "        if not ParserElement._packratEnabled:\n",
            "            ParserElement._packratEnabled = True\n",
            "            if cache_size_limit is None:\n",
            "                ParserElement.packrat_cache = ParserElement._UnboundedCache()\n",
            "            else:\n",
            "                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)\n",
            "            ParserElement._parse = ParserElement._parseCache\n",
            "\n",
            "    def parseString(self, instring, parseAll=False):\n",
            "        \"\"\"\n",
            "        Execute the parse expression with the given string.\n",
            "        This is the main interface to the client code, once the complete\n",
            "        expression has been built.\n",
            "\n",
            "        Returns the parsed data as a :class:`ParseResults` object, which may be\n",
            "        accessed as a list, or as a dict or object with attributes if the given parser\n",
            "        includes results names.\n",
            "\n",
            "        If you want the grammar to require that the entire input string be\n",
            "        successfully parsed, then set ``parseAll`` to True (equivalent to ending\n",
            "        the grammar with ``StringEnd()``).\n",
            "\n",
            "        Note: ``parseString`` implicitly calls ``expandtabs()`` on the input string,\n",
            "        in order to report proper column numbers in parse actions.\n",
            "        If the input string contains tabs and\n",
            "        the grammar uses parse actions that use the ``loc`` argument to index into the\n",
            "        string being parsed, you can ensure you have a consistent view of the input\n",
            "        string by:\n",
            "\n",
            "        - calling ``parseWithTabs`` on your grammar before calling ``parseString``\n",
            "          (see :class:`parseWithTabs`)\n",
            "        - define your parse action using the full ``(s, loc, toks)`` signature, and\n",
            "          reference the input string using the parse action's ``s`` argument\n",
            "        - explictly expand the tabs in your input string before calling\n",
            "          ``parseString``\n",
            "\n",
            "        Example::\n",
            "\n",
            "            Word('a').parseString('aaaaabaaa')  # -> ['aaaaa']\n",
            "            Word('a').parseString('aaaaabaaa', parseAll=True)  # -> Exception: Expected end of text\n",
            "        \"\"\"\n",
            "        ParserElement.resetCache()\n",
            "        if not self.streamlined:\n",
            "            self.streamline()\n",
            "            # ~ self.saveAsList = True\n",
            "        for e in self.ignoreExprs:\n",
            "            e.streamline()\n",
            "        if not self.keepTabs:\n",
            "            instring = instring.expandtabs()\n",
            "        try:\n",
            "            loc, tokens = self._parse(instring, 0)\n",
            "            if parseAll:\n",
            "                loc = self.preParse(instring, loc)\n",
            "                se = Empty() + StringEnd()\n",
            "                se._parse(instring, loc)\n",
            "        except ParseBaseException as exc:\n",
            "            if ParserElement.verbose_stacktrace:\n",
            "                raise\n",
            "            else:\n",
            "                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n",
            "                raise exc\n",
            "        else:\n",
            "            return tokens\n",
            "\n",
            "    def scanString(self, instring, maxMatches=_MAX_INT, overlap=False):\n",
            "        \"\"\"\n",
            "        Scan the input string for expression matches.  Each match will return the\n",
            "        matching tokens, start location, and end location.  May be called with optional\n",
            "        ``maxMatches`` argument, to clip scanning after 'n' matches are found.  If\n",
            "        ``overlap`` is specified, then overlapping matches will be reported.\n",
            "\n",
            "        Note that the start and end locations are reported relative to the string\n",
            "        being parsed.  See :class:`parseString` for more information on parsing\n",
            "        strings with embedded tabs.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            source = \"sldjf123lsdjjkf345sldkjf879lkjsfd987\"\n",
            "            print(source)\n",
            "            for tokens, start, end in Word(alphas).scanString(source):\n",
            "                print(' '*start + '^'*(end-start))\n",
            "                print(' '*start + tokens[0])\n",
            "\n",
            "        prints::\n",
            "\n",
            "            sldjf123lsdjjkf345sldkjf879lkjsfd987\n",
            "            ^^^^^\n",
            "            sldjf\n",
            "                    ^^^^^^^\n",
            "                    lsdjjkf\n",
            "                              ^^^^^^\n",
            "                              sldkjf\n",
            "                                       ^^^^^^\n",
            "                                       lkjsfd\n",
            "        \"\"\"\n",
            "        if not self.streamlined:\n",
            "            self.streamline()\n",
            "        for e in self.ignoreExprs:\n",
            "            e.streamline()\n",
            "\n",
            "        if not self.keepTabs:\n",
            "            instring = _ustr(instring).expandtabs()\n",
            "        instrlen = len(instring)\n",
            "        loc = 0\n",
            "        preparseFn = self.preParse\n",
            "        parseFn = self._parse\n",
            "        ParserElement.resetCache()\n",
            "        matches = 0\n",
            "        try:\n",
            "            while loc <= instrlen and matches < maxMatches:\n",
            "                try:\n",
            "                    preloc = preparseFn(instring, loc)\n",
            "                    nextLoc, tokens = parseFn(instring, preloc, callPreParse=False)\n",
            "                except ParseException:\n",
            "                    loc = preloc + 1\n",
            "                else:\n",
            "                    if nextLoc > loc:\n",
            "                        matches += 1\n",
            "                        yield tokens, preloc, nextLoc\n",
            "                        if overlap:\n",
            "                            nextloc = preparseFn(instring, loc)\n",
            "                            if nextloc > loc:\n",
            "                                loc = nextLoc\n",
            "                            else:\n",
            "                                loc += 1\n",
            "                        else:\n",
            "                            loc = nextLoc\n",
            "                    else:\n",
            "                        loc = preloc + 1\n",
            "        except ParseBaseException as exc:\n",
            "            if ParserElement.verbose_stacktrace:\n",
            "                raise\n",
            "            else:\n",
            "                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n",
            "                raise exc\n",
            "\n",
            "    def transformString(self, instring):\n",
            "        \"\"\"\n",
            "        Extension to :class:`scanString`, to modify matching text with modified tokens that may\n",
            "        be returned from a parse action.  To use ``transformString``, define a grammar and\n",
            "        attach a parse action to it that modifies the returned token list.\n",
            "        Invoking ``transformString()`` on a target string will then scan for matches,\n",
            "        and replace the matched text patterns according to the logic in the parse\n",
            "        action.  ``transformString()`` returns the resulting transformed string.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            wd = Word(alphas)\n",
            "            wd.setParseAction(lambda toks: toks[0].title())\n",
            "\n",
            "            print(wd.transformString(\"now is the winter of our discontent made glorious summer by this sun of york.\"))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.\n",
            "        \"\"\"\n",
            "        out = []\n",
            "        lastE = 0\n",
            "        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to\n",
            "        # keep string locs straight between transformString and scanString\n",
            "        self.keepTabs = True\n",
            "        try:\n",
            "            for t, s, e in self.scanString(instring):\n",
            "                out.append(instring[lastE:s])\n",
            "                if t:\n",
            "                    if isinstance(t, ParseResults):\n",
            "                        out += t.asList()\n",
            "                    elif isinstance(t, list):\n",
            "                        out += t\n",
            "                    else:\n",
            "                        out.append(t)\n",
            "                lastE = e\n",
            "            out.append(instring[lastE:])\n",
            "            out = [o for o in out if o]\n",
            "            return \"\".join(map(_ustr, _flatten(out)))\n",
            "        except ParseBaseException as exc:\n",
            "            if ParserElement.verbose_stacktrace:\n",
            "                raise\n",
            "            else:\n",
            "                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n",
            "                raise exc\n",
            "\n",
            "    def searchString(self, instring, maxMatches=_MAX_INT):\n",
            "        \"\"\"\n",
            "        Another extension to :class:`scanString`, simplifying the access to the tokens found\n",
            "        to match the given parse expression.  May be called with optional\n",
            "        ``maxMatches`` argument, to clip searching after 'n' matches are found.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters\n",
            "            cap_word = Word(alphas.upper(), alphas.lower())\n",
            "\n",
            "            print(cap_word.searchString(\"More than Iron, more than Lead, more than Gold I need Electricity\"))\n",
            "\n",
            "            # the sum() builtin can be used to merge results into a single ParseResults object\n",
            "            print(sum(cap_word.searchString(\"More than Iron, more than Lead, more than Gold I need Electricity\")))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            [['More'], ['Iron'], ['Lead'], ['Gold'], ['I'], ['Electricity']]\n",
            "            ['More', 'Iron', 'Lead', 'Gold', 'I', 'Electricity']\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return ParseResults([t for t, s, e in self.scanString(instring, maxMatches)])\n",
            "        except ParseBaseException as exc:\n",
            "            if ParserElement.verbose_stacktrace:\n",
            "                raise\n",
            "            else:\n",
            "                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n",
            "                raise exc\n",
            "\n",
            "    def split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):\n",
            "        \"\"\"\n",
            "        Generator method to split a string using the given expression as a separator.\n",
            "        May be called with optional ``maxsplit`` argument, to limit the number of splits;\n",
            "        and the optional ``includeSeparators`` argument (default= ``False``), if the separating\n",
            "        matching text should be included in the split results.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            punc = oneOf(list(\".,;:/-!?\"))\n",
            "            print(list(punc.split(\"This, this?, this sentence, is badly punctuated!\")))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']\n",
            "        \"\"\"\n",
            "        splits = 0\n",
            "        last = 0\n",
            "        for t, s, e in self.scanString(instring, maxMatches=maxsplit):\n",
            "            yield instring[last:s]\n",
            "            if includeSeparators:\n",
            "                yield t[0]\n",
            "            last = e\n",
            "        yield instring[last:]\n",
            "\n",
            "    def __add__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of + operator - returns :class:`And`. Adding strings to a ParserElement\n",
            "        converts them to :class:`Literal`s by default.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            greet = Word(alphas) + \",\" + Word(alphas) + \"!\"\n",
            "            hello = \"Hello, World!\"\n",
            "            print (hello, \"->\", greet.parseString(hello))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            Hello, World! -> ['Hello', ',', 'World', '!']\n",
            "\n",
            "        ``...`` may be used as a parse expression as a short form of :class:`SkipTo`.\n",
            "\n",
            "            Literal('start') + ... + Literal('end')\n",
            "\n",
            "        is equivalent to:\n",
            "\n",
            "            Literal('start') + SkipTo('end')(\"_skipped*\") + Literal('end')\n",
            "\n",
            "        Note that the skipped text is returned with '_skipped' as a results name,\n",
            "        and to support having multiple skips in the same parser, the value returned is\n",
            "        a list of all skipped text.\n",
            "        \"\"\"\n",
            "        if other is Ellipsis:\n",
            "            return _PendingSkip(self)\n",
            "\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return And([self, other])\n",
            "\n",
            "    def __radd__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of + operator when left operand is not a :class:`ParserElement`\n",
            "        \"\"\"\n",
            "        if other is Ellipsis:\n",
            "            return SkipTo(self)(\"_skipped*\") + self\n",
            "\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return other + self\n",
            "\n",
            "    def __sub__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of - operator, returns :class:`And` with error stop\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return self + And._ErrorStop() + other\n",
            "\n",
            "    def __rsub__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of - operator when left operand is not a :class:`ParserElement`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return other - self\n",
            "\n",
            "    def __mul__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of * operator, allows use of ``expr * 3`` in place of\n",
            "        ``expr + expr + expr``.  Expressions may also me multiplied by a 2-integer\n",
            "        tuple, similar to ``{min, max}`` multipliers in regular expressions.  Tuples\n",
            "        may also include ``None`` as in:\n",
            "         - ``expr*(n, None)`` or ``expr*(n, )`` is equivalent\n",
            "              to ``expr*n + ZeroOrMore(expr)``\n",
            "              (read as \"at least n instances of ``expr``\")\n",
            "         - ``expr*(None, n)`` is equivalent to ``expr*(0, n)``\n",
            "              (read as \"0 to n instances of ``expr``\")\n",
            "         - ``expr*(None, None)`` is equivalent to ``ZeroOrMore(expr)``\n",
            "         - ``expr*(1, None)`` is equivalent to ``OneOrMore(expr)``\n",
            "\n",
            "        Note that ``expr*(None, n)`` does not raise an exception if\n",
            "        more than n exprs exist in the input stream; that is,\n",
            "        ``expr*(None, n)`` does not enforce a maximum number of expr\n",
            "        occurrences.  If this behavior is desired, then write\n",
            "        ``expr*(None, n) + ~expr``\n",
            "        \"\"\"\n",
            "        if other is Ellipsis:\n",
            "            other = (0, None)\n",
            "        elif isinstance(other, tuple) and other[:1] == (Ellipsis,):\n",
            "            other = ((0, ) + other[1:] + (None,))[:2]\n",
            "\n",
            "        if isinstance(other, int):\n",
            "            minElements, optElements = other, 0\n",
            "        elif isinstance(other, tuple):\n",
            "            other = tuple(o if o is not Ellipsis else None for o in other)\n",
            "            other = (other + (None, None))[:2]\n",
            "            if other[0] is None:\n",
            "                other = (0, other[1])\n",
            "            if isinstance(other[0], int) and other[1] is None:\n",
            "                if other[0] == 0:\n",
            "                    return ZeroOrMore(self)\n",
            "                if other[0] == 1:\n",
            "                    return OneOrMore(self)\n",
            "                else:\n",
            "                    return self * other[0] + ZeroOrMore(self)\n",
            "            elif isinstance(other[0], int) and isinstance(other[1], int):\n",
            "                minElements, optElements = other\n",
            "                optElements -= minElements\n",
            "            else:\n",
            "                raise TypeError(\"cannot multiply 'ParserElement' and ('%s', '%s') objects\", type(other[0]), type(other[1]))\n",
            "        else:\n",
            "            raise TypeError(\"cannot multiply 'ParserElement' and '%s' objects\", type(other))\n",
            "\n",
            "        if minElements < 0:\n",
            "            raise ValueError(\"cannot multiply ParserElement by negative value\")\n",
            "        if optElements < 0:\n",
            "            raise ValueError(\"second tuple value must be greater or equal to first tuple value\")\n",
            "        if minElements == optElements == 0:\n",
            "            raise ValueError(\"cannot multiply ParserElement by 0 or (0, 0)\")\n",
            "\n",
            "        if optElements:\n",
            "            def makeOptionalList(n):\n",
            "                if n > 1:\n",
            "                    return Optional(self + makeOptionalList(n - 1))\n",
            "                else:\n",
            "                    return Optional(self)\n",
            "            if minElements:\n",
            "                if minElements == 1:\n",
            "                    ret = self + makeOptionalList(optElements)\n",
            "                else:\n",
            "                    ret = And([self] * minElements) + makeOptionalList(optElements)\n",
            "            else:\n",
            "                ret = makeOptionalList(optElements)\n",
            "        else:\n",
            "            if minElements == 1:\n",
            "                ret = self\n",
            "            else:\n",
            "                ret = And([self] * minElements)\n",
            "        return ret\n",
            "\n",
            "    def __rmul__(self, other):\n",
            "        return self.__mul__(other)\n",
            "\n",
            "    def __or__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of | operator - returns :class:`MatchFirst`\n",
            "        \"\"\"\n",
            "        if other is Ellipsis:\n",
            "            return _PendingSkip(self, must_skip=True)\n",
            "\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return MatchFirst([self, other])\n",
            "\n",
            "    def __ror__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of | operator when left operand is not a :class:`ParserElement`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return other | self\n",
            "\n",
            "    def __xor__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of ^ operator - returns :class:`Or`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return Or([self, other])\n",
            "\n",
            "    def __rxor__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of ^ operator when left operand is not a :class:`ParserElement`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return other ^ self\n",
            "\n",
            "    def __and__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of & operator - returns :class:`Each`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return Each([self, other])\n",
            "\n",
            "    def __rand__(self, other):\n",
            "        \"\"\"\n",
            "        Implementation of & operator when left operand is not a :class:`ParserElement`\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        if not isinstance(other, ParserElement):\n",
            "            warnings.warn(\"Cannot combine element of type %s with ParserElement\" % type(other),\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            return None\n",
            "        return other & self\n",
            "\n",
            "    def __invert__(self):\n",
            "        \"\"\"\n",
            "        Implementation of ~ operator - returns :class:`NotAny`\n",
            "        \"\"\"\n",
            "        return NotAny(self)\n",
            "\n",
            "    def __iter__(self):\n",
            "        # must implement __iter__ to override legacy use of sequential access to __getitem__ to\n",
            "        # iterate over a sequence\n",
            "        raise TypeError('%r object is not iterable' % self.__class__.__name__)\n",
            "\n",
            "    def __getitem__(self, key):\n",
            "        \"\"\"\n",
            "        use ``[]`` indexing notation as a short form for expression repetition:\n",
            "         - ``expr[n]`` is equivalent to ``expr*n``\n",
            "         - ``expr[m, n]`` is equivalent to ``expr*(m, n)``\n",
            "         - ``expr[n, ...]`` or ``expr[n,]`` is equivalent\n",
            "              to ``expr*n + ZeroOrMore(expr)``\n",
            "              (read as \"at least n instances of ``expr``\")\n",
            "         - ``expr[..., n]`` is equivalent to ``expr*(0, n)``\n",
            "              (read as \"0 to n instances of ``expr``\")\n",
            "         - ``expr[...]`` and ``expr[0, ...]`` are equivalent to ``ZeroOrMore(expr)``\n",
            "         - ``expr[1, ...]`` is equivalent to ``OneOrMore(expr)``\n",
            "         ``None`` may be used in place of ``...``.\n",
            "\n",
            "        Note that ``expr[..., n]`` and ``expr[m, n]``do not raise an exception\n",
            "        if more than ``n`` ``expr``s exist in the input stream.  If this behavior is\n",
            "        desired, then write ``expr[..., n] + ~expr``.\n",
            "       \"\"\"\n",
            "\n",
            "        # convert single arg keys to tuples\n",
            "        try:\n",
            "            if isinstance(key, str):\n",
            "                key = (key,)\n",
            "            iter(key)\n",
            "        except TypeError:\n",
            "            key = (key, key)\n",
            "\n",
            "        if len(key) > 2:\n",
            "            warnings.warn(\"only 1 or 2 index arguments supported ({0}{1})\".format(key[:5],\n",
            "                                                                                '... [{0}]'.format(len(key))\n",
            "                                                                                if len(key) > 5 else ''))\n",
            "\n",
            "        # clip to 2 elements\n",
            "        ret = self * tuple(key[:2])\n",
            "        return ret\n",
            "\n",
            "    def __call__(self, name=None):\n",
            "        \"\"\"\n",
            "        Shortcut for :class:`setResultsName`, with ``listAllMatches=False``.\n",
            "\n",
            "        If ``name`` is given with a trailing ``'*'`` character, then ``listAllMatches`` will be\n",
            "        passed as ``True``.\n",
            "\n",
            "        If ``name` is omitted, same as calling :class:`copy`.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            # these are equivalent\n",
            "            userdata = Word(alphas).setResultsName(\"name\") + Word(nums + \"-\").setResultsName(\"socsecno\")\n",
            "            userdata = Word(alphas)(\"name\") + Word(nums + \"-\")(\"socsecno\")\n",
            "        \"\"\"\n",
            "        if name is not None:\n",
            "            return self._setResultsName(name)\n",
            "        else:\n",
            "            return self.copy()\n",
            "\n",
            "    def suppress(self):\n",
            "        \"\"\"\n",
            "        Suppresses the output of this :class:`ParserElement`; useful to keep punctuation from\n",
            "        cluttering up returned output.\n",
            "        \"\"\"\n",
            "        return Suppress(self)\n",
            "\n",
            "    def leaveWhitespace(self):\n",
            "        \"\"\"\n",
            "        Disables the skipping of whitespace before matching the characters in the\n",
            "        :class:`ParserElement`'s defined pattern.  This is normally only used internally by\n",
            "        the pyparsing module, but may be needed in some whitespace-sensitive grammars.\n",
            "        \"\"\"\n",
            "        self.skipWhitespace = False\n",
            "        return self\n",
            "\n",
            "    def setWhitespaceChars(self, chars):\n",
            "        \"\"\"\n",
            "        Overrides the default whitespace chars\n",
            "        \"\"\"\n",
            "        self.skipWhitespace = True\n",
            "        self.whiteChars = chars\n",
            "        self.copyDefaultWhiteChars = False\n",
            "        return self\n",
            "\n",
            "    def parseWithTabs(self):\n",
            "        \"\"\"\n",
            "        Overrides default behavior to expand ``<TAB>``s to spaces before parsing the input string.\n",
            "        Must be called before ``parseString`` when the input grammar contains elements that\n",
            "        match ``<TAB>`` characters.\n",
            "        \"\"\"\n",
            "        self.keepTabs = True\n",
            "        return self\n",
            "\n",
            "    def ignore(self, other):\n",
            "        \"\"\"\n",
            "        Define expression to be ignored (e.g., comments) while doing pattern\n",
            "        matching; may be called repeatedly, to define multiple comment or other\n",
            "        ignorable patterns.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            patt = OneOrMore(Word(alphas))\n",
            "            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']\n",
            "\n",
            "            patt.ignore(cStyleComment)\n",
            "            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']\n",
            "        \"\"\"\n",
            "        if isinstance(other, basestring):\n",
            "            other = Suppress(other)\n",
            "\n",
            "        if isinstance(other, Suppress):\n",
            "            if other not in self.ignoreExprs:\n",
            "                self.ignoreExprs.append(other)\n",
            "        else:\n",
            "            self.ignoreExprs.append(Suppress(other.copy()))\n",
            "        return self\n",
            "\n",
            "    def setDebugActions(self, startAction, successAction, exceptionAction):\n",
            "        \"\"\"\n",
            "        Enable display of debugging messages while doing pattern matching.\n",
            "        \"\"\"\n",
            "        self.debugActions = (startAction or _defaultStartDebugAction,\n",
            "                             successAction or _defaultSuccessDebugAction,\n",
            "                             exceptionAction or _defaultExceptionDebugAction)\n",
            "        self.debug = True\n",
            "        return self\n",
            "\n",
            "    def setDebug(self, flag=True):\n",
            "        \"\"\"\n",
            "        Enable display of debugging messages while doing pattern matching.\n",
            "        Set ``flag`` to True to enable, False to disable.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            wd = Word(alphas).setName(\"alphaword\")\n",
            "            integer = Word(nums).setName(\"numword\")\n",
            "            term = wd | integer\n",
            "\n",
            "            # turn on debugging for wd\n",
            "            wd.setDebug()\n",
            "\n",
            "            OneOrMore(term).parseString(\"abc 123 xyz 890\")\n",
            "\n",
            "        prints::\n",
            "\n",
            "            Match alphaword at loc 0(1,1)\n",
            "            Matched alphaword -> ['abc']\n",
            "            Match alphaword at loc 3(1,4)\n",
            "            Exception raised:Expected alphaword (at char 4), (line:1, col:5)\n",
            "            Match alphaword at loc 7(1,8)\n",
            "            Matched alphaword -> ['xyz']\n",
            "            Match alphaword at loc 11(1,12)\n",
            "            Exception raised:Expected alphaword (at char 12), (line:1, col:13)\n",
            "            Match alphaword at loc 15(1,16)\n",
            "            Exception raised:Expected alphaword (at char 15), (line:1, col:16)\n",
            "\n",
            "        The output shown is that produced by the default debug actions - custom debug actions can be\n",
            "        specified using :class:`setDebugActions`. Prior to attempting\n",
            "        to match the ``wd`` expression, the debugging message ``\"Match <exprname> at loc <n>(<line>,<col>)\"``\n",
            "        is shown. Then if the parse succeeds, a ``\"Matched\"`` message is shown, or an ``\"Exception raised\"``\n",
            "        message is shown. Also note the use of :class:`setName` to assign a human-readable name to the expression,\n",
            "        which makes debugging and exception messages easier to understand - for instance, the default\n",
            "        name created for the :class:`Word` expression without calling ``setName`` is ``\"W:(ABCD...)\"``.\n",
            "        \"\"\"\n",
            "        if flag:\n",
            "            self.setDebugActions(_defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction)\n",
            "        else:\n",
            "            self.debug = False\n",
            "        return self\n",
            "\n",
            "    def __str__(self):\n",
            "        return self.name\n",
            "\n",
            "    def __repr__(self):\n",
            "        return _ustr(self)\n",
            "\n",
            "    def streamline(self):\n",
            "        self.streamlined = True\n",
            "        self.strRepr = None\n",
            "        return self\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        pass\n",
            "\n",
            "    def validate(self, validateTrace=None):\n",
            "        \"\"\"\n",
            "        Check defined expressions for valid structure, check for infinite recursive definitions.\n",
            "        \"\"\"\n",
            "        self.checkRecursion([])\n",
            "\n",
            "    def parseFile(self, file_or_filename, parseAll=False):\n",
            "        \"\"\"\n",
            "        Execute the parse expression on the given file or filename.\n",
            "        If a filename is specified (instead of a file object),\n",
            "        the entire file is opened, read, and closed before parsing.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            file_contents = file_or_filename.read()\n",
            "        except AttributeError:\n",
            "            with open(file_or_filename, \"r\") as f:\n",
            "                file_contents = f.read()\n",
            "        try:\n",
            "            return self.parseString(file_contents, parseAll)\n",
            "        except ParseBaseException as exc:\n",
            "            if ParserElement.verbose_stacktrace:\n",
            "                raise\n",
            "            else:\n",
            "                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n",
            "                raise exc\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if isinstance(other, ParserElement):\n",
            "            if PY_3:\n",
            "                self is other or super(ParserElement, self).__eq__(other)\n",
            "            else:\n",
            "                return self is other or vars(self) == vars(other)\n",
            "        elif isinstance(other, basestring):\n",
            "            return self.matches(other)\n",
            "        else:\n",
            "            return super(ParserElement, self) == other\n",
            "\n",
            "    def __ne__(self, other):\n",
            "        return not (self == other)\n",
            "\n",
            "    def __hash__(self):\n",
            "        return id(self)\n",
            "\n",
            "    def __req__(self, other):\n",
            "        return self == other\n",
            "\n",
            "    def __rne__(self, other):\n",
            "        return not (self == other)\n",
            "\n",
            "    def matches(self, testString, parseAll=True):\n",
            "        \"\"\"\n",
            "        Method for quick testing of a parser against a test string. Good for simple\n",
            "        inline microtests of sub expressions while building up larger parser.\n",
            "\n",
            "        Parameters:\n",
            "         - testString - to test against this expression for a match\n",
            "         - parseAll - (default= ``True``) - flag to pass to :class:`parseString` when running tests\n",
            "\n",
            "        Example::\n",
            "\n",
            "            expr = Word(nums)\n",
            "            assert expr.matches(\"100\")\n",
            "        \"\"\"\n",
            "        try:\n",
            "            self.parseString(_ustr(testString), parseAll=parseAll)\n",
            "            return True\n",
            "        except ParseBaseException:\n",
            "            return False\n",
            "\n",
            "    def runTests(self, tests, parseAll=True, comment='#',\n",
            "                 fullDump=True, printResults=True, failureTests=False, postParse=None,\n",
            "                 file=None):\n",
            "        \"\"\"\n",
            "        Execute the parse expression on a series of test strings, showing each\n",
            "        test, the parsed results or where the parse failed. Quick and easy way to\n",
            "        run a parse expression against a list of sample strings.\n",
            "\n",
            "        Parameters:\n",
            "         - tests - a list of separate test strings, or a multiline string of test strings\n",
            "         - parseAll - (default= ``True``) - flag to pass to :class:`parseString` when running tests\n",
            "         - comment - (default= ``'#'``) - expression for indicating embedded comments in the test\n",
            "              string; pass None to disable comment filtering\n",
            "         - fullDump - (default= ``True``) - dump results as list followed by results names in nested outline;\n",
            "              if False, only dump nested list\n",
            "         - printResults - (default= ``True``) prints test output to stdout\n",
            "         - failureTests - (default= ``False``) indicates if these tests are expected to fail parsing\n",
            "         - postParse - (default= ``None``) optional callback for successful parse results; called as\n",
            "              `fn(test_string, parse_results)` and returns a string to be added to the test output\n",
            "         - file - (default=``None``) optional file-like object to which test output will be written;\n",
            "              if None, will default to ``sys.stdout``\n",
            "\n",
            "        Returns: a (success, results) tuple, where success indicates that all tests succeeded\n",
            "        (or failed if ``failureTests`` is True), and the results contain a list of lines of each\n",
            "        test's output\n",
            "\n",
            "        Example::\n",
            "\n",
            "            number_expr = pyparsing_common.number.copy()\n",
            "\n",
            "            result = number_expr.runTests('''\n",
            "                # unsigned integer\n",
            "                100\n",
            "                # negative integer\n",
            "                -100\n",
            "                # float with scientific notation\n",
            "                6.02e23\n",
            "                # integer with scientific notation\n",
            "                1e-12\n",
            "                ''')\n",
            "            print(\"Success\" if result[0] else \"Failed!\")\n",
            "\n",
            "            result = number_expr.runTests('''\n",
            "                # stray character\n",
            "                100Z\n",
            "                # missing leading digit before '.'\n",
            "                -.100\n",
            "                # too many '.'\n",
            "                3.14.159\n",
            "                ''', failureTests=True)\n",
            "            print(\"Success\" if result[0] else \"Failed!\")\n",
            "\n",
            "        prints::\n",
            "\n",
            "            # unsigned integer\n",
            "            100\n",
            "            [100]\n",
            "\n",
            "            # negative integer\n",
            "            -100\n",
            "            [-100]\n",
            "\n",
            "            # float with scientific notation\n",
            "            6.02e23\n",
            "            [6.02e+23]\n",
            "\n",
            "            # integer with scientific notation\n",
            "            1e-12\n",
            "            [1e-12]\n",
            "\n",
            "            Success\n",
            "\n",
            "            # stray character\n",
            "            100Z\n",
            "               ^\n",
            "            FAIL: Expected end of text (at char 3), (line:1, col:4)\n",
            "\n",
            "            # missing leading digit before '.'\n",
            "            -.100\n",
            "            ^\n",
            "            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)\n",
            "\n",
            "            # too many '.'\n",
            "            3.14.159\n",
            "                ^\n",
            "            FAIL: Expected end of text (at char 4), (line:1, col:5)\n",
            "\n",
            "            Success\n",
            "\n",
            "        Each test string must be on a single line. If you want to test a string that spans multiple\n",
            "        lines, create a test like this::\n",
            "\n",
            "            expr.runTest(r\"this is a test\\\\n of strings that spans \\\\n 3 lines\")\n",
            "\n",
            "        (Note that this is a raw string literal, you must include the leading 'r'.)\n",
            "        \"\"\"\n",
            "        if isinstance(tests, basestring):\n",
            "            tests = list(map(str.strip, tests.rstrip().splitlines()))\n",
            "        if isinstance(comment, basestring):\n",
            "            comment = Literal(comment)\n",
            "        if file is None:\n",
            "            file = sys.stdout\n",
            "        print_ = file.write\n",
            "\n",
            "        allResults = []\n",
            "        comments = []\n",
            "        success = True\n",
            "        NL = Literal(r'\\n').addParseAction(replaceWith('\\n')).ignore(quotedString)\n",
            "        BOM = u'\\ufeff'\n",
            "        for t in tests:\n",
            "            if comment is not None and comment.matches(t, False) or comments and not t:\n",
            "                comments.append(t)\n",
            "                continue\n",
            "            if not t:\n",
            "                continue\n",
            "            out = ['\\n'.join(comments), t]\n",
            "            comments = []\n",
            "            try:\n",
            "                # convert newline marks to actual newlines, and strip leading BOM if present\n",
            "                t = NL.transformString(t.lstrip(BOM))\n",
            "                result = self.parseString(t, parseAll=parseAll)\n",
            "            except ParseBaseException as pe:\n",
            "                fatal = \"(FATAL)\" if isinstance(pe, ParseFatalException) else \"\"\n",
            "                if '\\n' in t:\n",
            "                    out.append(line(pe.loc, t))\n",
            "                    out.append(' ' * (col(pe.loc, t) - 1) + '^' + fatal)\n",
            "                else:\n",
            "                    out.append(' ' * pe.loc + '^' + fatal)\n",
            "                out.append(\"FAIL: \" + str(pe))\n",
            "                success = success and failureTests\n",
            "                result = pe\n",
            "            except Exception as exc:\n",
            "                out.append(\"FAIL-EXCEPTION: \" + str(exc))\n",
            "                success = success and failureTests\n",
            "                result = exc\n",
            "            else:\n",
            "                success = success and not failureTests\n",
            "                if postParse is not None:\n",
            "                    try:\n",
            "                        pp_value = postParse(t, result)\n",
            "                        if pp_value is not None:\n",
            "                            if isinstance(pp_value, ParseResults):\n",
            "                                out.append(pp_value.dump())\n",
            "                            else:\n",
            "                                out.append(str(pp_value))\n",
            "                        else:\n",
            "                            out.append(result.dump())\n",
            "                    except Exception as e:\n",
            "                        out.append(result.dump(full=fullDump))\n",
            "                        out.append(\"{0} failed: {1}: {2}\".format(postParse.__name__, type(e).__name__, e))\n",
            "                else:\n",
            "                    out.append(result.dump(full=fullDump))\n",
            "\n",
            "            if printResults:\n",
            "                if fullDump:\n",
            "                    out.append('')\n",
            "                print_('\\n'.join(out))\n",
            "\n",
            "            allResults.append((t, result))\n",
            "\n",
            "        return success, allResults\n",
            "\n",
            "\n",
            "class _PendingSkip(ParserElement):\n",
            "    # internal placeholder class to hold a place were '...' is added to a parser element,\n",
            "    # once another ParserElement is added, this placeholder will be replaced with a SkipTo\n",
            "    def __init__(self, expr, must_skip=False):\n",
            "        super(_PendingSkip, self).__init__()\n",
            "        self.strRepr = str(expr + Empty()).replace('Empty', '...')\n",
            "        self.name = self.strRepr\n",
            "        self.anchor = expr\n",
            "        self.must_skip = must_skip\n",
            "\n",
            "    def __add__(self, other):\n",
            "        skipper = SkipTo(other).setName(\"...\")(\"_skipped*\")\n",
            "        if self.must_skip:\n",
            "            def must_skip(t):\n",
            "                if not t._skipped or t._skipped.asList() == ['']:\n",
            "                    del t[0]\n",
            "                    t.pop(\"_skipped\", None)\n",
            "            def show_skip(t):\n",
            "                if t._skipped.asList()[-1:] == ['']:\n",
            "                    skipped = t.pop('_skipped')\n",
            "                    t['_skipped'] = 'missing <' + repr(self.anchor) + '>'\n",
            "            return (self.anchor + skipper().addParseAction(must_skip)\n",
            "                    | skipper().addParseAction(show_skip)) + other\n",
            "\n",
            "        return self.anchor + skipper + other\n",
            "\n",
            "    def __repr__(self):\n",
            "        return self.strRepr\n",
            "\n",
            "    def parseImpl(self, *args):\n",
            "        raise Exception(\"use of `...` expression without following SkipTo target expression\")\n",
            "\n",
            "\n",
            "class Token(ParserElement):\n",
            "    \"\"\"Abstract :class:`ParserElement` subclass, for defining atomic\n",
            "    matching patterns.\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(Token, self).__init__(savelist=False)\n",
            "\n",
            "\n",
            "class Empty(Token):\n",
            "    \"\"\"An empty token, will always match.\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(Empty, self).__init__()\n",
            "        self.name = \"Empty\"\n",
            "        self.mayReturnEmpty = True\n",
            "        self.mayIndexError = False\n",
            "\n",
            "\n",
            "class NoMatch(Token):\n",
            "    \"\"\"A token that will never match.\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(NoMatch, self).__init__()\n",
            "        self.name = \"NoMatch\"\n",
            "        self.mayReturnEmpty = True\n",
            "        self.mayIndexError = False\n",
            "        self.errmsg = \"Unmatchable token\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "\n",
            "class Literal(Token):\n",
            "    \"\"\"Token to exactly match a specified string.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        Literal('blah').parseString('blah')  # -> ['blah']\n",
            "        Literal('blah').parseString('blahfooblah')  # -> ['blah']\n",
            "        Literal('blah').parseString('bla')  # -> Exception: Expected \"blah\"\n",
            "\n",
            "    For case-insensitive matching, use :class:`CaselessLiteral`.\n",
            "\n",
            "    For keyword matching (force word break before and after the matched string),\n",
            "    use :class:`Keyword` or :class:`CaselessKeyword`.\n",
            "    \"\"\"\n",
            "    def __init__(self, matchString):\n",
            "        super(Literal, self).__init__()\n",
            "        self.match = matchString\n",
            "        self.matchLen = len(matchString)\n",
            "        try:\n",
            "            self.firstMatchChar = matchString[0]\n",
            "        except IndexError:\n",
            "            warnings.warn(\"null string passed to Literal; use Empty() instead\",\n",
            "                            SyntaxWarning, stacklevel=2)\n",
            "            self.__class__ = Empty\n",
            "        self.name = '\"%s\"' % _ustr(self.match)\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayReturnEmpty = False\n",
            "        self.mayIndexError = False\n",
            "\n",
            "        # Performance tuning: modify __class__ to select\n",
            "        # a parseImpl optimized for single-character check\n",
            "        if self.matchLen == 1 and type(self) is Literal:\n",
            "            self.__class__ = _SingleCharLiteral\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc] == self.firstMatchChar and instring.startswith(self.match, loc):\n",
            "            return loc + self.matchLen, self.match\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "class _SingleCharLiteral(Literal):\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc] == self.firstMatchChar:\n",
            "            return loc + 1, self.match\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "_L = Literal\n",
            "ParserElement._literalStringClass = Literal\n",
            "\n",
            "class Keyword(Token):\n",
            "    \"\"\"Token to exactly match a specified string as a keyword, that is,\n",
            "    it must be immediately followed by a non-keyword character.  Compare\n",
            "    with :class:`Literal`:\n",
            "\n",
            "     - ``Literal(\"if\")`` will match the leading ``'if'`` in\n",
            "       ``'ifAndOnlyIf'``.\n",
            "     - ``Keyword(\"if\")`` will not; it will only match the leading\n",
            "       ``'if'`` in ``'if x=1'``, or ``'if(y==2)'``\n",
            "\n",
            "    Accepts two optional constructor arguments in addition to the\n",
            "    keyword string:\n",
            "\n",
            "     - ``identChars`` is a string of characters that would be valid\n",
            "       identifier characters, defaulting to all alphanumerics + \"_\" and\n",
            "       \"$\"\n",
            "     - ``caseless`` allows case-insensitive matching, default is ``False``.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        Keyword(\"start\").parseString(\"start\")  # -> ['start']\n",
            "        Keyword(\"start\").parseString(\"starting\")  # -> Exception\n",
            "\n",
            "    For case-insensitive matching, use :class:`CaselessKeyword`.\n",
            "    \"\"\"\n",
            "    DEFAULT_KEYWORD_CHARS = alphanums + \"_$\"\n",
            "\n",
            "    def __init__(self, matchString, identChars=None, caseless=False):\n",
            "        super(Keyword, self).__init__()\n",
            "        if identChars is None:\n",
            "            identChars = Keyword.DEFAULT_KEYWORD_CHARS\n",
            "        self.match = matchString\n",
            "        self.matchLen = len(matchString)\n",
            "        try:\n",
            "            self.firstMatchChar = matchString[0]\n",
            "        except IndexError:\n",
            "            warnings.warn(\"null string passed to Keyword; use Empty() instead\",\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "        self.name = '\"%s\"' % self.match\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayReturnEmpty = False\n",
            "        self.mayIndexError = False\n",
            "        self.caseless = caseless\n",
            "        if caseless:\n",
            "            self.caselessmatch = matchString.upper()\n",
            "            identChars = identChars.upper()\n",
            "        self.identChars = set(identChars)\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if self.caseless:\n",
            "            if ((instring[loc:loc + self.matchLen].upper() == self.caselessmatch)\n",
            "                    and (loc >= len(instring) - self.matchLen\n",
            "                         or instring[loc + self.matchLen].upper() not in self.identChars)\n",
            "                    and (loc == 0\n",
            "                         or instring[loc - 1].upper() not in self.identChars)):\n",
            "                return loc + self.matchLen, self.match\n",
            "\n",
            "        else:\n",
            "            if instring[loc] == self.firstMatchChar:\n",
            "                if ((self.matchLen == 1 or instring.startswith(self.match, loc))\n",
            "                        and (loc >= len(instring) - self.matchLen\n",
            "                             or instring[loc + self.matchLen] not in self.identChars)\n",
            "                        and (loc == 0 or instring[loc - 1] not in self.identChars)):\n",
            "                    return loc + self.matchLen, self.match\n",
            "\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "    def copy(self):\n",
            "        c = super(Keyword, self).copy()\n",
            "        c.identChars = Keyword.DEFAULT_KEYWORD_CHARS\n",
            "        return c\n",
            "\n",
            "    @staticmethod\n",
            "    def setDefaultKeywordChars(chars):\n",
            "        \"\"\"Overrides the default Keyword chars\n",
            "        \"\"\"\n",
            "        Keyword.DEFAULT_KEYWORD_CHARS = chars\n",
            "\n",
            "class CaselessLiteral(Literal):\n",
            "    \"\"\"Token to match a specified string, ignoring case of letters.\n",
            "    Note: the matched results will always be in the case of the given\n",
            "    match string, NOT the case of the input text.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        OneOrMore(CaselessLiteral(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD', 'CMD']\n",
            "\n",
            "    (Contrast with example for :class:`CaselessKeyword`.)\n",
            "    \"\"\"\n",
            "    def __init__(self, matchString):\n",
            "        super(CaselessLiteral, self).__init__(matchString.upper())\n",
            "        # Preserve the defining literal.\n",
            "        self.returnString = matchString\n",
            "        self.name = \"'%s'\" % self.returnString\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc:loc + self.matchLen].upper() == self.match:\n",
            "            return loc + self.matchLen, self.returnString\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "class CaselessKeyword(Keyword):\n",
            "    \"\"\"\n",
            "    Caseless version of :class:`Keyword`.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        OneOrMore(CaselessKeyword(\"CMD\")).parseString(\"cmd CMD Cmd10\") # -> ['CMD', 'CMD']\n",
            "\n",
            "    (Contrast with example for :class:`CaselessLiteral`.)\n",
            "    \"\"\"\n",
            "    def __init__(self, matchString, identChars=None):\n",
            "        super(CaselessKeyword, self).__init__(matchString, identChars, caseless=True)\n",
            "\n",
            "class CloseMatch(Token):\n",
            "    \"\"\"A variation on :class:`Literal` which matches \"close\" matches,\n",
            "    that is, strings with at most 'n' mismatching characters.\n",
            "    :class:`CloseMatch` takes parameters:\n",
            "\n",
            "     - ``match_string`` - string to be matched\n",
            "     - ``maxMismatches`` - (``default=1``) maximum number of\n",
            "       mismatches allowed to count as a match\n",
            "\n",
            "    The results from a successful parse will contain the matched text\n",
            "    from the input string and the following named results:\n",
            "\n",
            "     - ``mismatches`` - a list of the positions within the\n",
            "       match_string where mismatches were found\n",
            "     - ``original`` - the original match_string used to compare\n",
            "       against the input string\n",
            "\n",
            "    If ``mismatches`` is an empty list, then the match was an exact\n",
            "    match.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        patt = CloseMatch(\"ATCATCGAATGGA\")\n",
            "        patt.parseString(\"ATCATCGAAXGGA\") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})\n",
            "        patt.parseString(\"ATCAXCGAAXGGA\") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)\n",
            "\n",
            "        # exact match\n",
            "        patt.parseString(\"ATCATCGAATGGA\") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})\n",
            "\n",
            "        # close match allowing up to 2 mismatches\n",
            "        patt = CloseMatch(\"ATCATCGAATGGA\", maxMismatches=2)\n",
            "        patt.parseString(\"ATCAXCGAAXGGA\") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})\n",
            "    \"\"\"\n",
            "    def __init__(self, match_string, maxMismatches=1):\n",
            "        super(CloseMatch, self).__init__()\n",
            "        self.name = match_string\n",
            "        self.match_string = match_string\n",
            "        self.maxMismatches = maxMismatches\n",
            "        self.errmsg = \"Expected %r (with up to %d mismatches)\" % (self.match_string, self.maxMismatches)\n",
            "        self.mayIndexError = False\n",
            "        self.mayReturnEmpty = False\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        start = loc\n",
            "        instrlen = len(instring)\n",
            "        maxloc = start + len(self.match_string)\n",
            "\n",
            "        if maxloc <= instrlen:\n",
            "            match_string = self.match_string\n",
            "            match_stringloc = 0\n",
            "            mismatches = []\n",
            "            maxMismatches = self.maxMismatches\n",
            "\n",
            "            for match_stringloc, s_m in enumerate(zip(instring[loc:maxloc], match_string)):\n",
            "                src, mat = s_m\n",
            "                if src != mat:\n",
            "                    mismatches.append(match_stringloc)\n",
            "                    if len(mismatches) > maxMismatches:\n",
            "                        break\n",
            "            else:\n",
            "                loc = match_stringloc + 1\n",
            "                results = ParseResults([instring[start:loc]])\n",
            "                results['original'] = match_string\n",
            "                results['mismatches'] = mismatches\n",
            "                return loc, results\n",
            "\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "\n",
            "class Word(Token):\n",
            "    \"\"\"Token for matching words composed of allowed character sets.\n",
            "    Defined with string containing all allowed initial characters, an\n",
            "    optional string containing allowed body characters (if omitted,\n",
            "    defaults to the initial character set), and an optional minimum,\n",
            "    maximum, and/or exact length.  The default value for ``min`` is\n",
            "    1 (a minimum value < 1 is not valid); the default values for\n",
            "    ``max`` and ``exact`` are 0, meaning no maximum or exact\n",
            "    length restriction. An optional ``excludeChars`` parameter can\n",
            "    list characters that might be found in the input ``bodyChars``\n",
            "    string; useful to define a word of all printables except for one or\n",
            "    two characters, for instance.\n",
            "\n",
            "    :class:`srange` is useful for defining custom character set strings\n",
            "    for defining ``Word`` expressions, using range notation from\n",
            "    regular expression character sets.\n",
            "\n",
            "    A common mistake is to use :class:`Word` to match a specific literal\n",
            "    string, as in ``Word(\"Address\")``. Remember that :class:`Word`\n",
            "    uses the string argument to define *sets* of matchable characters.\n",
            "    This expression would match \"Add\", \"AAA\", \"dAred\", or any other word\n",
            "    made up of the characters 'A', 'd', 'r', 'e', and 's'. To match an\n",
            "    exact literal string, use :class:`Literal` or :class:`Keyword`.\n",
            "\n",
            "    pyparsing includes helper strings for building Words:\n",
            "\n",
            "     - :class:`alphas`\n",
            "     - :class:`nums`\n",
            "     - :class:`alphanums`\n",
            "     - :class:`hexnums`\n",
            "     - :class:`alphas8bit` (alphabetic characters in ASCII range 128-255\n",
            "       - accented, tilded, umlauted, etc.)\n",
            "     - :class:`punc8bit` (non-alphabetic characters in ASCII range\n",
            "       128-255 - currency, symbols, superscripts, diacriticals, etc.)\n",
            "     - :class:`printables` (any non-whitespace character)\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # a word composed of digits\n",
            "        integer = Word(nums) # equivalent to Word(\"0123456789\") or Word(srange(\"0-9\"))\n",
            "\n",
            "        # a word with a leading capital, and zero or more lowercase\n",
            "        capital_word = Word(alphas.upper(), alphas.lower())\n",
            "\n",
            "        # hostnames are alphanumeric, with leading alpha, and '-'\n",
            "        hostname = Word(alphas, alphanums + '-')\n",
            "\n",
            "        # roman numeral (not a strict parser, accepts invalid mix of characters)\n",
            "        roman = Word(\"IVXLCDM\")\n",
            "\n",
            "        # any string of non-whitespace characters, except for ','\n",
            "        csv_value = Word(printables, excludeChars=\",\")\n",
            "    \"\"\"\n",
            "    def __init__(self, initChars, bodyChars=None, min=1, max=0, exact=0, asKeyword=False, excludeChars=None):\n",
            "        super(Word, self).__init__()\n",
            "        if excludeChars:\n",
            "            excludeChars = set(excludeChars)\n",
            "            initChars = ''.join(c for c in initChars if c not in excludeChars)\n",
            "            if bodyChars:\n",
            "                bodyChars = ''.join(c for c in bodyChars if c not in excludeChars)\n",
            "        self.initCharsOrig = initChars\n",
            "        self.initChars = set(initChars)\n",
            "        if bodyChars:\n",
            "            self.bodyCharsOrig = bodyChars\n",
            "            self.bodyChars = set(bodyChars)\n",
            "        else:\n",
            "            self.bodyCharsOrig = initChars\n",
            "            self.bodyChars = set(initChars)\n",
            "\n",
            "        self.maxSpecified = max > 0\n",
            "\n",
            "        if min < 1:\n",
            "            raise ValueError(\"cannot specify a minimum length < 1; use Optional(Word()) if zero-length word is permitted\")\n",
            "\n",
            "        self.minLen = min\n",
            "\n",
            "        if max > 0:\n",
            "            self.maxLen = max\n",
            "        else:\n",
            "            self.maxLen = _MAX_INT\n",
            "\n",
            "        if exact > 0:\n",
            "            self.maxLen = exact\n",
            "            self.minLen = exact\n",
            "\n",
            "        self.name = _ustr(self)\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayIndexError = False\n",
            "        self.asKeyword = asKeyword\n",
            "\n",
            "        if ' ' not in self.initCharsOrig + self.bodyCharsOrig and (min == 1 and max == 0 and exact == 0):\n",
            "            if self.bodyCharsOrig == self.initCharsOrig:\n",
            "                self.reString = \"[%s]+\" % _escapeRegexRangeChars(self.initCharsOrig)\n",
            "            elif len(self.initCharsOrig) == 1:\n",
            "                self.reString = \"%s[%s]*\" % (re.escape(self.initCharsOrig),\n",
            "                                             _escapeRegexRangeChars(self.bodyCharsOrig),)\n",
            "            else:\n",
            "                self.reString = \"[%s][%s]*\" % (_escapeRegexRangeChars(self.initCharsOrig),\n",
            "                                               _escapeRegexRangeChars(self.bodyCharsOrig),)\n",
            "            if self.asKeyword:\n",
            "                self.reString = r\"\\b\" + self.reString + r\"\\b\"\n",
            "\n",
            "            try:\n",
            "                self.re = re.compile(self.reString)\n",
            "            except Exception:\n",
            "                self.re = None\n",
            "            else:\n",
            "                self.re_match = self.re.match\n",
            "                self.__class__ = _WordRegex\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc] not in self.initChars:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        start = loc\n",
            "        loc += 1\n",
            "        instrlen = len(instring)\n",
            "        bodychars = self.bodyChars\n",
            "        maxloc = start + self.maxLen\n",
            "        maxloc = min(maxloc, instrlen)\n",
            "        while loc < maxloc and instring[loc] in bodychars:\n",
            "            loc += 1\n",
            "\n",
            "        throwException = False\n",
            "        if loc - start < self.minLen:\n",
            "            throwException = True\n",
            "        elif self.maxSpecified and loc < instrlen and instring[loc] in bodychars:\n",
            "            throwException = True\n",
            "        elif self.asKeyword:\n",
            "            if (start > 0 and instring[start - 1] in bodychars\n",
            "                    or loc < instrlen and instring[loc] in bodychars):\n",
            "                throwException = True\n",
            "\n",
            "        if throwException:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        return loc, instring[start:loc]\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(Word, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None:\n",
            "\n",
            "            def charsAsStr(s):\n",
            "                if len(s) > 4:\n",
            "                    return s[:4] + \"...\"\n",
            "                else:\n",
            "                    return s\n",
            "\n",
            "            if self.initCharsOrig != self.bodyCharsOrig:\n",
            "                self.strRepr = \"W:(%s, %s)\" % (charsAsStr(self.initCharsOrig), charsAsStr(self.bodyCharsOrig))\n",
            "            else:\n",
            "                self.strRepr = \"W:(%s)\" % charsAsStr(self.initCharsOrig)\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "class _WordRegex(Word):\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        result = self.re_match(instring, loc)\n",
            "        if not result:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        loc = result.end()\n",
            "        return loc, result.group()\n",
            "\n",
            "\n",
            "class Char(_WordRegex):\n",
            "    \"\"\"A short-cut class for defining ``Word(characters, exact=1)``,\n",
            "    when defining a match of any single character in a string of\n",
            "    characters.\n",
            "    \"\"\"\n",
            "    def __init__(self, charset, asKeyword=False, excludeChars=None):\n",
            "        super(Char, self).__init__(charset, exact=1, asKeyword=asKeyword, excludeChars=excludeChars)\n",
            "        self.reString = \"[%s]\" % _escapeRegexRangeChars(''.join(self.initChars))\n",
            "        if asKeyword:\n",
            "            self.reString = r\"\\b%s\\b\" % self.reString\n",
            "        self.re = re.compile(self.reString)\n",
            "        self.re_match = self.re.match\n",
            "\n",
            "\n",
            "class Regex(Token):\n",
            "    r\"\"\"Token for matching strings that match a given regular\n",
            "    expression. Defined with string specifying the regular expression in\n",
            "    a form recognized by the stdlib Python  `re module <https://docs.python.org/3/library/re.html>`_.\n",
            "    If the given regex contains named groups (defined using ``(?P<name>...)``),\n",
            "    these will be preserved as named parse results.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        realnum = Regex(r\"[+-]?\\d+\\.\\d*\")\n",
            "        date = Regex(r'(?P<year>\\d{4})-(?P<month>\\d\\d?)-(?P<day>\\d\\d?)')\n",
            "        # ref: https://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression\n",
            "        roman = Regex(r\"M{0,4}(CM|CD|D?{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\")\n",
            "    \"\"\"\n",
            "    compiledREtype = type(re.compile(\"[A-Z]\"))\n",
            "    def __init__(self, pattern, flags=0, asGroupList=False, asMatch=False):\n",
            "        \"\"\"The parameters ``pattern`` and ``flags`` are passed\n",
            "        to the ``re.compile()`` function as-is. See the Python\n",
            "        `re module <https://docs.python.org/3/library/re.html>`_ module for an\n",
            "        explanation of the acceptable patterns and flags.\n",
            "        \"\"\"\n",
            "        super(Regex, self).__init__()\n",
            "\n",
            "        if isinstance(pattern, basestring):\n",
            "            if not pattern:\n",
            "                warnings.warn(\"null string passed to Regex; use Empty() instead\",\n",
            "                              SyntaxWarning, stacklevel=2)\n",
            "\n",
            "            self.pattern = pattern\n",
            "            self.flags = flags\n",
            "\n",
            "            try:\n",
            "                self.re = re.compile(self.pattern, self.flags)\n",
            "                self.reString = self.pattern\n",
            "            except sre_constants.error:\n",
            "                warnings.warn(\"invalid pattern (%s) passed to Regex\" % pattern,\n",
            "                              SyntaxWarning, stacklevel=2)\n",
            "                raise\n",
            "\n",
            "        elif isinstance(pattern, Regex.compiledREtype):\n",
            "            self.re = pattern\n",
            "            self.pattern = self.reString = str(pattern)\n",
            "            self.flags = flags\n",
            "\n",
            "        else:\n",
            "            raise ValueError(\"Regex may only be constructed with a string or a compiled RE object\")\n",
            "\n",
            "        self.re_match = self.re.match\n",
            "\n",
            "        self.name = _ustr(self)\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayIndexError = False\n",
            "        self.mayReturnEmpty = True\n",
            "        self.asGroupList = asGroupList\n",
            "        self.asMatch = asMatch\n",
            "        if self.asGroupList:\n",
            "            self.parseImpl = self.parseImplAsGroupList\n",
            "        if self.asMatch:\n",
            "            self.parseImpl = self.parseImplAsMatch\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        result = self.re_match(instring, loc)\n",
            "        if not result:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        loc = result.end()\n",
            "        ret = ParseResults(result.group())\n",
            "        d = result.groupdict()\n",
            "        if d:\n",
            "            for k, v in d.items():\n",
            "                ret[k] = v\n",
            "        return loc, ret\n",
            "\n",
            "    def parseImplAsGroupList(self, instring, loc, doActions=True):\n",
            "        result = self.re_match(instring, loc)\n",
            "        if not result:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        loc = result.end()\n",
            "        ret = result.groups()\n",
            "        return loc, ret\n",
            "\n",
            "    def parseImplAsMatch(self, instring, loc, doActions=True):\n",
            "        result = self.re_match(instring, loc)\n",
            "        if not result:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        loc = result.end()\n",
            "        ret = result\n",
            "        return loc, ret\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(Regex, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"Re:(%s)\" % repr(self.pattern)\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "    def sub(self, repl):\n",
            "        r\"\"\"\n",
            "        Return Regex with an attached parse action to transform the parsed\n",
            "        result as if called using `re.sub(expr, repl, string) <https://docs.python.org/3/library/re.html#re.sub>`_.\n",
            "\n",
            "        Example::\n",
            "\n",
            "            make_html = Regex(r\"(\\w+):(.*?):\").sub(r\"<\\1>\\2</\\1>\")\n",
            "            print(make_html.transformString(\"h1:main title:\"))\n",
            "            # prints \"<h1>main title</h1>\"\n",
            "        \"\"\"\n",
            "        if self.asGroupList:\n",
            "            warnings.warn(\"cannot use sub() with Regex(asGroupList=True)\",\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            raise SyntaxError()\n",
            "\n",
            "        if self.asMatch and callable(repl):\n",
            "            warnings.warn(\"cannot use sub() with a callable with Regex(asMatch=True)\",\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            raise SyntaxError()\n",
            "\n",
            "        if self.asMatch:\n",
            "            def pa(tokens):\n",
            "                return tokens[0].expand(repl)\n",
            "        else:\n",
            "            def pa(tokens):\n",
            "                return self.re.sub(repl, tokens[0])\n",
            "        return self.addParseAction(pa)\n",
            "\n",
            "class QuotedString(Token):\n",
            "    r\"\"\"\n",
            "    Token for matching strings that are delimited by quoting characters.\n",
            "\n",
            "    Defined with the following parameters:\n",
            "\n",
            "        - quoteChar - string of one or more characters defining the\n",
            "          quote delimiting string\n",
            "        - escChar - character to escape quotes, typically backslash\n",
            "          (default= ``None``)\n",
            "        - escQuote - special quote sequence to escape an embedded quote\n",
            "          string (such as SQL's ``\"\"`` to escape an embedded ``\"``)\n",
            "          (default= ``None``)\n",
            "        - multiline - boolean indicating whether quotes can span\n",
            "          multiple lines (default= ``False``)\n",
            "        - unquoteResults - boolean indicating whether the matched text\n",
            "          should be unquoted (default= ``True``)\n",
            "        - endQuoteChar - string of one or more characters defining the\n",
            "          end of the quote delimited string (default= ``None``  => same as\n",
            "          quoteChar)\n",
            "        - convertWhitespaceEscapes - convert escaped whitespace\n",
            "          (``'\\t'``, ``'\\n'``, etc.) to actual whitespace\n",
            "          (default= ``True``)\n",
            "\n",
            "    Example::\n",
            "\n",
            "        qs = QuotedString('\"')\n",
            "        print(qs.searchString('lsjdf \"This is the quote\" sldjf'))\n",
            "        complex_qs = QuotedString('{{', endQuoteChar='}}')\n",
            "        print(complex_qs.searchString('lsjdf {{This is the \"quote\"}} sldjf'))\n",
            "        sql_qs = QuotedString('\"', escQuote='\"\"')\n",
            "        print(sql_qs.searchString('lsjdf \"This is the quote with \"\"embedded\"\" quotes\" sldjf'))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['This is the quote']]\n",
            "        [['This is the \"quote\"']]\n",
            "        [['This is the quote with \"embedded\" quotes']]\n",
            "    \"\"\"\n",
            "    def __init__(self, quoteChar, escChar=None, escQuote=None, multiline=False,\n",
            "                 unquoteResults=True, endQuoteChar=None, convertWhitespaceEscapes=True):\n",
            "        super(QuotedString, self).__init__()\n",
            "\n",
            "        # remove white space from quote chars - wont work anyway\n",
            "        quoteChar = quoteChar.strip()\n",
            "        if not quoteChar:\n",
            "            warnings.warn(\"quoteChar cannot be the empty string\", SyntaxWarning, stacklevel=2)\n",
            "            raise SyntaxError()\n",
            "\n",
            "        if endQuoteChar is None:\n",
            "            endQuoteChar = quoteChar\n",
            "        else:\n",
            "            endQuoteChar = endQuoteChar.strip()\n",
            "            if not endQuoteChar:\n",
            "                warnings.warn(\"endQuoteChar cannot be the empty string\", SyntaxWarning, stacklevel=2)\n",
            "                raise SyntaxError()\n",
            "\n",
            "        self.quoteChar = quoteChar\n",
            "        self.quoteCharLen = len(quoteChar)\n",
            "        self.firstQuoteChar = quoteChar[0]\n",
            "        self.endQuoteChar = endQuoteChar\n",
            "        self.endQuoteCharLen = len(endQuoteChar)\n",
            "        self.escChar = escChar\n",
            "        self.escQuote = escQuote\n",
            "        self.unquoteResults = unquoteResults\n",
            "        self.convertWhitespaceEscapes = convertWhitespaceEscapes\n",
            "\n",
            "        if multiline:\n",
            "            self.flags = re.MULTILINE | re.DOTALL\n",
            "            self.pattern = r'%s(?:[^%s%s]' % (re.escape(self.quoteChar),\n",
            "                                              _escapeRegexRangeChars(self.endQuoteChar[0]),\n",
            "                                              (escChar is not None and _escapeRegexRangeChars(escChar) or ''))\n",
            "        else:\n",
            "            self.flags = 0\n",
            "            self.pattern = r'%s(?:[^%s\\n\\r%s]' % (re.escape(self.quoteChar),\n",
            "                                                  _escapeRegexRangeChars(self.endQuoteChar[0]),\n",
            "                                                  (escChar is not None and _escapeRegexRangeChars(escChar) or ''))\n",
            "        if len(self.endQuoteChar) > 1:\n",
            "            self.pattern += (\n",
            "                '|(?:' + ')|(?:'.join(\"%s[^%s]\" % (re.escape(self.endQuoteChar[:i]),\n",
            "                                                   _escapeRegexRangeChars(self.endQuoteChar[i]))\n",
            "                                      for i in range(len(self.endQuoteChar) - 1, 0, -1)) + ')')\n",
            "\n",
            "        if escQuote:\n",
            "            self.pattern += (r'|(?:%s)' % re.escape(escQuote))\n",
            "        if escChar:\n",
            "            self.pattern += (r'|(?:%s.)' % re.escape(escChar))\n",
            "            self.escCharReplacePattern = re.escape(self.escChar) + \"(.)\"\n",
            "        self.pattern += (r')*%s' % re.escape(self.endQuoteChar))\n",
            "\n",
            "        try:\n",
            "            self.re = re.compile(self.pattern, self.flags)\n",
            "            self.reString = self.pattern\n",
            "            self.re_match = self.re.match\n",
            "        except sre_constants.error:\n",
            "            warnings.warn(\"invalid pattern (%s) passed to Regex\" % self.pattern,\n",
            "                          SyntaxWarning, stacklevel=2)\n",
            "            raise\n",
            "\n",
            "        self.name = _ustr(self)\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayIndexError = False\n",
            "        self.mayReturnEmpty = True\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        result = instring[loc] == self.firstQuoteChar and self.re_match(instring, loc) or None\n",
            "        if not result:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        loc = result.end()\n",
            "        ret = result.group()\n",
            "\n",
            "        if self.unquoteResults:\n",
            "\n",
            "            # strip off quotes\n",
            "            ret = ret[self.quoteCharLen: -self.endQuoteCharLen]\n",
            "\n",
            "            if isinstance(ret, basestring):\n",
            "                # replace escaped whitespace\n",
            "                if '\\\\' in ret and self.convertWhitespaceEscapes:\n",
            "                    ws_map = {\n",
            "                        r'\\t': '\\t',\n",
            "                        r'\\n': '\\n',\n",
            "                        r'\\f': '\\f',\n",
            "                        r'\\r': '\\r',\n",
            "                    }\n",
            "                    for wslit, wschar in ws_map.items():\n",
            "                        ret = ret.replace(wslit, wschar)\n",
            "\n",
            "                # replace escaped characters\n",
            "                if self.escChar:\n",
            "                    ret = re.sub(self.escCharReplacePattern, r\"\\g<1>\", ret)\n",
            "\n",
            "                # replace escaped quotes\n",
            "                if self.escQuote:\n",
            "                    ret = ret.replace(self.escQuote, self.endQuoteChar)\n",
            "\n",
            "        return loc, ret\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(QuotedString, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"quoted string, starting with %s ending with %s\" % (self.quoteChar, self.endQuoteChar)\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "\n",
            "class CharsNotIn(Token):\n",
            "    \"\"\"Token for matching words composed of characters *not* in a given\n",
            "    set (will include whitespace in matched characters if not listed in\n",
            "    the provided exclusion set - see example). Defined with string\n",
            "    containing all disallowed characters, and an optional minimum,\n",
            "    maximum, and/or exact length.  The default value for ``min`` is\n",
            "    1 (a minimum value < 1 is not valid); the default values for\n",
            "    ``max`` and ``exact`` are 0, meaning no maximum or exact\n",
            "    length restriction.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # define a comma-separated-value as anything that is not a ','\n",
            "        csv_value = CharsNotIn(',')\n",
            "        print(delimitedList(csv_value).parseString(\"dkls,lsdkjf,s12 34,@!#,213\"))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']\n",
            "    \"\"\"\n",
            "    def __init__(self, notChars, min=1, max=0, exact=0):\n",
            "        super(CharsNotIn, self).__init__()\n",
            "        self.skipWhitespace = False\n",
            "        self.notChars = notChars\n",
            "\n",
            "        if min < 1:\n",
            "            raise ValueError(\"cannot specify a minimum length < 1; use \"\n",
            "                             \"Optional(CharsNotIn()) if zero-length char group is permitted\")\n",
            "\n",
            "        self.minLen = min\n",
            "\n",
            "        if max > 0:\n",
            "            self.maxLen = max\n",
            "        else:\n",
            "            self.maxLen = _MAX_INT\n",
            "\n",
            "        if exact > 0:\n",
            "            self.maxLen = exact\n",
            "            self.minLen = exact\n",
            "\n",
            "        self.name = _ustr(self)\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "        self.mayReturnEmpty = (self.minLen == 0)\n",
            "        self.mayIndexError = False\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc] in self.notChars:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        start = loc\n",
            "        loc += 1\n",
            "        notchars = self.notChars\n",
            "        maxlen = min(start + self.maxLen, len(instring))\n",
            "        while loc < maxlen and instring[loc] not in notchars:\n",
            "            loc += 1\n",
            "\n",
            "        if loc - start < self.minLen:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        return loc, instring[start:loc]\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(CharsNotIn, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            if len(self.notChars) > 4:\n",
            "                self.strRepr = \"!W:(%s...)\" % self.notChars[:4]\n",
            "            else:\n",
            "                self.strRepr = \"!W:(%s)\" % self.notChars\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "class White(Token):\n",
            "    \"\"\"Special matching class for matching whitespace.  Normally,\n",
            "    whitespace is ignored by pyparsing grammars.  This class is included\n",
            "    when some whitespace structures are significant.  Define with\n",
            "    a string containing the whitespace characters to be matched; default\n",
            "    is ``\" \\\\t\\\\r\\\\n\"``.  Also takes optional ``min``,\n",
            "    ``max``, and ``exact`` arguments, as defined for the\n",
            "    :class:`Word` class.\n",
            "    \"\"\"\n",
            "    whiteStrs = {\n",
            "        ' ' : '<SP>',\n",
            "        '\\t': '<TAB>',\n",
            "        '\\n': '<LF>',\n",
            "        '\\r': '<CR>',\n",
            "        '\\f': '<FF>',\n",
            "        'u\\00A0': '<NBSP>',\n",
            "        'u\\1680': '<OGHAM_SPACE_MARK>',\n",
            "        'u\\180E': '<MONGOLIAN_VOWEL_SEPARATOR>',\n",
            "        'u\\2000': '<EN_QUAD>',\n",
            "        'u\\2001': '<EM_QUAD>',\n",
            "        'u\\2002': '<EN_SPACE>',\n",
            "        'u\\2003': '<EM_SPACE>',\n",
            "        'u\\2004': '<THREE-PER-EM_SPACE>',\n",
            "        'u\\2005': '<FOUR-PER-EM_SPACE>',\n",
            "        'u\\2006': '<SIX-PER-EM_SPACE>',\n",
            "        'u\\2007': '<FIGURE_SPACE>',\n",
            "        'u\\2008': '<PUNCTUATION_SPACE>',\n",
            "        'u\\2009': '<THIN_SPACE>',\n",
            "        'u\\200A': '<HAIR_SPACE>',\n",
            "        'u\\200B': '<ZERO_WIDTH_SPACE>',\n",
            "        'u\\202F': '<NNBSP>',\n",
            "        'u\\205F': '<MMSP>',\n",
            "        'u\\3000': '<IDEOGRAPHIC_SPACE>',\n",
            "        }\n",
            "    def __init__(self, ws=\" \\t\\r\\n\", min=1, max=0, exact=0):\n",
            "        super(White, self).__init__()\n",
            "        self.matchWhite = ws\n",
            "        self.setWhitespaceChars(\"\".join(c for c in self.whiteChars if c not in self.matchWhite))\n",
            "        # ~ self.leaveWhitespace()\n",
            "        self.name = (\"\".join(White.whiteStrs[c] for c in self.matchWhite))\n",
            "        self.mayReturnEmpty = True\n",
            "        self.errmsg = \"Expected \" + self.name\n",
            "\n",
            "        self.minLen = min\n",
            "\n",
            "        if max > 0:\n",
            "            self.maxLen = max\n",
            "        else:\n",
            "            self.maxLen = _MAX_INT\n",
            "\n",
            "        if exact > 0:\n",
            "            self.maxLen = exact\n",
            "            self.minLen = exact\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if instring[loc] not in self.matchWhite:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "        start = loc\n",
            "        loc += 1\n",
            "        maxloc = start + self.maxLen\n",
            "        maxloc = min(maxloc, len(instring))\n",
            "        while loc < maxloc and instring[loc] in self.matchWhite:\n",
            "            loc += 1\n",
            "\n",
            "        if loc - start < self.minLen:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        return loc, instring[start:loc]\n",
            "\n",
            "\n",
            "class _PositionToken(Token):\n",
            "    def __init__(self):\n",
            "        super(_PositionToken, self).__init__()\n",
            "        self.name = self.__class__.__name__\n",
            "        self.mayReturnEmpty = True\n",
            "        self.mayIndexError = False\n",
            "\n",
            "class GoToColumn(_PositionToken):\n",
            "    \"\"\"Token to advance to a specific column of input text; useful for\n",
            "    tabular report scraping.\n",
            "    \"\"\"\n",
            "    def __init__(self, colno):\n",
            "        super(GoToColumn, self).__init__()\n",
            "        self.col = colno\n",
            "\n",
            "    def preParse(self, instring, loc):\n",
            "        if col(loc, instring) != self.col:\n",
            "            instrlen = len(instring)\n",
            "            if self.ignoreExprs:\n",
            "                loc = self._skipIgnorables(instring, loc)\n",
            "            while loc < instrlen and instring[loc].isspace() and col(loc, instring) != self.col:\n",
            "                loc += 1\n",
            "        return loc\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        thiscol = col(loc, instring)\n",
            "        if thiscol > self.col:\n",
            "            raise ParseException(instring, loc, \"Text not in expected column\", self)\n",
            "        newloc = loc + self.col - thiscol\n",
            "        ret = instring[loc: newloc]\n",
            "        return newloc, ret\n",
            "\n",
            "\n",
            "class LineStart(_PositionToken):\n",
            "    r\"\"\"Matches if current position is at the beginning of a line within\n",
            "    the parse string\n",
            "\n",
            "    Example::\n",
            "\n",
            "        test = '''\\\n",
            "        AAA this line\n",
            "        AAA and this line\n",
            "          AAA but not this one\n",
            "        B AAA and definitely not this one\n",
            "        '''\n",
            "\n",
            "        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):\n",
            "            print(t)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['AAA', ' this line']\n",
            "        ['AAA', ' and this line']\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(LineStart, self).__init__()\n",
            "        self.errmsg = \"Expected start of line\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if col(loc, instring) == 1:\n",
            "            return loc, []\n",
            "        raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "class LineEnd(_PositionToken):\n",
            "    \"\"\"Matches if current position is at the end of a line within the\n",
            "    parse string\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(LineEnd, self).__init__()\n",
            "        self.setWhitespaceChars(ParserElement.DEFAULT_WHITE_CHARS.replace(\"\\n\", \"\"))\n",
            "        self.errmsg = \"Expected end of line\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if loc < len(instring):\n",
            "            if instring[loc] == \"\\n\":\n",
            "                return loc + 1, \"\\n\"\n",
            "            else:\n",
            "                raise ParseException(instring, loc, self.errmsg, self)\n",
            "        elif loc == len(instring):\n",
            "            return loc + 1, []\n",
            "        else:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "class StringStart(_PositionToken):\n",
            "    \"\"\"Matches if current position is at the beginning of the parse\n",
            "    string\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(StringStart, self).__init__()\n",
            "        self.errmsg = \"Expected start of text\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if loc != 0:\n",
            "            # see if entire string up to here is just whitespace and ignoreables\n",
            "            if loc != self.preParse(instring, 0):\n",
            "                raise ParseException(instring, loc, self.errmsg, self)\n",
            "        return loc, []\n",
            "\n",
            "class StringEnd(_PositionToken):\n",
            "    \"\"\"Matches if current position is at the end of the parse string\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        super(StringEnd, self).__init__()\n",
            "        self.errmsg = \"Expected end of text\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if loc < len(instring):\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "        elif loc == len(instring):\n",
            "            return loc + 1, []\n",
            "        elif loc > len(instring):\n",
            "            return loc, []\n",
            "        else:\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "class WordStart(_PositionToken):\n",
            "    \"\"\"Matches if the current position is at the beginning of a Word,\n",
            "    and is not preceded by any character in a given set of\n",
            "    ``wordChars`` (default= ``printables``). To emulate the\n",
            "    ``\\b`` behavior of regular expressions, use\n",
            "    ``WordStart(alphanums)``. ``WordStart`` will also match at\n",
            "    the beginning of the string being parsed, or at the beginning of\n",
            "    a line.\n",
            "    \"\"\"\n",
            "    def __init__(self, wordChars=printables):\n",
            "        super(WordStart, self).__init__()\n",
            "        self.wordChars = set(wordChars)\n",
            "        self.errmsg = \"Not at the start of a word\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if loc != 0:\n",
            "            if (instring[loc - 1] in self.wordChars\n",
            "                    or instring[loc] not in self.wordChars):\n",
            "                raise ParseException(instring, loc, self.errmsg, self)\n",
            "        return loc, []\n",
            "\n",
            "class WordEnd(_PositionToken):\n",
            "    \"\"\"Matches if the current position is at the end of a Word, and is\n",
            "    not followed by any character in a given set of ``wordChars``\n",
            "    (default= ``printables``). To emulate the ``\\b`` behavior of\n",
            "    regular expressions, use ``WordEnd(alphanums)``. ``WordEnd``\n",
            "    will also match at the end of the string being parsed, or at the end\n",
            "    of a line.\n",
            "    \"\"\"\n",
            "    def __init__(self, wordChars=printables):\n",
            "        super(WordEnd, self).__init__()\n",
            "        self.wordChars = set(wordChars)\n",
            "        self.skipWhitespace = False\n",
            "        self.errmsg = \"Not at the end of a word\"\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        instrlen = len(instring)\n",
            "        if instrlen > 0 and loc < instrlen:\n",
            "            if (instring[loc] in self.wordChars or\n",
            "                    instring[loc - 1] not in self.wordChars):\n",
            "                raise ParseException(instring, loc, self.errmsg, self)\n",
            "        return loc, []\n",
            "\n",
            "\n",
            "class ParseExpression(ParserElement):\n",
            "    \"\"\"Abstract subclass of ParserElement, for combining and\n",
            "    post-processing parsed tokens.\n",
            "    \"\"\"\n",
            "    def __init__(self, exprs, savelist=False):\n",
            "        super(ParseExpression, self).__init__(savelist)\n",
            "        if isinstance(exprs, _generatorType):\n",
            "            exprs = list(exprs)\n",
            "\n",
            "        if isinstance(exprs, basestring):\n",
            "            self.exprs = [self._literalStringClass(exprs)]\n",
            "        elif isinstance(exprs, ParserElement):\n",
            "            self.exprs = [exprs]\n",
            "        elif isinstance(exprs, Iterable):\n",
            "            exprs = list(exprs)\n",
            "            # if sequence of strings provided, wrap with Literal\n",
            "            if any(isinstance(expr, basestring) for expr in exprs):\n",
            "                exprs = (self._literalStringClass(e) if isinstance(e, basestring) else e for e in exprs)\n",
            "            self.exprs = list(exprs)\n",
            "        else:\n",
            "            try:\n",
            "                self.exprs = list(exprs)\n",
            "            except TypeError:\n",
            "                self.exprs = [exprs]\n",
            "        self.callPreparse = False\n",
            "\n",
            "    def append(self, other):\n",
            "        self.exprs.append(other)\n",
            "        self.strRepr = None\n",
            "        return self\n",
            "\n",
            "    def leaveWhitespace(self):\n",
            "        \"\"\"Extends ``leaveWhitespace`` defined in base class, and also invokes ``leaveWhitespace`` on\n",
            "           all contained expressions.\"\"\"\n",
            "        self.skipWhitespace = False\n",
            "        self.exprs = [e.copy() for e in self.exprs]\n",
            "        for e in self.exprs:\n",
            "            e.leaveWhitespace()\n",
            "        return self\n",
            "\n",
            "    def ignore(self, other):\n",
            "        if isinstance(other, Suppress):\n",
            "            if other not in self.ignoreExprs:\n",
            "                super(ParseExpression, self).ignore(other)\n",
            "                for e in self.exprs:\n",
            "                    e.ignore(self.ignoreExprs[-1])\n",
            "        else:\n",
            "            super(ParseExpression, self).ignore(other)\n",
            "            for e in self.exprs:\n",
            "                e.ignore(self.ignoreExprs[-1])\n",
            "        return self\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(ParseExpression, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"%s:(%s)\" % (self.__class__.__name__, _ustr(self.exprs))\n",
            "        return self.strRepr\n",
            "\n",
            "    def streamline(self):\n",
            "        super(ParseExpression, self).streamline()\n",
            "\n",
            "        for e in self.exprs:\n",
            "            e.streamline()\n",
            "\n",
            "        # collapse nested And's of the form And(And(And(a, b), c), d) to And(a, b, c, d)\n",
            "        # but only if there are no parse actions or resultsNames on the nested And's\n",
            "        # (likewise for Or's and MatchFirst's)\n",
            "        if len(self.exprs) == 2:\n",
            "            other = self.exprs[0]\n",
            "            if (isinstance(other, self.__class__)\n",
            "                    and not other.parseAction\n",
            "                    and other.resultsName is None\n",
            "                    and not other.debug):\n",
            "                self.exprs = other.exprs[:] + [self.exprs[1]]\n",
            "                self.strRepr = None\n",
            "                self.mayReturnEmpty |= other.mayReturnEmpty\n",
            "                self.mayIndexError  |= other.mayIndexError\n",
            "\n",
            "            other = self.exprs[-1]\n",
            "            if (isinstance(other, self.__class__)\n",
            "                    and not other.parseAction\n",
            "                    and other.resultsName is None\n",
            "                    and not other.debug):\n",
            "                self.exprs = self.exprs[:-1] + other.exprs[:]\n",
            "                self.strRepr = None\n",
            "                self.mayReturnEmpty |= other.mayReturnEmpty\n",
            "                self.mayIndexError  |= other.mayIndexError\n",
            "\n",
            "        self.errmsg = \"Expected \" + _ustr(self)\n",
            "\n",
            "        return self\n",
            "\n",
            "    def validate(self, validateTrace=None):\n",
            "        tmp = (validateTrace if validateTrace is not None else [])[:] + [self]\n",
            "        for e in self.exprs:\n",
            "            e.validate(tmp)\n",
            "        self.checkRecursion([])\n",
            "\n",
            "    def copy(self):\n",
            "        ret = super(ParseExpression, self).copy()\n",
            "        ret.exprs = [e.copy() for e in self.exprs]\n",
            "        return ret\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        if __diag__.warn_ungrouped_named_tokens_in_collection:\n",
            "            for e in self.exprs:\n",
            "                if isinstance(e, ParserElement) and e.resultsName:\n",
            "                    warnings.warn(\"{0}: setting results name {1!r} on {2} expression \"\n",
            "                                  \"collides with {3!r} on contained expression\".format(\"warn_ungrouped_named_tokens_in_collection\",\n",
            "                                                                                       name,\n",
            "                                                                                       type(self).__name__,\n",
            "                                                                                       e.resultsName),\n",
            "                                  stacklevel=3)\n",
            "\n",
            "        return super(ParseExpression, self)._setResultsName(name, listAllMatches)\n",
            "\n",
            "\n",
            "class And(ParseExpression):\n",
            "    \"\"\"\n",
            "    Requires all given :class:`ParseExpression` s to be found in the given order.\n",
            "    Expressions may be separated by whitespace.\n",
            "    May be constructed using the ``'+'`` operator.\n",
            "    May also be constructed using the ``'-'`` operator, which will\n",
            "    suppress backtracking.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        integer = Word(nums)\n",
            "        name_expr = OneOrMore(Word(alphas))\n",
            "\n",
            "        expr = And([integer(\"id\"), name_expr(\"name\"), integer(\"age\")])\n",
            "        # more easily written as:\n",
            "        expr = integer(\"id\") + name_expr(\"name\") + integer(\"age\")\n",
            "    \"\"\"\n",
            "\n",
            "    class _ErrorStop(Empty):\n",
            "        def __init__(self, *args, **kwargs):\n",
            "            super(And._ErrorStop, self).__init__(*args, **kwargs)\n",
            "            self.name = '-'\n",
            "            self.leaveWhitespace()\n",
            "\n",
            "    def __init__(self, exprs, savelist=True):\n",
            "        if exprs and Ellipsis in exprs:\n",
            "            tmp = []\n",
            "            for i, expr in enumerate(exprs):\n",
            "                if expr is Ellipsis:\n",
            "                    if i < len(exprs) - 1:\n",
            "                        skipto_arg = (Empty() + exprs[i + 1]).exprs[-1]\n",
            "                        tmp.append(SkipTo(skipto_arg)(\"_skipped*\"))\n",
            "                    else:\n",
            "                        raise Exception(\"cannot construct And with sequence ending in ...\")\n",
            "                else:\n",
            "                    tmp.append(expr)\n",
            "            exprs[:] = tmp\n",
            "        super(And, self).__init__(exprs, savelist)\n",
            "        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)\n",
            "        self.setWhitespaceChars(self.exprs[0].whiteChars)\n",
            "        self.skipWhitespace = self.exprs[0].skipWhitespace\n",
            "        self.callPreparse = True\n",
            "\n",
            "    def streamline(self):\n",
            "        # collapse any _PendingSkip's\n",
            "        if self.exprs:\n",
            "            if any(isinstance(e, ParseExpression) and e.exprs and isinstance(e.exprs[-1], _PendingSkip)\n",
            "                   for e in self.exprs[:-1]):\n",
            "                for i, e in enumerate(self.exprs[:-1]):\n",
            "                    if e is None:\n",
            "                        continue\n",
            "                    if (isinstance(e, ParseExpression)\n",
            "                            and e.exprs and isinstance(e.exprs[-1], _PendingSkip)):\n",
            "                        e.exprs[-1] = e.exprs[-1] + self.exprs[i + 1]\n",
            "                        self.exprs[i + 1] = None\n",
            "                self.exprs = [e for e in self.exprs if e is not None]\n",
            "\n",
            "        super(And, self).streamline()\n",
            "        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)\n",
            "        return self\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        # pass False as last arg to _parse for first element, since we already\n",
            "        # pre-parsed the string as part of our And pre-parsing\n",
            "        loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "        errorStop = False\n",
            "        for e in self.exprs[1:]:\n",
            "            if isinstance(e, And._ErrorStop):\n",
            "                errorStop = True\n",
            "                continue\n",
            "            if errorStop:\n",
            "                try:\n",
            "                    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "                except ParseSyntaxException:\n",
            "                    raise\n",
            "                except ParseBaseException as pe:\n",
            "                    pe.__traceback__ = None\n",
            "                    raise ParseSyntaxException._from_exception(pe)\n",
            "                except IndexError:\n",
            "                    raise ParseSyntaxException(instring, len(instring), self.errmsg, self)\n",
            "            else:\n",
            "                loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "            if exprtokens or exprtokens.haskeys():\n",
            "                resultlist += exprtokens\n",
            "        return loc, resultlist\n",
            "\n",
            "    def __iadd__(self, other):\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        return self.append(other)  # And([self, other])\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        subRecCheckList = parseElementList[:] + [self]\n",
            "        for e in self.exprs:\n",
            "            e.checkRecursion(subRecCheckList)\n",
            "            if not e.mayReturnEmpty:\n",
            "                break\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"{\" + \" \".join(_ustr(e) for e in self.exprs) + \"}\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "\n",
            "class Or(ParseExpression):\n",
            "    \"\"\"Requires that at least one :class:`ParseExpression` is found. If\n",
            "    two expressions match, the expression that matches the longest\n",
            "    string will be used. May be constructed using the ``'^'``\n",
            "    operator.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # construct Or using '^' operator\n",
            "\n",
            "        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))\n",
            "        print(number.searchString(\"123 3.1416 789\"))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['123'], ['3.1416'], ['789']]\n",
            "    \"\"\"\n",
            "    def __init__(self, exprs, savelist=False):\n",
            "        super(Or, self).__init__(exprs, savelist)\n",
            "        if self.exprs:\n",
            "            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)\n",
            "        else:\n",
            "            self.mayReturnEmpty = True\n",
            "\n",
            "    def streamline(self):\n",
            "        super(Or, self).streamline()\n",
            "        if __compat__.collect_all_And_tokens:\n",
            "            self.saveAsList = any(e.saveAsList for e in self.exprs)\n",
            "        return self\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        maxExcLoc = -1\n",
            "        maxException = None\n",
            "        matches = []\n",
            "        for e in self.exprs:\n",
            "            try:\n",
            "                loc2 = e.tryParse(instring, loc)\n",
            "            except ParseException as err:\n",
            "                err.__traceback__ = None\n",
            "                if err.loc > maxExcLoc:\n",
            "                    maxException = err\n",
            "                    maxExcLoc = err.loc\n",
            "            except IndexError:\n",
            "                if len(instring) > maxExcLoc:\n",
            "                    maxException = ParseException(instring, len(instring), e.errmsg, self)\n",
            "                    maxExcLoc = len(instring)\n",
            "            else:\n",
            "                # save match among all matches, to retry longest to shortest\n",
            "                matches.append((loc2, e))\n",
            "\n",
            "        if matches:\n",
            "            # re-evaluate all matches in descending order of length of match, in case attached actions\n",
            "            # might change whether or how much they match of the input.\n",
            "            matches.sort(key=itemgetter(0), reverse=True)\n",
            "\n",
            "            if not doActions:\n",
            "                # no further conditions or parse actions to change the selection of\n",
            "                # alternative, so the first match will be the best match\n",
            "                best_expr = matches[0][1]\n",
            "                return best_expr._parse(instring, loc, doActions)\n",
            "\n",
            "            longest = -1, None\n",
            "            for loc1, expr1 in matches:\n",
            "                if loc1 <= longest[0]:\n",
            "                    # already have a longer match than this one will deliver, we are done\n",
            "                    return longest\n",
            "\n",
            "                try:\n",
            "                    loc2, toks = expr1._parse(instring, loc, doActions)\n",
            "                except ParseException as err:\n",
            "                    err.__traceback__ = None\n",
            "                    if err.loc > maxExcLoc:\n",
            "                        maxException = err\n",
            "                        maxExcLoc = err.loc\n",
            "                else:\n",
            "                    if loc2 >= loc1:\n",
            "                        return loc2, toks\n",
            "                    # didn't match as much as before\n",
            "                    elif loc2 > longest[0]:\n",
            "                        longest = loc2, toks\n",
            "\n",
            "            if longest != (-1, None):\n",
            "                return longest\n",
            "\n",
            "        if maxException is not None:\n",
            "            maxException.msg = self.errmsg\n",
            "            raise maxException\n",
            "        else:\n",
            "            raise ParseException(instring, loc, \"no defined alternatives to match\", self)\n",
            "\n",
            "\n",
            "    def __ixor__(self, other):\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        return self.append(other)  # Or([self, other])\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"{\" + \" ^ \".join(_ustr(e) for e in self.exprs) + \"}\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        subRecCheckList = parseElementList[:] + [self]\n",
            "        for e in self.exprs:\n",
            "            e.checkRecursion(subRecCheckList)\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        if (not __compat__.collect_all_And_tokens\n",
            "                and __diag__.warn_multiple_tokens_in_named_alternation):\n",
            "            if any(isinstance(e, And) for e in self.exprs):\n",
            "                warnings.warn(\"{0}: setting results name {1!r} on {2} expression \"\n",
            "                              \"may only return a single token for an And alternative, \"\n",
            "                              \"in future will return the full list of tokens\".format(\n",
            "                    \"warn_multiple_tokens_in_named_alternation\", name, type(self).__name__),\n",
            "                    stacklevel=3)\n",
            "\n",
            "        return super(Or, self)._setResultsName(name, listAllMatches)\n",
            "\n",
            "\n",
            "class MatchFirst(ParseExpression):\n",
            "    \"\"\"Requires that at least one :class:`ParseExpression` is found. If\n",
            "    two expressions match, the first one listed is the one that will\n",
            "    match. May be constructed using the ``'|'`` operator.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # construct MatchFirst using '|' operator\n",
            "\n",
            "        # watch the order of expressions to match\n",
            "        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))\n",
            "        print(number.searchString(\"123 3.1416 789\")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]\n",
            "\n",
            "        # put more selective expression first\n",
            "        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)\n",
            "        print(number.searchString(\"123 3.1416 789\")) #  Better -> [['123'], ['3.1416'], ['789']]\n",
            "    \"\"\"\n",
            "    def __init__(self, exprs, savelist=False):\n",
            "        super(MatchFirst, self).__init__(exprs, savelist)\n",
            "        if self.exprs:\n",
            "            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)\n",
            "        else:\n",
            "            self.mayReturnEmpty = True\n",
            "\n",
            "    def streamline(self):\n",
            "        super(MatchFirst, self).streamline()\n",
            "        if __compat__.collect_all_And_tokens:\n",
            "            self.saveAsList = any(e.saveAsList for e in self.exprs)\n",
            "        return self\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        maxExcLoc = -1\n",
            "        maxException = None\n",
            "        for e in self.exprs:\n",
            "            try:\n",
            "                ret = e._parse(instring, loc, doActions)\n",
            "                return ret\n",
            "            except ParseException as err:\n",
            "                if err.loc > maxExcLoc:\n",
            "                    maxException = err\n",
            "                    maxExcLoc = err.loc\n",
            "            except IndexError:\n",
            "                if len(instring) > maxExcLoc:\n",
            "                    maxException = ParseException(instring, len(instring), e.errmsg, self)\n",
            "                    maxExcLoc = len(instring)\n",
            "\n",
            "        # only got here if no expression matched, raise exception for match that made it the furthest\n",
            "        else:\n",
            "            if maxException is not None:\n",
            "                maxException.msg = self.errmsg\n",
            "                raise maxException\n",
            "            else:\n",
            "                raise ParseException(instring, loc, \"no defined alternatives to match\", self)\n",
            "\n",
            "    def __ior__(self, other):\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        return self.append(other)  # MatchFirst([self, other])\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"{\" + \" | \".join(_ustr(e) for e in self.exprs) + \"}\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        subRecCheckList = parseElementList[:] + [self]\n",
            "        for e in self.exprs:\n",
            "            e.checkRecursion(subRecCheckList)\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        if (not __compat__.collect_all_And_tokens\n",
            "                and __diag__.warn_multiple_tokens_in_named_alternation):\n",
            "            if any(isinstance(e, And) for e in self.exprs):\n",
            "                warnings.warn(\"{0}: setting results name {1!r} on {2} expression \"\n",
            "                              \"may only return a single token for an And alternative, \"\n",
            "                              \"in future will return the full list of tokens\".format(\n",
            "                    \"warn_multiple_tokens_in_named_alternation\", name, type(self).__name__),\n",
            "                    stacklevel=3)\n",
            "\n",
            "        return super(MatchFirst, self)._setResultsName(name, listAllMatches)\n",
            "\n",
            "\n",
            "class Each(ParseExpression):\n",
            "    \"\"\"Requires all given :class:`ParseExpression` s to be found, but in\n",
            "    any order. Expressions may be separated by whitespace.\n",
            "\n",
            "    May be constructed using the ``'&'`` operator.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        color = oneOf(\"RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN\")\n",
            "        shape_type = oneOf(\"SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON\")\n",
            "        integer = Word(nums)\n",
            "        shape_attr = \"shape:\" + shape_type(\"shape\")\n",
            "        posn_attr = \"posn:\" + Group(integer(\"x\") + ',' + integer(\"y\"))(\"posn\")\n",
            "        color_attr = \"color:\" + color(\"color\")\n",
            "        size_attr = \"size:\" + integer(\"size\")\n",
            "\n",
            "        # use Each (using operator '&') to accept attributes in any order\n",
            "        # (shape and posn are required, color and size are optional)\n",
            "        shape_spec = shape_attr & posn_attr & Optional(color_attr) & Optional(size_attr)\n",
            "\n",
            "        shape_spec.runTests('''\n",
            "            shape: SQUARE color: BLACK posn: 100, 120\n",
            "            shape: CIRCLE size: 50 color: BLUE posn: 50,80\n",
            "            color:GREEN size:20 shape:TRIANGLE posn:20,40\n",
            "            '''\n",
            "            )\n",
            "\n",
            "    prints::\n",
            "\n",
            "        shape: SQUARE color: BLACK posn: 100, 120\n",
            "        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]\n",
            "        - color: BLACK\n",
            "        - posn: ['100', ',', '120']\n",
            "          - x: 100\n",
            "          - y: 120\n",
            "        - shape: SQUARE\n",
            "\n",
            "\n",
            "        shape: CIRCLE size: 50 color: BLUE posn: 50,80\n",
            "        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]\n",
            "        - color: BLUE\n",
            "        - posn: ['50', ',', '80']\n",
            "          - x: 50\n",
            "          - y: 80\n",
            "        - shape: CIRCLE\n",
            "        - size: 50\n",
            "\n",
            "\n",
            "        color: GREEN size: 20 shape: TRIANGLE posn: 20,40\n",
            "        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]\n",
            "        - color: GREEN\n",
            "        - posn: ['20', ',', '40']\n",
            "          - x: 20\n",
            "          - y: 40\n",
            "        - shape: TRIANGLE\n",
            "        - size: 20\n",
            "    \"\"\"\n",
            "    def __init__(self, exprs, savelist=True):\n",
            "        super(Each, self).__init__(exprs, savelist)\n",
            "        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)\n",
            "        self.skipWhitespace = True\n",
            "        self.initExprGroups = True\n",
            "        self.saveAsList = True\n",
            "\n",
            "    def streamline(self):\n",
            "        super(Each, self).streamline()\n",
            "        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)\n",
            "        return self\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if self.initExprGroups:\n",
            "            self.opt1map = dict((id(e.expr), e) for e in self.exprs if isinstance(e, Optional))\n",
            "            opt1 = [e.expr for e in self.exprs if isinstance(e, Optional)]\n",
            "            opt2 = [e for e in self.exprs if e.mayReturnEmpty and not isinstance(e, Optional)]\n",
            "            self.optionals = opt1 + opt2\n",
            "            self.multioptionals = [e.expr for e in self.exprs if isinstance(e, ZeroOrMore)]\n",
            "            self.multirequired = [e.expr for e in self.exprs if isinstance(e, OneOrMore)]\n",
            "            self.required = [e for e in self.exprs if not isinstance(e, (Optional, ZeroOrMore, OneOrMore))]\n",
            "            self.required += self.multirequired\n",
            "            self.initExprGroups = False\n",
            "        tmpLoc = loc\n",
            "        tmpReqd = self.required[:]\n",
            "        tmpOpt  = self.optionals[:]\n",
            "        matchOrder = []\n",
            "\n",
            "        keepMatching = True\n",
            "        while keepMatching:\n",
            "            tmpExprs = tmpReqd + tmpOpt + self.multioptionals + self.multirequired\n",
            "            failed = []\n",
            "            for e in tmpExprs:\n",
            "                try:\n",
            "                    tmpLoc = e.tryParse(instring, tmpLoc)\n",
            "                except ParseException:\n",
            "                    failed.append(e)\n",
            "                else:\n",
            "                    matchOrder.append(self.opt1map.get(id(e), e))\n",
            "                    if e in tmpReqd:\n",
            "                        tmpReqd.remove(e)\n",
            "                    elif e in tmpOpt:\n",
            "                        tmpOpt.remove(e)\n",
            "            if len(failed) == len(tmpExprs):\n",
            "                keepMatching = False\n",
            "\n",
            "        if tmpReqd:\n",
            "            missing = \", \".join(_ustr(e) for e in tmpReqd)\n",
            "            raise ParseException(instring, loc, \"Missing one or more required elements (%s)\" % missing)\n",
            "\n",
            "        # add any unmatched Optionals, in case they have default values defined\n",
            "        matchOrder += [e for e in self.exprs if isinstance(e, Optional) and e.expr in tmpOpt]\n",
            "\n",
            "        resultlist = []\n",
            "        for e in matchOrder:\n",
            "            loc, results = e._parse(instring, loc, doActions)\n",
            "            resultlist.append(results)\n",
            "\n",
            "        finalResults = sum(resultlist, ParseResults([]))\n",
            "        return loc, finalResults\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"{\" + \" & \".join(_ustr(e) for e in self.exprs) + \"}\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        subRecCheckList = parseElementList[:] + [self]\n",
            "        for e in self.exprs:\n",
            "            e.checkRecursion(subRecCheckList)\n",
            "\n",
            "\n",
            "class ParseElementEnhance(ParserElement):\n",
            "    \"\"\"Abstract subclass of :class:`ParserElement`, for combining and\n",
            "    post-processing parsed tokens.\n",
            "    \"\"\"\n",
            "    def __init__(self, expr, savelist=False):\n",
            "        super(ParseElementEnhance, self).__init__(savelist)\n",
            "        if isinstance(expr, basestring):\n",
            "            if issubclass(self._literalStringClass, Token):\n",
            "                expr = self._literalStringClass(expr)\n",
            "            else:\n",
            "                expr = self._literalStringClass(Literal(expr))\n",
            "        self.expr = expr\n",
            "        self.strRepr = None\n",
            "        if expr is not None:\n",
            "            self.mayIndexError = expr.mayIndexError\n",
            "            self.mayReturnEmpty = expr.mayReturnEmpty\n",
            "            self.setWhitespaceChars(expr.whiteChars)\n",
            "            self.skipWhitespace = expr.skipWhitespace\n",
            "            self.saveAsList = expr.saveAsList\n",
            "            self.callPreparse = expr.callPreparse\n",
            "            self.ignoreExprs.extend(expr.ignoreExprs)\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if self.expr is not None:\n",
            "            return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "        else:\n",
            "            raise ParseException(\"\", loc, self.errmsg, self)\n",
            "\n",
            "    def leaveWhitespace(self):\n",
            "        self.skipWhitespace = False\n",
            "        self.expr = self.expr.copy()\n",
            "        if self.expr is not None:\n",
            "            self.expr.leaveWhitespace()\n",
            "        return self\n",
            "\n",
            "    def ignore(self, other):\n",
            "        if isinstance(other, Suppress):\n",
            "            if other not in self.ignoreExprs:\n",
            "                super(ParseElementEnhance, self).ignore(other)\n",
            "                if self.expr is not None:\n",
            "                    self.expr.ignore(self.ignoreExprs[-1])\n",
            "        else:\n",
            "            super(ParseElementEnhance, self).ignore(other)\n",
            "            if self.expr is not None:\n",
            "                self.expr.ignore(self.ignoreExprs[-1])\n",
            "        return self\n",
            "\n",
            "    def streamline(self):\n",
            "        super(ParseElementEnhance, self).streamline()\n",
            "        if self.expr is not None:\n",
            "            self.expr.streamline()\n",
            "        return self\n",
            "\n",
            "    def checkRecursion(self, parseElementList):\n",
            "        if self in parseElementList:\n",
            "            raise RecursiveGrammarException(parseElementList + [self])\n",
            "        subRecCheckList = parseElementList[:] + [self]\n",
            "        if self.expr is not None:\n",
            "            self.expr.checkRecursion(subRecCheckList)\n",
            "\n",
            "    def validate(self, validateTrace=None):\n",
            "        if validateTrace is None:\n",
            "            validateTrace = []\n",
            "        tmp = validateTrace[:] + [self]\n",
            "        if self.expr is not None:\n",
            "            self.expr.validate(tmp)\n",
            "        self.checkRecursion([])\n",
            "\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            return super(ParseElementEnhance, self).__str__()\n",
            "        except Exception:\n",
            "            pass\n",
            "\n",
            "        if self.strRepr is None and self.expr is not None:\n",
            "            self.strRepr = \"%s:(%s)\" % (self.__class__.__name__, _ustr(self.expr))\n",
            "        return self.strRepr\n",
            "\n",
            "\n",
            "class FollowedBy(ParseElementEnhance):\n",
            "    \"\"\"Lookahead matching of the given parse expression.\n",
            "    ``FollowedBy`` does *not* advance the parsing position within\n",
            "    the input string, it only verifies that the specified parse\n",
            "    expression matches at the current position.  ``FollowedBy``\n",
            "    always returns a null token list. If any results names are defined\n",
            "    in the lookahead expression, those *will* be returned for access by\n",
            "    name.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # use FollowedBy to match a label only if it is followed by a ':'\n",
            "        data_word = Word(alphas)\n",
            "        label = data_word + FollowedBy(':')\n",
            "        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n",
            "\n",
            "        OneOrMore(attr_expr).parseString(\"shape: SQUARE color: BLACK posn: upper left\").pprint()\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]\n",
            "    \"\"\"\n",
            "    def __init__(self, expr):\n",
            "        super(FollowedBy, self).__init__(expr)\n",
            "        self.mayReturnEmpty = True\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        # by using self._expr.parse and deleting the contents of the returned ParseResults list\n",
            "        # we keep any named results that were defined in the FollowedBy expression\n",
            "        _, ret = self.expr._parse(instring, loc, doActions=doActions)\n",
            "        del ret[:]\n",
            "\n",
            "        return loc, ret\n",
            "\n",
            "\n",
            "class PrecededBy(ParseElementEnhance):\n",
            "    \"\"\"Lookbehind matching of the given parse expression.\n",
            "    ``PrecededBy`` does not advance the parsing position within the\n",
            "    input string, it only verifies that the specified parse expression\n",
            "    matches prior to the current position.  ``PrecededBy`` always\n",
            "    returns a null token list, but if a results name is defined on the\n",
            "    given expression, it is returned.\n",
            "\n",
            "    Parameters:\n",
            "\n",
            "     - expr - expression that must match prior to the current parse\n",
            "       location\n",
            "     - retreat - (default= ``None``) - (int) maximum number of characters\n",
            "       to lookbehind prior to the current parse location\n",
            "\n",
            "    If the lookbehind expression is a string, Literal, Keyword, or\n",
            "    a Word or CharsNotIn with a specified exact or maximum length, then\n",
            "    the retreat parameter is not required. Otherwise, retreat must be\n",
            "    specified to give a maximum number of characters to look back from\n",
            "    the current parse position for a lookbehind match.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # VB-style variable names with type prefixes\n",
            "        int_var = PrecededBy(\"#\") + pyparsing_common.identifier\n",
            "        str_var = PrecededBy(\"$\") + pyparsing_common.identifier\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, expr, retreat=None):\n",
            "        super(PrecededBy, self).__init__(expr)\n",
            "        self.expr = self.expr().leaveWhitespace()\n",
            "        self.mayReturnEmpty = True\n",
            "        self.mayIndexError = False\n",
            "        self.exact = False\n",
            "        if isinstance(expr, str):\n",
            "            retreat = len(expr)\n",
            "            self.exact = True\n",
            "        elif isinstance(expr, (Literal, Keyword)):\n",
            "            retreat = expr.matchLen\n",
            "            self.exact = True\n",
            "        elif isinstance(expr, (Word, CharsNotIn)) and expr.maxLen != _MAX_INT:\n",
            "            retreat = expr.maxLen\n",
            "            self.exact = True\n",
            "        elif isinstance(expr, _PositionToken):\n",
            "            retreat = 0\n",
            "            self.exact = True\n",
            "        self.retreat = retreat\n",
            "        self.errmsg = \"not preceded by \" + str(expr)\n",
            "        self.skipWhitespace = False\n",
            "\n",
            "    def parseImpl(self, instring, loc=0, doActions=True):\n",
            "        if self.exact:\n",
            "            if loc < self.retreat:\n",
            "                raise ParseException(instring, loc, self.errmsg)\n",
            "            start = loc - self.retreat\n",
            "            _, ret = self.expr._parse(instring, start)\n",
            "        else:\n",
            "            # retreat specified a maximum lookbehind window, iterate\n",
            "            test_expr = self.expr + StringEnd()\n",
            "            instring_slice = instring[:loc]\n",
            "            last_expr = ParseException(instring, loc, self.errmsg)\n",
            "            for offset in range(1, min(loc, self.retreat + 1)):\n",
            "                try:\n",
            "                    _, ret = test_expr._parse(instring_slice, loc - offset)\n",
            "                except ParseBaseException as pbe:\n",
            "                    last_expr = pbe\n",
            "                else:\n",
            "                    break\n",
            "            else:\n",
            "                raise last_expr\n",
            "        # return empty list of tokens, but preserve any defined results names\n",
            "        del ret[:]\n",
            "        return loc, ret\n",
            "\n",
            "\n",
            "class NotAny(ParseElementEnhance):\n",
            "    \"\"\"Lookahead to disallow matching with the given parse expression.\n",
            "    ``NotAny`` does *not* advance the parsing position within the\n",
            "    input string, it only verifies that the specified parse expression\n",
            "    does *not* match at the current position.  Also, ``NotAny`` does\n",
            "    *not* skip over leading whitespace. ``NotAny`` always returns\n",
            "    a null token list.  May be constructed using the '~' operator.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        AND, OR, NOT = map(CaselessKeyword, \"AND OR NOT\".split())\n",
            "\n",
            "        # take care not to mistake keywords for identifiers\n",
            "        ident = ~(AND | OR | NOT) + Word(alphas)\n",
            "        boolean_term = Optional(NOT) + ident\n",
            "\n",
            "        # very crude boolean expression - to support parenthesis groups and\n",
            "        # operation hierarchy, use infixNotation\n",
            "        boolean_expr = boolean_term + ZeroOrMore((AND | OR) + boolean_term)\n",
            "\n",
            "        # integers that are followed by \".\" are actually floats\n",
            "        integer = Word(nums) + ~Char(\".\")\n",
            "    \"\"\"\n",
            "    def __init__(self, expr):\n",
            "        super(NotAny, self).__init__(expr)\n",
            "        # ~ self.leaveWhitespace()\n",
            "        self.skipWhitespace = False  # do NOT use self.leaveWhitespace(), don't want to propagate to exprs\n",
            "        self.mayReturnEmpty = True\n",
            "        self.errmsg = \"Found unwanted token, \" + _ustr(self.expr)\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        if self.expr.canParseNext(instring, loc):\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "        return loc, []\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"~{\" + _ustr(self.expr) + \"}\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "class _MultipleMatch(ParseElementEnhance):\n",
            "    def __init__(self, expr, stopOn=None):\n",
            "        super(_MultipleMatch, self).__init__(expr)\n",
            "        self.saveAsList = True\n",
            "        ender = stopOn\n",
            "        if isinstance(ender, basestring):\n",
            "            ender = self._literalStringClass(ender)\n",
            "        self.stopOn(ender)\n",
            "\n",
            "    def stopOn(self, ender):\n",
            "        if isinstance(ender, basestring):\n",
            "            ender = self._literalStringClass(ender)\n",
            "        self.not_ender = ~ender if ender is not None else None\n",
            "        return self\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        self_expr_parse = self.expr._parse\n",
            "        self_skip_ignorables = self._skipIgnorables\n",
            "        check_ender = self.not_ender is not None\n",
            "        if check_ender:\n",
            "            try_not_ender = self.not_ender.tryParse\n",
            "\n",
            "        # must be at least one (but first see if we are the stopOn sentinel;\n",
            "        # if so, fail)\n",
            "        if check_ender:\n",
            "            try_not_ender(instring, loc)\n",
            "        loc, tokens = self_expr_parse(instring, loc, doActions, callPreParse=False)\n",
            "        try:\n",
            "            hasIgnoreExprs = (not not self.ignoreExprs)\n",
            "            while 1:\n",
            "                if check_ender:\n",
            "                    try_not_ender(instring, loc)\n",
            "                if hasIgnoreExprs:\n",
            "                    preloc = self_skip_ignorables(instring, loc)\n",
            "                else:\n",
            "                    preloc = loc\n",
            "                loc, tmptokens = self_expr_parse(instring, preloc, doActions)\n",
            "                if tmptokens or tmptokens.haskeys():\n",
            "                    tokens += tmptokens\n",
            "        except (ParseException, IndexError):\n",
            "            pass\n",
            "\n",
            "        return loc, tokens\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        if __diag__.warn_ungrouped_named_tokens_in_collection:\n",
            "            for e in [self.expr] + getattr(self.expr, 'exprs', []):\n",
            "                if isinstance(e, ParserElement) and e.resultsName:\n",
            "                    warnings.warn(\"{0}: setting results name {1!r} on {2} expression \"\n",
            "                                  \"collides with {3!r} on contained expression\".format(\"warn_ungrouped_named_tokens_in_collection\",\n",
            "                                                                                       name,\n",
            "                                                                                       type(self).__name__,\n",
            "                                                                                       e.resultsName),\n",
            "                                  stacklevel=3)\n",
            "\n",
            "        return super(_MultipleMatch, self)._setResultsName(name, listAllMatches)\n",
            "\n",
            "\n",
            "class OneOrMore(_MultipleMatch):\n",
            "    \"\"\"Repetition of one or more of the given expression.\n",
            "\n",
            "    Parameters:\n",
            "     - expr - expression that must match one or more times\n",
            "     - stopOn - (default= ``None``) - expression for a terminating sentinel\n",
            "          (only required if the sentinel would ordinarily match the repetition\n",
            "          expression)\n",
            "\n",
            "    Example::\n",
            "\n",
            "        data_word = Word(alphas)\n",
            "        label = data_word + FollowedBy(':')\n",
            "        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))\n",
            "\n",
            "        text = \"shape: SQUARE posn: upper left color: BLACK\"\n",
            "        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]\n",
            "\n",
            "        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data\n",
            "        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n",
            "        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]\n",
            "\n",
            "        # could also be written as\n",
            "        (attr_expr * (1,)).parseString(text).pprint()\n",
            "    \"\"\"\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"{\" + _ustr(self.expr) + \"}...\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "class ZeroOrMore(_MultipleMatch):\n",
            "    \"\"\"Optional repetition of zero or more of the given expression.\n",
            "\n",
            "    Parameters:\n",
            "     - expr - expression that must match zero or more times\n",
            "     - stopOn - (default= ``None``) - expression for a terminating sentinel\n",
            "          (only required if the sentinel would ordinarily match the repetition\n",
            "          expression)\n",
            "\n",
            "    Example: similar to :class:`OneOrMore`\n",
            "    \"\"\"\n",
            "    def __init__(self, expr, stopOn=None):\n",
            "        super(ZeroOrMore, self).__init__(expr, stopOn=stopOn)\n",
            "        self.mayReturnEmpty = True\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        try:\n",
            "            return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)\n",
            "        except (ParseException, IndexError):\n",
            "            return loc, []\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"[\" + _ustr(self.expr) + \"]...\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "\n",
            "class _NullToken(object):\n",
            "    def __bool__(self):\n",
            "        return False\n",
            "    __nonzero__ = __bool__\n",
            "    def __str__(self):\n",
            "        return \"\"\n",
            "\n",
            "class Optional(ParseElementEnhance):\n",
            "    \"\"\"Optional matching of the given expression.\n",
            "\n",
            "    Parameters:\n",
            "     - expr - expression that must match zero or more times\n",
            "     - default (optional) - value to be returned if the optional expression is not found.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier\n",
            "        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))\n",
            "        zip.runTests('''\n",
            "            # traditional ZIP code\n",
            "            12345\n",
            "\n",
            "            # ZIP+4 form\n",
            "            12101-0001\n",
            "\n",
            "            # invalid ZIP\n",
            "            98765-\n",
            "            ''')\n",
            "\n",
            "    prints::\n",
            "\n",
            "        # traditional ZIP code\n",
            "        12345\n",
            "        ['12345']\n",
            "\n",
            "        # ZIP+4 form\n",
            "        12101-0001\n",
            "        ['12101-0001']\n",
            "\n",
            "        # invalid ZIP\n",
            "        98765-\n",
            "             ^\n",
            "        FAIL: Expected end of text (at char 5), (line:1, col:6)\n",
            "    \"\"\"\n",
            "    __optionalNotMatched = _NullToken()\n",
            "\n",
            "    def __init__(self, expr, default=__optionalNotMatched):\n",
            "        super(Optional, self).__init__(expr, savelist=False)\n",
            "        self.saveAsList = self.expr.saveAsList\n",
            "        self.defaultValue = default\n",
            "        self.mayReturnEmpty = True\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        try:\n",
            "            loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "        except (ParseException, IndexError):\n",
            "            if self.defaultValue is not self.__optionalNotMatched:\n",
            "                if self.expr.resultsName:\n",
            "                    tokens = ParseResults([self.defaultValue])\n",
            "                    tokens[self.expr.resultsName] = self.defaultValue\n",
            "                else:\n",
            "                    tokens = [self.defaultValue]\n",
            "            else:\n",
            "                tokens = []\n",
            "        return loc, tokens\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "\n",
            "        if self.strRepr is None:\n",
            "            self.strRepr = \"[\" + _ustr(self.expr) + \"]\"\n",
            "\n",
            "        return self.strRepr\n",
            "\n",
            "class SkipTo(ParseElementEnhance):\n",
            "    \"\"\"Token for skipping over all undefined text until the matched\n",
            "    expression is found.\n",
            "\n",
            "    Parameters:\n",
            "     - expr - target expression marking the end of the data to be skipped\n",
            "     - include - (default= ``False``) if True, the target expression is also parsed\n",
            "          (the skipped text and target expression are returned as a 2-element list).\n",
            "     - ignore - (default= ``None``) used to define grammars (typically quoted strings and\n",
            "          comments) that might contain false matches to the target expression\n",
            "     - failOn - (default= ``None``) define expressions that are not allowed to be\n",
            "          included in the skipped test; if found before the target expression is found,\n",
            "          the SkipTo is not a match\n",
            "\n",
            "    Example::\n",
            "\n",
            "        report = '''\n",
            "            Outstanding Issues Report - 1 Jan 2000\n",
            "\n",
            "               # | Severity | Description                               |  Days Open\n",
            "            -----+----------+-------------------------------------------+-----------\n",
            "             101 | Critical | Intermittent system crash                 |          6\n",
            "              94 | Cosmetic | Spelling error on Login ('log|n')         |         14\n",
            "              79 | Minor    | System slow when running too many reports |         47\n",
            "            '''\n",
            "        integer = Word(nums)\n",
            "        SEP = Suppress('|')\n",
            "        # use SkipTo to simply match everything up until the next SEP\n",
            "        # - ignore quoted strings, so that a '|' character inside a quoted string does not match\n",
            "        # - parse action will call token.strip() for each matched token, i.e., the description body\n",
            "        string_data = SkipTo(SEP, ignore=quotedString)\n",
            "        string_data.setParseAction(tokenMap(str.strip))\n",
            "        ticket_expr = (integer(\"issue_num\") + SEP\n",
            "                      + string_data(\"sev\") + SEP\n",
            "                      + string_data(\"desc\") + SEP\n",
            "                      + integer(\"days_open\"))\n",
            "\n",
            "        for tkt in ticket_expr.searchString(report):\n",
            "            print tkt.dump()\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['101', 'Critical', 'Intermittent system crash', '6']\n",
            "        - days_open: 6\n",
            "        - desc: Intermittent system crash\n",
            "        - issue_num: 101\n",
            "        - sev: Critical\n",
            "        ['94', 'Cosmetic', \"Spelling error on Login ('log|n')\", '14']\n",
            "        - days_open: 14\n",
            "        - desc: Spelling error on Login ('log|n')\n",
            "        - issue_num: 94\n",
            "        - sev: Cosmetic\n",
            "        ['79', 'Minor', 'System slow when running too many reports', '47']\n",
            "        - days_open: 47\n",
            "        - desc: System slow when running too many reports\n",
            "        - issue_num: 79\n",
            "        - sev: Minor\n",
            "    \"\"\"\n",
            "    def __init__(self, other, include=False, ignore=None, failOn=None):\n",
            "        super(SkipTo, self).__init__(other)\n",
            "        self.ignoreExpr = ignore\n",
            "        self.mayReturnEmpty = True\n",
            "        self.mayIndexError = False\n",
            "        self.includeMatch = include\n",
            "        self.saveAsList = False\n",
            "        if isinstance(failOn, basestring):\n",
            "            self.failOn = self._literalStringClass(failOn)\n",
            "        else:\n",
            "            self.failOn = failOn\n",
            "        self.errmsg = \"No match found for \" + _ustr(self.expr)\n",
            "\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "        startloc = loc\n",
            "        instrlen = len(instring)\n",
            "        expr = self.expr\n",
            "        expr_parse = self.expr._parse\n",
            "        self_failOn_canParseNext = self.failOn.canParseNext if self.failOn is not None else None\n",
            "        self_ignoreExpr_tryParse = self.ignoreExpr.tryParse if self.ignoreExpr is not None else None\n",
            "\n",
            "        tmploc = loc\n",
            "        while tmploc <= instrlen:\n",
            "            if self_failOn_canParseNext is not None:\n",
            "                # break if failOn expression matches\n",
            "                if self_failOn_canParseNext(instring, tmploc):\n",
            "                    break\n",
            "\n",
            "            if self_ignoreExpr_tryParse is not None:\n",
            "                # advance past ignore expressions\n",
            "                while 1:\n",
            "                    try:\n",
            "                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)\n",
            "                    except ParseBaseException:\n",
            "                        break\n",
            "\n",
            "            try:\n",
            "                expr_parse(instring, tmploc, doActions=False, callPreParse=False)\n",
            "            except (ParseException, IndexError):\n",
            "                # no match, advance loc in string\n",
            "                tmploc += 1\n",
            "            else:\n",
            "                # matched skipto expr, done\n",
            "                break\n",
            "\n",
            "        else:\n",
            "            # ran off the end of the input string without matching skipto expr, fail\n",
            "            raise ParseException(instring, loc, self.errmsg, self)\n",
            "\n",
            "        # build up return values\n",
            "        loc = tmploc\n",
            "        skiptext = instring[startloc:loc]\n",
            "        skipresult = ParseResults(skiptext)\n",
            "\n",
            "        if self.includeMatch:\n",
            "            loc, mat = expr_parse(instring, loc, doActions, callPreParse=False)\n",
            "            skipresult += mat\n",
            "\n",
            "        return loc, skipresult\n",
            "\n",
            "class Forward(ParseElementEnhance):\n",
            "    \"\"\"Forward declaration of an expression to be defined later -\n",
            "    used for recursive grammars, such as algebraic infix notation.\n",
            "    When the expression is known, it is assigned to the ``Forward``\n",
            "    variable using the '<<' operator.\n",
            "\n",
            "    Note: take care when assigning to ``Forward`` not to overlook\n",
            "    precedence of operators.\n",
            "\n",
            "    Specifically, '|' has a lower precedence than '<<', so that::\n",
            "\n",
            "        fwdExpr << a | b | c\n",
            "\n",
            "    will actually be evaluated as::\n",
            "\n",
            "        (fwdExpr << a) | b | c\n",
            "\n",
            "    thereby leaving b and c out as parseable alternatives.  It is recommended that you\n",
            "    explicitly group the values inserted into the ``Forward``::\n",
            "\n",
            "        fwdExpr << (a | b | c)\n",
            "\n",
            "    Converting to use the '<<=' operator instead will avoid this problem.\n",
            "\n",
            "    See :class:`ParseResults.pprint` for an example of a recursive\n",
            "    parser created using ``Forward``.\n",
            "    \"\"\"\n",
            "    def __init__(self, other=None):\n",
            "        super(Forward, self).__init__(other, savelist=False)\n",
            "\n",
            "    def __lshift__(self, other):\n",
            "        if isinstance(other, basestring):\n",
            "            other = self._literalStringClass(other)\n",
            "        self.expr = other\n",
            "        self.strRepr = None\n",
            "        self.mayIndexError = self.expr.mayIndexError\n",
            "        self.mayReturnEmpty = self.expr.mayReturnEmpty\n",
            "        self.setWhitespaceChars(self.expr.whiteChars)\n",
            "        self.skipWhitespace = self.expr.skipWhitespace\n",
            "        self.saveAsList = self.expr.saveAsList\n",
            "        self.ignoreExprs.extend(self.expr.ignoreExprs)\n",
            "        return self\n",
            "\n",
            "    def __ilshift__(self, other):\n",
            "        return self << other\n",
            "\n",
            "    def leaveWhitespace(self):\n",
            "        self.skipWhitespace = False\n",
            "        return self\n",
            "\n",
            "    def streamline(self):\n",
            "        if not self.streamlined:\n",
            "            self.streamlined = True\n",
            "            if self.expr is not None:\n",
            "                self.expr.streamline()\n",
            "        return self\n",
            "\n",
            "    def validate(self, validateTrace=None):\n",
            "        if validateTrace is None:\n",
            "            validateTrace = []\n",
            "\n",
            "        if self not in validateTrace:\n",
            "            tmp = validateTrace[:] + [self]\n",
            "            if self.expr is not None:\n",
            "                self.expr.validate(tmp)\n",
            "        self.checkRecursion([])\n",
            "\n",
            "    def __str__(self):\n",
            "        if hasattr(self, \"name\"):\n",
            "            return self.name\n",
            "        if self.strRepr is not None:\n",
            "            return self.strRepr\n",
            "\n",
            "        # Avoid infinite recursion by setting a temporary strRepr\n",
            "        self.strRepr = \": ...\"\n",
            "\n",
            "        # Use the string representation of main expression.\n",
            "        retString = '...'\n",
            "        try:\n",
            "            if self.expr is not None:\n",
            "                retString = _ustr(self.expr)[:1000]\n",
            "            else:\n",
            "                retString = \"None\"\n",
            "        finally:\n",
            "            self.strRepr = self.__class__.__name__ + \": \" + retString\n",
            "        return self.strRepr\n",
            "\n",
            "    def copy(self):\n",
            "        if self.expr is not None:\n",
            "            return super(Forward, self).copy()\n",
            "        else:\n",
            "            ret = Forward()\n",
            "            ret <<= self\n",
            "            return ret\n",
            "\n",
            "    def _setResultsName(self, name, listAllMatches=False):\n",
            "        if __diag__.warn_name_set_on_empty_Forward:\n",
            "            if self.expr is None:\n",
            "                warnings.warn(\"{0}: setting results name {0!r} on {1} expression \"\n",
            "                              \"that has no contained expression\".format(\"warn_name_set_on_empty_Forward\",\n",
            "                                                                        name,\n",
            "                                                                        type(self).__name__),\n",
            "                              stacklevel=3)\n",
            "\n",
            "        return super(Forward, self)._setResultsName(name, listAllMatches)\n",
            "\n",
            "class TokenConverter(ParseElementEnhance):\n",
            "    \"\"\"\n",
            "    Abstract subclass of :class:`ParseExpression`, for converting parsed results.\n",
            "    \"\"\"\n",
            "    def __init__(self, expr, savelist=False):\n",
            "        super(TokenConverter, self).__init__(expr)  # , savelist)\n",
            "        self.saveAsList = False\n",
            "\n",
            "class Combine(TokenConverter):\n",
            "    \"\"\"Converter to concatenate all matching tokens to a single string.\n",
            "    By default, the matching patterns must also be contiguous in the\n",
            "    input string; this can be disabled by specifying\n",
            "    ``'adjacent=False'`` in the constructor.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        real = Word(nums) + '.' + Word(nums)\n",
            "        print(real.parseString('3.1416')) # -> ['3', '.', '1416']\n",
            "        # will also erroneously match the following\n",
            "        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']\n",
            "\n",
            "        real = Combine(Word(nums) + '.' + Word(nums))\n",
            "        print(real.parseString('3.1416')) # -> ['3.1416']\n",
            "        # no match when there are internal spaces\n",
            "        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)\n",
            "    \"\"\"\n",
            "    def __init__(self, expr, joinString=\"\", adjacent=True):\n",
            "        super(Combine, self).__init__(expr)\n",
            "        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself\n",
            "        if adjacent:\n",
            "            self.leaveWhitespace()\n",
            "        self.adjacent = adjacent\n",
            "        self.skipWhitespace = True\n",
            "        self.joinString = joinString\n",
            "        self.callPreparse = True\n",
            "\n",
            "    def ignore(self, other):\n",
            "        if self.adjacent:\n",
            "            ParserElement.ignore(self, other)\n",
            "        else:\n",
            "            super(Combine, self).ignore(other)\n",
            "        return self\n",
            "\n",
            "    def postParse(self, instring, loc, tokenlist):\n",
            "        retToks = tokenlist.copy()\n",
            "        del retToks[:]\n",
            "        retToks += ParseResults([\"\".join(tokenlist._asStringList(self.joinString))], modal=self.modalResults)\n",
            "\n",
            "        if self.resultsName and retToks.haskeys():\n",
            "            return [retToks]\n",
            "        else:\n",
            "            return retToks\n",
            "\n",
            "class Group(TokenConverter):\n",
            "    \"\"\"Converter to return the matched tokens as a list - useful for\n",
            "    returning tokens of :class:`ZeroOrMore` and :class:`OneOrMore` expressions.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        ident = Word(alphas)\n",
            "        num = Word(nums)\n",
            "        term = ident | num\n",
            "        func = ident + Optional(delimitedList(term))\n",
            "        print(func.parseString(\"fn a, b, 100\"))  # -> ['fn', 'a', 'b', '100']\n",
            "\n",
            "        func = ident + Group(Optional(delimitedList(term)))\n",
            "        print(func.parseString(\"fn a, b, 100\"))  # -> ['fn', ['a', 'b', '100']]\n",
            "    \"\"\"\n",
            "    def __init__(self, expr):\n",
            "        super(Group, self).__init__(expr)\n",
            "        self.saveAsList = True\n",
            "\n",
            "    def postParse(self, instring, loc, tokenlist):\n",
            "        return [tokenlist]\n",
            "\n",
            "class Dict(TokenConverter):\n",
            "    \"\"\"Converter to return a repetitive expression as a list, but also\n",
            "    as a dictionary. Each element can also be referenced using the first\n",
            "    token in the expression as its key. Useful for tabular report\n",
            "    scraping when the first column can be used as a item key.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        data_word = Word(alphas)\n",
            "        label = data_word + FollowedBy(':')\n",
            "        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))\n",
            "\n",
            "        text = \"shape: SQUARE posn: upper left color: light blue texture: burlap\"\n",
            "        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n",
            "\n",
            "        # print attributes as plain groups\n",
            "        print(OneOrMore(attr_expr).parseString(text).dump())\n",
            "\n",
            "        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names\n",
            "        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)\n",
            "        print(result.dump())\n",
            "\n",
            "        # access named fields as dict entries, or output as dict\n",
            "        print(result['shape'])\n",
            "        print(result.asDict())\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']\n",
            "        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]\n",
            "        - color: light blue\n",
            "        - posn: upper left\n",
            "        - shape: SQUARE\n",
            "        - texture: burlap\n",
            "        SQUARE\n",
            "        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}\n",
            "\n",
            "    See more examples at :class:`ParseResults` of accessing fields by results name.\n",
            "    \"\"\"\n",
            "    def __init__(self, expr):\n",
            "        super(Dict, self).__init__(expr)\n",
            "        self.saveAsList = True\n",
            "\n",
            "    def postParse(self, instring, loc, tokenlist):\n",
            "        for i, tok in enumerate(tokenlist):\n",
            "            if len(tok) == 0:\n",
            "                continue\n",
            "            ikey = tok[0]\n",
            "            if isinstance(ikey, int):\n",
            "                ikey = _ustr(tok[0]).strip()\n",
            "            if len(tok) == 1:\n",
            "                tokenlist[ikey] = _ParseResultsWithOffset(\"\", i)\n",
            "            elif len(tok) == 2 and not isinstance(tok[1], ParseResults):\n",
            "                tokenlist[ikey] = _ParseResultsWithOffset(tok[1], i)\n",
            "            else:\n",
            "                dictvalue = tok.copy()  # ParseResults(i)\n",
            "                del dictvalue[0]\n",
            "                if len(dictvalue) != 1 or (isinstance(dictvalue, ParseResults) and dictvalue.haskeys()):\n",
            "                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue, i)\n",
            "                else:\n",
            "                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0], i)\n",
            "\n",
            "        if self.resultsName:\n",
            "            return [tokenlist]\n",
            "        else:\n",
            "            return tokenlist\n",
            "\n",
            "\n",
            "class Suppress(TokenConverter):\n",
            "    \"\"\"Converter for ignoring the results of a parsed expression.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        source = \"a, b, c,d\"\n",
            "        wd = Word(alphas)\n",
            "        wd_list1 = wd + ZeroOrMore(',' + wd)\n",
            "        print(wd_list1.parseString(source))\n",
            "\n",
            "        # often, delimiters that are useful during parsing are just in the\n",
            "        # way afterward - use Suppress to keep them out of the parsed output\n",
            "        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)\n",
            "        print(wd_list2.parseString(source))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['a', ',', 'b', ',', 'c', ',', 'd']\n",
            "        ['a', 'b', 'c', 'd']\n",
            "\n",
            "    (See also :class:`delimitedList`.)\n",
            "    \"\"\"\n",
            "    def postParse(self, instring, loc, tokenlist):\n",
            "        return []\n",
            "\n",
            "    def suppress(self):\n",
            "        return self\n",
            "\n",
            "\n",
            "class OnlyOnce(object):\n",
            "    \"\"\"Wrapper for parse actions, to ensure they are only called once.\n",
            "    \"\"\"\n",
            "    def __init__(self, methodCall):\n",
            "        self.callable = _trim_arity(methodCall)\n",
            "        self.called = False\n",
            "    def __call__(self, s, l, t):\n",
            "        if not self.called:\n",
            "            results = self.callable(s, l, t)\n",
            "            self.called = True\n",
            "            return results\n",
            "        raise ParseException(s, l, \"\")\n",
            "    def reset(self):\n",
            "        self.called = False\n",
            "\n",
            "def traceParseAction(f):\n",
            "    \"\"\"Decorator for debugging parse actions.\n",
            "\n",
            "    When the parse action is called, this decorator will print\n",
            "    ``\">> entering method-name(line:<current_source_line>, <parse_location>, <matched_tokens>)\"``.\n",
            "    When the parse action completes, the decorator will print\n",
            "    ``\"<<\"`` followed by the returned value, or any exception that the parse action raised.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        wd = Word(alphas)\n",
            "\n",
            "        @traceParseAction\n",
            "        def remove_duplicate_chars(tokens):\n",
            "            return ''.join(sorted(set(''.join(tokens))))\n",
            "\n",
            "        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)\n",
            "        print(wds.parseString(\"slkdjs sld sldd sdlf sdljf\"))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))\n",
            "        <<leaving remove_duplicate_chars (ret: 'dfjkls')\n",
            "        ['dfjkls']\n",
            "    \"\"\"\n",
            "    f = _trim_arity(f)\n",
            "    def z(*paArgs):\n",
            "        thisFunc = f.__name__\n",
            "        s, l, t = paArgs[-3:]\n",
            "        if len(paArgs) > 3:\n",
            "            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n",
            "        sys.stderr.write(\">>entering %s(line: '%s', %d, %r)\\n\" % (thisFunc, line(l, s), l, t))\n",
            "        try:\n",
            "            ret = f(*paArgs)\n",
            "        except Exception as exc:\n",
            "            sys.stderr.write(\"<<leaving %s (exception: %s)\\n\" % (thisFunc, exc))\n",
            "            raise\n",
            "        sys.stderr.write(\"<<leaving %s (ret: %r)\\n\" % (thisFunc, ret))\n",
            "        return ret\n",
            "    try:\n",
            "        z.__name__ = f.__name__\n",
            "    except AttributeError:\n",
            "        pass\n",
            "    return z\n",
            "\n",
            "#\n",
            "# global helpers\n",
            "#\n",
            "def delimitedList(expr, delim=\",\", combine=False):\n",
            "    \"\"\"Helper to define a delimited list of expressions - the delimiter\n",
            "    defaults to ','. By default, the list elements and delimiters can\n",
            "    have intervening whitespace, and comments, but this can be\n",
            "    overridden by passing ``combine=True`` in the constructor. If\n",
            "    ``combine`` is set to ``True``, the matching tokens are\n",
            "    returned as a single token string, with the delimiters included;\n",
            "    otherwise, the matching tokens are returned as a list of tokens,\n",
            "    with the delimiters suppressed.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        delimitedList(Word(alphas)).parseString(\"aa,bb,cc\") # -> ['aa', 'bb', 'cc']\n",
            "        delimitedList(Word(hexnums), delim=':', combine=True).parseString(\"AA:BB:CC:DD:EE\") # -> ['AA:BB:CC:DD:EE']\n",
            "    \"\"\"\n",
            "    dlName = _ustr(expr) + \" [\" + _ustr(delim) + \" \" + _ustr(expr) + \"]...\"\n",
            "    if combine:\n",
            "        return Combine(expr + ZeroOrMore(delim + expr)).setName(dlName)\n",
            "    else:\n",
            "        return (expr + ZeroOrMore(Suppress(delim) + expr)).setName(dlName)\n",
            "\n",
            "def countedArray(expr, intExpr=None):\n",
            "    \"\"\"Helper to define a counted list of expressions.\n",
            "\n",
            "    This helper defines a pattern of the form::\n",
            "\n",
            "        integer expr expr expr...\n",
            "\n",
            "    where the leading integer tells how many expr expressions follow.\n",
            "    The matched tokens returns the array of expr tokens as a list - the\n",
            "    leading count token is suppressed.\n",
            "\n",
            "    If ``intExpr`` is specified, it should be a pyparsing expression\n",
            "    that produces an integer value.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        countedArray(Word(alphas)).parseString('2 ab cd ef')  # -> ['ab', 'cd']\n",
            "\n",
            "        # in this parser, the leading integer value is given in binary,\n",
            "        # '10' indicating that 2 values are in the array\n",
            "        binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))\n",
            "        countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')  # -> ['ab', 'cd']\n",
            "    \"\"\"\n",
            "    arrayExpr = Forward()\n",
            "    def countFieldParseAction(s, l, t):\n",
            "        n = t[0]\n",
            "        arrayExpr << (n and Group(And([expr] * n)) or Group(empty))\n",
            "        return []\n",
            "    if intExpr is None:\n",
            "        intExpr = Word(nums).setParseAction(lambda t: int(t[0]))\n",
            "    else:\n",
            "        intExpr = intExpr.copy()\n",
            "    intExpr.setName(\"arrayLen\")\n",
            "    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)\n",
            "    return (intExpr + arrayExpr).setName('(len) ' + _ustr(expr) + '...')\n",
            "\n",
            "def _flatten(L):\n",
            "    ret = []\n",
            "    for i in L:\n",
            "        if isinstance(i, list):\n",
            "            ret.extend(_flatten(i))\n",
            "        else:\n",
            "            ret.append(i)\n",
            "    return ret\n",
            "\n",
            "def matchPreviousLiteral(expr):\n",
            "    \"\"\"Helper to define an expression that is indirectly defined from\n",
            "    the tokens matched in a previous expression, that is, it looks for\n",
            "    a 'repeat' of a previous expression.  For example::\n",
            "\n",
            "        first = Word(nums)\n",
            "        second = matchPreviousLiteral(first)\n",
            "        matchExpr = first + \":\" + second\n",
            "\n",
            "    will match ``\"1:1\"``, but not ``\"1:2\"``.  Because this\n",
            "    matches a previous literal, will also match the leading\n",
            "    ``\"1:1\"`` in ``\"1:10\"``. If this is not desired, use\n",
            "    :class:`matchPreviousExpr`. Do *not* use with packrat parsing\n",
            "    enabled.\n",
            "    \"\"\"\n",
            "    rep = Forward()\n",
            "    def copyTokenToRepeater(s, l, t):\n",
            "        if t:\n",
            "            if len(t) == 1:\n",
            "                rep << t[0]\n",
            "            else:\n",
            "                # flatten t tokens\n",
            "                tflat = _flatten(t.asList())\n",
            "                rep << And(Literal(tt) for tt in tflat)\n",
            "        else:\n",
            "            rep << Empty()\n",
            "    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)\n",
            "    rep.setName('(prev) ' + _ustr(expr))\n",
            "    return rep\n",
            "\n",
            "def matchPreviousExpr(expr):\n",
            "    \"\"\"Helper to define an expression that is indirectly defined from\n",
            "    the tokens matched in a previous expression, that is, it looks for\n",
            "    a 'repeat' of a previous expression.  For example::\n",
            "\n",
            "        first = Word(nums)\n",
            "        second = matchPreviousExpr(first)\n",
            "        matchExpr = first + \":\" + second\n",
            "\n",
            "    will match ``\"1:1\"``, but not ``\"1:2\"``.  Because this\n",
            "    matches by expressions, will *not* match the leading ``\"1:1\"``\n",
            "    in ``\"1:10\"``; the expressions are evaluated first, and then\n",
            "    compared, so ``\"1\"`` is compared with ``\"10\"``. Do *not* use\n",
            "    with packrat parsing enabled.\n",
            "    \"\"\"\n",
            "    rep = Forward()\n",
            "    e2 = expr.copy()\n",
            "    rep <<= e2\n",
            "    def copyTokenToRepeater(s, l, t):\n",
            "        matchTokens = _flatten(t.asList())\n",
            "        def mustMatchTheseTokens(s, l, t):\n",
            "            theseTokens = _flatten(t.asList())\n",
            "            if theseTokens != matchTokens:\n",
            "                raise ParseException('', 0, '')\n",
            "        rep.setParseAction(mustMatchTheseTokens, callDuringTry=True)\n",
            "    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)\n",
            "    rep.setName('(prev) ' + _ustr(expr))\n",
            "    return rep\n",
            "\n",
            "def _escapeRegexRangeChars(s):\n",
            "    # ~  escape these chars: ^-]\n",
            "    for c in r\"\\^-]\":\n",
            "        s = s.replace(c, _bslash + c)\n",
            "    s = s.replace(\"\\n\", r\"\\n\")\n",
            "    s = s.replace(\"\\t\", r\"\\t\")\n",
            "    return _ustr(s)\n",
            "\n",
            "def oneOf(strs, caseless=False, useRegex=True, asKeyword=False):\n",
            "    \"\"\"Helper to quickly define a set of alternative Literals, and makes\n",
            "    sure to do longest-first testing when there is a conflict,\n",
            "    regardless of the input order, but returns\n",
            "    a :class:`MatchFirst` for best performance.\n",
            "\n",
            "    Parameters:\n",
            "\n",
            "     - strs - a string of space-delimited literals, or a collection of\n",
            "       string literals\n",
            "     - caseless - (default= ``False``) - treat all literals as\n",
            "       caseless\n",
            "     - useRegex - (default= ``True``) - as an optimization, will\n",
            "       generate a Regex object; otherwise, will generate\n",
            "       a :class:`MatchFirst` object (if ``caseless=True`` or ``asKeyword=True``, or if\n",
            "       creating a :class:`Regex` raises an exception)\n",
            "     - asKeyword - (default=``False``) - enforce Keyword-style matching on the\n",
            "       generated expressions\n",
            "\n",
            "    Example::\n",
            "\n",
            "        comp_oper = oneOf(\"< = > <= >= !=\")\n",
            "        var = Word(alphas)\n",
            "        number = Word(nums)\n",
            "        term = var | number\n",
            "        comparison_expr = term + comp_oper + term\n",
            "        print(comparison_expr.searchString(\"B = 12  AA=23 B<=AA AA>12\"))\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]\n",
            "    \"\"\"\n",
            "    if isinstance(caseless, basestring):\n",
            "        warnings.warn(\"More than one string argument passed to oneOf, pass \"\n",
            "                      \"choices as a list or space-delimited string\", stacklevel=2)\n",
            "\n",
            "    if caseless:\n",
            "        isequal = (lambda a, b: a.upper() == b.upper())\n",
            "        masks = (lambda a, b: b.upper().startswith(a.upper()))\n",
            "        parseElementClass = CaselessKeyword if asKeyword else CaselessLiteral\n",
            "    else:\n",
            "        isequal = (lambda a, b: a == b)\n",
            "        masks = (lambda a, b: b.startswith(a))\n",
            "        parseElementClass = Keyword if asKeyword else Literal\n",
            "\n",
            "    symbols = []\n",
            "    if isinstance(strs, basestring):\n",
            "        symbols = strs.split()\n",
            "    elif isinstance(strs, Iterable):\n",
            "        symbols = list(strs)\n",
            "    else:\n",
            "        warnings.warn(\"Invalid argument to oneOf, expected string or iterable\",\n",
            "                      SyntaxWarning, stacklevel=2)\n",
            "    if not symbols:\n",
            "        return NoMatch()\n",
            "\n",
            "    if not asKeyword:\n",
            "        # if not producing keywords, need to reorder to take care to avoid masking\n",
            "        # longer choices with shorter ones\n",
            "        i = 0\n",
            "        while i < len(symbols) - 1:\n",
            "            cur = symbols[i]\n",
            "            for j, other in enumerate(symbols[i + 1:]):\n",
            "                if isequal(other, cur):\n",
            "                    del symbols[i + j + 1]\n",
            "                    break\n",
            "                elif masks(cur, other):\n",
            "                    del symbols[i + j + 1]\n",
            "                    symbols.insert(i, other)\n",
            "                    break\n",
            "            else:\n",
            "                i += 1\n",
            "\n",
            "    if not (caseless or asKeyword) and useRegex:\n",
            "        # ~ print (strs, \"->\", \"|\".join([_escapeRegexChars(sym) for sym in symbols]))\n",
            "        try:\n",
            "            if len(symbols) == len(\"\".join(symbols)):\n",
            "                return Regex(\"[%s]\" % \"\".join(_escapeRegexRangeChars(sym) for sym in symbols)).setName(' | '.join(symbols))\n",
            "            else:\n",
            "                return Regex(\"|\".join(re.escape(sym) for sym in symbols)).setName(' | '.join(symbols))\n",
            "        except Exception:\n",
            "            warnings.warn(\"Exception creating Regex for oneOf, building MatchFirst\",\n",
            "                    SyntaxWarning, stacklevel=2)\n",
            "\n",
            "    # last resort, just use MatchFirst\n",
            "    return MatchFirst(parseElementClass(sym) for sym in symbols).setName(' | '.join(symbols))\n",
            "\n",
            "def dictOf(key, value):\n",
            "    \"\"\"Helper to easily and clearly define a dictionary by specifying\n",
            "    the respective patterns for the key and value.  Takes care of\n",
            "    defining the :class:`Dict`, :class:`ZeroOrMore`, and\n",
            "    :class:`Group` tokens in the proper order.  The key pattern\n",
            "    can include delimiting markers or punctuation, as long as they are\n",
            "    suppressed, thereby leaving the significant key text.  The value\n",
            "    pattern can include named results, so that the :class:`Dict` results\n",
            "    can include named token fields.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        text = \"shape: SQUARE posn: upper left color: light blue texture: burlap\"\n",
            "        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))\n",
            "        print(OneOrMore(attr_expr).parseString(text).dump())\n",
            "\n",
            "        attr_label = label\n",
            "        attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)\n",
            "\n",
            "        # similar to Dict, but simpler call format\n",
            "        result = dictOf(attr_label, attr_value).parseString(text)\n",
            "        print(result.dump())\n",
            "        print(result['shape'])\n",
            "        print(result.shape)  # object attribute access works too\n",
            "        print(result.asDict())\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]\n",
            "        - color: light blue\n",
            "        - posn: upper left\n",
            "        - shape: SQUARE\n",
            "        - texture: burlap\n",
            "        SQUARE\n",
            "        SQUARE\n",
            "        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}\n",
            "    \"\"\"\n",
            "    return Dict(OneOrMore(Group(key + value)))\n",
            "\n",
            "def originalTextFor(expr, asString=True):\n",
            "    \"\"\"Helper to return the original, untokenized text for a given\n",
            "    expression.  Useful to restore the parsed fields of an HTML start\n",
            "    tag into the raw tag text itself, or to revert separate tokens with\n",
            "    intervening whitespace back to the original matching input text. By\n",
            "    default, returns astring containing the original parsed text.\n",
            "\n",
            "    If the optional ``asString`` argument is passed as\n",
            "    ``False``, then the return value is\n",
            "    a :class:`ParseResults` containing any results names that\n",
            "    were originally matched, and a single token containing the original\n",
            "    matched text from the input string.  So if the expression passed to\n",
            "    :class:`originalTextFor` contains expressions with defined\n",
            "    results names, you must set ``asString`` to ``False`` if you\n",
            "    want to preserve those results name values.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        src = \"this is test <b> bold <i>text</i> </b> normal text \"\n",
            "        for tag in (\"b\", \"i\"):\n",
            "            opener, closer = makeHTMLTags(tag)\n",
            "            patt = originalTextFor(opener + SkipTo(closer) + closer)\n",
            "            print(patt.searchString(src)[0])\n",
            "\n",
            "    prints::\n",
            "\n",
            "        ['<b> bold <i>text</i> </b>']\n",
            "        ['<i>text</i>']\n",
            "    \"\"\"\n",
            "    locMarker = Empty().setParseAction(lambda s, loc, t: loc)\n",
            "    endlocMarker = locMarker.copy()\n",
            "    endlocMarker.callPreparse = False\n",
            "    matchExpr = locMarker(\"_original_start\") + expr + endlocMarker(\"_original_end\")\n",
            "    if asString:\n",
            "        extractText = lambda s, l, t: s[t._original_start: t._original_end]\n",
            "    else:\n",
            "        def extractText(s, l, t):\n",
            "            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]\n",
            "    matchExpr.setParseAction(extractText)\n",
            "    matchExpr.ignoreExprs = expr.ignoreExprs\n",
            "    return matchExpr\n",
            "\n",
            "def ungroup(expr):\n",
            "    \"\"\"Helper to undo pyparsing's default grouping of And expressions,\n",
            "    even if all but one are non-empty.\n",
            "    \"\"\"\n",
            "    return TokenConverter(expr).addParseAction(lambda t: t[0])\n",
            "\n",
            "def locatedExpr(expr):\n",
            "    \"\"\"Helper to decorate a returned token with its starting and ending\n",
            "    locations in the input string.\n",
            "\n",
            "    This helper adds the following results names:\n",
            "\n",
            "     - locn_start = location where matched expression begins\n",
            "     - locn_end = location where matched expression ends\n",
            "     - value = the actual parsed results\n",
            "\n",
            "    Be careful if the input text contains ``<TAB>`` characters, you\n",
            "    may want to call :class:`ParserElement.parseWithTabs`\n",
            "\n",
            "    Example::\n",
            "\n",
            "        wd = Word(alphas)\n",
            "        for match in locatedExpr(wd).searchString(\"ljsdf123lksdjjf123lkkjj1222\"):\n",
            "            print(match)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [[0, 'ljsdf', 5]]\n",
            "        [[8, 'lksdjjf', 15]]\n",
            "        [[18, 'lkkjj', 23]]\n",
            "    \"\"\"\n",
            "    locator = Empty().setParseAction(lambda s, l, t: l)\n",
            "    return Group(locator(\"locn_start\") + expr(\"value\") + locator.copy().leaveWhitespace()(\"locn_end\"))\n",
            "\n",
            "\n",
            "# convenience constants for positional expressions\n",
            "empty       = Empty().setName(\"empty\")\n",
            "lineStart   = LineStart().setName(\"lineStart\")\n",
            "lineEnd     = LineEnd().setName(\"lineEnd\")\n",
            "stringStart = StringStart().setName(\"stringStart\")\n",
            "stringEnd   = StringEnd().setName(\"stringEnd\")\n",
            "\n",
            "_escapedPunc = Word(_bslash, r\"\\[]-*.$+^?()~ \", exact=2).setParseAction(lambda s, l, t: t[0][1])\n",
            "_escapedHexChar = Regex(r\"\\\\0?[xX][0-9a-fA-F]+\").setParseAction(lambda s, l, t: unichr(int(t[0].lstrip(r'\\0x'), 16)))\n",
            "_escapedOctChar = Regex(r\"\\\\0[0-7]+\").setParseAction(lambda s, l, t: unichr(int(t[0][1:], 8)))\n",
            "_singleChar = _escapedPunc | _escapedHexChar | _escapedOctChar | CharsNotIn(r'\\]', exact=1)\n",
            "_charRange = Group(_singleChar + Suppress(\"-\") + _singleChar)\n",
            "_reBracketExpr = Literal(\"[\") + Optional(\"^\").setResultsName(\"negate\") + Group(OneOrMore(_charRange | _singleChar)).setResultsName(\"body\") + \"]\"\n",
            "\n",
            "def srange(s):\n",
            "    r\"\"\"Helper to easily define string ranges for use in Word\n",
            "    construction. Borrows syntax from regexp '[]' string range\n",
            "    definitions::\n",
            "\n",
            "        srange(\"[0-9]\")   -> \"0123456789\"\n",
            "        srange(\"[a-z]\")   -> \"abcdefghijklmnopqrstuvwxyz\"\n",
            "        srange(\"[a-z$_]\") -> \"abcdefghijklmnopqrstuvwxyz$_\"\n",
            "\n",
            "    The input string must be enclosed in []'s, and the returned string\n",
            "    is the expanded character set joined into a single string. The\n",
            "    values enclosed in the []'s may be:\n",
            "\n",
            "     - a single character\n",
            "     - an escaped character with a leading backslash (such as ``\\-``\n",
            "       or ``\\]``)\n",
            "     - an escaped hex character with a leading ``'\\x'``\n",
            "       (``\\x21``, which is a ``'!'`` character) (``\\0x##``\n",
            "       is also supported for backwards compatibility)\n",
            "     - an escaped octal character with a leading ``'\\0'``\n",
            "       (``\\041``, which is a ``'!'`` character)\n",
            "     - a range of any of the above, separated by a dash (``'a-z'``,\n",
            "       etc.)\n",
            "     - any combination of the above (``'aeiouy'``,\n",
            "       ``'a-zA-Z0-9_$'``, etc.)\n",
            "    \"\"\"\n",
            "    _expanded = lambda p: p if not isinstance(p, ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]), ord(p[1]) + 1))\n",
            "    try:\n",
            "        return \"\".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)\n",
            "    except Exception:\n",
            "        return \"\"\n",
            "\n",
            "def matchOnlyAtCol(n):\n",
            "    \"\"\"Helper method for defining parse actions that require matching at\n",
            "    a specific column in the input text.\n",
            "    \"\"\"\n",
            "    def verifyCol(strg, locn, toks):\n",
            "        if col(locn, strg) != n:\n",
            "            raise ParseException(strg, locn, \"matched token not at column %d\" % n)\n",
            "    return verifyCol\n",
            "\n",
            "def replaceWith(replStr):\n",
            "    \"\"\"Helper method for common parse actions that simply return\n",
            "    a literal value.  Especially useful when used with\n",
            "    :class:`transformString<ParserElement.transformString>` ().\n",
            "\n",
            "    Example::\n",
            "\n",
            "        num = Word(nums).setParseAction(lambda toks: int(toks[0]))\n",
            "        na = oneOf(\"N/A NA\").setParseAction(replaceWith(math.nan))\n",
            "        term = na | num\n",
            "\n",
            "        OneOrMore(term).parseString(\"324 234 N/A 234\") # -> [324, 234, nan, 234]\n",
            "    \"\"\"\n",
            "    return lambda s, l, t: [replStr]\n",
            "\n",
            "def removeQuotes(s, l, t):\n",
            "    \"\"\"Helper parse action for removing quotation marks from parsed\n",
            "    quoted strings.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # by default, quotation marks are included in parsed results\n",
            "        quotedString.parseString(\"'Now is the Winter of our Discontent'\") # -> [\"'Now is the Winter of our Discontent'\"]\n",
            "\n",
            "        # use removeQuotes to strip quotation marks from parsed results\n",
            "        quotedString.setParseAction(removeQuotes)\n",
            "        quotedString.parseString(\"'Now is the Winter of our Discontent'\") # -> [\"Now is the Winter of our Discontent\"]\n",
            "    \"\"\"\n",
            "    return t[0][1:-1]\n",
            "\n",
            "def tokenMap(func, *args):\n",
            "    \"\"\"Helper to define a parse action by mapping a function to all\n",
            "    elements of a ParseResults list. If any additional args are passed,\n",
            "    they are forwarded to the given function as additional arguments\n",
            "    after the token, as in\n",
            "    ``hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))``,\n",
            "    which will convert the parsed data to an integer using base 16.\n",
            "\n",
            "    Example (compare the last to example in :class:`ParserElement.transformString`::\n",
            "\n",
            "        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))\n",
            "        hex_ints.runTests('''\n",
            "            00 11 22 aa FF 0a 0d 1a\n",
            "            ''')\n",
            "\n",
            "        upperword = Word(alphas).setParseAction(tokenMap(str.upper))\n",
            "        OneOrMore(upperword).runTests('''\n",
            "            my kingdom for a horse\n",
            "            ''')\n",
            "\n",
            "        wd = Word(alphas).setParseAction(tokenMap(str.title))\n",
            "        OneOrMore(wd).setParseAction(' '.join).runTests('''\n",
            "            now is the winter of our discontent made glorious summer by this sun of york\n",
            "            ''')\n",
            "\n",
            "    prints::\n",
            "\n",
            "        00 11 22 aa FF 0a 0d 1a\n",
            "        [0, 17, 34, 170, 255, 10, 13, 26]\n",
            "\n",
            "        my kingdom for a horse\n",
            "        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']\n",
            "\n",
            "        now is the winter of our discontent made glorious summer by this sun of york\n",
            "        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']\n",
            "    \"\"\"\n",
            "    def pa(s, l, t):\n",
            "        return [func(tokn, *args) for tokn in t]\n",
            "\n",
            "    try:\n",
            "        func_name = getattr(func, '__name__',\n",
            "                            getattr(func, '__class__').__name__)\n",
            "    except Exception:\n",
            "        func_name = str(func)\n",
            "    pa.__name__ = func_name\n",
            "\n",
            "    return pa\n",
            "\n",
            "upcaseTokens = tokenMap(lambda t: _ustr(t).upper())\n",
            "\"\"\"(Deprecated) Helper parse action to convert tokens to upper case.\n",
            "Deprecated in favor of :class:`pyparsing_common.upcaseTokens`\"\"\"\n",
            "\n",
            "downcaseTokens = tokenMap(lambda t: _ustr(t).lower())\n",
            "\"\"\"(Deprecated) Helper parse action to convert tokens to lower case.\n",
            "Deprecated in favor of :class:`pyparsing_common.downcaseTokens`\"\"\"\n",
            "\n",
            "def _makeTags(tagStr, xml,\n",
            "              suppress_LT=Suppress(\"<\"),\n",
            "              suppress_GT=Suppress(\">\")):\n",
            "    \"\"\"Internal helper to construct opening and closing tag expressions, given a tag name\"\"\"\n",
            "    if isinstance(tagStr, basestring):\n",
            "        resname = tagStr\n",
            "        tagStr = Keyword(tagStr, caseless=not xml)\n",
            "    else:\n",
            "        resname = tagStr.name\n",
            "\n",
            "    tagAttrName = Word(alphas, alphanums + \"_-:\")\n",
            "    if xml:\n",
            "        tagAttrValue = dblQuotedString.copy().setParseAction(removeQuotes)\n",
            "        openTag = (suppress_LT\n",
            "                   + tagStr(\"tag\")\n",
            "                   + Dict(ZeroOrMore(Group(tagAttrName + Suppress(\"=\") + tagAttrValue)))\n",
            "                   + Optional(\"/\", default=[False])(\"empty\").setParseAction(lambda s, l, t: t[0] == '/')\n",
            "                   + suppress_GT)\n",
            "    else:\n",
            "        tagAttrValue = quotedString.copy().setParseAction(removeQuotes) | Word(printables, excludeChars=\">\")\n",
            "        openTag = (suppress_LT\n",
            "                   + tagStr(\"tag\")\n",
            "                   + Dict(ZeroOrMore(Group(tagAttrName.setParseAction(downcaseTokens)\n",
            "                                           + Optional(Suppress(\"=\") + tagAttrValue))))\n",
            "                   + Optional(\"/\", default=[False])(\"empty\").setParseAction(lambda s, l, t: t[0] == '/')\n",
            "                   + suppress_GT)\n",
            "    closeTag = Combine(_L(\"</\") + tagStr + \">\", adjacent=False)\n",
            "\n",
            "    openTag.setName(\"<%s>\" % resname)\n",
            "    # add start<tagname> results name in parse action now that ungrouped names are not reported at two levels\n",
            "    openTag.addParseAction(lambda t: t.__setitem__(\"start\" + \"\".join(resname.replace(\":\", \" \").title().split()), t.copy()))\n",
            "    closeTag = closeTag(\"end\" + \"\".join(resname.replace(\":\", \" \").title().split())).setName(\"</%s>\" % resname)\n",
            "    openTag.tag = resname\n",
            "    closeTag.tag = resname\n",
            "    openTag.tag_body = SkipTo(closeTag())\n",
            "    return openTag, closeTag\n",
            "\n",
            "def makeHTMLTags(tagStr):\n",
            "    \"\"\"Helper to construct opening and closing tag expressions for HTML,\n",
            "    given a tag name. Matches tags in either upper or lower case,\n",
            "    attributes with namespaces and with quoted or unquoted values.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        text = '<td>More info at the <a href=\"https://github.com/pyparsing/pyparsing/wiki\">pyparsing</a> wiki page</td>'\n",
            "        # makeHTMLTags returns pyparsing expressions for the opening and\n",
            "        # closing tags as a 2-tuple\n",
            "        a, a_end = makeHTMLTags(\"A\")\n",
            "        link_expr = a + SkipTo(a_end)(\"link_text\") + a_end\n",
            "\n",
            "        for link in link_expr.searchString(text):\n",
            "            # attributes in the <A> tag (like \"href\" shown here) are\n",
            "            # also accessible as named results\n",
            "            print(link.link_text, '->', link.href)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        pyparsing -> https://github.com/pyparsing/pyparsing/wiki\n",
            "    \"\"\"\n",
            "    return _makeTags(tagStr, False)\n",
            "\n",
            "def makeXMLTags(tagStr):\n",
            "    \"\"\"Helper to construct opening and closing tag expressions for XML,\n",
            "    given a tag name. Matches tags only in the given upper/lower case.\n",
            "\n",
            "    Example: similar to :class:`makeHTMLTags`\n",
            "    \"\"\"\n",
            "    return _makeTags(tagStr, True)\n",
            "\n",
            "def withAttribute(*args, **attrDict):\n",
            "    \"\"\"Helper to create a validating parse action to be used with start\n",
            "    tags created with :class:`makeXMLTags` or\n",
            "    :class:`makeHTMLTags`. Use ``withAttribute`` to qualify\n",
            "    a starting tag with a required attribute value, to avoid false\n",
            "    matches on common tags such as ``<TD>`` or ``<DIV>``.\n",
            "\n",
            "    Call ``withAttribute`` with a series of attribute names and\n",
            "    values. Specify the list of filter attributes names and values as:\n",
            "\n",
            "     - keyword arguments, as in ``(align=\"right\")``, or\n",
            "     - as an explicit dict with ``**`` operator, when an attribute\n",
            "       name is also a Python reserved word, as in ``**{\"class\":\"Customer\", \"align\":\"right\"}``\n",
            "     - a list of name-value tuples, as in ``((\"ns1:class\", \"Customer\"), (\"ns2:align\", \"right\"))``\n",
            "\n",
            "    For attribute names with a namespace prefix, you must use the second\n",
            "    form.  Attribute names are matched insensitive to upper/lower case.\n",
            "\n",
            "    If just testing for ``class`` (with or without a namespace), use\n",
            "    :class:`withClass`.\n",
            "\n",
            "    To verify that the attribute exists, but without specifying a value,\n",
            "    pass ``withAttribute.ANY_VALUE`` as the value.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        html = '''\n",
            "            <div>\n",
            "            Some text\n",
            "            <div type=\"grid\">1 4 0 1 0</div>\n",
            "            <div type=\"graph\">1,3 2,3 1,1</div>\n",
            "            <div>this has no type</div>\n",
            "            </div>\n",
            "\n",
            "        '''\n",
            "        div,div_end = makeHTMLTags(\"div\")\n",
            "\n",
            "        # only match div tag having a type attribute with value \"grid\"\n",
            "        div_grid = div().setParseAction(withAttribute(type=\"grid\"))\n",
            "        grid_expr = div_grid + SkipTo(div | div_end)(\"body\")\n",
            "        for grid_header in grid_expr.searchString(html):\n",
            "            print(grid_header.body)\n",
            "\n",
            "        # construct a match with any div tag having a type attribute, regardless of the value\n",
            "        div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))\n",
            "        div_expr = div_any_type + SkipTo(div | div_end)(\"body\")\n",
            "        for div_header in div_expr.searchString(html):\n",
            "            print(div_header.body)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        1 4 0 1 0\n",
            "\n",
            "        1 4 0 1 0\n",
            "        1,3 2,3 1,1\n",
            "    \"\"\"\n",
            "    if args:\n",
            "        attrs = args[:]\n",
            "    else:\n",
            "        attrs = attrDict.items()\n",
            "    attrs = [(k, v) for k, v in attrs]\n",
            "    def pa(s, l, tokens):\n",
            "        for attrName, attrValue in attrs:\n",
            "            if attrName not in tokens:\n",
            "                raise ParseException(s, l, \"no matching attribute \" + attrName)\n",
            "            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:\n",
            "                raise ParseException(s, l, \"attribute '%s' has value '%s', must be '%s'\" %\n",
            "                                            (attrName, tokens[attrName], attrValue))\n",
            "    return pa\n",
            "withAttribute.ANY_VALUE = object()\n",
            "\n",
            "def withClass(classname, namespace=''):\n",
            "    \"\"\"Simplified version of :class:`withAttribute` when\n",
            "    matching on a div class - made difficult because ``class`` is\n",
            "    a reserved word in Python.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        html = '''\n",
            "            <div>\n",
            "            Some text\n",
            "            <div class=\"grid\">1 4 0 1 0</div>\n",
            "            <div class=\"graph\">1,3 2,3 1,1</div>\n",
            "            <div>this &lt;div&gt; has no class</div>\n",
            "            </div>\n",
            "\n",
            "        '''\n",
            "        div,div_end = makeHTMLTags(\"div\")\n",
            "        div_grid = div().setParseAction(withClass(\"grid\"))\n",
            "\n",
            "        grid_expr = div_grid + SkipTo(div | div_end)(\"body\")\n",
            "        for grid_header in grid_expr.searchString(html):\n",
            "            print(grid_header.body)\n",
            "\n",
            "        div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))\n",
            "        div_expr = div_any_type + SkipTo(div | div_end)(\"body\")\n",
            "        for div_header in div_expr.searchString(html):\n",
            "            print(div_header.body)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        1 4 0 1 0\n",
            "\n",
            "        1 4 0 1 0\n",
            "        1,3 2,3 1,1\n",
            "    \"\"\"\n",
            "    classattr = \"%s:class\" % namespace if namespace else \"class\"\n",
            "    return withAttribute(**{classattr: classname})\n",
            "\n",
            "opAssoc = SimpleNamespace()\n",
            "opAssoc.LEFT = object()\n",
            "opAssoc.RIGHT = object()\n",
            "\n",
            "def infixNotation(baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')')):\n",
            "    \"\"\"Helper method for constructing grammars of expressions made up of\n",
            "    operators working in a precedence hierarchy.  Operators may be unary\n",
            "    or binary, left- or right-associative.  Parse actions can also be\n",
            "    attached to operator expressions. The generated parser will also\n",
            "    recognize the use of parentheses to override operator precedences\n",
            "    (see example below).\n",
            "\n",
            "    Note: if you define a deep operator list, you may see performance\n",
            "    issues when using infixNotation. See\n",
            "    :class:`ParserElement.enablePackrat` for a mechanism to potentially\n",
            "    improve your parser performance.\n",
            "\n",
            "    Parameters:\n",
            "     - baseExpr - expression representing the most basic element for the\n",
            "       nested\n",
            "     - opList - list of tuples, one for each operator precedence level\n",
            "       in the expression grammar; each tuple is of the form ``(opExpr,\n",
            "       numTerms, rightLeftAssoc, parseAction)``, where:\n",
            "\n",
            "       - opExpr is the pyparsing expression for the operator; may also\n",
            "         be a string, which will be converted to a Literal; if numTerms\n",
            "         is 3, opExpr is a tuple of two expressions, for the two\n",
            "         operators separating the 3 terms\n",
            "       - numTerms is the number of terms for this operator (must be 1,\n",
            "         2, or 3)\n",
            "       - rightLeftAssoc is the indicator whether the operator is right\n",
            "         or left associative, using the pyparsing-defined constants\n",
            "         ``opAssoc.RIGHT`` and ``opAssoc.LEFT``.\n",
            "       - parseAction is the parse action to be associated with\n",
            "         expressions matching this operator expression (the parse action\n",
            "         tuple member may be omitted); if the parse action is passed\n",
            "         a tuple or list of functions, this is equivalent to calling\n",
            "         ``setParseAction(*fn)``\n",
            "         (:class:`ParserElement.setParseAction`)\n",
            "     - lpar - expression for matching left-parentheses\n",
            "       (default= ``Suppress('(')``)\n",
            "     - rpar - expression for matching right-parentheses\n",
            "       (default= ``Suppress(')')``)\n",
            "\n",
            "    Example::\n",
            "\n",
            "        # simple example of four-function arithmetic with ints and\n",
            "        # variable names\n",
            "        integer = pyparsing_common.signed_integer\n",
            "        varname = pyparsing_common.identifier\n",
            "\n",
            "        arith_expr = infixNotation(integer | varname,\n",
            "            [\n",
            "            ('-', 1, opAssoc.RIGHT),\n",
            "            (oneOf('* /'), 2, opAssoc.LEFT),\n",
            "            (oneOf('+ -'), 2, opAssoc.LEFT),\n",
            "            ])\n",
            "\n",
            "        arith_expr.runTests('''\n",
            "            5+3*6\n",
            "            (5+3)*6\n",
            "            -2--11\n",
            "            ''', fullDump=False)\n",
            "\n",
            "    prints::\n",
            "\n",
            "        5+3*6\n",
            "        [[5, '+', [3, '*', 6]]]\n",
            "\n",
            "        (5+3)*6\n",
            "        [[[5, '+', 3], '*', 6]]\n",
            "\n",
            "        -2--11\n",
            "        [[['-', 2], '-', ['-', 11]]]\n",
            "    \"\"\"\n",
            "    # captive version of FollowedBy that does not do parse actions or capture results names\n",
            "    class _FB(FollowedBy):\n",
            "        def parseImpl(self, instring, loc, doActions=True):\n",
            "            self.expr.tryParse(instring, loc)\n",
            "            return loc, []\n",
            "\n",
            "    ret = Forward()\n",
            "    lastExpr = baseExpr | (lpar + ret + rpar)\n",
            "    for i, operDef in enumerate(opList):\n",
            "        opExpr, arity, rightLeftAssoc, pa = (operDef + (None, ))[:4]\n",
            "        termName = \"%s term\" % opExpr if arity < 3 else \"%s%s term\" % opExpr\n",
            "        if arity == 3:\n",
            "            if opExpr is None or len(opExpr) != 2:\n",
            "                raise ValueError(\n",
            "                    \"if numterms=3, opExpr must be a tuple or list of two expressions\")\n",
            "            opExpr1, opExpr2 = opExpr\n",
            "        thisExpr = Forward().setName(termName)\n",
            "        if rightLeftAssoc == opAssoc.LEFT:\n",
            "            if arity == 1:\n",
            "                matchExpr = _FB(lastExpr + opExpr) + Group(lastExpr + OneOrMore(opExpr))\n",
            "            elif arity == 2:\n",
            "                if opExpr is not None:\n",
            "                    matchExpr = _FB(lastExpr + opExpr + lastExpr) + Group(lastExpr + OneOrMore(opExpr + lastExpr))\n",
            "                else:\n",
            "                    matchExpr = _FB(lastExpr + lastExpr) + Group(lastExpr + OneOrMore(lastExpr))\n",
            "            elif arity == 3:\n",
            "                matchExpr = (_FB(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr)\n",
            "                             + Group(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr))\n",
            "            else:\n",
            "                raise ValueError(\"operator must be unary (1), binary (2), or ternary (3)\")\n",
            "        elif rightLeftAssoc == opAssoc.RIGHT:\n",
            "            if arity == 1:\n",
            "                # try to avoid LR with this extra test\n",
            "                if not isinstance(opExpr, Optional):\n",
            "                    opExpr = Optional(opExpr)\n",
            "                matchExpr = _FB(opExpr.expr + thisExpr) + Group(opExpr + thisExpr)\n",
            "            elif arity == 2:\n",
            "                if opExpr is not None:\n",
            "                    matchExpr = _FB(lastExpr + opExpr + thisExpr) + Group(lastExpr + OneOrMore(opExpr + thisExpr))\n",
            "                else:\n",
            "                    matchExpr = _FB(lastExpr + thisExpr) + Group(lastExpr + OneOrMore(thisExpr))\n",
            "            elif arity == 3:\n",
            "                matchExpr = (_FB(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr)\n",
            "                             + Group(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr))\n",
            "            else:\n",
            "                raise ValueError(\"operator must be unary (1), binary (2), or ternary (3)\")\n",
            "        else:\n",
            "            raise ValueError(\"operator must indicate right or left associativity\")\n",
            "        if pa:\n",
            "            if isinstance(pa, (tuple, list)):\n",
            "                matchExpr.setParseAction(*pa)\n",
            "            else:\n",
            "                matchExpr.setParseAction(pa)\n",
            "        thisExpr <<= (matchExpr.setName(termName) | lastExpr)\n",
            "        lastExpr = thisExpr\n",
            "    ret <<= lastExpr\n",
            "    return ret\n",
            "\n",
            "operatorPrecedence = infixNotation\n",
            "\"\"\"(Deprecated) Former name of :class:`infixNotation`, will be\n",
            "dropped in a future release.\"\"\"\n",
            "\n",
            "dblQuotedString = Combine(Regex(r'\"(?:[^\"\\n\\r\\\\]|(?:\"\")|(?:\\\\(?:[^x]|x[0-9a-fA-F]+)))*') + '\"').setName(\"string enclosed in double quotes\")\n",
            "sglQuotedString = Combine(Regex(r\"'(?:[^'\\n\\r\\\\]|(?:'')|(?:\\\\(?:[^x]|x[0-9a-fA-F]+)))*\") + \"'\").setName(\"string enclosed in single quotes\")\n",
            "quotedString = Combine(Regex(r'\"(?:[^\"\\n\\r\\\\]|(?:\"\")|(?:\\\\(?:[^x]|x[0-9a-fA-F]+)))*') + '\"'\n",
            "                       | Regex(r\"'(?:[^'\\n\\r\\\\]|(?:'')|(?:\\\\(?:[^x]|x[0-9a-fA-F]+)))*\") + \"'\").setName(\"quotedString using single or double quotes\")\n",
            "unicodeString = Combine(_L('u') + quotedString.copy()).setName(\"unicode string literal\")\n",
            "\n",
            "def nestedExpr(opener=\"(\", closer=\")\", content=None, ignoreExpr=quotedString.copy()):\n",
            "    \"\"\"Helper method for defining nested lists enclosed in opening and\n",
            "    closing delimiters (\"(\" and \")\" are the default).\n",
            "\n",
            "    Parameters:\n",
            "     - opener - opening character for a nested list\n",
            "       (default= ``\"(\"``); can also be a pyparsing expression\n",
            "     - closer - closing character for a nested list\n",
            "       (default= ``\")\"``); can also be a pyparsing expression\n",
            "     - content - expression for items within the nested lists\n",
            "       (default= ``None``)\n",
            "     - ignoreExpr - expression for ignoring opening and closing\n",
            "       delimiters (default= :class:`quotedString`)\n",
            "\n",
            "    If an expression is not provided for the content argument, the\n",
            "    nested expression will capture all whitespace-delimited content\n",
            "    between delimiters as a list of separate values.\n",
            "\n",
            "    Use the ``ignoreExpr`` argument to define expressions that may\n",
            "    contain opening or closing characters that should not be treated as\n",
            "    opening or closing characters for nesting, such as quotedString or\n",
            "    a comment expression.  Specify multiple expressions using an\n",
            "    :class:`Or` or :class:`MatchFirst`. The default is\n",
            "    :class:`quotedString`, but if no expressions are to be ignored, then\n",
            "    pass ``None`` for this argument.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        data_type = oneOf(\"void int short long char float double\")\n",
            "        decl_data_type = Combine(data_type + Optional(Word('*')))\n",
            "        ident = Word(alphas+'_', alphanums+'_')\n",
            "        number = pyparsing_common.number\n",
            "        arg = Group(decl_data_type + ident)\n",
            "        LPAR, RPAR = map(Suppress, \"()\")\n",
            "\n",
            "        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))\n",
            "\n",
            "        c_function = (decl_data_type(\"type\")\n",
            "                      + ident(\"name\")\n",
            "                      + LPAR + Optional(delimitedList(arg), [])(\"args\") + RPAR\n",
            "                      + code_body(\"body\"))\n",
            "        c_function.ignore(cStyleComment)\n",
            "\n",
            "        source_code = '''\n",
            "            int is_odd(int x) {\n",
            "                return (x%2);\n",
            "            }\n",
            "\n",
            "            int dec_to_hex(char hchar) {\n",
            "                if (hchar >= '0' && hchar <= '9') {\n",
            "                    return (ord(hchar)-ord('0'));\n",
            "                } else {\n",
            "                    return (10+ord(hchar)-ord('A'));\n",
            "                }\n",
            "            }\n",
            "        '''\n",
            "        for func in c_function.searchString(source_code):\n",
            "            print(\"%(name)s (%(type)s) args: %(args)s\" % func)\n",
            "\n",
            "\n",
            "    prints::\n",
            "\n",
            "        is_odd (int) args: [['int', 'x']]\n",
            "        dec_to_hex (int) args: [['char', 'hchar']]\n",
            "    \"\"\"\n",
            "    if opener == closer:\n",
            "        raise ValueError(\"opening and closing strings cannot be the same\")\n",
            "    if content is None:\n",
            "        if isinstance(opener, basestring) and isinstance(closer, basestring):\n",
            "            if len(opener) == 1 and len(closer) == 1:\n",
            "                if ignoreExpr is not None:\n",
            "                    content = (Combine(OneOrMore(~ignoreExpr\n",
            "                                                 + CharsNotIn(opener\n",
            "                                                              + closer\n",
            "                                                              + ParserElement.DEFAULT_WHITE_CHARS, exact=1)\n",
            "                                                 )\n",
            "                                       ).setParseAction(lambda t: t[0].strip()))\n",
            "                else:\n",
            "                    content = (empty.copy() + CharsNotIn(opener\n",
            "                                                         + closer\n",
            "                                                         + ParserElement.DEFAULT_WHITE_CHARS\n",
            "                                                         ).setParseAction(lambda t: t[0].strip()))\n",
            "            else:\n",
            "                if ignoreExpr is not None:\n",
            "                    content = (Combine(OneOrMore(~ignoreExpr\n",
            "                                                 + ~Literal(opener)\n",
            "                                                 + ~Literal(closer)\n",
            "                                                 + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))\n",
            "                                       ).setParseAction(lambda t: t[0].strip()))\n",
            "                else:\n",
            "                    content = (Combine(OneOrMore(~Literal(opener)\n",
            "                                                 + ~Literal(closer)\n",
            "                                                 + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1))\n",
            "                                       ).setParseAction(lambda t: t[0].strip()))\n",
            "        else:\n",
            "            raise ValueError(\"opening and closing arguments must be strings if no content expression is given\")\n",
            "    ret = Forward()\n",
            "    if ignoreExpr is not None:\n",
            "        ret <<= Group(Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer))\n",
            "    else:\n",
            "        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content)  + Suppress(closer))\n",
            "    ret.setName('nested %s%s expression' % (opener, closer))\n",
            "    return ret\n",
            "\n",
            "def indentedBlock(blockStatementExpr, indentStack, indent=True):\n",
            "    \"\"\"Helper method for defining space-delimited indentation blocks,\n",
            "    such as those used to define block statements in Python source code.\n",
            "\n",
            "    Parameters:\n",
            "\n",
            "     - blockStatementExpr - expression defining syntax of statement that\n",
            "       is repeated within the indented block\n",
            "     - indentStack - list created by caller to manage indentation stack\n",
            "       (multiple statementWithIndentedBlock expressions within a single\n",
            "       grammar should share a common indentStack)\n",
            "     - indent - boolean indicating whether block must be indented beyond\n",
            "       the current level; set to False for block of left-most\n",
            "       statements (default= ``True``)\n",
            "\n",
            "    A valid block must contain at least one ``blockStatement``.\n",
            "\n",
            "    Example::\n",
            "\n",
            "        data = '''\n",
            "        def A(z):\n",
            "          A1\n",
            "          B = 100\n",
            "          G = A2\n",
            "          A2\n",
            "          A3\n",
            "        B\n",
            "        def BB(a,b,c):\n",
            "          BB1\n",
            "          def BBA():\n",
            "            bba1\n",
            "            bba2\n",
            "            bba3\n",
            "        C\n",
            "        D\n",
            "        def spam(x,y):\n",
            "             def eggs(z):\n",
            "                 pass\n",
            "        '''\n",
            "\n",
            "\n",
            "        indentStack = [1]\n",
            "        stmt = Forward()\n",
            "\n",
            "        identifier = Word(alphas, alphanums)\n",
            "        funcDecl = (\"def\" + identifier + Group(\"(\" + Optional(delimitedList(identifier)) + \")\") + \":\")\n",
            "        func_body = indentedBlock(stmt, indentStack)\n",
            "        funcDef = Group(funcDecl + func_body)\n",
            "\n",
            "        rvalue = Forward()\n",
            "        funcCall = Group(identifier + \"(\" + Optional(delimitedList(rvalue)) + \")\")\n",
            "        rvalue << (funcCall | identifier | Word(nums))\n",
            "        assignment = Group(identifier + \"=\" + rvalue)\n",
            "        stmt << (funcDef | assignment | identifier)\n",
            "\n",
            "        module_body = OneOrMore(stmt)\n",
            "\n",
            "        parseTree = module_body.parseString(data)\n",
            "        parseTree.pprint()\n",
            "\n",
            "    prints::\n",
            "\n",
            "        [['def',\n",
            "          'A',\n",
            "          ['(', 'z', ')'],\n",
            "          ':',\n",
            "          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],\n",
            "         'B',\n",
            "         ['def',\n",
            "          'BB',\n",
            "          ['(', 'a', 'b', 'c', ')'],\n",
            "          ':',\n",
            "          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],\n",
            "         'C',\n",
            "         'D',\n",
            "         ['def',\n",
            "          'spam',\n",
            "          ['(', 'x', 'y', ')'],\n",
            "          ':',\n",
            "          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]]\n",
            "    \"\"\"\n",
            "    backup_stack = indentStack[:]\n",
            "\n",
            "    def reset_stack():\n",
            "        indentStack[:] = backup_stack\n",
            "\n",
            "    def checkPeerIndent(s, l, t):\n",
            "        if l >= len(s): return\n",
            "        curCol = col(l, s)\n",
            "        if curCol != indentStack[-1]:\n",
            "            if curCol > indentStack[-1]:\n",
            "                raise ParseException(s, l, \"illegal nesting\")\n",
            "            raise ParseException(s, l, \"not a peer entry\")\n",
            "\n",
            "    def checkSubIndent(s, l, t):\n",
            "        curCol = col(l, s)\n",
            "        if curCol > indentStack[-1]:\n",
            "            indentStack.append(curCol)\n",
            "        else:\n",
            "            raise ParseException(s, l, \"not a subentry\")\n",
            "\n",
            "    def checkUnindent(s, l, t):\n",
            "        if l >= len(s): return\n",
            "        curCol = col(l, s)\n",
            "        if not(indentStack and curCol in indentStack):\n",
            "            raise ParseException(s, l, \"not an unindent\")\n",
            "        if curCol < indentStack[-1]:\n",
            "            indentStack.pop()\n",
            "\n",
            "    NL = OneOrMore(LineEnd().setWhitespaceChars(\"\\t \").suppress())\n",
            "    INDENT = (Empty() + Empty().setParseAction(checkSubIndent)).setName('INDENT')\n",
            "    PEER   = Empty().setParseAction(checkPeerIndent).setName('')\n",
            "    UNDENT = Empty().setParseAction(checkUnindent).setName('UNINDENT')\n",
            "    if indent:\n",
            "        smExpr = Group(Optional(NL)\n",
            "                       + INDENT\n",
            "                       + OneOrMore(PEER + Group(blockStatementExpr) + Optional(NL))\n",
            "                       + UNDENT)\n",
            "    else:\n",
            "        smExpr = Group(Optional(NL)\n",
            "                       + OneOrMore(PEER + Group(blockStatementExpr) + Optional(NL))\n",
            "                       + UNDENT)\n",
            "    smExpr.setFailAction(lambda a, b, c, d: reset_stack())\n",
            "    blockStatementExpr.ignore(_bslash + LineEnd())\n",
            "    return smExpr.setName('indented block')\n",
            "\n",
            "alphas8bit = srange(r\"[\\0xc0-\\0xd6\\0xd8-\\0xf6\\0xf8-\\0xff]\")\n",
            "punc8bit = srange(r\"[\\0xa1-\\0xbf\\0xd7\\0xf7]\")\n",
            "\n",
            "anyOpenTag, anyCloseTag = makeHTMLTags(Word(alphas, alphanums + \"_:\").setName('any tag'))\n",
            "_htmlEntityMap = dict(zip(\"gt lt amp nbsp quot apos\".split(), '><& \"\\''))\n",
            "commonHTMLEntity = Regex('&(?P<entity>' + '|'.join(_htmlEntityMap.keys()) +\");\").setName(\"common HTML entity\")\n",
            "def replaceHTMLEntity(t):\n",
            "    \"\"\"Helper parser action to replace common HTML entities with their special characters\"\"\"\n",
            "    return _htmlEntityMap.get(t.entity)\n",
            "\n",
            "# it's easy to get these comment structures wrong - they're very common, so may as well make them available\n",
            "cStyleComment = Combine(Regex(r\"/\\*(?:[^*]|\\*(?!/))*\") + '*/').setName(\"C style comment\")\n",
            "\"Comment of the form ``/* ... */``\"\n",
            "\n",
            "htmlComment = Regex(r\"<!--[\\s\\S]*?-->\").setName(\"HTML comment\")\n",
            "\"Comment of the form ``<!-- ... -->``\"\n",
            "\n",
            "restOfLine = Regex(r\".*\").leaveWhitespace().setName(\"rest of line\")\n",
            "dblSlashComment = Regex(r\"//(?:\\\\\\n|[^\\n])*\").setName(\"// comment\")\n",
            "\"Comment of the form ``// ... (to end of line)``\"\n",
            "\n",
            "cppStyleComment = Combine(Regex(r\"/\\*(?:[^*]|\\*(?!/))*\") + '*/' | dblSlashComment).setName(\"C++ style comment\")\n",
            "\"Comment of either form :class:`cStyleComment` or :class:`dblSlashComment`\"\n",
            "\n",
            "javaStyleComment = cppStyleComment\n",
            "\"Same as :class:`cppStyleComment`\"\n",
            "\n",
            "pythonStyleComment = Regex(r\"#.*\").setName(\"Python style comment\")\n",
            "\"Comment of the form ``# ... (to end of line)``\"\n",
            "\n",
            "_commasepitem = Combine(OneOrMore(Word(printables, excludeChars=',')\n",
            "                                  + Optional(Word(\" \\t\")\n",
            "                                             + ~Literal(\",\") + ~LineEnd()))).streamline().setName(\"commaItem\")\n",
            "commaSeparatedList = delimitedList(Optional(quotedString.copy() | _commasepitem, default=\"\")).setName(\"commaSeparatedList\")\n",
            "\"\"\"(Deprecated) Predefined expression of 1 or more printable words or\n",
            "quoted strings, separated by commas.\n",
            "\n",
            "This expression is deprecated in favor of :class:`pyparsing_common.comma_separated_list`.\n",
            "\"\"\"\n",
            "\n",
            "# some other useful expressions - using lower-case class name since we are really using this as a namespace\n",
            "class pyparsing_common:\n",
            "    \"\"\"Here are some common low-level expressions that may be useful in\n",
            "    jump-starting parser development:\n",
            "\n",
            "     - numeric forms (:class:`integers<integer>`, :class:`reals<real>`,\n",
            "       :class:`scientific notation<sci_real>`)\n",
            "     - common :class:`programming identifiers<identifier>`\n",
            "     - network addresses (:class:`MAC<mac_address>`,\n",
            "       :class:`IPv4<ipv4_address>`, :class:`IPv6<ipv6_address>`)\n",
            "     - ISO8601 :class:`dates<iso8601_date>` and\n",
            "       :class:`datetime<iso8601_datetime>`\n",
            "     - :class:`UUID<uuid>`\n",
            "     - :class:`comma-separated list<comma_separated_list>`\n",
            "\n",
            "    Parse actions:\n",
            "\n",
            "     - :class:`convertToInteger`\n",
            "     - :class:`convertToFloat`\n",
            "     - :class:`convertToDate`\n",
            "     - :class:`convertToDatetime`\n",
            "     - :class:`stripHTMLTags`\n",
            "     - :class:`upcaseTokens`\n",
            "     - :class:`downcaseTokens`\n",
            "\n",
            "    Example::\n",
            "\n",
            "        pyparsing_common.number.runTests('''\n",
            "            # any int or real number, returned as the appropriate type\n",
            "            100\n",
            "            -100\n",
            "            +100\n",
            "            3.14159\n",
            "            6.02e23\n",
            "            1e-12\n",
            "            ''')\n",
            "\n",
            "        pyparsing_common.fnumber.runTests('''\n",
            "            # any int or real number, returned as float\n",
            "            100\n",
            "            -100\n",
            "            +100\n",
            "            3.14159\n",
            "            6.02e23\n",
            "            1e-12\n",
            "            ''')\n",
            "\n",
            "        pyparsing_common.hex_integer.runTests('''\n",
            "            # hex numbers\n",
            "            100\n",
            "            FF\n",
            "            ''')\n",
            "\n",
            "        pyparsing_common.fraction.runTests('''\n",
            "            # fractions\n",
            "            1/2\n",
            "            -3/4\n",
            "            ''')\n",
            "\n",
            "        pyparsing_common.mixed_integer.runTests('''\n",
            "            # mixed fractions\n",
            "            1\n",
            "            1/2\n",
            "            -3/4\n",
            "            1-3/4\n",
            "            ''')\n",
            "\n",
            "        import uuid\n",
            "        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))\n",
            "        pyparsing_common.uuid.runTests('''\n",
            "            # uuid\n",
            "            12345678-1234-5678-1234-567812345678\n",
            "            ''')\n",
            "\n",
            "    prints::\n",
            "\n",
            "        # any int or real number, returned as the appropriate type\n",
            "        100\n",
            "        [100]\n",
            "\n",
            "        -100\n",
            "        [-100]\n",
            "\n",
            "        +100\n",
            "        [100]\n",
            "\n",
            "        3.14159\n",
            "        [3.14159]\n",
            "\n",
            "        6.02e23\n",
            "        [6.02e+23]\n",
            "\n",
            "        1e-12\n",
            "        [1e-12]\n",
            "\n",
            "        # any int or real number, returned as float\n",
            "        100\n",
            "        [100.0]\n",
            "\n",
            "        -100\n",
            "        [-100.0]\n",
            "\n",
            "        +100\n",
            "        [100.0]\n",
            "\n",
            "        3.14159\n",
            "        [3.14159]\n",
            "\n",
            "        6.02e23\n",
            "        [6.02e+23]\n",
            "\n",
            "        1e-12\n",
            "        [1e-12]\n",
            "\n",
            "        # hex numbers\n",
            "        100\n",
            "        [256]\n",
            "\n",
            "        FF\n",
            "        [255]\n",
            "\n",
            "        # fractions\n",
            "        1/2\n",
            "        [0.5]\n",
            "\n",
            "        -3/4\n",
            "        [-0.75]\n",
            "\n",
            "        # mixed fractions\n",
            "        1\n",
            "        [1]\n",
            "\n",
            "        1/2\n",
            "        [0.5]\n",
            "\n",
            "        -3/4\n",
            "        [-0.75]\n",
            "\n",
            "        1-3/4\n",
            "        [1.75]\n",
            "\n",
            "        # uuid\n",
            "        12345678-1234-5678-1234-567812345678\n",
            "        [UUID('12345678-1234-5678-1234-567812345678')]\n",
            "    \"\"\"\n",
            "\n",
            "    convertToInteger = tokenMap(int)\n",
            "    \"\"\"\n",
            "    Parse action for converting parsed integers to Python int\n",
            "    \"\"\"\n",
            "\n",
            "    convertToFloat = tokenMap(float)\n",
            "    \"\"\"\n",
            "    Parse action for converting parsed numbers to Python float\n",
            "    \"\"\"\n",
            "\n",
            "    integer = Word(nums).setName(\"integer\").setParseAction(convertToInteger)\n",
            "    \"\"\"expression that parses an unsigned integer, returns an int\"\"\"\n",
            "\n",
            "    hex_integer = Word(hexnums).setName(\"hex integer\").setParseAction(tokenMap(int, 16))\n",
            "    \"\"\"expression that parses a hexadecimal integer, returns an int\"\"\"\n",
            "\n",
            "    signed_integer = Regex(r'[+-]?\\d+').setName(\"signed integer\").setParseAction(convertToInteger)\n",
            "    \"\"\"expression that parses an integer with optional leading sign, returns an int\"\"\"\n",
            "\n",
            "    fraction = (signed_integer().setParseAction(convertToFloat) + '/' + signed_integer().setParseAction(convertToFloat)).setName(\"fraction\")\n",
            "    \"\"\"fractional expression of an integer divided by an integer, returns a float\"\"\"\n",
            "    fraction.addParseAction(lambda t: t[0]/t[-1])\n",
            "\n",
            "    mixed_integer = (fraction | signed_integer + Optional(Optional('-').suppress() + fraction)).setName(\"fraction or mixed integer-fraction\")\n",
            "    \"\"\"mixed integer of the form 'integer - fraction', with optional leading integer, returns float\"\"\"\n",
            "    mixed_integer.addParseAction(sum)\n",
            "\n",
            "    real = Regex(r'[+-]?(:?\\d+\\.\\d*|\\.\\d+)').setName(\"real number\").setParseAction(convertToFloat)\n",
            "    \"\"\"expression that parses a floating point number and returns a float\"\"\"\n",
            "\n",
            "    sci_real = Regex(r'[+-]?(:?\\d+(:?[eE][+-]?\\d+)|(:?\\d+\\.\\d*|\\.\\d+)(:?[eE][+-]?\\d+)?)').setName(\"real number with scientific notation\").setParseAction(convertToFloat)\n",
            "    \"\"\"expression that parses a floating point number with optional\n",
            "    scientific notation and returns a float\"\"\"\n",
            "\n",
            "    # streamlining this expression makes the docs nicer-looking\n",
            "    number = (sci_real | real | signed_integer).streamline()\n",
            "    \"\"\"any numeric expression, returns the corresponding Python type\"\"\"\n",
            "\n",
            "    fnumber = Regex(r'[+-]?\\d+\\.?\\d*([eE][+-]?\\d+)?').setName(\"fnumber\").setParseAction(convertToFloat)\n",
            "    \"\"\"any int or real number, returned as float\"\"\"\n",
            "\n",
            "    identifier = Word(alphas + '_', alphanums + '_').setName(\"identifier\")\n",
            "    \"\"\"typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')\"\"\"\n",
            "\n",
            "    ipv4_address = Regex(r'(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}').setName(\"IPv4 address\")\n",
            "    \"IPv4 address (``0.0.0.0 - 255.255.255.255``)\"\n",
            "\n",
            "    _ipv6_part = Regex(r'[0-9a-fA-F]{1,4}').setName(\"hex_integer\")\n",
            "    _full_ipv6_address = (_ipv6_part + (':' + _ipv6_part) * 7).setName(\"full IPv6 address\")\n",
            "    _short_ipv6_address = (Optional(_ipv6_part + (':' + _ipv6_part) * (0, 6))\n",
            "                           + \"::\"\n",
            "                           + Optional(_ipv6_part + (':' + _ipv6_part) * (0, 6))\n",
            "                           ).setName(\"short IPv6 address\")\n",
            "    _short_ipv6_address.addCondition(lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8)\n",
            "    _mixed_ipv6_address = (\"::ffff:\" + ipv4_address).setName(\"mixed IPv6 address\")\n",
            "    ipv6_address = Combine((_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).setName(\"IPv6 address\")).setName(\"IPv6 address\")\n",
            "    \"IPv6 address (long, short, or mixed form)\"\n",
            "\n",
            "    mac_address = Regex(r'[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\\1[0-9a-fA-F]{2}){4}').setName(\"MAC address\")\n",
            "    \"MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)\"\n",
            "\n",
            "    @staticmethod\n",
            "    def convertToDate(fmt=\"%Y-%m-%d\"):\n",
            "        \"\"\"\n",
            "        Helper to create a parse action for converting parsed date string to Python datetime.date\n",
            "\n",
            "        Params -\n",
            "         - fmt - format to be passed to datetime.strptime (default= ``\"%Y-%m-%d\"``)\n",
            "\n",
            "        Example::\n",
            "\n",
            "            date_expr = pyparsing_common.iso8601_date.copy()\n",
            "            date_expr.setParseAction(pyparsing_common.convertToDate())\n",
            "            print(date_expr.parseString(\"1999-12-31\"))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            [datetime.date(1999, 12, 31)]\n",
            "        \"\"\"\n",
            "        def cvt_fn(s, l, t):\n",
            "            try:\n",
            "                return datetime.strptime(t[0], fmt).date()\n",
            "            except ValueError as ve:\n",
            "                raise ParseException(s, l, str(ve))\n",
            "        return cvt_fn\n",
            "\n",
            "    @staticmethod\n",
            "    def convertToDatetime(fmt=\"%Y-%m-%dT%H:%M:%S.%f\"):\n",
            "        \"\"\"Helper to create a parse action for converting parsed\n",
            "        datetime string to Python datetime.datetime\n",
            "\n",
            "        Params -\n",
            "         - fmt - format to be passed to datetime.strptime (default= ``\"%Y-%m-%dT%H:%M:%S.%f\"``)\n",
            "\n",
            "        Example::\n",
            "\n",
            "            dt_expr = pyparsing_common.iso8601_datetime.copy()\n",
            "            dt_expr.setParseAction(pyparsing_common.convertToDatetime())\n",
            "            print(dt_expr.parseString(\"1999-12-31T23:59:59.999\"))\n",
            "\n",
            "        prints::\n",
            "\n",
            "            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]\n",
            "        \"\"\"\n",
            "        def cvt_fn(s, l, t):\n",
            "            try:\n",
            "                return datetime.strptime(t[0], fmt)\n",
            "            except ValueError as ve:\n",
            "                raise ParseException(s, l, str(ve))\n",
            "        return cvt_fn\n",
            "\n",
            "    iso8601_date = Regex(r'(?P<year>\\d{4})(?:-(?P<month>\\d\\d)(?:-(?P<day>\\d\\d))?)?').setName(\"ISO8601 date\")\n",
            "    \"ISO8601 date (``yyyy-mm-dd``)\"\n",
            "\n",
            "    iso8601_datetime = Regex(r'(?P<year>\\d{4})-(?P<month>\\d\\d)-(?P<day>\\d\\d)[T ](?P<hour>\\d\\d):(?P<minute>\\d\\d)(:(?P<second>\\d\\d(\\.\\d*)?)?)?(?P<tz>Z|[+-]\\d\\d:?\\d\\d)?').setName(\"ISO8601 datetime\")\n",
            "    \"ISO8601 datetime (``yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)``) - trailing seconds, milliseconds, and timezone optional; accepts separating ``'T'`` or ``' '``\"\n",
            "\n",
            "    uuid = Regex(r'[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}').setName(\"UUID\")\n",
            "    \"UUID (``xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx``)\"\n",
            "\n",
            "    _html_stripper = anyOpenTag.suppress() | anyCloseTag.suppress()\n",
            "    @staticmethod\n",
            "    def stripHTMLTags(s, l, tokens):\n",
            "        \"\"\"Parse action to remove HTML tags from web page HTML source\n",
            "\n",
            "        Example::\n",
            "\n",
            "            # strip HTML links from normal text\n",
            "            text = '<td>More info at the <a href=\"https://github.com/pyparsing/pyparsing/wiki\">pyparsing</a> wiki page</td>'\n",
            "            td, td_end = makeHTMLTags(\"TD\")\n",
            "            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)(\"body\") + td_end\n",
            "            print(table_text.parseString(text).body)\n",
            "\n",
            "        Prints::\n",
            "\n",
            "            More info at the pyparsing wiki page\n",
            "        \"\"\"\n",
            "        return pyparsing_common._html_stripper.transformString(tokens[0])\n",
            "\n",
            "    _commasepitem = Combine(OneOrMore(~Literal(\",\")\n",
            "                                      + ~LineEnd()\n",
            "                                      + Word(printables, excludeChars=',')\n",
            "                                      + Optional(White(\" \\t\")))).streamline().setName(\"commaItem\")\n",
            "    comma_separated_list = delimitedList(Optional(quotedString.copy()\n",
            "                                                  | _commasepitem, default='')\n",
            "                                         ).setName(\"comma separated list\")\n",
            "    \"\"\"Predefined expression of 1 or more printable words or quoted strings, separated by commas.\"\"\"\n",
            "\n",
            "    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))\n",
            "    \"\"\"Parse action to convert tokens to upper case.\"\"\"\n",
            "\n",
            "    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))\n",
            "    \"\"\"Parse action to convert tokens to lower case.\"\"\"\n",
            "\n",
            "\n",
            "class _lazyclassproperty(object):\n",
            "    def __init__(self, fn):\n",
            "        self.fn = fn\n",
            "        self.__doc__ = fn.__doc__\n",
            "        self.__name__ = fn.__name__\n",
            "\n",
            "    def __get__(self, obj, cls):\n",
            "        if cls is None:\n",
            "            cls = type(obj)\n",
            "        if not hasattr(cls, '_intern') or any(cls._intern is getattr(superclass, '_intern', [])\n",
            "                                              for superclass in cls.__mro__[1:]):\n",
            "            cls._intern = {}\n",
            "        attrname = self.fn.__name__\n",
            "        if attrname not in cls._intern:\n",
            "            cls._intern[attrname] = self.fn(cls)\n",
            "        return cls._intern[attrname]\n",
            "\n",
            "\n",
            "class unicode_set(object):\n",
            "    \"\"\"\n",
            "    A set of Unicode characters, for language-specific strings for\n",
            "    ``alphas``, ``nums``, ``alphanums``, and ``printables``.\n",
            "    A unicode_set is defined by a list of ranges in the Unicode character\n",
            "    set, in a class attribute ``_ranges``, such as::\n",
            "\n",
            "        _ranges = [(0x0020, 0x007e), (0x00a0, 0x00ff),]\n",
            "\n",
            "    A unicode set can also be defined using multiple inheritance of other unicode sets::\n",
            "\n",
            "        class CJK(Chinese, Japanese, Korean):\n",
            "            pass\n",
            "    \"\"\"\n",
            "    _ranges = []\n",
            "\n",
            "    @classmethod\n",
            "    def _get_chars_for_ranges(cls):\n",
            "        ret = []\n",
            "        for cc in cls.__mro__:\n",
            "            if cc is unicode_set:\n",
            "                break\n",
            "            for rr in cc._ranges:\n",
            "                ret.extend(range(rr[0], rr[-1] + 1))\n",
            "        return [unichr(c) for c in sorted(set(ret))]\n",
            "\n",
            "    @_lazyclassproperty\n",
            "    def printables(cls):\n",
            "        \"all non-whitespace characters in this range\"\n",
            "        return u''.join(filterfalse(unicode.isspace, cls._get_chars_for_ranges()))\n",
            "\n",
            "    @_lazyclassproperty\n",
            "    def alphas(cls):\n",
            "        \"all alphabetic characters in this range\"\n",
            "        return u''.join(filter(unicode.isalpha, cls._get_chars_for_ranges()))\n",
            "\n",
            "    @_lazyclassproperty\n",
            "    def nums(cls):\n",
            "        \"all numeric digit characters in this range\"\n",
            "        return u''.join(filter(unicode.isdigit, cls._get_chars_for_ranges()))\n",
            "\n",
            "    @_lazyclassproperty\n",
            "    def alphanums(cls):\n",
            "        \"all alphanumeric characters in this range\"\n",
            "        return cls.alphas + cls.nums\n",
            "\n",
            "\n",
            "class pyparsing_unicode(unicode_set):\n",
            "    \"\"\"\n",
            "    A namespace class for defining common language unicode_sets.\n",
            "    \"\"\"\n",
            "    _ranges = [(32, sys.maxunicode)]\n",
            "\n",
            "    class Latin1(unicode_set):\n",
            "        \"Unicode set for Latin-1 Unicode Character Range\"\n",
            "        _ranges = [(0x0020, 0x007e), (0x00a0, 0x00ff),]\n",
            "\n",
            "    class LatinA(unicode_set):\n",
            "        \"Unicode set for Latin-A Unicode Character Range\"\n",
            "        _ranges = [(0x0100, 0x017f),]\n",
            "\n",
            "    class LatinB(unicode_set):\n",
            "        \"Unicode set for Latin-B Unicode Character Range\"\n",
            "        _ranges = [(0x0180, 0x024f),]\n",
            "\n",
            "    class Greek(unicode_set):\n",
            "        \"Unicode set for Greek Unicode Character Ranges\"\n",
            "        _ranges = [\n",
            "            (0x0370, 0x03ff), (0x1f00, 0x1f15), (0x1f18, 0x1f1d), (0x1f20, 0x1f45), (0x1f48, 0x1f4d),\n",
            "            (0x1f50, 0x1f57), (0x1f59,), (0x1f5b,), (0x1f5d,), (0x1f5f, 0x1f7d), (0x1f80, 0x1fb4), (0x1fb6, 0x1fc4),\n",
            "            (0x1fc6, 0x1fd3), (0x1fd6, 0x1fdb), (0x1fdd, 0x1fef), (0x1ff2, 0x1ff4), (0x1ff6, 0x1ffe),\n",
            "        ]\n",
            "\n",
            "    class Cyrillic(unicode_set):\n",
            "        \"Unicode set for Cyrillic Unicode Character Range\"\n",
            "        _ranges = [(0x0400, 0x04ff)]\n",
            "\n",
            "    class Chinese(unicode_set):\n",
            "        \"Unicode set for Chinese Unicode Character Range\"\n",
            "        _ranges = [(0x4e00, 0x9fff), (0x3000, 0x303f),]\n",
            "\n",
            "    class Japanese(unicode_set):\n",
            "        \"Unicode set for Japanese Unicode Character Range, combining Kanji, Hiragana, and Katakana ranges\"\n",
            "        _ranges = []\n",
            "\n",
            "        class Kanji(unicode_set):\n",
            "            \"Unicode set for Kanji Unicode Character Range\"\n",
            "            _ranges = [(0x4E00, 0x9Fbf), (0x3000, 0x303f),]\n",
            "\n",
            "        class Hiragana(unicode_set):\n",
            "            \"Unicode set for Hiragana Unicode Character Range\"\n",
            "            _ranges = [(0x3040, 0x309f),]\n",
            "\n",
            "        class Katakana(unicode_set):\n",
            "            \"Unicode set for Katakana  Unicode Character Range\"\n",
            "            _ranges = [(0x30a0, 0x30ff),]\n",
            "\n",
            "    class Korean(unicode_set):\n",
            "        \"Unicode set for Korean Unicode Character Range\"\n",
            "        _ranges = [(0xac00, 0xd7af), (0x1100, 0x11ff), (0x3130, 0x318f), (0xa960, 0xa97f), (0xd7b0, 0xd7ff), (0x3000, 0x303f),]\n",
            "\n",
            "    class CJK(Chinese, Japanese, Korean):\n",
            "        \"Unicode set for combined Chinese, Japanese, and Korean (CJK) Unicode Character Range\"\n",
            "        pass\n",
            "\n",
            "    class Thai(unicode_set):\n",
            "        \"Unicode set for Thai Unicode Character Range\"\n",
            "        _ranges = [(0x0e01, 0x0e3a), (0x0e3f, 0x0e5b),]\n",
            "\n",
            "    class Arabic(unicode_set):\n",
            "        \"Unicode set for Arabic Unicode Character Range\"\n",
            "        _ranges = [(0x0600, 0x061b), (0x061e, 0x06ff), (0x0700, 0x077f),]\n",
            "\n",
            "    class Hebrew(unicode_set):\n",
            "        \"Unicode set for Hebrew Unicode Character Range\"\n",
            "        _ranges = [(0x0590, 0x05ff),]\n",
            "\n",
            "    class Devanagari(unicode_set):\n",
            "        \"Unicode set for Devanagari Unicode Character Range\"\n",
            "        _ranges = [(0x0900, 0x097f), (0xa8e0, 0xa8ff)]\n",
            "\n",
            "pyparsing_unicode.Japanese._ranges = (pyparsing_unicode.Japanese.Kanji._ranges\n",
            "                                      + pyparsing_unicode.Japanese.Hiragana._ranges\n",
            "                                      + pyparsing_unicode.Japanese.Katakana._ranges)\n",
            "\n",
            "# define ranges in language character sets\n",
            "if PY_3:\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Arabic)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Chinese)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Cyrillic)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Greek)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Hebrew)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Japanese)\n",
            "    setattr(pyparsing_unicode.Japanese, u\"\", pyparsing_unicode.Japanese.Kanji)\n",
            "    setattr(pyparsing_unicode.Japanese, u\"\", pyparsing_unicode.Japanese.Katakana)\n",
            "    setattr(pyparsing_unicode.Japanese, u\"\", pyparsing_unicode.Japanese.Hiragana)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Korean)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Thai)\n",
            "    setattr(pyparsing_unicode, u\"\", pyparsing_unicode.Devanagari)\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "\n",
            "    selectToken    = CaselessLiteral(\"select\")\n",
            "    fromToken      = CaselessLiteral(\"from\")\n",
            "\n",
            "    ident          = Word(alphas, alphanums + \"_$\")\n",
            "\n",
            "    columnName     = delimitedList(ident, \".\", combine=True).setParseAction(upcaseTokens)\n",
            "    columnNameList = Group(delimitedList(columnName)).setName(\"columns\")\n",
            "    columnSpec     = ('*' | columnNameList)\n",
            "\n",
            "    tableName      = delimitedList(ident, \".\", combine=True).setParseAction(upcaseTokens)\n",
            "    tableNameList  = Group(delimitedList(tableName)).setName(\"tables\")\n",
            "\n",
            "    simpleSQL      = selectToken(\"command\") + columnSpec(\"columns\") + fromToken + tableNameList(\"tables\")\n",
            "\n",
            "    # demo runTests method, including embedded comments in test string\n",
            "    simpleSQL.runTests(\"\"\"\n",
            "        # '*' as column list and dotted table name\n",
            "        select * from SYS.XYZZY\n",
            "\n",
            "        # caseless match on \"SELECT\", and casts back to \"select\"\n",
            "        SELECT * from XYZZY, ABC\n",
            "\n",
            "        # list of column names, and mixed case SELECT keyword\n",
            "        Select AA,BB,CC from Sys.dual\n",
            "\n",
            "        # multiple tables\n",
            "        Select A, B, C from Sys.dual, Table2\n",
            "\n",
            "        # invalid SELECT keyword - should fail\n",
            "        Xelect A, B, C from Sys.dual\n",
            "\n",
            "        # incomplete command - should fail\n",
            "        Select\n",
            "\n",
            "        # invalid column name - should fail\n",
            "        Select ^^^ frox Sys.dual\n",
            "\n",
            "        \"\"\")\n",
            "\n",
            "    pyparsing_common.number.runTests(\"\"\"\n",
            "        100\n",
            "        -100\n",
            "        +100\n",
            "        3.14159\n",
            "        6.02e23\n",
            "        1e-12\n",
            "        \"\"\")\n",
            "\n",
            "    # any int or real number, returned as float\n",
            "    pyparsing_common.fnumber.runTests(\"\"\"\n",
            "        100\n",
            "        -100\n",
            "        +100\n",
            "        3.14159\n",
            "        6.02e23\n",
            "        1e-12\n",
            "        \"\"\")\n",
            "\n",
            "    pyparsing_common.hex_integer.runTests(\"\"\"\n",
            "        100\n",
            "        FF\n",
            "        \"\"\")\n",
            "\n",
            "    import uuid\n",
            "    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))\n",
            "    pyparsing_common.uuid.runTests(\"\"\"\n",
            "        12345678-1234-5678-1234-567812345678\n",
            "        \"\"\")\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "\"\"\"Pretty-print tabular data.\"\"\"\n",
            "\n",
            "from __future__ import print_function\n",
            "from __future__ import unicode_literals\n",
            "from collections import namedtuple\n",
            "from platform import python_version_tuple\n",
            "import re\n",
            "import math\n",
            "\n",
            "\n",
            "if python_version_tuple() >= (\"3\", \"3\", \"0\"):\n",
            "    from collections.abc import Iterable\n",
            "else:\n",
            "    from collections import Iterable\n",
            "\n",
            "if python_version_tuple()[0] < \"3\":\n",
            "    from itertools import izip_longest\n",
            "    from functools import partial\n",
            "    _none_type = type(None)\n",
            "    _bool_type = bool\n",
            "    _int_type = int\n",
            "    _long_type = long\n",
            "    _float_type = float\n",
            "    _text_type = unicode\n",
            "    _binary_type = str\n",
            "\n",
            "    def _is_file(f):\n",
            "        return isinstance(f, file)\n",
            "\n",
            "else:\n",
            "    from itertools import zip_longest as izip_longest\n",
            "    from functools import reduce, partial\n",
            "    _none_type = type(None)\n",
            "    _bool_type = bool\n",
            "    _int_type = int\n",
            "    _long_type = int\n",
            "    _float_type = float\n",
            "    _text_type = str\n",
            "    _binary_type = bytes\n",
            "    basestring = str\n",
            "\n",
            "    import io\n",
            "    def _is_file(f):\n",
            "        return isinstance(f, io.IOBase)\n",
            "\n",
            "try:\n",
            "    import wcwidth  # optional wide-character (CJK) support\n",
            "except ImportError:\n",
            "    wcwidth = None\n",
            "\n",
            "\n",
            "__all__ = [\"tabulate\", \"tabulate_formats\", \"simple_separated_format\"]\n",
            "__version__ = \"0.8.3\"\n",
            "\n",
            "\n",
            "# minimum extra space in headers\n",
            "MIN_PADDING = 2\n",
            "\n",
            "# Whether or not to preserve leading/trailing whitespace in data.\n",
            "PRESERVE_WHITESPACE = False\n",
            "\n",
            "_DEFAULT_FLOATFMT=\"g\"\n",
            "_DEFAULT_MISSINGVAL=\"\"\n",
            "\n",
            "\n",
            "# if True, enable wide-character (CJK) support\n",
            "WIDE_CHARS_MODE = wcwidth is not None\n",
            "\n",
            "\n",
            "Line = namedtuple(\"Line\", [\"begin\", \"hline\", \"sep\", \"end\"])\n",
            "\n",
            "\n",
            "DataRow = namedtuple(\"DataRow\", [\"begin\", \"sep\", \"end\"])\n",
            "\n",
            "\n",
            "# A table structure is suppposed to be:\n",
            "#\n",
            "#     --- lineabove ---------\n",
            "#         headerrow\n",
            "#     --- linebelowheader ---\n",
            "#         datarow\n",
            "#     --- linebewteenrows ---\n",
            "#     ... (more datarows) ...\n",
            "#     --- linebewteenrows ---\n",
            "#         last datarow\n",
            "#     --- linebelow ---------\n",
            "#\n",
            "# TableFormat's line* elements can be\n",
            "#\n",
            "#   - either None, if the element is not used,\n",
            "#   - or a Line tuple,\n",
            "#   - or a function: [col_widths], [col_alignments] -> string.\n",
            "#\n",
            "# TableFormat's *row elements can be\n",
            "#\n",
            "#   - either None, if the element is not used,\n",
            "#   - or a DataRow tuple,\n",
            "#   - or a function: [cell_values], [col_widths], [col_alignments] -> string.\n",
            "#\n",
            "# padding (an integer) is the amount of white space around data values.\n",
            "#\n",
            "# with_header_hide:\n",
            "#\n",
            "#   - either None, to display all table elements unconditionally,\n",
            "#   - or a list of elements not to be displayed if the table has column headers.\n",
            "#\n",
            "TableFormat = namedtuple(\"TableFormat\", [\"lineabove\", \"linebelowheader\",\n",
            "                                         \"linebetweenrows\", \"linebelow\",\n",
            "                                         \"headerrow\", \"datarow\",\n",
            "                                         \"padding\", \"with_header_hide\"])\n",
            "\n",
            "\n",
            "def _pipe_segment_with_colons(align, colwidth):\n",
            "    \"\"\"Return a segment of a horizontal line with optional colons which\n",
            "    indicate column's alignment (as in `pipe` output format).\"\"\"\n",
            "    w = colwidth\n",
            "    if align in [\"right\", \"decimal\"]:\n",
            "        return ('-' * (w - 1)) + \":\"\n",
            "    elif align == \"center\":\n",
            "        return \":\" + ('-' * (w - 2)) + \":\"\n",
            "    elif align == \"left\":\n",
            "        return \":\" + ('-' * (w - 1))\n",
            "    else:\n",
            "        return '-' * w\n",
            "\n",
            "\n",
            "def _pipe_line_with_colons(colwidths, colaligns):\n",
            "    \"\"\"Return a horizontal line with optional colons to indicate column's\n",
            "    alignment (as in `pipe` output format).\"\"\"\n",
            "    segments = [_pipe_segment_with_colons(a, w) for a, w in zip(colaligns, colwidths)]\n",
            "    return \"|\" + \"|\".join(segments) + \"|\"\n",
            "\n",
            "\n",
            "def _mediawiki_row_with_attrs(separator, cell_values, colwidths, colaligns):\n",
            "    alignment = { \"left\":    '',\n",
            "                  \"right\":   'align=\"right\"| ',\n",
            "                  \"center\":  'align=\"center\"| ',\n",
            "                  \"decimal\": 'align=\"right\"| ' }\n",
            "    # hard-coded padding _around_ align attribute and value together\n",
            "    # rather than padding parameter which affects only the value\n",
            "    values_with_attrs = [' ' + alignment.get(a, '') + c + ' '\n",
            "                         for c, a in zip(cell_values, colaligns)]\n",
            "    colsep = separator*2\n",
            "    return (separator + colsep.join(values_with_attrs)).rstrip()\n",
            "\n",
            "\n",
            "def _textile_row_with_attrs(cell_values, colwidths, colaligns):\n",
            "    cell_values[0] += ' '\n",
            "    alignment = { \"left\": \"<.\", \"right\": \">.\", \"center\": \"=.\", \"decimal\": \">.\" }\n",
            "    values = (alignment.get(a, '') + v for a, v in zip(colaligns, cell_values))\n",
            "    return '|' + '|'.join(values) + '|'\n",
            "\n",
            "\n",
            "def _html_begin_table_without_header(colwidths_ignore, colaligns_ignore):\n",
            "    # this table header will be suppressed if there is a header row\n",
            "    return \"\\n\".join([\"<table>\", \"<tbody>\"])\n",
            "\n",
            "\n",
            "def _html_row_with_attrs(celltag, cell_values, colwidths, colaligns):\n",
            "    alignment = { \"left\":    '',\n",
            "                  \"right\":   ' style=\"text-align: right;\"',\n",
            "                  \"center\":  ' style=\"text-align: center;\"',\n",
            "                  \"decimal\": ' style=\"text-align: right;\"' }\n",
            "    values_with_attrs = [\"<{0}{1}>{2}</{0}>\".format(celltag, alignment.get(a, ''), c)\n",
            "                         for c, a in zip(cell_values, colaligns)]\n",
            "    rowhtml = \"<tr>\" + \"\".join(values_with_attrs).rstrip() + \"</tr>\"\n",
            "    if celltag == \"th\":  # it's a header row, create a new table header\n",
            "        rowhtml = \"\\n\".join([\"<table>\",\n",
            "                             \"<thead>\",\n",
            "                             rowhtml,\n",
            "                             \"</thead>\",\n",
            "                             \"<tbody>\"])\n",
            "    return rowhtml\n",
            "\n",
            "def _moin_row_with_attrs(celltag, cell_values, colwidths, colaligns, header=''):\n",
            "    alignment = { \"left\":    '',\n",
            "                  \"right\":   '<style=\"text-align: right;\">',\n",
            "                  \"center\":  '<style=\"text-align: center;\">',\n",
            "                  \"decimal\": '<style=\"text-align: right;\">' }\n",
            "    values_with_attrs = [\"{0}{1} {2} \".format(celltag,\n",
            "                                              alignment.get(a, ''),\n",
            "                                              header+c+header)\n",
            "                         for c, a in zip(cell_values, colaligns)]\n",
            "    return \"\".join(values_with_attrs)+\"||\"\n",
            "\n",
            "def _latex_line_begin_tabular(colwidths, colaligns, booktabs=False):\n",
            "    alignment = { \"left\": \"l\", \"right\": \"r\", \"center\": \"c\", \"decimal\": \"r\" }\n",
            "    tabular_columns_fmt = \"\".join([alignment.get(a, \"l\") for a in colaligns])\n",
            "    return \"\\n\".join([\"\\\\begin{tabular}{\" + tabular_columns_fmt + \"}\",\n",
            "                      \"\\\\toprule\" if booktabs else \"\\hline\"])\n",
            "\n",
            "LATEX_ESCAPE_RULES = {r\"&\": r\"\\&\", r\"%\": r\"\\%\", r\"$\": r\"\\$\", r\"#\": r\"\\#\",\n",
            "                      r\"_\": r\"\\_\", r\"^\": r\"\\^{}\", r\"{\": r\"\\{\", r\"}\": r\"\\}\",\n",
            "                      r\"~\": r\"\\textasciitilde{}\", \"\\\\\": r\"\\textbackslash{}\",\n",
            "                      r\"<\": r\"\\ensuremath{<}\", r\">\": r\"\\ensuremath{>}\"}\n",
            "\n",
            "def _latex_row(cell_values, colwidths, colaligns, escrules=LATEX_ESCAPE_RULES):\n",
            "    def escape_char(c):\n",
            "        return escrules.get(c, c)\n",
            "    escaped_values = [\"\".join(map(escape_char, cell)) for cell in cell_values]\n",
            "    rowfmt = DataRow(\"\", \"&\", \"\\\\\\\\\")\n",
            "    return _build_simple_row(escaped_values, rowfmt)\n",
            "\n",
            "def _rst_escape_first_column(rows, headers):\n",
            "    def escape_empty(val):\n",
            "        if isinstance(val, (_text_type, _binary_type)) and not val.strip():\n",
            "            return \"..\"\n",
            "        else:\n",
            "            return val\n",
            "    new_headers = list(headers)\n",
            "    new_rows = []\n",
            "    if headers:\n",
            "        new_headers[0] = escape_empty(headers[0])\n",
            "    for row in rows:\n",
            "        new_row = list(row)\n",
            "        if new_row:\n",
            "            new_row[0] = escape_empty(row[0])\n",
            "        new_rows.append(new_row)\n",
            "    return new_rows, new_headers\n",
            "\n",
            "\n",
            "_table_formats = {\"simple\":\n",
            "                  TableFormat(lineabove=Line(\"\", \"-\", \"  \", \"\"),\n",
            "                              linebelowheader=Line(\"\", \"-\", \"  \", \"\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"\", \"-\", \"  \", \"\"),\n",
            "                              headerrow=DataRow(\"\", \"  \", \"\"),\n",
            "                              datarow=DataRow(\"\", \"  \", \"\"),\n",
            "                              padding=0,\n",
            "                              with_header_hide=[\"lineabove\", \"linebelow\"]),\n",
            "                  \"plain\":\n",
            "                  TableFormat(lineabove=None, linebelowheader=None,\n",
            "                              linebetweenrows=None, linebelow=None,\n",
            "                              headerrow=DataRow(\"\", \"  \", \"\"),\n",
            "                              datarow=DataRow(\"\", \"  \", \"\"),\n",
            "                              padding=0, with_header_hide=None),\n",
            "                  \"grid\":\n",
            "                  TableFormat(lineabove=Line(\"+\", \"-\", \"+\", \"+\"),\n",
            "                              linebelowheader=Line(\"+\", \"=\", \"+\", \"+\"),\n",
            "                              linebetweenrows=Line(\"+\", \"-\", \"+\", \"+\"),\n",
            "                              linebelow=Line(\"+\", \"-\", \"+\", \"+\"),\n",
            "                              headerrow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"fancy_grid\":\n",
            "                  TableFormat(lineabove=Line(\"\", \"\", \"\", \"\"),\n",
            "                              linebelowheader=Line(\"\", \"\", \"\", \"\"),\n",
            "                              linebetweenrows=Line(\"\", \"\", \"\", \"\"),\n",
            "                              linebelow=Line(\"\", \"\", \"\", \"\"),\n",
            "                              headerrow=DataRow(\"\", \"\", \"\"),\n",
            "                              datarow=DataRow(\"\", \"\", \"\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"github\":\n",
            "                  TableFormat(lineabove=Line(\"|\", \"-\", \"|\", \"|\"),\n",
            "                              linebelowheader=Line(\"|\", \"-\", \"|\", \"|\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1,\n",
            "                              with_header_hide=[\"lineabove\"]),\n",
            "                  \"pipe\":\n",
            "                  TableFormat(lineabove=_pipe_line_with_colons,\n",
            "                              linebelowheader=_pipe_line_with_colons,\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1,\n",
            "                              with_header_hide=[\"lineabove\"]),\n",
            "                  \"orgtbl\":\n",
            "                  TableFormat(lineabove=None,\n",
            "                              linebelowheader=Line(\"|\", \"-\", \"+\", \"|\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"jira\":\n",
            "                  TableFormat(lineabove=None,\n",
            "                              linebelowheader=None,\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=DataRow(\"||\", \"||\", \"||\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"presto\":\n",
            "                      TableFormat(lineabove=None,\n",
            "                                  linebelowheader=Line(\"\", \"-\", \"+\", \"\"),\n",
            "                                  linebetweenrows=None,\n",
            "                                  linebelow=None,\n",
            "                                  headerrow=DataRow(\"\", \"|\", \"\"),\n",
            "                                  datarow=DataRow(\"\", \"|\", \"\"),\n",
            "                                  padding=1, with_header_hide=None),\n",
            "                  \"psql\":\n",
            "                  TableFormat(lineabove=Line(\"+\", \"-\", \"+\", \"+\"),\n",
            "                              linebelowheader=Line(\"|\", \"-\", \"+\", \"|\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"+\", \"-\", \"+\", \"+\"),\n",
            "                              headerrow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              datarow=DataRow(\"|\", \"|\", \"|\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"rst\":\n",
            "                  TableFormat(lineabove=Line(\"\", \"=\", \"  \", \"\"),\n",
            "                              linebelowheader=Line(\"\", \"=\", \"  \", \"\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"\", \"=\", \"  \", \"\"),\n",
            "                              headerrow=DataRow(\"\", \"  \", \"\"),\n",
            "                              datarow=DataRow(\"\", \"  \", \"\"),\n",
            "                              padding=0, with_header_hide=None),\n",
            "                  \"mediawiki\":\n",
            "                  TableFormat(lineabove=Line(\"{| class=\\\"wikitable\\\" style=\\\"text-align: left;\\\"\",\n",
            "                                             \"\", \"\", \"\\n|+ <!-- caption -->\\n|-\"),\n",
            "                              linebelowheader=Line(\"|-\", \"\", \"\", \"\"),\n",
            "                              linebetweenrows=Line(\"|-\", \"\", \"\", \"\"),\n",
            "                              linebelow=Line(\"|}\", \"\", \"\", \"\"),\n",
            "                              headerrow=partial(_mediawiki_row_with_attrs, \"!\"),\n",
            "                              datarow=partial(_mediawiki_row_with_attrs, \"|\"),\n",
            "                              padding=0, with_header_hide=None),\n",
            "                  \"moinmoin\":\n",
            "                  TableFormat(lineabove=None,\n",
            "                              linebelowheader=None,\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=partial(_moin_row_with_attrs,\"||\",header=\"'''\"),\n",
            "                              datarow=partial(_moin_row_with_attrs,\"||\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"youtrack\":\n",
            "                  TableFormat(lineabove=None,\n",
            "                              linebelowheader=None,\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=None,\n",
            "                              headerrow=DataRow(\"|| \", \" || \", \" || \"),\n",
            "                              datarow=DataRow(\"| \", \" | \", \" |\"),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"html\":\n",
            "                  TableFormat(lineabove=_html_begin_table_without_header,\n",
            "                              linebelowheader=\"\",\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"</tbody>\\n</table>\", \"\", \"\", \"\"),\n",
            "                              headerrow=partial(_html_row_with_attrs, \"th\"),\n",
            "                              datarow=partial(_html_row_with_attrs, \"td\"),\n",
            "                              padding=0, with_header_hide=[\"lineabove\"]),\n",
            "                  \"latex\":\n",
            "                  TableFormat(lineabove=_latex_line_begin_tabular,\n",
            "                              linebelowheader=Line(\"\\\\hline\", \"\", \"\", \"\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"\\\\hline\\n\\\\end{tabular}\", \"\", \"\", \"\"),\n",
            "                              headerrow=_latex_row,\n",
            "                              datarow=_latex_row,\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"latex_raw\":\n",
            "                  TableFormat(lineabove=_latex_line_begin_tabular,\n",
            "                              linebelowheader=Line(\"\\\\hline\", \"\", \"\", \"\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"\\\\hline\\n\\\\end{tabular}\", \"\", \"\", \"\"),\n",
            "                              headerrow=partial(_latex_row, escrules={}),\n",
            "                              datarow=partial(_latex_row, escrules={}),\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"latex_booktabs\":\n",
            "                  TableFormat(lineabove=partial(_latex_line_begin_tabular, booktabs=True),\n",
            "                              linebelowheader=Line(\"\\\\midrule\", \"\", \"\", \"\"),\n",
            "                              linebetweenrows=None,\n",
            "                              linebelow=Line(\"\\\\bottomrule\\n\\\\end{tabular}\", \"\", \"\", \"\"),\n",
            "                              headerrow=_latex_row,\n",
            "                              datarow=_latex_row,\n",
            "                              padding=1, with_header_hide=None),\n",
            "                  \"tsv\":\n",
            "                  TableFormat(lineabove=None, linebelowheader=None,\n",
            "                              linebetweenrows=None, linebelow=None,\n",
            "                              headerrow=DataRow(\"\", \"\\t\", \"\"),\n",
            "                              datarow=DataRow(\"\", \"\\t\", \"\"),\n",
            "                              padding=0, with_header_hide=None),\n",
            "                  \"textile\":\n",
            "                  TableFormat(lineabove=None, linebelowheader=None,\n",
            "                              linebetweenrows=None, linebelow=None,\n",
            "                              headerrow=DataRow(\"|_. \", \"|_.\", \"|\"),\n",
            "                              datarow=_textile_row_with_attrs,\n",
            "                              padding=1, with_header_hide=None)}\n",
            "\n",
            "\n",
            "tabulate_formats = list(sorted(_table_formats.keys()))\n",
            "\n",
            "# The table formats for which multiline cells will be folded into subsequent\n",
            "# table rows. The key is the original format specified at the API. The value is\n",
            "# the format that will be used to represent the original format.\n",
            "multiline_formats = {\n",
            "        \"plain\": \"plain\",\n",
            "        \"simple\": \"simple\",\n",
            "        \"grid\": \"grid\",\n",
            "        \"fancy_grid\": \"fancy_grid\",\n",
            "        \"pipe\": \"pipe\",\n",
            "        \"orgtbl\": \"orgtbl\",\n",
            "        \"jira\": \"jira\",\n",
            "        \"presto\": \"presto\",\n",
            "        \"psql\": \"psql\",\n",
            "        \"rst\": \"rst\",\n",
            "}\n",
            "\n",
            "# TODO: Add multiline support for the remaining table formats:\n",
            "#       - mediawiki: Replace \\n with <br>\n",
            "#       - moinmoin: TBD\n",
            "#       - youtrack: TBD\n",
            "#       - html: Replace \\n with <br>\n",
            "#       - latex*: Use \"makecell\" package: In header, replace X\\nY with\n",
            "#         \\thead{X\\\\Y} and in data row, replace X\\nY with \\makecell{X\\\\Y}\n",
            "#       - tsv: TBD\n",
            "#       - textile: Replace \\n with <br/> (must be well-formed XML)\n",
            "\n",
            "_multiline_codes = re.compile(r\"\\r|\\n|\\r\\n\")\n",
            "_multiline_codes_bytes = re.compile(b\"\\r|\\n|\\r\\n\")\n",
            "_invisible_codes = re.compile(r\"\\x1b\\[\\d+[;\\d]*m|\\x1b\\[\\d*\\;\\d*\\;\\d*m\")  # ANSI color codes\n",
            "_invisible_codes_bytes = re.compile(b\"\\x1b\\[\\d+[;\\d]*m|\\x1b\\[\\d*\\;\\d*\\;\\d*m\")  # ANSI color codes\n",
            "\n",
            "\n",
            "def simple_separated_format(separator):\n",
            "    \"\"\"Construct a simple TableFormat with columns separated by a separator.\n",
            "\n",
            "    >>> tsv = simple_separated_format(\"\\\\t\") ; \\\n",
            "        tabulate([[\"foo\", 1], [\"spam\", 23]], tablefmt=tsv) == 'foo \\\\t 1\\\\nspam\\\\t23'\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    return TableFormat(None, None, None, None,\n",
            "                       headerrow=DataRow('', separator, ''),\n",
            "                       datarow=DataRow('', separator, ''),\n",
            "                       padding=0, with_header_hide=None)\n",
            "\n",
            "\n",
            "def _isconvertible(conv, string):\n",
            "    try:\n",
            "        n = conv(string)\n",
            "        return True\n",
            "    except (ValueError, TypeError):\n",
            "        return False\n",
            "\n",
            "\n",
            "def _isnumber(string):\n",
            "    \"\"\"\n",
            "    >>> _isnumber(\"123.45\")\n",
            "    True\n",
            "    >>> _isnumber(\"123\")\n",
            "    True\n",
            "    >>> _isnumber(\"spam\")\n",
            "    False\n",
            "    >>> _isnumber(\"123e45678\")\n",
            "    False\n",
            "    >>> _isnumber(\"inf\")\n",
            "    True\n",
            "    \"\"\"\n",
            "    if not _isconvertible(float, string):\n",
            "        return False\n",
            "    elif isinstance(string, (_text_type, _binary_type)) and (\n",
            "            math.isinf(float(string)) or math.isnan(float(string))):\n",
            "        return string.lower() in ['inf', '-inf', 'nan']\n",
            "    return True\n",
            "\n",
            "\n",
            "def _isint(string, inttype=int):\n",
            "    \"\"\"\n",
            "    >>> _isint(\"123\")\n",
            "    True\n",
            "    >>> _isint(\"123.45\")\n",
            "    False\n",
            "    \"\"\"\n",
            "    return type(string) is inttype or\\\n",
            "           (isinstance(string, _binary_type) or isinstance(string, _text_type))\\\n",
            "            and\\\n",
            "            _isconvertible(inttype, string)\n",
            "\n",
            "\n",
            "def _isbool(string):\n",
            "    \"\"\"\n",
            "    >>> _isbool(True)\n",
            "    True\n",
            "    >>> _isbool(\"False\")\n",
            "    True\n",
            "    >>> _isbool(1)\n",
            "    False\n",
            "    \"\"\"\n",
            "    return type(string) is _bool_type or\\\n",
            "           (isinstance(string, (_binary_type, _text_type))\\\n",
            "            and\\\n",
            "            string in (\"True\", \"False\"))\n",
            "\n",
            "\n",
            "def _type(string, has_invisible=True, numparse=True):\n",
            "    \"\"\"The least generic type (type(None), int, float, str, unicode).\n",
            "\n",
            "    >>> _type(None) is type(None)\n",
            "    True\n",
            "    >>> _type(\"foo\") is type(\"\")\n",
            "    True\n",
            "    >>> _type(\"1\") is type(1)\n",
            "    True\n",
            "    >>> _type('\\x1b[31m42\\x1b[0m') is type(42)\n",
            "    True\n",
            "    >>> _type('\\x1b[31m42\\x1b[0m') is type(42)\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    if has_invisible and \\\n",
            "       (isinstance(string, _text_type) or isinstance(string, _binary_type)):\n",
            "        string = _strip_invisible(string)\n",
            "\n",
            "    if string is None:\n",
            "        return _none_type\n",
            "    elif hasattr(string, \"isoformat\"):  # datetime.datetime, date, and time\n",
            "        return _text_type\n",
            "    elif _isbool(string):\n",
            "        return _bool_type\n",
            "    elif _isint(string) and numparse:\n",
            "        return int\n",
            "    elif _isint(string, _long_type) and numparse:\n",
            "        return int\n",
            "    elif _isnumber(string) and numparse:\n",
            "        return float\n",
            "    elif isinstance(string, _binary_type):\n",
            "        return _binary_type\n",
            "    else:\n",
            "        return _text_type\n",
            "\n",
            "\n",
            "def _afterpoint(string):\n",
            "    \"\"\"Symbols after a decimal point, -1 if the string lacks the decimal point.\n",
            "\n",
            "    >>> _afterpoint(\"123.45\")\n",
            "    2\n",
            "    >>> _afterpoint(\"1001\")\n",
            "    -1\n",
            "    >>> _afterpoint(\"eggs\")\n",
            "    -1\n",
            "    >>> _afterpoint(\"123e45\")\n",
            "    2\n",
            "\n",
            "    \"\"\"\n",
            "    if _isnumber(string):\n",
            "        if _isint(string):\n",
            "            return -1\n",
            "        else:\n",
            "            pos = string.rfind(\".\")\n",
            "            pos = string.lower().rfind(\"e\") if pos < 0 else pos\n",
            "            if pos >= 0:\n",
            "                return len(string) - pos - 1\n",
            "            else:\n",
            "                return -1  # no point\n",
            "    else:\n",
            "        return -1  # not a number\n",
            "\n",
            "\n",
            "def _padleft(width, s):\n",
            "    \"\"\"Flush right.\n",
            "\n",
            "    >>> _padleft(6, '\\u044f\\u0439\\u0446\\u0430') == '  \\u044f\\u0439\\u0446\\u0430'\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    fmt = \"{0:>%ds}\" % width\n",
            "    return fmt.format(s)\n",
            "\n",
            "\n",
            "def _padright(width, s):\n",
            "    \"\"\"Flush left.\n",
            "\n",
            "    >>> _padright(6, '\\u044f\\u0439\\u0446\\u0430') == '\\u044f\\u0439\\u0446\\u0430  '\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    fmt = \"{0:<%ds}\" % width\n",
            "    return fmt.format(s)\n",
            "\n",
            "\n",
            "def _padboth(width, s):\n",
            "    \"\"\"Center string.\n",
            "\n",
            "    >>> _padboth(6, '\\u044f\\u0439\\u0446\\u0430') == ' \\u044f\\u0439\\u0446\\u0430 '\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    fmt = \"{0:^%ds}\" % width\n",
            "    return fmt.format(s)\n",
            "\n",
            "\n",
            "def _padnone(ignore_width, s):\n",
            "    return s\n",
            "\n",
            "\n",
            "def _strip_invisible(s):\n",
            "    \"Remove invisible ANSI color codes.\"\n",
            "    if isinstance(s, _text_type):\n",
            "        return re.sub(_invisible_codes, \"\", s)\n",
            "    else:  # a bytestring\n",
            "        return re.sub(_invisible_codes_bytes, \"\", s)\n",
            "\n",
            "\n",
            "def _visible_width(s):\n",
            "    \"\"\"Visible width of a printed string. ANSI color codes are removed.\n",
            "\n",
            "    >>> _visible_width('\\x1b[31mhello\\x1b[0m'), _visible_width(\"world\")\n",
            "    (5, 5)\n",
            "\n",
            "    \"\"\"\n",
            "    # optional wide-character support\n",
            "    if wcwidth is not None and WIDE_CHARS_MODE:\n",
            "        len_fn = wcwidth.wcswidth\n",
            "    else:\n",
            "        len_fn = len\n",
            "    if isinstance(s, _text_type) or isinstance(s, _binary_type):\n",
            "        return len_fn(_strip_invisible(s))\n",
            "    else:\n",
            "        return len_fn(_text_type(s))\n",
            "\n",
            "\n",
            "def _is_multiline(s):\n",
            "    if isinstance(s, _text_type):\n",
            "        return bool(re.search(_multiline_codes, s))\n",
            "    else:  # a bytestring\n",
            "        return bool(re.search(_multiline_codes_bytes, s))\n",
            "\n",
            "\n",
            "def _multiline_width(multiline_s, line_width_fn=len):\n",
            "    \"\"\"Visible width of a potentially multiline content.\"\"\"\n",
            "    return max(map(line_width_fn, re.split(\"[\\r\\n]\", multiline_s)))\n",
            "\n",
            "\n",
            "def _choose_width_fn(has_invisible, enable_widechars, is_multiline):\n",
            "    \"\"\"Return a function to calculate visible cell width.\"\"\"\n",
            "    if has_invisible:\n",
            "        line_width_fn = _visible_width\n",
            "    elif enable_widechars: # optional wide-character support if available\n",
            "        line_width_fn = wcwidth.wcswidth\n",
            "    else:\n",
            "        line_width_fn = len\n",
            "    if is_multiline:\n",
            "        width_fn = lambda s: _multiline_width(s, line_width_fn)\n",
            "    else:\n",
            "        width_fn = line_width_fn\n",
            "    return width_fn\n",
            "\n",
            "\n",
            "def _align_column_choose_padfn(strings, alignment, has_invisible):\n",
            "    if alignment == \"right\":\n",
            "        if not PRESERVE_WHITESPACE:\n",
            "            strings = [s.strip() for s in strings]\n",
            "        padfn = _padleft\n",
            "    elif alignment == \"center\":\n",
            "        if not PRESERVE_WHITESPACE:\n",
            "            strings = [s.strip() for s in strings]\n",
            "        padfn = _padboth\n",
            "    elif alignment == \"decimal\":\n",
            "        if has_invisible:\n",
            "            decimals = [_afterpoint(_strip_invisible(s)) for s in strings]\n",
            "        else:\n",
            "            decimals = [_afterpoint(s) for s in strings]\n",
            "        maxdecimals = max(decimals)\n",
            "        strings = [s + (maxdecimals - decs) * \" \"\n",
            "                   for s, decs in zip(strings, decimals)]\n",
            "        padfn = _padleft\n",
            "    elif not alignment:\n",
            "        padfn = _padnone\n",
            "    else:\n",
            "        if not PRESERVE_WHITESPACE:\n",
            "            strings = [s.strip() for s in strings]\n",
            "        padfn = _padright\n",
            "    return strings, padfn\n",
            "\n",
            "\n",
            "def _align_column(strings, alignment, minwidth=0,\n",
            "                  has_invisible=True, enable_widechars=False, is_multiline=False):\n",
            "    \"\"\"[string] -> [padded_string]\"\"\"\n",
            "    strings, padfn = _align_column_choose_padfn(strings, alignment, has_invisible)\n",
            "    width_fn = _choose_width_fn(has_invisible, enable_widechars, is_multiline)\n",
            "\n",
            "    s_widths = list(map(width_fn, strings))\n",
            "    maxwidth = max(max(s_widths), minwidth)\n",
            "    # TODO: refactor column alignment in single-line and multiline modes\n",
            "    if is_multiline:\n",
            "        if not enable_widechars and not has_invisible:\n",
            "            padded_strings = [\n",
            "                \"\\n\".join([padfn(maxwidth, s) for s in ms.splitlines()])\n",
            "                for ms in strings]\n",
            "        else:\n",
            "            # enable wide-character width corrections\n",
            "            s_lens = [max((len(s) for s in re.split(\"[\\r\\n]\", ms))) for ms in strings]\n",
            "            visible_widths = [maxwidth - (w - l) for w, l in zip(s_widths, s_lens)]\n",
            "            # wcswidth and _visible_width don't count invisible characters;\n",
            "            # padfn doesn't need to apply another correction\n",
            "            padded_strings = [\"\\n\".join([padfn(w, s) for s in (ms.splitlines() or ms)])\n",
            "                              for ms, w in zip(strings, visible_widths)]\n",
            "    else:  # single-line cell values\n",
            "        if not enable_widechars and not has_invisible:\n",
            "            padded_strings = [padfn(maxwidth, s) for s in strings]\n",
            "        else:\n",
            "            # enable wide-character width corrections\n",
            "            s_lens = list(map(len, strings))\n",
            "            visible_widths = [maxwidth - (w - l) for w, l in zip(s_widths, s_lens)]\n",
            "            # wcswidth and _visible_width don't count invisible characters;\n",
            "            # padfn doesn't need to apply another correction\n",
            "            padded_strings = [padfn(w, s) for s, w in zip(strings, visible_widths)]\n",
            "    return padded_strings\n",
            "\n",
            "\n",
            "def _more_generic(type1, type2):\n",
            "    types = { _none_type: 0, _bool_type: 1, int: 2, float: 3, _binary_type: 4, _text_type: 5 }\n",
            "    invtypes = { 5: _text_type, 4: _binary_type, 3: float, 2: int, 1: _bool_type, 0: _none_type }\n",
            "    moregeneric = max(types.get(type1, 5), types.get(type2, 5))\n",
            "    return invtypes[moregeneric]\n",
            "\n",
            "\n",
            "def _column_type(strings, has_invisible=True, numparse=True):\n",
            "    \"\"\"The least generic type all column values are convertible to.\n",
            "\n",
            "    >>> _column_type([True, False]) is _bool_type\n",
            "    True\n",
            "    >>> _column_type([\"1\", \"2\"]) is _int_type\n",
            "    True\n",
            "    >>> _column_type([\"1\", \"2.3\"]) is _float_type\n",
            "    True\n",
            "    >>> _column_type([\"1\", \"2.3\", \"four\"]) is _text_type\n",
            "    True\n",
            "    >>> _column_type([\"four\", '\\u043f\\u044f\\u0442\\u044c']) is _text_type\n",
            "    True\n",
            "    >>> _column_type([None, \"brux\"]) is _text_type\n",
            "    True\n",
            "    >>> _column_type([1, 2, None]) is _int_type\n",
            "    True\n",
            "    >>> import datetime as dt\n",
            "    >>> _column_type([dt.datetime(1991,2,19), dt.time(17,35)]) is _text_type\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    types = [_type(s, has_invisible, numparse) for s in strings ]\n",
            "    return reduce(_more_generic, types, _bool_type)\n",
            "\n",
            "\n",
            "def _format(val, valtype, floatfmt, missingval=\"\", has_invisible=True):\n",
            "    \"\"\"Format a value accoding to its type.\n",
            "\n",
            "    Unicode is supported:\n",
            "\n",
            "    >>> hrow = ['\\u0431\\u0443\\u043a\\u0432\\u0430', '\\u0446\\u0438\\u0444\\u0440\\u0430'] ; \\\n",
            "        tbl = [['\\u0430\\u0437', 2], ['\\u0431\\u0443\\u043a\\u0438', 4]] ; \\\n",
            "        good_result = '\\\\u0431\\\\u0443\\\\u043a\\\\u0432\\\\u0430      \\\\u0446\\\\u0438\\\\u0444\\\\u0440\\\\u0430\\\\n-------  -------\\\\n\\\\u0430\\\\u0437             2\\\\n\\\\u0431\\\\u0443\\\\u043a\\\\u0438           4' ; \\\n",
            "        tabulate(tbl, headers=hrow) == good_result\n",
            "    True\n",
            "\n",
            "    \"\"\"\n",
            "    if val is None:\n",
            "        return missingval\n",
            "\n",
            "    if valtype in [int, _text_type]:\n",
            "        return \"{0}\".format(val)\n",
            "    elif valtype is _binary_type:\n",
            "        try:\n",
            "            return _text_type(val, \"ascii\")\n",
            "        except TypeError:\n",
            "            return _text_type(val)\n",
            "    elif valtype is float:\n",
            "        is_a_colored_number = has_invisible and isinstance(val, (_text_type, _binary_type))\n",
            "        if is_a_colored_number:\n",
            "            raw_val = _strip_invisible(val)\n",
            "            formatted_val = format(float(raw_val), floatfmt)\n",
            "            return val.replace(raw_val, formatted_val)\n",
            "        else:\n",
            "            return format(float(val), floatfmt)\n",
            "    else:\n",
            "        return \"{0}\".format(val)\n",
            "\n",
            "\n",
            "def _align_header(header, alignment, width, visible_width, is_multiline=False, width_fn=None):\n",
            "    \"Pad string header to width chars given known visible_width of the header.\"\n",
            "    if is_multiline:\n",
            "        header_lines = re.split(_multiline_codes, header)\n",
            "        padded_lines = [_align_header(h, alignment, width, width_fn(h)) for h in header_lines]\n",
            "        return \"\\n\".join(padded_lines)\n",
            "    # else: not multiline\n",
            "    ninvisible = len(header) - visible_width\n",
            "    width += ninvisible\n",
            "    if alignment == \"left\":\n",
            "        return _padright(width, header)\n",
            "    elif alignment == \"center\":\n",
            "        return _padboth(width, header)\n",
            "    elif not alignment:\n",
            "        return \"{0}\".format(header)\n",
            "    else:\n",
            "        return _padleft(width, header)\n",
            "\n",
            "\n",
            "def _prepend_row_index(rows, index):\n",
            "    \"\"\"Add a left-most index column.\"\"\"\n",
            "    if index is None or index is False:\n",
            "        return rows\n",
            "    if len(index) != len(rows):\n",
            "        print('index=', index)\n",
            "        print('rows=', rows)\n",
            "        raise ValueError('index must be as long as the number of data rows')\n",
            "    rows = [[v]+list(row) for v,row in zip(index, rows)]\n",
            "    return rows\n",
            "\n",
            "\n",
            "def _bool(val):\n",
            "    \"A wrapper around standard bool() which doesn't throw on NumPy arrays\"\n",
            "    try:\n",
            "        return bool(val)\n",
            "    except ValueError:  # val is likely to be a numpy array with many elements\n",
            "        return False\n",
            "\n",
            "\n",
            "def _normalize_tabular_data(tabular_data, headers, showindex=\"default\"):\n",
            "    \"\"\"Transform a supported data type to a list of lists, and a list of headers.\n",
            "\n",
            "    Supported tabular data types:\n",
            "\n",
            "    * list-of-lists or another iterable of iterables\n",
            "\n",
            "    * list of named tuples (usually used with headers=\"keys\")\n",
            "\n",
            "    * list of dicts (usually used with headers=\"keys\")\n",
            "\n",
            "    * list of OrderedDicts (usually used with headers=\"keys\")\n",
            "\n",
            "    * 2D NumPy arrays\n",
            "\n",
            "    * NumPy record arrays (usually used with headers=\"keys\")\n",
            "\n",
            "    * dict of iterables (usually used with headers=\"keys\")\n",
            "\n",
            "    * pandas.DataFrame (usually used with headers=\"keys\")\n",
            "\n",
            "    The first row can be used as headers if headers=\"firstrow\",\n",
            "    column indices can be used as headers if headers=\"keys\".\n",
            "\n",
            "    If showindex=\"default\", show row indices of the pandas.DataFrame.\n",
            "    If showindex=\"always\", show row indices for all types of data.\n",
            "    If showindex=\"never\", don't show row indices for all types of data.\n",
            "    If showindex is an iterable, show its values as row indices.\n",
            "\n",
            "    \"\"\"\n",
            "\n",
            "    try:\n",
            "        bool(headers)\n",
            "        is_headers2bool_broken = False\n",
            "    except ValueError: # numpy.ndarray, pandas.core.index.Index, ...\n",
            "        is_headers2bool_broken = True\n",
            "        headers = list(headers)\n",
            "\n",
            "    index = None\n",
            "    if hasattr(tabular_data, \"keys\") and hasattr(tabular_data, \"values\"):\n",
            "        # dict-like and pandas.DataFrame?\n",
            "        if hasattr(tabular_data.values, \"__call__\"):\n",
            "            # likely a conventional dict\n",
            "            keys = tabular_data.keys()\n",
            "            rows = list(izip_longest(*tabular_data.values()))  # columns have to be transposed\n",
            "        elif hasattr(tabular_data, \"index\"):\n",
            "            # values is a property, has .index => it's likely a pandas.DataFrame (pandas 0.11.0)\n",
            "            keys = list(tabular_data)\n",
            "            if tabular_data.index.name is not None:\n",
            "                if isinstance(tabular_data.index.name, list):\n",
            "                    keys[:0] = tabular_data.index.name\n",
            "                else:\n",
            "                    keys[:0] = [tabular_data.index.name]\n",
            "            vals = tabular_data.values  # values matrix doesn't need to be transposed\n",
            "            # for DataFrames add an index per default\n",
            "            index = list(tabular_data.index)\n",
            "            rows = [list(row) for row in vals]\n",
            "        else:\n",
            "            raise ValueError(\"tabular data doesn't appear to be a dict or a DataFrame\")\n",
            "\n",
            "        if headers == \"keys\":\n",
            "            headers = list(map(_text_type,keys))  # headers should be strings\n",
            "\n",
            "    else:  # it's a usual an iterable of iterables, or a NumPy array\n",
            "        rows = list(tabular_data)\n",
            "\n",
            "        if (headers == \"keys\" and not rows):\n",
            "            # an empty table (issue #81)\n",
            "            headers = []\n",
            "        elif (headers == \"keys\" and\n",
            "            hasattr(tabular_data, \"dtype\") and\n",
            "            getattr(tabular_data.dtype, \"names\")):\n",
            "            # numpy record array\n",
            "            headers = tabular_data.dtype.names\n",
            "        elif (headers == \"keys\"\n",
            "              and len(rows) > 0\n",
            "              and isinstance(rows[0], tuple)\n",
            "              and hasattr(rows[0], \"_fields\")):\n",
            "            # namedtuple\n",
            "            headers = list(map(_text_type, rows[0]._fields))\n",
            "        elif (len(rows) > 0\n",
            "              and isinstance(rows[0], dict)):\n",
            "            # dict or OrderedDict\n",
            "            uniq_keys = set() # implements hashed lookup\n",
            "            keys = [] # storage for set\n",
            "            if headers == \"firstrow\":\n",
            "                firstdict = rows[0] if len(rows) > 0 else {}\n",
            "                keys.extend(firstdict.keys())\n",
            "                uniq_keys.update(keys)\n",
            "                rows = rows[1:]\n",
            "            for row in rows:\n",
            "                for k in row.keys():\n",
            "                    #Save unique items in input order\n",
            "                    if k not in uniq_keys:\n",
            "                        keys.append(k)\n",
            "                        uniq_keys.add(k)\n",
            "            if headers == 'keys':\n",
            "                headers = keys\n",
            "            elif isinstance(headers, dict):\n",
            "                # a dict of headers for a list of dicts\n",
            "                headers = [headers.get(k, k) for k in keys]\n",
            "                headers = list(map(_text_type, headers))\n",
            "            elif headers == \"firstrow\":\n",
            "                if len(rows) > 0:\n",
            "                    headers = [firstdict.get(k, k) for k in keys]\n",
            "                    headers = list(map(_text_type, headers))\n",
            "                else:\n",
            "                    headers = []\n",
            "            elif headers:\n",
            "                raise ValueError('headers for a list of dicts is not a dict or a keyword')\n",
            "            rows = [[row.get(k) for k in keys] for row in rows]\n",
            "\n",
            "        elif (headers == \"keys\"\n",
            "              and hasattr(tabular_data, \"description\")\n",
            "              and hasattr(tabular_data, \"fetchone\")\n",
            "              and hasattr(tabular_data, \"rowcount\")):\n",
            "            # Python Database API cursor object (PEP 0249)\n",
            "            # print tabulate(cursor, headers='keys')\n",
            "            headers = [column[0] for column in tabular_data.description]\n",
            "\n",
            "        elif headers == \"keys\" and len(rows) > 0:\n",
            "            # keys are column indices\n",
            "            headers = list(map(_text_type, range(len(rows[0]))))\n",
            "\n",
            "    # take headers from the first row if necessary\n",
            "    if headers == \"firstrow\" and len(rows) > 0:\n",
            "        if index is not None:\n",
            "            headers = [index[0]] + list(rows[0])\n",
            "            index = index[1:]\n",
            "        else:\n",
            "            headers = rows[0]\n",
            "        headers = list(map(_text_type, headers)) # headers should be strings\n",
            "        rows = rows[1:]\n",
            "\n",
            "    headers = list(map(_text_type,headers))\n",
            "    rows = list(map(list,rows))\n",
            "\n",
            "    # add or remove an index column\n",
            "    showindex_is_a_str = type(showindex) in [_text_type, _binary_type]\n",
            "    if showindex == \"default\" and index is not None:\n",
            "        rows = _prepend_row_index(rows, index)\n",
            "    elif isinstance(showindex, Iterable) and not showindex_is_a_str:\n",
            "        rows = _prepend_row_index(rows, list(showindex))\n",
            "    elif showindex == \"always\" or (_bool(showindex) and not showindex_is_a_str):\n",
            "        if index is None:\n",
            "            index = list(range(len(rows)))\n",
            "        rows = _prepend_row_index(rows, index)\n",
            "    elif showindex == \"never\" or (not _bool(showindex) and not showindex_is_a_str):\n",
            "        pass\n",
            "\n",
            "    # pad with empty headers for initial columns if necessary\n",
            "    if headers and len(rows) > 0:\n",
            "       nhs = len(headers)\n",
            "       ncols = len(rows[0])\n",
            "       if nhs < ncols:\n",
            "           headers = [\"\"]*(ncols - nhs) + headers\n",
            "\n",
            "    return rows, headers\n",
            "\n",
            "\n",
            "\n",
            "def tabulate(tabular_data, headers=(), tablefmt=\"simple\",\n",
            "             floatfmt=_DEFAULT_FLOATFMT, numalign=\"decimal\", stralign=\"left\",\n",
            "             missingval=_DEFAULT_MISSINGVAL, showindex=\"default\", disable_numparse=False,\n",
            "             colalign=None):\n",
            "    \"\"\"Format a fixed width table for pretty printing.\n",
            "\n",
            "    >>> print(tabulate([[1, 2.34], [-56, \"8.999\"], [\"2\", \"10001\"]]))\n",
            "    ---  ---------\n",
            "      1      2.34\n",
            "    -56      8.999\n",
            "      2  10001\n",
            "    ---  ---------\n",
            "\n",
            "    The first required argument (`tabular_data`) can be a\n",
            "    list-of-lists (or another iterable of iterables), a list of named\n",
            "    tuples, a dictionary of iterables, an iterable of dictionaries,\n",
            "    a two-dimensional NumPy array, NumPy record array, or a Pandas'\n",
            "    dataframe.\n",
            "\n",
            "\n",
            "    Table headers\n",
            "    -------------\n",
            "\n",
            "    To print nice column headers, supply the second argument (`headers`):\n",
            "\n",
            "      - `headers` can be an explicit list of column headers\n",
            "      - if `headers=\"firstrow\"`, then the first row of data is used\n",
            "      - if `headers=\"keys\"`, then dictionary keys or column indices are used\n",
            "\n",
            "    Otherwise a headerless table is produced.\n",
            "\n",
            "    If the number of headers is less than the number of columns, they\n",
            "    are supposed to be names of the last columns. This is consistent\n",
            "    with the plain-text format of R and Pandas' dataframes.\n",
            "\n",
            "    >>> print(tabulate([[\"sex\",\"age\"],[\"Alice\",\"F\",24],[\"Bob\",\"M\",19]],\n",
            "    ...       headers=\"firstrow\"))\n",
            "           sex      age\n",
            "    -----  -----  -----\n",
            "    Alice  F         24\n",
            "    Bob    M         19\n",
            "\n",
            "    By default, pandas.DataFrame data have an additional column called\n",
            "    row index. To add a similar column to all other types of data,\n",
            "    use `showindex=\"always\"` or `showindex=True`. To suppress row indices\n",
            "    for all types of data, pass `showindex=\"never\" or `showindex=False`.\n",
            "    To add a custom row index column, pass `showindex=some_iterable`.\n",
            "\n",
            "    >>> print(tabulate([[\"F\",24],[\"M\",19]], showindex=\"always\"))\n",
            "    -  -  --\n",
            "    0  F  24\n",
            "    1  M  19\n",
            "    -  -  --\n",
            "\n",
            "\n",
            "    Column alignment\n",
            "    ----------------\n",
            "\n",
            "    `tabulate` tries to detect column types automatically, and aligns\n",
            "    the values properly. By default it aligns decimal points of the\n",
            "    numbers (or flushes integer numbers to the right), and flushes\n",
            "    everything else to the left. Possible column alignments\n",
            "    (`numalign`, `stralign`) are: \"right\", \"center\", \"left\", \"decimal\"\n",
            "    (only for `numalign`), and None (to disable alignment).\n",
            "\n",
            "\n",
            "    Table formats\n",
            "    -------------\n",
            "\n",
            "    `floatfmt` is a format specification used for columns which\n",
            "    contain numeric data with a decimal point. This can also be\n",
            "    a list or tuple of format strings, one per column.\n",
            "\n",
            "    `None` values are replaced with a `missingval` string (like\n",
            "    `floatfmt`, this can also be a list of values for different\n",
            "    columns):\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 1, None],\n",
            "    ...                 [\"eggs\", 42, 3.14],\n",
            "    ...                 [\"other\", None, 2.7]], missingval=\"?\"))\n",
            "    -----  --  ----\n",
            "    spam    1  ?\n",
            "    eggs   42  3.14\n",
            "    other   ?  2.7\n",
            "    -----  --  ----\n",
            "\n",
            "    Various plain-text table formats (`tablefmt`) are supported:\n",
            "    'plain', 'simple', 'grid', 'pipe', 'orgtbl', 'rst', 'mediawiki',\n",
            "    'latex', 'latex_raw' and 'latex_booktabs'. Variable `tabulate_formats`\n",
            "    contains the list of currently supported formats.\n",
            "\n",
            "    \"plain\" format doesn't use any pseudographics to draw tables,\n",
            "    it separates columns with a double space:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                 [\"strings\", \"numbers\"], \"plain\"))\n",
            "    strings      numbers\n",
            "    spam         41.9999\n",
            "    eggs        451\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"plain\"))\n",
            "    spam   41.9999\n",
            "    eggs  451\n",
            "\n",
            "    \"simple\" format is like Pandoc simple_tables:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                 [\"strings\", \"numbers\"], \"simple\"))\n",
            "    strings      numbers\n",
            "    ---------  ---------\n",
            "    spam         41.9999\n",
            "    eggs        451\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"simple\"))\n",
            "    ----  --------\n",
            "    spam   41.9999\n",
            "    eggs  451\n",
            "    ----  --------\n",
            "\n",
            "    \"grid\" is similar to tables produced by Emacs table.el package or\n",
            "    Pandoc grid_tables:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"grid\"))\n",
            "    +-----------+-----------+\n",
            "    | strings   |   numbers |\n",
            "    +===========+===========+\n",
            "    | spam      |   41.9999 |\n",
            "    +-----------+-----------+\n",
            "    | eggs      |  451      |\n",
            "    +-----------+-----------+\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"grid\"))\n",
            "    +------+----------+\n",
            "    | spam |  41.9999 |\n",
            "    +------+----------+\n",
            "    | eggs | 451      |\n",
            "    +------+----------+\n",
            "\n",
            "    \"fancy_grid\" draws a grid using box-drawing characters:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"fancy_grid\"))\n",
            "    \n",
            "     strings      numbers \n",
            "    \n",
            "     spam         41.9999 \n",
            "    \n",
            "     eggs        451      \n",
            "    \n",
            "\n",
            "    \"pipe\" is like tables in PHP Markdown Extra extension or Pandoc\n",
            "    pipe_tables:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"pipe\"))\n",
            "    | strings   |   numbers |\n",
            "    |:----------|----------:|\n",
            "    | spam      |   41.9999 |\n",
            "    | eggs      |  451      |\n",
            "\n",
            "    \"presto\" is like tables produce by the Presto CLI:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"presto\"))\n",
            "     strings   |   numbers\n",
            "    -----------+-----------\n",
            "     spam      |   41.9999\n",
            "     eggs      |  451\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"pipe\"))\n",
            "    |:-----|---------:|\n",
            "    | spam |  41.9999 |\n",
            "    | eggs | 451      |\n",
            "\n",
            "    \"orgtbl\" is like tables in Emacs org-mode and orgtbl-mode. They\n",
            "    are slightly different from \"pipe\" format by not using colons to\n",
            "    define column alignment, and using a \"+\" sign to indicate line\n",
            "    intersections:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"orgtbl\"))\n",
            "    | strings   |   numbers |\n",
            "    |-----------+-----------|\n",
            "    | spam      |   41.9999 |\n",
            "    | eggs      |  451      |\n",
            "\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"orgtbl\"))\n",
            "    | spam |  41.9999 |\n",
            "    | eggs | 451      |\n",
            "\n",
            "    \"rst\" is like a simple table format from reStructuredText; please\n",
            "    note that reStructuredText accepts also \"grid\" tables:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                [\"strings\", \"numbers\"], \"rst\"))\n",
            "    =========  =========\n",
            "    strings      numbers\n",
            "    =========  =========\n",
            "    spam         41.9999\n",
            "    eggs        451\n",
            "    =========  =========\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"rst\"))\n",
            "    ====  ========\n",
            "    spam   41.9999\n",
            "    eggs  451\n",
            "    ====  ========\n",
            "\n",
            "    \"mediawiki\" produces a table markup used in Wikipedia and on other\n",
            "    MediaWiki-based sites:\n",
            "\n",
            "    >>> print(tabulate([[\"strings\", \"numbers\"], [\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                headers=\"firstrow\", tablefmt=\"mediawiki\"))\n",
            "    {| class=\"wikitable\" style=\"text-align: left;\"\n",
            "    |+ <!-- caption -->\n",
            "    |-\n",
            "    ! strings   !! align=\"right\"|   numbers\n",
            "    |-\n",
            "    | spam      || align=\"right\"|   41.9999\n",
            "    |-\n",
            "    | eggs      || align=\"right\"|  451\n",
            "    |}\n",
            "\n",
            "    \"html\" produces HTML markup:\n",
            "\n",
            "    >>> print(tabulate([[\"strings\", \"numbers\"], [\"spam\", 41.9999], [\"eggs\", \"451.0\"]],\n",
            "    ...                headers=\"firstrow\", tablefmt=\"html\"))\n",
            "    <table>\n",
            "    <thead>\n",
            "    <tr><th>strings  </th><th style=\"text-align: right;\">  numbers</th></tr>\n",
            "    </thead>\n",
            "    <tbody>\n",
            "    <tr><td>spam     </td><td style=\"text-align: right;\">  41.9999</td></tr>\n",
            "    <tr><td>eggs     </td><td style=\"text-align: right;\"> 451     </td></tr>\n",
            "    </tbody>\n",
            "    </table>\n",
            "\n",
            "    \"latex\" produces a tabular environment of LaTeX document markup:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"latex\"))\n",
            "    \\\\begin{tabular}{lr}\n",
            "    \\\\hline\n",
            "     spam &  41.9999 \\\\\\\\\n",
            "     eggs & 451      \\\\\\\\\n",
            "    \\\\hline\n",
            "    \\\\end{tabular}\n",
            "\n",
            "    \"latex_raw\" is similar to \"latex\", but doesn't escape special characters,\n",
            "    such as backslash and underscore, so LaTeX commands may embedded into\n",
            "    cells' values:\n",
            "\n",
            "    >>> print(tabulate([[\"spam$_9$\", 41.9999], [\"\\\\\\\\emph{eggs}\", \"451.0\"]], tablefmt=\"latex_raw\"))\n",
            "    \\\\begin{tabular}{lr}\n",
            "    \\\\hline\n",
            "     spam$_9$    &  41.9999 \\\\\\\\\n",
            "     \\\\emph{eggs} & 451      \\\\\\\\\n",
            "    \\\\hline\n",
            "    \\\\end{tabular}\n",
            "\n",
            "    \"latex_booktabs\" produces a tabular environment of LaTeX document markup\n",
            "    using the booktabs.sty package:\n",
            "\n",
            "    >>> print(tabulate([[\"spam\", 41.9999], [\"eggs\", \"451.0\"]], tablefmt=\"latex_booktabs\"))\n",
            "    \\\\begin{tabular}{lr}\n",
            "    \\\\toprule\n",
            "     spam &  41.9999 \\\\\\\\\n",
            "     eggs & 451      \\\\\\\\\n",
            "    \\\\bottomrule\n",
            "    \\end{tabular}\n",
            "\n",
            "    Number parsing\n",
            "    --------------\n",
            "    By default, anything which can be parsed as a number is a number.\n",
            "    This ensures numbers represented as strings are aligned properly.\n",
            "    This can lead to weird results for particular strings such as\n",
            "    specific git SHAs e.g. \"42992e1\" will be parsed into the number\n",
            "    429920 and aligned as such.\n",
            "\n",
            "    To completely disable number parsing (and alignment), use\n",
            "    `disable_numparse=True`. For more fine grained control, a list column\n",
            "    indices is used to disable number parsing only on those columns\n",
            "    e.g. `disable_numparse=[0, 2]` would disable number parsing only on the\n",
            "    first and third columns.\n",
            "    \"\"\"\n",
            "    if tabular_data is None:\n",
            "        tabular_data = []\n",
            "    list_of_lists, headers = _normalize_tabular_data(\n",
            "            tabular_data, headers, showindex=showindex)\n",
            "\n",
            "    # empty values in the first column of RST tables should be escaped (issue #82)\n",
            "    # \"\" should be escaped as \"\\\\ \" or \"..\"\n",
            "    if tablefmt == 'rst':\n",
            "        list_of_lists, headers = _rst_escape_first_column(list_of_lists, headers)\n",
            "\n",
            "    # optimization: look for ANSI control codes once,\n",
            "    # enable smart width functions only if a control code is found\n",
            "    plain_text = '\\t'.join(['\\t'.join(map(_text_type, headers))] + \\\n",
            "                            ['\\t'.join(map(_text_type, row)) for row in list_of_lists])\n",
            "\n",
            "    has_invisible = re.search(_invisible_codes, plain_text)\n",
            "    enable_widechars = wcwidth is not None and WIDE_CHARS_MODE\n",
            "    if tablefmt in multiline_formats and _is_multiline(plain_text):\n",
            "        tablefmt = multiline_formats.get(tablefmt, tablefmt)\n",
            "        is_multiline = True\n",
            "    else:\n",
            "        is_multiline = False\n",
            "    width_fn = _choose_width_fn(has_invisible, enable_widechars, is_multiline)\n",
            "\n",
            "    # format rows and columns, convert numeric values to strings\n",
            "    cols = list(izip_longest(*list_of_lists))\n",
            "    numparses = _expand_numparse(disable_numparse, len(cols))\n",
            "    coltypes = [_column_type(col, numparse=np) for col, np in\n",
            "                zip(cols, numparses)]\n",
            "    if isinstance(floatfmt, basestring): #old version\n",
            "        float_formats = len(cols) * [floatfmt] # just duplicate the string to use in each column\n",
            "    else: # if floatfmt is list, tuple etc we have one per column\n",
            "        float_formats = list(floatfmt)\n",
            "        if len(float_formats) < len(cols):\n",
            "            float_formats.extend( (len(cols)-len(float_formats)) * [_DEFAULT_FLOATFMT] )\n",
            "    if isinstance(missingval, basestring):\n",
            "        missing_vals = len(cols) * [missingval]\n",
            "    else:\n",
            "        missing_vals = list(missingval)\n",
            "        if len(missing_vals) < len(cols):\n",
            "            missing_vals.extend( (len(cols)-len(missing_vals)) * [_DEFAULT_MISSINGVAL] )\n",
            "    cols = [[_format(v, ct, fl_fmt, miss_v, has_invisible) for v in c]\n",
            "             for c, ct, fl_fmt, miss_v in zip(cols, coltypes, float_formats, missing_vals)]\n",
            "\n",
            "    # align columns\n",
            "    aligns = [numalign if ct in [int,float] else stralign for ct in coltypes]\n",
            "    if colalign is not None:\n",
            "        assert isinstance(colalign, Iterable)\n",
            "        for idx, align in enumerate(colalign):\n",
            "            aligns[idx] = align\n",
            "    minwidths = [width_fn(h) + MIN_PADDING for h in headers] if headers else [0]*len(cols)\n",
            "    cols = [_align_column(c, a, minw, has_invisible, enable_widechars, is_multiline)\n",
            "            for c, a, minw in zip(cols, aligns, minwidths)]\n",
            "\n",
            "    if headers:\n",
            "        # align headers and add headers\n",
            "        t_cols = cols or [['']] * len(headers)\n",
            "        t_aligns = aligns or [stralign] * len(headers)\n",
            "        minwidths = [max(minw, max(width_fn(cl) for cl in c)) for minw, c in zip(minwidths, t_cols)]\n",
            "        headers = [_align_header(h, a, minw, width_fn(h), is_multiline, width_fn)\n",
            "                   for h, a, minw in zip(headers, t_aligns, minwidths)]\n",
            "        rows = list(zip(*cols))\n",
            "    else:\n",
            "        minwidths = [max(width_fn(cl) for cl in c) for c in cols]\n",
            "        rows = list(zip(*cols))\n",
            "\n",
            "    if not isinstance(tablefmt, TableFormat):\n",
            "        tablefmt = _table_formats.get(tablefmt, _table_formats[\"simple\"])\n",
            "\n",
            "    return _format_table(tablefmt, headers, rows, minwidths, aligns, is_multiline)\n",
            "\n",
            "\n",
            "def _expand_numparse(disable_numparse, column_count):\n",
            "    \"\"\"\n",
            "    Return a list of bools of length `column_count` which indicates whether\n",
            "    number parsing should be used on each column.\n",
            "    If `disable_numparse` is a list of indices, each of those indices are False,\n",
            "    and everything else is True.\n",
            "    If `disable_numparse` is a bool, then the returned list is all the same.\n",
            "    \"\"\"\n",
            "    if isinstance(disable_numparse, Iterable):\n",
            "        numparses = [True] * column_count\n",
            "        for index in disable_numparse:\n",
            "            numparses[index] = False\n",
            "        return numparses\n",
            "    else:\n",
            "        return [not disable_numparse] * column_count\n",
            "\n",
            "\n",
            "def _pad_row(cells, padding):\n",
            "    if cells:\n",
            "        pad = \" \"*padding\n",
            "        padded_cells = [pad + cell + pad for cell in cells]\n",
            "        return padded_cells\n",
            "    else:\n",
            "        return cells\n",
            "\n",
            "\n",
            "def _build_simple_row(padded_cells, rowfmt):\n",
            "    \"Format row according to DataRow format without padding.\"\n",
            "    begin, sep, end = rowfmt\n",
            "    return (begin + sep.join(padded_cells) + end).rstrip()\n",
            "\n",
            "\n",
            "def _build_row(padded_cells, colwidths, colaligns, rowfmt):\n",
            "    \"Return a string which represents a row of data cells.\"\n",
            "    if not rowfmt:\n",
            "        return None\n",
            "    if hasattr(rowfmt, \"__call__\"):\n",
            "        return rowfmt(padded_cells, colwidths, colaligns)\n",
            "    else:\n",
            "        return _build_simple_row(padded_cells, rowfmt)\n",
            "\n",
            "\n",
            "def _append_basic_row(lines, padded_cells, colwidths, colaligns, rowfmt):\n",
            "    lines.append(_build_row(padded_cells, colwidths, colaligns, rowfmt))\n",
            "    return lines\n",
            "\n",
            "\n",
            "def _append_multiline_row(lines, padded_multiline_cells, padded_widths, colaligns, rowfmt, pad):\n",
            "    colwidths = [w - 2*pad for w in padded_widths]\n",
            "    cells_lines = [c.splitlines() for c in padded_multiline_cells]\n",
            "    nlines = max(map(len, cells_lines)) # number of lines in the row\n",
            "    # vertically pad cells where some lines are missing\n",
            "    cells_lines = [(cl + [' '*w]*(nlines - len(cl))) for cl, w in zip(cells_lines, colwidths)]\n",
            "    lines_cells = [[cl[i] for cl in cells_lines] for i in range(nlines)]\n",
            "    for ln in lines_cells:\n",
            "        padded_ln = _pad_row(ln, pad)\n",
            "        _append_basic_row(lines, padded_ln, colwidths, colaligns, rowfmt)\n",
            "    return lines\n",
            "\n",
            "\n",
            "def _build_line(colwidths, colaligns, linefmt):\n",
            "    \"Return a string which represents a horizontal line.\"\n",
            "    if not linefmt:\n",
            "        return None\n",
            "    if hasattr(linefmt, \"__call__\"):\n",
            "        return linefmt(colwidths, colaligns)\n",
            "    else:\n",
            "        begin, fill, sep,  end = linefmt\n",
            "        cells = [fill*w for w in colwidths]\n",
            "        return _build_simple_row(cells, (begin, sep, end))\n",
            "\n",
            "\n",
            "def _append_line(lines, colwidths, colaligns, linefmt):\n",
            "    lines.append(_build_line(colwidths, colaligns, linefmt))\n",
            "    return lines\n",
            "\n",
            "\n",
            "def _format_table(fmt, headers, rows, colwidths, colaligns, is_multiline):\n",
            "    \"\"\"Produce a plain-text representation of the table.\"\"\"\n",
            "    lines = []\n",
            "    hidden = fmt.with_header_hide if (headers and fmt.with_header_hide) else []\n",
            "    pad = fmt.padding\n",
            "    headerrow = fmt.headerrow\n",
            "\n",
            "    padded_widths = [(w + 2*pad) for w in colwidths]\n",
            "    if is_multiline:\n",
            "        pad_row = lambda row, _: row  # do it later, in _append_multiline_row\n",
            "        append_row = partial(_append_multiline_row, pad=pad)\n",
            "    else:\n",
            "        pad_row = _pad_row\n",
            "        append_row = _append_basic_row\n",
            "\n",
            "    padded_headers = pad_row(headers, pad)\n",
            "    padded_rows = [pad_row(row, pad) for row in rows]\n",
            "\n",
            "    if fmt.lineabove and \"lineabove\" not in hidden:\n",
            "        _append_line(lines, padded_widths, colaligns, fmt.lineabove)\n",
            "\n",
            "    if padded_headers:\n",
            "        append_row(lines, padded_headers, padded_widths, colaligns, headerrow)\n",
            "        if fmt.linebelowheader and \"linebelowheader\" not in hidden:\n",
            "            _append_line(lines, padded_widths, colaligns, fmt.linebelowheader)\n",
            "\n",
            "    if padded_rows and fmt.linebetweenrows and \"linebetweenrows\" not in hidden:\n",
            "        # initial rows with a line below\n",
            "        for row in padded_rows[:-1]:\n",
            "            append_row(lines, row, padded_widths, colaligns, fmt.datarow)\n",
            "            _append_line(lines, padded_widths, colaligns, fmt.linebetweenrows)\n",
            "        # the last row without a line below\n",
            "        append_row(lines, padded_rows[-1], padded_widths, colaligns, fmt.datarow)\n",
            "    else:\n",
            "        for row in padded_rows:\n",
            "            append_row(lines, row, padded_widths, colaligns, fmt.datarow)\n",
            "\n",
            "    if fmt.linebelow and \"linebelow\" not in hidden:\n",
            "        _append_line(lines, padded_widths, colaligns, fmt.linebelow)\n",
            "\n",
            "    if headers or rows:\n",
            "        return \"\\n\".join(lines)\n",
            "    else: # a completely empty table\n",
            "        return \"\"\n",
            "\n",
            "\n",
            "def _main():\n",
            "    \"\"\"\\\n",
            "    Usage: tabulate [options] [FILE ...]\n",
            "\n",
            "    Pretty-print tabular data.\n",
            "    See also https://bitbucket.org/astanin/python-tabulate\n",
            "\n",
            "    FILE                      a filename of the file with tabular data;\n",
            "                              if \"-\" or missing, read data from stdin.\n",
            "\n",
            "    Options:\n",
            "\n",
            "    -h, --help                show this message\n",
            "    -1, --header              use the first row of data as a table header\n",
            "    -o FILE, --output FILE    print table to FILE (default: stdout)\n",
            "    -s REGEXP, --sep REGEXP   use a custom column separator (default: whitespace)\n",
            "    -F FPFMT, --float FPFMT   floating point number format (default: g)\n",
            "    -f FMT, --format FMT      set output table format; supported formats:\n",
            "                              plain, simple, grid, fancy_grid, pipe, orgtbl,\n",
            "                              rst, mediawiki, html, latex, latex_raw,\n",
            "                              latex_booktabs, tsv\n",
            "                              (default: simple)\n",
            "    \"\"\"\n",
            "    import getopt\n",
            "    import sys\n",
            "    import textwrap\n",
            "    usage = textwrap.dedent(_main.__doc__)\n",
            "    try:\n",
            "        opts, args = getopt.getopt(sys.argv[1:],\n",
            "                     \"h1o:s:F:A:f:\",\n",
            "                     [\"help\", \"header\", \"output\", \"sep=\", \"float=\", \"align=\",\n",
            "                      \"format=\"])\n",
            "    except getopt.GetoptError as e:\n",
            "        print(e)\n",
            "        print(usage)\n",
            "        sys.exit(2)\n",
            "    headers = []\n",
            "    floatfmt = _DEFAULT_FLOATFMT\n",
            "    colalign = None\n",
            "    tablefmt = \"simple\"\n",
            "    sep = r\"\\s+\"\n",
            "    outfile = \"-\"\n",
            "    for opt, value in opts:\n",
            "        if opt in [\"-1\", \"--header\"]:\n",
            "            headers = \"firstrow\"\n",
            "        elif opt in [\"-o\", \"--output\"]:\n",
            "            outfile = value\n",
            "        elif opt in [\"-F\", \"--float\"]:\n",
            "            floatfmt = value\n",
            "        elif opt in [\"-C\", \"--colalign\"]:\n",
            "            colalign = value.split()\n",
            "        elif opt in [\"-f\", \"--format\"]:\n",
            "            if value not in tabulate_formats:\n",
            "                print(\"%s is not a supported table format\" % value)\n",
            "                print(usage)\n",
            "                sys.exit(3)\n",
            "            tablefmt = value\n",
            "        elif opt in [\"-s\", \"--sep\"]:\n",
            "            sep = value\n",
            "        elif opt in [\"-h\", \"--help\"]:\n",
            "            print(usage)\n",
            "            sys.exit(0)\n",
            "    files = [sys.stdin] if not args else args\n",
            "    with (sys.stdout if outfile == \"-\" else open(outfile, \"w\")) as out:\n",
            "        for f in files:\n",
            "            if f == \"-\":\n",
            "                f = sys.stdin\n",
            "            if _is_file(f):\n",
            "                _pprint_file(f, headers=headers, tablefmt=tablefmt,\n",
            "                             sep=sep, floatfmt=floatfmt, file=out,\n",
            "                             colalign=colalign)\n",
            "            else:\n",
            "                with open(f) as fobj:\n",
            "                    _pprint_file(fobj, headers=headers, tablefmt=tablefmt,\n",
            "                                 sep=sep, floatfmt=floatfmt, file=out,\n",
            "                                 colalign=colalign)\n",
            "\n",
            "\n",
            "def _pprint_file(fobject, headers, tablefmt, sep, floatfmt, file, colalign):\n",
            "    rows = fobject.readlines()\n",
            "    table = [re.split(sep, r.rstrip()) for r in rows if r.strip()]\n",
            "    print(tabulate(table, headers, tablefmt, floatfmt=floatfmt,\n",
            "          colalign=colalign), file=file)\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    _main()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#!/usr/bin/env python\n",
            "\n",
            "\"\"\" PickleShare - a small 'shelve' like datastore with concurrency support\n",
            "\n",
            "Like shelve, a PickleShareDB object acts like a normal dictionary. Unlike\n",
            "shelve, many processes can access the database simultaneously. Changing a\n",
            "value in database is immediately visible to other processes accessing the\n",
            "same database.\n",
            "\n",
            "Concurrency is possible because the values are stored in separate files. Hence\n",
            "the \"database\" is a directory where *all* files are governed by PickleShare.\n",
            "\n",
            "Example usage::\n",
            "\n",
            "    from pickleshare import *\n",
            "    db = PickleShareDB('~/testpickleshare')\n",
            "    db.clear()\n",
            "    print \"Should be empty:\",db.items()\n",
            "    db['hello'] = 15\n",
            "    db['aku ankka'] = [1,2,313]\n",
            "    db['paths/are/ok/key'] = [1,(5,46)]\n",
            "    print db.keys()\n",
            "    del db['aku ankka']\n",
            "\n",
            "This module is certainly not ZODB, but can be used for low-load\n",
            "(non-mission-critical) situations where tiny code size trumps the\n",
            "advanced features of a \"real\" object database.\n",
            "\n",
            "Installation guide: pip install pickleshare\n",
            "\n",
            "Author: Ville Vainio <vivainio@gmail.com>\n",
            "License: MIT open source license.\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "\n",
            "__version__ = \"0.7.5\"\n",
            "\n",
            "try:\n",
            "    from pathlib import Path\n",
            "except ImportError:\n",
            "    # Python 2 backport\n",
            "    from pathlib2 import Path\n",
            "\n",
            "import os,stat,time\n",
            "try:\n",
            "    import collections.abc as collections_abc\n",
            "except ImportError:\n",
            "    import collections as collections_abc\n",
            "try:\n",
            "    import cPickle as pickle\n",
            "except ImportError:\n",
            "    import pickle\n",
            "import errno\n",
            "import sys\n",
            "\n",
            "if sys.version_info[0] >= 3:\n",
            "    string_types = (str,)\n",
            "else:\n",
            "    string_types = (str, unicode)\n",
            "\n",
            "def gethashfile(key):\n",
            "    return (\"%02x\" % abs(hash(key) % 256))[-2:]\n",
            "\n",
            "_sentinel = object()\n",
            "\n",
            "class PickleShareDB(collections_abc.MutableMapping):\n",
            "    \"\"\" The main 'connection' object for PickleShare database \"\"\"\n",
            "    def __init__(self,root):\n",
            "        \"\"\" Return a db object that will manage the specied directory\"\"\"\n",
            "        if not isinstance(root, string_types):\n",
            "            root = str(root)\n",
            "        root = os.path.abspath(os.path.expanduser(root))\n",
            "        self.root = Path(root)\n",
            "        if not self.root.is_dir():\n",
            "            # catching the exception is necessary if multiple processes are concurrently trying to create a folder\n",
            "            # exists_ok keyword argument of mkdir does the same but only from Python 3.5\n",
            "            try:\n",
            "                self.root.mkdir(parents=True)\n",
            "            except OSError as e:\n",
            "                if e.errno != errno.EEXIST:\n",
            "                    raise\n",
            "        # cache has { 'key' : (obj, orig_mod_time) }\n",
            "        self.cache = {}\n",
            "\n",
            "\n",
            "    def __getitem__(self,key):\n",
            "        \"\"\" db['key'] reading \"\"\"\n",
            "        fil = self.root / key\n",
            "        try:\n",
            "            mtime = (fil.stat()[stat.ST_MTIME])\n",
            "        except OSError:\n",
            "            raise KeyError(key)\n",
            "\n",
            "        if fil in self.cache and mtime == self.cache[fil][1]:\n",
            "            return self.cache[fil][0]\n",
            "        try:\n",
            "            # The cached item has expired, need to read\n",
            "            with fil.open(\"rb\") as f:\n",
            "                obj = pickle.loads(f.read())\n",
            "        except:\n",
            "            raise KeyError(key)\n",
            "\n",
            "        self.cache[fil] = (obj,mtime)\n",
            "        return obj\n",
            "\n",
            "    def __setitem__(self,key,value):\n",
            "        \"\"\" db['key'] = 5 \"\"\"\n",
            "        fil = self.root / key\n",
            "        parent = fil.parent\n",
            "        if parent and not parent.is_dir():\n",
            "            parent.mkdir(parents=True)\n",
            "        # We specify protocol 2, so that we can mostly go between Python 2\n",
            "        # and Python 3. We can upgrade to protocol 3 when Python 2 is obsolete.\n",
            "        with fil.open('wb') as f:\n",
            "            pickle.dump(value, f, protocol=2)\n",
            "        try:\n",
            "            self.cache[fil] = (value, fil.stat().st_mtime)\n",
            "        except OSError as e:\n",
            "            if e.errno != errno.ENOENT:\n",
            "                raise\n",
            "\n",
            "    def hset(self, hashroot, key, value):\n",
            "        \"\"\" hashed set \"\"\"\n",
            "        hroot = self.root / hashroot\n",
            "        if not hroot.is_dir():\n",
            "            hroot.mkdir()\n",
            "        hfile = hroot / gethashfile(key)\n",
            "        d = self.get(hfile, {})\n",
            "        d.update( {key : value})\n",
            "        self[hfile] = d\n",
            "\n",
            "\n",
            "\n",
            "    def hget(self, hashroot, key, default = _sentinel, fast_only = True):\n",
            "        \"\"\" hashed get \"\"\"\n",
            "        hroot = self.root / hashroot\n",
            "        hfile = hroot / gethashfile(key)\n",
            "\n",
            "        d = self.get(hfile, _sentinel )\n",
            "        #print \"got dict\",d,\"from\",hfile\n",
            "        if d is _sentinel:\n",
            "            if fast_only:\n",
            "                if default is _sentinel:\n",
            "                    raise KeyError(key)\n",
            "\n",
            "                return default\n",
            "\n",
            "            # slow mode ok, works even after hcompress()\n",
            "            d = self.hdict(hashroot)\n",
            "\n",
            "        return d.get(key, default)\n",
            "\n",
            "    def hdict(self, hashroot):\n",
            "        \"\"\" Get all data contained in hashed category 'hashroot' as dict \"\"\"\n",
            "        hfiles = self.keys(hashroot + \"/*\")\n",
            "        hfiles.sort()\n",
            "        last = len(hfiles) and hfiles[-1] or ''\n",
            "        if last.endswith('xx'):\n",
            "            # print \"using xx\"\n",
            "            hfiles = [last] + hfiles[:-1]\n",
            "\n",
            "        all = {}\n",
            "\n",
            "        for f in hfiles:\n",
            "            # print \"using\",f\n",
            "            try:\n",
            "                all.update(self[f])\n",
            "            except KeyError:\n",
            "                print(\"Corrupt\",f,\"deleted - hset is not threadsafe!\")\n",
            "                del self[f]\n",
            "\n",
            "            self.uncache(f)\n",
            "\n",
            "        return all\n",
            "\n",
            "    def hcompress(self, hashroot):\n",
            "        \"\"\" Compress category 'hashroot', so hset is fast again\n",
            "\n",
            "        hget will fail if fast_only is True for compressed items (that were\n",
            "        hset before hcompress).\n",
            "\n",
            "        \"\"\"\n",
            "        hfiles = self.keys(hashroot + \"/*\")\n",
            "        all = {}\n",
            "        for f in hfiles:\n",
            "            # print \"using\",f\n",
            "            all.update(self[f])\n",
            "            self.uncache(f)\n",
            "\n",
            "        self[hashroot + '/xx'] = all\n",
            "        for f in hfiles:\n",
            "            p = self.root / f\n",
            "            if p.name == 'xx':\n",
            "                continue\n",
            "            p.unlink()\n",
            "\n",
            "\n",
            "\n",
            "    def __delitem__(self,key):\n",
            "        \"\"\" del db[\"key\"] \"\"\"\n",
            "        fil = self.root / key\n",
            "        self.cache.pop(fil,None)\n",
            "        try:\n",
            "            fil.unlink()\n",
            "        except OSError:\n",
            "            # notfound and permission denied are ok - we\n",
            "            # lost, the other process wins the conflict\n",
            "            pass\n",
            "\n",
            "    def _normalized(self, p):\n",
            "        \"\"\" Make a key suitable for user's eyes \"\"\"\n",
            "        return str(p.relative_to(self.root)).replace('\\\\','/')\n",
            "\n",
            "    def keys(self, globpat = None):\n",
            "        \"\"\" All keys in DB, or all keys matching a glob\"\"\"\n",
            "\n",
            "        if globpat is None:\n",
            "            files = self.root.rglob('*')\n",
            "        else:\n",
            "            files = self.root.glob(globpat)\n",
            "        return [self._normalized(p) for p in files if p.is_file()]\n",
            "\n",
            "    def __iter__(self):\n",
            "        return iter(self.keys())\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.keys())\n",
            "\n",
            "    def uncache(self,*items):\n",
            "        \"\"\" Removes all, or specified items from cache\n",
            "\n",
            "        Use this after reading a large amount of large objects\n",
            "        to free up memory, when you won't be needing the objects\n",
            "        for a while.\n",
            "\n",
            "        \"\"\"\n",
            "        if not items:\n",
            "            self.cache = {}\n",
            "        for it in items:\n",
            "            self.cache.pop(it,None)\n",
            "\n",
            "    def waitget(self,key, maxwaittime = 60 ):\n",
            "        \"\"\" Wait (poll) for a key to get a value\n",
            "\n",
            "        Will wait for `maxwaittime` seconds before raising a KeyError.\n",
            "        The call exits normally if the `key` field in db gets a value\n",
            "        within the timeout period.\n",
            "\n",
            "        Use this for synchronizing different processes or for ensuring\n",
            "        that an unfortunately timed \"db['key'] = newvalue\" operation\n",
            "        in another process (which causes all 'get' operation to cause a\n",
            "        KeyError for the duration of pickling) won't screw up your program\n",
            "        logic.\n",
            "        \"\"\"\n",
            "\n",
            "        wtimes = [0.2] * 3 + [0.5] * 2 + [1]\n",
            "        tries = 0\n",
            "        waited = 0\n",
            "        while 1:\n",
            "            try:\n",
            "                val = self[key]\n",
            "                return val\n",
            "            except KeyError:\n",
            "                pass\n",
            "\n",
            "            if waited > maxwaittime:\n",
            "                raise KeyError(key)\n",
            "\n",
            "            time.sleep(wtimes[tries])\n",
            "            waited+=wtimes[tries]\n",
            "            if tries < len(wtimes) -1:\n",
            "                tries+=1\n",
            "\n",
            "    def getlink(self,folder):\n",
            "        \"\"\" Get a convenient link for accessing items  \"\"\"\n",
            "        return PickleShareLink(self, folder)\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"PickleShareDB('%s')\" % self.root\n",
            "\n",
            "\n",
            "\n",
            "class PickleShareLink:\n",
            "    \"\"\" A shortdand for accessing nested PickleShare data conveniently.\n",
            "\n",
            "    Created through PickleShareDB.getlink(), example::\n",
            "\n",
            "        lnk = db.getlink('myobjects/test')\n",
            "        lnk.foo = 2\n",
            "        lnk.bar = lnk.foo + 5\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, db, keydir ):\n",
            "        self.__dict__.update(locals())\n",
            "\n",
            "    def __getattr__(self,key):\n",
            "        return self.__dict__['db'][self.__dict__['keydir']+'/' + key]\n",
            "    def __setattr__(self,key,val):\n",
            "        self.db[self.keydir+'/' + key] = val\n",
            "    def __repr__(self):\n",
            "        db = self.__dict__['db']\n",
            "        keys = db.keys( self.__dict__['keydir'] +\"/*\")\n",
            "        return \"<PickleShareLink '%s': %s>\" % (\n",
            "            self.__dict__['keydir'],\n",
            "            \";\".join([Path(k).basename() for k in keys]))\n",
            "\n",
            "def main():\n",
            "    import textwrap\n",
            "    usage = textwrap.dedent(\"\"\"\\\n",
            "    pickleshare - manage PickleShare databases\n",
            "\n",
            "    Usage:\n",
            "\n",
            "        pickleshare dump /path/to/db > dump.txt\n",
            "        pickleshare load /path/to/db < dump.txt\n",
            "        pickleshare test /path/to/db\n",
            "    \"\"\")\n",
            "    DB = PickleShareDB\n",
            "    import sys\n",
            "    if len(sys.argv) < 2:\n",
            "        print(usage)\n",
            "        return\n",
            "\n",
            "    cmd = sys.argv[1]\n",
            "    args = sys.argv[2:]\n",
            "    if cmd == 'dump':\n",
            "        if not args: args= ['.']\n",
            "        db = DB(args[0])\n",
            "        import pprint\n",
            "        pprint.pprint(db.items())\n",
            "    elif cmd == 'load':\n",
            "        cont = sys.stdin.read()\n",
            "        db = DB(args[0])\n",
            "        data = eval(cont)\n",
            "        db.clear()\n",
            "        for k,v in db.items():\n",
            "            db[k] = v\n",
            "    elif cmd == 'testwait':\n",
            "        db = DB(args[0])\n",
            "        db.clear()\n",
            "        print(db.waitget('250'))\n",
            "    elif cmd == 'test':\n",
            "        test()\n",
            "        stress()\n",
            "\n",
            "if __name__== \"__main__\":\n",
            "    main()\n",
            "\n",
            "\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# this file is derived from lunar project.\n",
            "#\n",
            "# lunar project:\n",
            "#   Copyright (C) 1988,1989,1991,1992,2001 Fung F. Lee and Ricky Yeung\n",
            "#   Licensed under GPLv2.\n",
            "#\n",
            "# Copyright (C) 2008 LI Daobing <lidaobing@gmail.com>\n",
            "\n",
            "'''\n",
            "A Chinese Calendar Library in Pure Python\n",
            "=========================================\n",
            "\n",
            "Chinese Calendar: http://en.wikipedia.org/wiki/Chinese_calendar\n",
            "\n",
            "Usage\n",
            "-----\n",
            "        >>> LunarDate.fromSolarDate(1976, 10, 1)\n",
            "        LunarDate(1976, 8, 8, 1)\n",
            "        >>> LunarDate(1976, 8, 8, 1).toSolarDate()\n",
            "        datetime.date(1976, 10, 1)\n",
            "        >>> LunarDate(1976, 8, 8, 1).year\n",
            "        1976\n",
            "        >>> LunarDate(1976, 8, 8, 1).month\n",
            "        8\n",
            "        >>> LunarDate(1976, 8, 8, 1).day\n",
            "        8\n",
            "        >>> LunarDate(1976, 8, 8, 1).isLeapMonth\n",
            "        True\n",
            "\n",
            "        >>> today = LunarDate.today()\n",
            "        >>> type(today).__name__\n",
            "        'LunarDate'\n",
            "\n",
            "        >>> # support '+' and '-' between datetime.date and datetime.timedelta\n",
            "        >>> ld = LunarDate(1976,8,8)\n",
            "        >>> sd = datetime.date(2008,1,1)\n",
            "        >>> td = datetime.timedelta(days=10)\n",
            "        >>> ld-ld\n",
            "        datetime.timedelta(0)\n",
            "        >>> (ld-sd).days\n",
            "        -11444\n",
            "        >>> ld-td\n",
            "        LunarDate(1976, 7, 27, 0)\n",
            "        >>> (sd-ld).days\n",
            "        11444\n",
            "        >>> ld+td\n",
            "        LunarDate(1976, 8, 18, 0)\n",
            "        >>> td+ld\n",
            "        LunarDate(1976, 8, 18, 0)\n",
            "        >>> ld2 = LunarDate.today()\n",
            "        >>> ld < ld2\n",
            "        True\n",
            "        >>> ld <= ld2\n",
            "        True\n",
            "        >>> ld > ld2\n",
            "        False\n",
            "        >>> ld >= ld2\n",
            "        False\n",
            "        >>> ld == ld2\n",
            "        False\n",
            "        >>> ld != ld2\n",
            "        True\n",
            "        >>> ld == ld\n",
            "        True\n",
            "        >>> LunarDate.today() == LunarDate.today()\n",
            "        True\n",
            "        >>> before_leap_month = LunarDate.fromSolarDate(2088, 5, 17)\n",
            "        >>> before_leap_month.year\n",
            "        2088\n",
            "        >>> before_leap_month.month\n",
            "        4\n",
            "        >>> before_leap_month.day\n",
            "        27\n",
            "        >>> before_leap_month.isLeapMonth\n",
            "        False\n",
            "        >>> leap_month = LunarDate.fromSolarDate(2088, 6, 17)\n",
            "        >>> leap_month.year\n",
            "        2088\n",
            "        >>> leap_month.month\n",
            "        4\n",
            "        >>> leap_month.day\n",
            "        28\n",
            "        >>> leap_month.isLeapMonth\n",
            "        True\n",
            "        >>> after_leap_month = LunarDate.fromSolarDate(2088, 7, 17)\n",
            "        >>> after_leap_month.year\n",
            "        2088\n",
            "        >>> after_leap_month.month\n",
            "        5\n",
            "        >>> after_leap_month.day\n",
            "        29\n",
            "        >>> after_leap_month.isLeapMonth\n",
            "        False\n",
            "\n",
            "Limits\n",
            "------\n",
            "\n",
            "this library can only deal with year from 1900 to 2099 (in chinese calendar).\n",
            "\n",
            "See also\n",
            "--------\n",
            "\n",
            "* lunar: http://packages.qa.debian.org/l/lunar.html,\n",
            "  A converter written in C, this program is derived from it.\n",
            "* python-lunar: http://code.google.com/p/liblunar/\n",
            "  Another library written in C, including a python binding.\n",
            "'''\n",
            "\n",
            "import datetime\n",
            "\n",
            "__version__ = \"0.2.0\"\n",
            "__all__ = ['LunarDate']\n",
            "\n",
            "class LunarDate(object):\n",
            "    _startDate = datetime.date(1900, 1, 31)\n",
            "\n",
            "    def __init__(self, year, month, day, isLeapMonth=False):\n",
            "        self.year = year\n",
            "        self.month = month\n",
            "        self.day = day\n",
            "        self.isLeapMonth = bool(isLeapMonth)\n",
            "\n",
            "    def __str__(self):\n",
            "        return 'LunarDate(%d, %d, %d, %d)' % (self.year, self.month, self.day, self.isLeapMonth)\n",
            "\n",
            "    __repr__ = __str__\n",
            "\n",
            "    @staticmethod\n",
            "    def fromSolarDate(year, month, day):\n",
            "        '''\n",
            "        >>> LunarDate.fromSolarDate(1900, 1, 31)\n",
            "        LunarDate(1900, 1, 1, 0)\n",
            "        >>> LunarDate.fromSolarDate(2008, 10, 2)\n",
            "        LunarDate(2008, 9, 4, 0)\n",
            "        >>> LunarDate.fromSolarDate(1976, 10, 1)\n",
            "        LunarDate(1976, 8, 8, 1)\n",
            "        >>> LunarDate.fromSolarDate(2033, 10, 23)\n",
            "        LunarDate(2033, 10, 1, 0)\n",
            "        '''\n",
            "        solarDate = datetime.date(year, month, day)\n",
            "        offset = (solarDate - LunarDate._startDate).days\n",
            "        return LunarDate._fromOffset(offset)\n",
            "\n",
            "    def toSolarDate(self):\n",
            "        '''\n",
            "        >>> LunarDate(1900, 1, 1).toSolarDate()\n",
            "        datetime.date(1900, 1, 31)\n",
            "        >>> LunarDate(2008, 9, 4).toSolarDate()\n",
            "        datetime.date(2008, 10, 2)\n",
            "        >>> LunarDate(1976, 8, 8, 1).toSolarDate()\n",
            "        datetime.date(1976, 10, 1)\n",
            "        >>> LunarDate(2004, 1, 30).toSolarDate()\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        ValueError: day out of range\n",
            "        >>> LunarDate(2004, 13, 1).toSolarDate()\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        ValueError: month out of range\n",
            "        >>> LunarDate(2100, 1, 1).toSolarDate()\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        ValueError: year out of range [1900, 2100)\n",
            "        >>>\n",
            "        '''\n",
            "        def _calcDays(yearInfo, month, day, isLeapMonth):\n",
            "            isLeapMonth = int(isLeapMonth)\n",
            "            res = 0\n",
            "            ok = False\n",
            "            for _month, _days, _isLeapMonth in self._enumMonth(yearInfo):\n",
            "                if (_month, _isLeapMonth) == (month, isLeapMonth):\n",
            "                    if 1 <= day <= _days:\n",
            "                        res += day - 1\n",
            "                        return res\n",
            "                    else:\n",
            "                        raise ValueError(\"day out of range\")\n",
            "                res += _days\n",
            "\n",
            "            raise ValueError(\"month out of range\")\n",
            "\n",
            "        offset = 0\n",
            "        start_year = 1900\n",
            "        end_year = start_year + len(yearInfos)\n",
            "        if start_year < 1900 or self.year >= end_year:\n",
            "            raise ValueError('year out of range [{}, {})'.format(start_year, end_year))\n",
            "        yearIdx = self.year - 1900\n",
            "        for i in range(yearIdx):\n",
            "            offset += yearDays[i]\n",
            "\n",
            "        offset += _calcDays(yearInfos[yearIdx], self.month, self.day, self.isLeapMonth)\n",
            "        return self._startDate + datetime.timedelta(days=offset)\n",
            "\n",
            "    def __sub__(self, other):\n",
            "        if isinstance(other, LunarDate):\n",
            "            return self.toSolarDate() - other.toSolarDate()\n",
            "        elif isinstance(other, datetime.date):\n",
            "            return self.toSolarDate() - other\n",
            "        elif isinstance(other, datetime.timedelta):\n",
            "            res = self.toSolarDate() - other\n",
            "            return LunarDate.fromSolarDate(res.year, res.month, res.day)\n",
            "        raise TypeError\n",
            "\n",
            "    def __rsub__(self, other):\n",
            "        if isinstance(other, datetime.date):\n",
            "            return other - self.toSolarDate()\n",
            "\n",
            "    def __add__(self, other):\n",
            "        if isinstance(other, datetime.timedelta):\n",
            "            res = self.toSolarDate() + other\n",
            "            return LunarDate.fromSolarDate(res.year, res.month, res.day)\n",
            "        raise TypeError\n",
            "\n",
            "    def __radd__(self, other):\n",
            "        return self + other\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        '''\n",
            "        >>> LunarDate.today() == 5\n",
            "        False\n",
            "        '''\n",
            "        if not isinstance(other, LunarDate):\n",
            "            return False\n",
            "\n",
            "        return self - other == datetime.timedelta(0)\n",
            "\n",
            "    def __lt__(self, other):\n",
            "        '''\n",
            "        >>> LunarDate.today() < LunarDate.today()\n",
            "        False\n",
            "        >>> LunarDate.today() < 5\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        TypeError: can't compare LunarDate to int\n",
            "        '''\n",
            "        try:\n",
            "            return self - other < datetime.timedelta(0)\n",
            "        except TypeError:\n",
            "            raise TypeError(\"can't compare LunarDate to %s\" % (type(other).__name__,))\n",
            "\n",
            "    def __le__(self, other):\n",
            "        # needed because the default implementation tries equality first,\n",
            "        # and that does not throw a type error\n",
            "        return self < other or self == other\n",
            "\n",
            "    def __gt__(self, other):\n",
            "        '''\n",
            "        >>> LunarDate.today() > LunarDate.today()\n",
            "        False\n",
            "        >>> LunarDate.today() > 5\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        TypeError: can't compare LunarDate to int\n",
            "        '''\n",
            "        return not self <= other\n",
            "\n",
            "    def __ge__(self, other):\n",
            "        '''\n",
            "        >>> LunarDate.today() >= LunarDate.today()\n",
            "        True\n",
            "        >>> LunarDate.today() >= 5\n",
            "        Traceback (most recent call last):\n",
            "        ...\n",
            "        TypeError: can't compare LunarDate to int\n",
            "        '''\n",
            "        return not self < other\n",
            "\n",
            "    @classmethod\n",
            "    def today(cls):\n",
            "        res = datetime.date.today()\n",
            "        return cls.fromSolarDate(res.year, res.month, res.day)\n",
            "\n",
            "    @staticmethod\n",
            "    def _enumMonth(yearInfo):\n",
            "        months = [(i, 0) for i in range(1, 13)]\n",
            "        leapMonth = yearInfo % 16\n",
            "        if leapMonth == 0:\n",
            "            pass\n",
            "        elif leapMonth <= 12:\n",
            "            months.insert(leapMonth, (leapMonth, 1))\n",
            "        else:\n",
            "            raise ValueError(\"yearInfo %r mod 16 should in [0, 12]\" % yearInfo)\n",
            "\n",
            "        for month, isLeapMonth in months:\n",
            "            if isLeapMonth:\n",
            "                days = (yearInfo >> 16) % 2 + 29\n",
            "            else:\n",
            "                days = (yearInfo >> (16 - month)) % 2 + 29\n",
            "            yield month, days, isLeapMonth\n",
            "\n",
            "    @classmethod\n",
            "    def _fromOffset(cls, offset):\n",
            "        def _calcMonthDay(yearInfo, offset):\n",
            "            for month, days, isLeapMonth in cls._enumMonth(yearInfo):\n",
            "                if offset < days:\n",
            "                    break\n",
            "                offset -= days\n",
            "            return (month, offset + 1, isLeapMonth)\n",
            "\n",
            "        offset = int(offset)\n",
            "\n",
            "        for idx, yearDay in enumerate(yearDays):\n",
            "            if offset < yearDay:\n",
            "                break\n",
            "            offset -= yearDay\n",
            "        year = 1900 + idx\n",
            "\n",
            "        yearInfo = yearInfos[idx]\n",
            "        month, day, isLeapMonth = _calcMonthDay(yearInfo, offset)\n",
            "        return LunarDate(year, month, day, isLeapMonth)\n",
            "\n",
            "yearInfos = [\n",
            "        #    /* encoding:\n",
            "        #               b bbbbbbbbbbbb bbbb\n",
            "        #       bit#    1 111111000000 0000\n",
            "        #               6 543210987654 3210\n",
            "        #               . ............ ....\n",
            "        #       month#    000000000111\n",
            "        #               M 123456789012   L\n",
            "        #\n",
            "        #    b_j = 1 for long month, b_j = 0 for short month\n",
            "        #    L is the leap month of the year if 1<=L<=12; NO leap month if L = 0.\n",
            "        #    The leap month (if exists) is long one iff M = 1.\n",
            "        #    */\n",
            "        0x04bd8,                                    #   /* 1900 */\n",
            "        0x04ae0, 0x0a570, 0x054d5, 0x0d260, 0x0d950,#   /* 1905 */\n",
            "        0x16554, 0x056a0, 0x09ad0, 0x055d2, 0x04ae0,#   /* 1910 */\n",
            "        0x0a5b6, 0x0a4d0, 0x0d250, 0x1d255, 0x0b540,#   /* 1915 */\n",
            "        0x0d6a0, 0x0ada2, 0x095b0, 0x14977, 0x04970,#   /* 1920 */\n",
            "        0x0a4b0, 0x0b4b5, 0x06a50, 0x06d40, 0x1ab54,#   /* 1925 */\n",
            "        0x02b60, 0x09570, 0x052f2, 0x04970, 0x06566,#   /* 1930 */\n",
            "        0x0d4a0, 0x0ea50, 0x06e95, 0x05ad0, 0x02b60,#   /* 1935 */\n",
            "        0x186e3, 0x092e0, 0x1c8d7, 0x0c950, 0x0d4a0,#   /* 1940 */\n",
            "        0x1d8a6, 0x0b550, 0x056a0, 0x1a5b4, 0x025d0,#   /* 1945 */\n",
            "        0x092d0, 0x0d2b2, 0x0a950, 0x0b557, 0x06ca0,#   /* 1950 */\n",
            "        0x0b550, 0x15355, 0x04da0, 0x0a5d0, 0x14573,#   /* 1955 */\n",
            "        0x052d0, 0x0a9a8, 0x0e950, 0x06aa0, 0x0aea6,#   /* 1960 */\n",
            "        0x0ab50, 0x04b60, 0x0aae4, 0x0a570, 0x05260,#   /* 1965 */\n",
            "        0x0f263, 0x0d950, 0x05b57, 0x056a0, 0x096d0,#   /* 1970 */\n",
            "        0x04dd5, 0x04ad0, 0x0a4d0, 0x0d4d4, 0x0d250,#   /* 1975 */\n",
            "        0x0d558, 0x0b540, 0x0b5a0, 0x195a6, 0x095b0,#   /* 1980 */\n",
            "        0x049b0, 0x0a974, 0x0a4b0, 0x0b27a, 0x06a50,#   /* 1985 */\n",
            "        0x06d40, 0x0af46, 0x0ab60, 0x09570, 0x04af5,#   /* 1990 */\n",
            "        0x04970, 0x064b0, 0x074a3, 0x0ea50, 0x06b58,#   /* 1995 */\n",
            "        0x05ac0, 0x0ab60, 0x096d5, 0x092e0, 0x0c960,#   /* 2000 */\n",
            "        0x0d954, 0x0d4a0, 0x0da50, 0x07552, 0x056a0,#   /* 2005 */\n",
            "        0x0abb7, 0x025d0, 0x092d0, 0x0cab5, 0x0a950,#   /* 2010 */\n",
            "        0x0b4a0, 0x0baa4, 0x0ad50, 0x055d9, 0x04ba0,#   /* 2015 */\n",
            "        0x0a5b0, 0x15176, 0x052b0, 0x0a930, 0x07954,#   /* 2020 */\n",
            "        0x06aa0, 0x0ad50, 0x05b52, 0x04b60, 0x0a6e6,#   /* 2025 */\n",
            "        0x0a4e0, 0x0d260, 0x0ea65, 0x0d530, 0x05aa0,#   /* 2030 */\n",
            "        0x076a3, 0x096d0, 0x04afb, 0x04ad0, 0x0a4d0,#   /* 2035 */\n",
            "        0x1d0b6, 0x0d250, 0x0d520, 0x0dd45, 0x0b5a0,#   /* 2040 */\n",
            "        0x056d0, 0x055b2, 0x049b0, 0x0a577, 0x0a4b0,#   /* 2045 */\n",
            "        0x0aa50, 0x1b255, 0x06d20, 0x0ada0, 0x14b63,#   /* 2050 */\n",
            "        0x09370, 0x049f8, 0x04970, 0x064b0, 0x168a6,#   /* 2055 */\n",
            "        0x0ea50, 0x06aa0, 0x1a6c4, 0x0aae0, 0x092e0,#   /* 2060 */\n",
            "        0x0d2e3, 0x0c960, 0x0d557, 0x0d4a0, 0x0da50,#   /* 2065 */\n",
            "        0x05d55, 0x056a0, 0x0a6d0, 0x055d4, 0x052d0,#   /* 2070 */\n",
            "        0x0a9b8, 0x0a950, 0x0b4a0, 0x0b6a6, 0x0ad50,#   /* 2075 */\n",
            "        0x055a0, 0x0aba4, 0x0a5b0, 0x052b0, 0x0b273,#   /* 2080 */\n",
            "        0x06930, 0x07337, 0x06aa0, 0x0ad50, 0x14b55,#   /* 2085 */\n",
            "        0x04b60, 0x0a570, 0x054e4, 0x0d160, 0x0e968,#   /* 2090 */\n",
            "        0x0d520, 0x0daa0, 0x16aa6, 0x056d0, 0x04ae0,#   /* 2095 */\n",
            "        0x0a9d4, 0x0a2d0, 0x0d150, 0x0f252,         #   /* 2099 */\n",
            "]\n",
            "\n",
            "def yearInfo2yearDay(yearInfo):\n",
            "    '''calculate the days in a lunar year from the lunar year's info\n",
            "\n",
            "    >>> yearInfo2yearDay(0) # no leap month, and every month has 29 days.\n",
            "    348\n",
            "    >>> yearInfo2yearDay(1) # 1 leap month, and every month has 29 days.\n",
            "    377\n",
            "    >>> yearInfo2yearDay((2**12-1)*16) # no leap month, and every month has 30 days.\n",
            "    360\n",
            "    >>> yearInfo2yearDay((2**13-1)*16+1) # 1 leap month, and every month has 30 days.\n",
            "    390\n",
            "    >>> # 1 leap month, and every normal month has 30 days, and leap month has 29 days.\n",
            "    >>> yearInfo2yearDay((2**12-1)*16+1)\n",
            "    389\n",
            "    '''\n",
            "    yearInfo = int(yearInfo)\n",
            "\n",
            "    res = 29 * 12\n",
            "\n",
            "    leap = False\n",
            "    if yearInfo % 16 != 0:\n",
            "        leap = True\n",
            "        res += 29\n",
            "\n",
            "    yearInfo //= 16\n",
            "\n",
            "    for i in range(12 + leap):\n",
            "        if yearInfo % 2 == 1:\n",
            "            res += 1\n",
            "        yearInfo //= 2\n",
            "    return res\n",
            "\n",
            "yearDays = [yearInfo2yearDay(x) for x in yearInfos]\n",
            "\n",
            "def day2LunarDate(offset):\n",
            "    offset = int(offset)\n",
            "    res = LunarDate()\n",
            "\n",
            "    for idx, yearDay in enumerate(yearDays):\n",
            "        if offset < yearDay:\n",
            "            break\n",
            "        offset -= yearDay\n",
            "    res.year = 1900 + idx\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    import doctest\n",
            "    failure_count, test_count = doctest.testmod()\n",
            "    if failure_count > 0:\n",
            "        import sys\n",
            "        sys.exit(1)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"\n",
            "    inflect.py: correctly generate plurals, ordinals, indefinite articles;\n",
            "                convert numbers to words\n",
            "    Copyright (C) 2010 Paul Dyson\n",
            "\n",
            "    Based upon the Perl module Lingua::EN::Inflect by Damian Conway.\n",
            "\n",
            "    The original Perl module Lingua::EN::Inflect by Damian Conway is\n",
            "    available from http://search.cpan.org/~dconway/\n",
            "\n",
            "    This module can be downloaded at http://pypi.org/project/inflect\n",
            "\n",
            "methods:\n",
            "          classical inflect\n",
            "          plural plural_noun plural_verb plural_adj singular_noun no num a an\n",
            "          compare compare_nouns compare_verbs compare_adjs\n",
            "          present_participle\n",
            "          ordinal\n",
            "          number_to_words\n",
            "          join\n",
            "          defnoun defverb defadj defa defan\n",
            "\n",
            "    INFLECTIONS:    classical inflect\n",
            "          plural plural_noun plural_verb plural_adj singular_noun compare\n",
            "          no num a an present_participle\n",
            "\n",
            "    PLURALS:   classical inflect\n",
            "          plural plural_noun plural_verb plural_adj singular_noun no num\n",
            "          compare compare_nouns compare_verbs compare_adjs\n",
            "\n",
            "    COMPARISONS:    classical\n",
            "          compare compare_nouns compare_verbs compare_adjs\n",
            "\n",
            "    ARTICLES:   classical inflect num a an\n",
            "\n",
            "    NUMERICAL:      ordinal number_to_words\n",
            "\n",
            "    USER_DEFINED:   defnoun defverb defadj defa defan\n",
            "\n",
            "Exceptions:\n",
            " UnknownClassicalModeError\n",
            " BadNumValueError\n",
            " BadChunkingOptionError\n",
            " NumOutOfRangeError\n",
            " BadUserDefinedPatternError\n",
            " BadRcFileError\n",
            " BadGenderError\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import unicode_literals\n",
            "\n",
            "import ast\n",
            "import sys\n",
            "import re\n",
            "\n",
            "\n",
            "class UnknownClassicalModeError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BadNumValueError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BadChunkingOptionError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class NumOutOfRangeError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BadUserDefinedPatternError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BadRcFileError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BadGenderError(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "__version__ = \"2.1.0\"\n",
            "\n",
            "\n",
            "STDOUT_ON = False\n",
            "\n",
            "\n",
            "def print3(txt):\n",
            "    if STDOUT_ON:\n",
            "        print(txt)\n",
            "\n",
            "\n",
            "def enclose(s):\n",
            "    return \"(?:%s)\" % s\n",
            "\n",
            "\n",
            "def joinstem(cutpoint=0, words=\"\"):\n",
            "    \"\"\"\n",
            "    join stem of each word in words into a string for regex\n",
            "    each word is truncated at cutpoint\n",
            "    cutpoint is usually negative indicating the number of letters to remove\n",
            "    from the end of each word\n",
            "\n",
            "    e.g.\n",
            "    joinstem(-2, [\"ephemeris\", \"iris\", \".*itis\"]) returns\n",
            "    (?:ephemer|ir|.*it)\n",
            "\n",
            "    \"\"\"\n",
            "    return enclose(\"|\".join(w[:cutpoint] for w in words))\n",
            "\n",
            "\n",
            "def bysize(words):\n",
            "    \"\"\"\n",
            "    take a list of words and return a dict of sets sorted by word length\n",
            "    e.g.\n",
            "    ret[3]=set(['ant', 'cat', 'dog', 'pig'])\n",
            "    ret[4]=set(['frog', 'goat'])\n",
            "    ret[5]=set(['horse'])\n",
            "    ret[8]=set(['elephant'])\n",
            "    \"\"\"\n",
            "    ret = {}\n",
            "    for w in words:\n",
            "        if len(w) not in ret:\n",
            "            ret[len(w)] = set()\n",
            "        ret[len(w)].add(w)\n",
            "    return ret\n",
            "\n",
            "\n",
            "def make_pl_si_lists(lst, plending, siendingsize, dojoinstem=True):\n",
            "    \"\"\"\n",
            "    given a list of singular words: lst\n",
            "    an ending to append to make the plural: plending\n",
            "    the number of characters to remove from the singular\n",
            "        before appending plending: siendingsize\n",
            "    a flag whether to create a joinstem: dojoinstem\n",
            "\n",
            "    return:\n",
            "    a list of pluralised words: si_list (called si because this is what you need to\n",
            "                                         look for to make the singular)\n",
            "    the pluralised words as a dict of sets sorted by word length: si_bysize\n",
            "    the singular words as a dict of sets sorted by word length: pl_bysize\n",
            "    if dojoinstem is True: a regular expression that matches any of the stems: stem\n",
            "    \"\"\"\n",
            "    if siendingsize is not None:\n",
            "        siendingsize = -siendingsize\n",
            "    si_list = [w[:siendingsize] + plending for w in lst]\n",
            "    pl_bysize = bysize(lst)\n",
            "    si_bysize = bysize(si_list)\n",
            "    if dojoinstem:\n",
            "        stem = joinstem(siendingsize, lst)\n",
            "        return si_list, si_bysize, pl_bysize, stem\n",
            "    else:\n",
            "        return si_list, si_bysize, pl_bysize\n",
            "\n",
            "\n",
            "# 1. PLURALS\n",
            "\n",
            "pl_sb_irregular_s = {\n",
            "    \"corpus\": \"corpuses|corpora\",\n",
            "    \"opus\": \"opuses|opera\",\n",
            "    \"genus\": \"genera\",\n",
            "    \"mythos\": \"mythoi\",\n",
            "    \"penis\": \"penises|penes\",\n",
            "    \"testis\": \"testes\",\n",
            "    \"atlas\": \"atlases|atlantes\",\n",
            "    \"yes\": \"yeses\",\n",
            "}\n",
            "\n",
            "pl_sb_irregular = {\n",
            "    \"child\": \"children\",\n",
            "    \"brother\": \"brothers|brethren\",\n",
            "    \"loaf\": \"loaves\",\n",
            "    \"hoof\": \"hoofs|hooves\",\n",
            "    \"beef\": \"beefs|beeves\",\n",
            "    \"thief\": \"thiefs|thieves\",\n",
            "    \"money\": \"monies\",\n",
            "    \"mongoose\": \"mongooses\",\n",
            "    \"ox\": \"oxen\",\n",
            "    \"cow\": \"cows|kine\",\n",
            "    \"graffito\": \"graffiti\",\n",
            "    \"octopus\": \"octopuses|octopodes\",\n",
            "    \"genie\": \"genies|genii\",\n",
            "    \"ganglion\": \"ganglions|ganglia\",\n",
            "    \"trilby\": \"trilbys\",\n",
            "    \"turf\": \"turfs|turves\",\n",
            "    \"numen\": \"numina\",\n",
            "    \"atman\": \"atmas\",\n",
            "    \"occiput\": \"occiputs|occipita\",\n",
            "    \"sabretooth\": \"sabretooths\",\n",
            "    \"sabertooth\": \"sabertooths\",\n",
            "    \"lowlife\": \"lowlifes\",\n",
            "    \"flatfoot\": \"flatfoots\",\n",
            "    \"tenderfoot\": \"tenderfoots\",\n",
            "    \"romany\": \"romanies\",\n",
            "    \"jerry\": \"jerries\",\n",
            "    \"mary\": \"maries\",\n",
            "    \"talouse\": \"talouses\",\n",
            "    \"blouse\": \"blouses\",\n",
            "    \"rom\": \"roma\",\n",
            "    \"carmen\": \"carmina\",\n",
            "}\n",
            "\n",
            "pl_sb_irregular.update(pl_sb_irregular_s)\n",
            "# pl_sb_irregular_keys = enclose('|'.join(pl_sb_irregular.keys()))\n",
            "\n",
            "pl_sb_irregular_caps = {\n",
            "    \"Romany\": \"Romanies\",\n",
            "    \"Jerry\": \"Jerrys\",\n",
            "    \"Mary\": \"Marys\",\n",
            "    \"Rom\": \"Roma\",\n",
            "}\n",
            "\n",
            "pl_sb_irregular_compound = {\"prima donna\": \"prima donnas|prime donne\"}\n",
            "\n",
            "si_sb_irregular = {v: k for (k, v) in pl_sb_irregular.items()}\n",
            "keys = list(si_sb_irregular.keys())\n",
            "for k in keys:\n",
            "    if \"|\" in k:\n",
            "        k1, k2 = k.split(\"|\")\n",
            "        si_sb_irregular[k1] = si_sb_irregular[k2] = si_sb_irregular[k]\n",
            "        del si_sb_irregular[k]\n",
            "si_sb_irregular_caps = {v: k for (k, v) in pl_sb_irregular_caps.items()}\n",
            "si_sb_irregular_compound = {v: k for (k, v) in pl_sb_irregular_compound.items()}\n",
            "keys = list(si_sb_irregular_compound.keys())\n",
            "for k in keys:\n",
            "    if \"|\" in k:\n",
            "        k1, k2 = k.split(\"|\")\n",
            "        si_sb_irregular_compound[k1] = si_sb_irregular_compound[\n",
            "            k2\n",
            "        ] = si_sb_irregular_compound[k]\n",
            "        del si_sb_irregular_compound[k]\n",
            "\n",
            "# si_sb_irregular_keys = enclose('|'.join(si_sb_irregular.keys()))\n",
            "\n",
            "# Z's that don't double\n",
            "\n",
            "pl_sb_z_zes_list = (\"quartz\", \"topaz\")\n",
            "pl_sb_z_zes_bysize = bysize(pl_sb_z_zes_list)\n",
            "\n",
            "pl_sb_ze_zes_list = (\"snooze\",)\n",
            "pl_sb_ze_zes_bysize = bysize(pl_sb_ze_zes_list)\n",
            "\n",
            "\n",
            "# CLASSICAL \"..is\" -> \"..ides\"\n",
            "\n",
            "pl_sb_C_is_ides_complete = [\n",
            "    # GENERAL WORDS...\n",
            "    \"ephemeris\",\n",
            "    \"iris\",\n",
            "    \"clitoris\",\n",
            "    \"chrysalis\",\n",
            "    \"epididymis\",\n",
            "]\n",
            "\n",
            "pl_sb_C_is_ides_endings = [\n",
            "    # INFLAMATIONS...\n",
            "    \"itis\"\n",
            "]\n",
            "\n",
            "pl_sb_C_is_ides = joinstem(\n",
            "    -2, pl_sb_C_is_ides_complete + [\".*%s\" % w for w in pl_sb_C_is_ides_endings]\n",
            ")\n",
            "\n",
            "pl_sb_C_is_ides_list = pl_sb_C_is_ides_complete + pl_sb_C_is_ides_endings\n",
            "\n",
            "(\n",
            "    si_sb_C_is_ides_list,\n",
            "    si_sb_C_is_ides_bysize,\n",
            "    pl_sb_C_is_ides_bysize,\n",
            ") = make_pl_si_lists(pl_sb_C_is_ides_list, \"ides\", 2, dojoinstem=False)\n",
            "\n",
            "\n",
            "# CLASSICAL \"..a\" -> \"..ata\"\n",
            "\n",
            "pl_sb_C_a_ata_list = (\n",
            "    \"anathema\",\n",
            "    \"bema\",\n",
            "    \"carcinoma\",\n",
            "    \"charisma\",\n",
            "    \"diploma\",\n",
            "    \"dogma\",\n",
            "    \"drama\",\n",
            "    \"edema\",\n",
            "    \"enema\",\n",
            "    \"enigma\",\n",
            "    \"lemma\",\n",
            "    \"lymphoma\",\n",
            "    \"magma\",\n",
            "    \"melisma\",\n",
            "    \"miasma\",\n",
            "    \"oedema\",\n",
            "    \"sarcoma\",\n",
            "    \"schema\",\n",
            "    \"soma\",\n",
            "    \"stigma\",\n",
            "    \"stoma\",\n",
            "    \"trauma\",\n",
            "    \"gumma\",\n",
            "    \"pragma\",\n",
            ")\n",
            "\n",
            "(\n",
            "    si_sb_C_a_ata_list,\n",
            "    si_sb_C_a_ata_bysize,\n",
            "    pl_sb_C_a_ata_bysize,\n",
            "    pl_sb_C_a_ata,\n",
            ") = make_pl_si_lists(pl_sb_C_a_ata_list, \"ata\", 1)\n",
            "\n",
            "# UNCONDITIONAL \"..a\" -> \"..ae\"\n",
            "\n",
            "pl_sb_U_a_ae_list = (\"alumna\", \"alga\", \"vertebra\", \"persona\")\n",
            "(\n",
            "    si_sb_U_a_ae_list,\n",
            "    si_sb_U_a_ae_bysize,\n",
            "    pl_sb_U_a_ae_bysize,\n",
            "    pl_sb_U_a_ae,\n",
            ") = make_pl_si_lists(pl_sb_U_a_ae_list, \"e\", None)\n",
            "\n",
            "# CLASSICAL \"..a\" -> \"..ae\"\n",
            "\n",
            "pl_sb_C_a_ae_list = (\n",
            "    \"amoeba\",\n",
            "    \"antenna\",\n",
            "    \"formula\",\n",
            "    \"hyperbola\",\n",
            "    \"medusa\",\n",
            "    \"nebula\",\n",
            "    \"parabola\",\n",
            "    \"abscissa\",\n",
            "    \"hydra\",\n",
            "    \"nova\",\n",
            "    \"lacuna\",\n",
            "    \"aurora\",\n",
            "    \"umbra\",\n",
            "    \"flora\",\n",
            "    \"fauna\",\n",
            ")\n",
            "(\n",
            "    si_sb_C_a_ae_list,\n",
            "    si_sb_C_a_ae_bysize,\n",
            "    pl_sb_C_a_ae_bysize,\n",
            "    pl_sb_C_a_ae,\n",
            ") = make_pl_si_lists(pl_sb_C_a_ae_list, \"e\", None)\n",
            "\n",
            "\n",
            "# CLASSICAL \"..en\" -> \"..ina\"\n",
            "\n",
            "pl_sb_C_en_ina_list = (\"stamen\", \"foramen\", \"lumen\")\n",
            "\n",
            "(\n",
            "    si_sb_C_en_ina_list,\n",
            "    si_sb_C_en_ina_bysize,\n",
            "    pl_sb_C_en_ina_bysize,\n",
            "    pl_sb_C_en_ina,\n",
            ") = make_pl_si_lists(pl_sb_C_en_ina_list, \"ina\", 2)\n",
            "\n",
            "\n",
            "# UNCONDITIONAL \"..um\" -> \"..a\"\n",
            "\n",
            "pl_sb_U_um_a_list = (\n",
            "    \"bacterium\",\n",
            "    \"agendum\",\n",
            "    \"desideratum\",\n",
            "    \"erratum\",\n",
            "    \"stratum\",\n",
            "    \"datum\",\n",
            "    \"ovum\",\n",
            "    \"extremum\",\n",
            "    \"candelabrum\",\n",
            ")\n",
            "(\n",
            "    si_sb_U_um_a_list,\n",
            "    si_sb_U_um_a_bysize,\n",
            "    pl_sb_U_um_a_bysize,\n",
            "    pl_sb_U_um_a,\n",
            ") = make_pl_si_lists(pl_sb_U_um_a_list, \"a\", 2)\n",
            "\n",
            "# CLASSICAL \"..um\" -> \"..a\"\n",
            "\n",
            "pl_sb_C_um_a_list = (\n",
            "    \"maximum\",\n",
            "    \"minimum\",\n",
            "    \"momentum\",\n",
            "    \"optimum\",\n",
            "    \"quantum\",\n",
            "    \"cranium\",\n",
            "    \"curriculum\",\n",
            "    \"dictum\",\n",
            "    \"phylum\",\n",
            "    \"aquarium\",\n",
            "    \"compendium\",\n",
            "    \"emporium\",\n",
            "    \"enconium\",\n",
            "    \"gymnasium\",\n",
            "    \"honorarium\",\n",
            "    \"interregnum\",\n",
            "    \"lustrum\",\n",
            "    \"memorandum\",\n",
            "    \"millennium\",\n",
            "    \"rostrum\",\n",
            "    \"spectrum\",\n",
            "    \"speculum\",\n",
            "    \"stadium\",\n",
            "    \"trapezium\",\n",
            "    \"ultimatum\",\n",
            "    \"medium\",\n",
            "    \"vacuum\",\n",
            "    \"velum\",\n",
            "    \"consortium\",\n",
            "    \"arboretum\",\n",
            ")\n",
            "\n",
            "(\n",
            "    si_sb_C_um_a_list,\n",
            "    si_sb_C_um_a_bysize,\n",
            "    pl_sb_C_um_a_bysize,\n",
            "    pl_sb_C_um_a,\n",
            ") = make_pl_si_lists(pl_sb_C_um_a_list, \"a\", 2)\n",
            "\n",
            "\n",
            "# UNCONDITIONAL \"..us\" -> \"i\"\n",
            "\n",
            "pl_sb_U_us_i_list = (\n",
            "    \"alumnus\",\n",
            "    \"alveolus\",\n",
            "    \"bacillus\",\n",
            "    \"bronchus\",\n",
            "    \"locus\",\n",
            "    \"nucleus\",\n",
            "    \"stimulus\",\n",
            "    \"meniscus\",\n",
            "    \"sarcophagus\",\n",
            ")\n",
            "(\n",
            "    si_sb_U_us_i_list,\n",
            "    si_sb_U_us_i_bysize,\n",
            "    pl_sb_U_us_i_bysize,\n",
            "    pl_sb_U_us_i,\n",
            ") = make_pl_si_lists(pl_sb_U_us_i_list, \"i\", 2)\n",
            "\n",
            "# CLASSICAL \"..us\" -> \"..i\"\n",
            "\n",
            "pl_sb_C_us_i_list = (\n",
            "    \"focus\",\n",
            "    \"radius\",\n",
            "    \"genius\",\n",
            "    \"incubus\",\n",
            "    \"succubus\",\n",
            "    \"nimbus\",\n",
            "    \"fungus\",\n",
            "    \"nucleolus\",\n",
            "    \"stylus\",\n",
            "    \"torus\",\n",
            "    \"umbilicus\",\n",
            "    \"uterus\",\n",
            "    \"hippopotamus\",\n",
            "    \"cactus\",\n",
            ")\n",
            "\n",
            "(\n",
            "    si_sb_C_us_i_list,\n",
            "    si_sb_C_us_i_bysize,\n",
            "    pl_sb_C_us_i_bysize,\n",
            "    pl_sb_C_us_i,\n",
            ") = make_pl_si_lists(pl_sb_C_us_i_list, \"i\", 2)\n",
            "\n",
            "\n",
            "# CLASSICAL \"..us\" -> \"..us\"  (ASSIMILATED 4TH DECLENSION LATIN NOUNS)\n",
            "\n",
            "pl_sb_C_us_us = (\n",
            "    \"status\",\n",
            "    \"apparatus\",\n",
            "    \"prospectus\",\n",
            "    \"sinus\",\n",
            "    \"hiatus\",\n",
            "    \"impetus\",\n",
            "    \"plexus\",\n",
            ")\n",
            "pl_sb_C_us_us_bysize = bysize(pl_sb_C_us_us)\n",
            "\n",
            "# UNCONDITIONAL \"..on\" -> \"a\"\n",
            "\n",
            "pl_sb_U_on_a_list = (\n",
            "    \"criterion\",\n",
            "    \"perihelion\",\n",
            "    \"aphelion\",\n",
            "    \"phenomenon\",\n",
            "    \"prolegomenon\",\n",
            "    \"noumenon\",\n",
            "    \"organon\",\n",
            "    \"asyndeton\",\n",
            "    \"hyperbaton\",\n",
            ")\n",
            "(\n",
            "    si_sb_U_on_a_list,\n",
            "    si_sb_U_on_a_bysize,\n",
            "    pl_sb_U_on_a_bysize,\n",
            "    pl_sb_U_on_a,\n",
            ") = make_pl_si_lists(pl_sb_U_on_a_list, \"a\", 2)\n",
            "\n",
            "# CLASSICAL \"..on\" -> \"..a\"\n",
            "\n",
            "pl_sb_C_on_a_list = (\"oxymoron\",)\n",
            "\n",
            "(\n",
            "    si_sb_C_on_a_list,\n",
            "    si_sb_C_on_a_bysize,\n",
            "    pl_sb_C_on_a_bysize,\n",
            "    pl_sb_C_on_a,\n",
            ") = make_pl_si_lists(pl_sb_C_on_a_list, \"a\", 2)\n",
            "\n",
            "\n",
            "# CLASSICAL \"..o\" -> \"..i\"  (BUT NORMALLY -> \"..os\")\n",
            "\n",
            "pl_sb_C_o_i = [\n",
            "    \"solo\",\n",
            "    \"soprano\",\n",
            "    \"basso\",\n",
            "    \"alto\",\n",
            "    \"contralto\",\n",
            "    \"tempo\",\n",
            "    \"piano\",\n",
            "    \"virtuoso\",\n",
            "]  # list not tuple so can concat for pl_sb_U_o_os\n",
            "\n",
            "pl_sb_C_o_i_bysize = bysize(pl_sb_C_o_i)\n",
            "si_sb_C_o_i_bysize = bysize([\"%si\" % w[:-1] for w in pl_sb_C_o_i])\n",
            "\n",
            "pl_sb_C_o_i_stems = joinstem(-1, pl_sb_C_o_i)\n",
            "\n",
            "# ALWAYS \"..o\" -> \"..os\"\n",
            "\n",
            "pl_sb_U_o_os_complete = {\"ado\", \"ISO\", \"NATO\", \"NCO\", \"NGO\", \"oto\"}\n",
            "si_sb_U_o_os_complete = {\"%ss\" % w for w in pl_sb_U_o_os_complete}\n",
            "\n",
            "\n",
            "pl_sb_U_o_os_endings = [\n",
            "    \"aficionado\",\n",
            "    \"aggro\",\n",
            "    \"albino\",\n",
            "    \"allegro\",\n",
            "    \"ammo\",\n",
            "    \"Antananarivo\",\n",
            "    \"archipelago\",\n",
            "    \"armadillo\",\n",
            "    \"auto\",\n",
            "    \"avocado\",\n",
            "    \"Bamako\",\n",
            "    \"Barquisimeto\",\n",
            "    \"bimbo\",\n",
            "    \"bingo\",\n",
            "    \"Biro\",\n",
            "    \"bolero\",\n",
            "    \"Bolzano\",\n",
            "    \"bongo\",\n",
            "    \"Boto\",\n",
            "    \"burro\",\n",
            "    \"Cairo\",\n",
            "    \"canto\",\n",
            "    \"cappuccino\",\n",
            "    \"casino\",\n",
            "    \"cello\",\n",
            "    \"Chicago\",\n",
            "    \"Chimango\",\n",
            "    \"cilantro\",\n",
            "    \"cochito\",\n",
            "    \"coco\",\n",
            "    \"Colombo\",\n",
            "    \"Colorado\",\n",
            "    \"commando\",\n",
            "    \"concertino\",\n",
            "    \"contango\",\n",
            "    \"credo\",\n",
            "    \"crescendo\",\n",
            "    \"cyano\",\n",
            "    \"demo\",\n",
            "    \"ditto\",\n",
            "    \"Draco\",\n",
            "    \"dynamo\",\n",
            "    \"embryo\",\n",
            "    \"Esperanto\",\n",
            "    \"espresso\",\n",
            "    \"euro\",\n",
            "    \"falsetto\",\n",
            "    \"Faro\",\n",
            "    \"fiasco\",\n",
            "    \"Filipino\",\n",
            "    \"flamenco\",\n",
            "    \"furioso\",\n",
            "    \"generalissimo\",\n",
            "    \"Gestapo\",\n",
            "    \"ghetto\",\n",
            "    \"gigolo\",\n",
            "    \"gizmo\",\n",
            "    \"Greensboro\",\n",
            "    \"gringo\",\n",
            "    \"Guaiabero\",\n",
            "    \"guano\",\n",
            "    \"gumbo\",\n",
            "    \"gyro\",\n",
            "    \"hairdo\",\n",
            "    \"hippo\",\n",
            "    \"Idaho\",\n",
            "    \"impetigo\",\n",
            "    \"inferno\",\n",
            "    \"info\",\n",
            "    \"intermezzo\",\n",
            "    \"intertrigo\",\n",
            "    \"Iquico\",\n",
            "    \"jumbo\",\n",
            "    \"junto\",\n",
            "    \"Kakapo\",\n",
            "    \"kilo\",\n",
            "    \"Kinkimavo\",\n",
            "    \"Kokako\",\n",
            "    \"Kosovo\",\n",
            "    \"Lesotho\",\n",
            "    \"libero\",\n",
            "    \"libido\",\n",
            "    \"libretto\",\n",
            "    \"lido\",\n",
            "    \"Lilo\",\n",
            "    \"limbo\",\n",
            "    \"limo\",\n",
            "    \"lineno\",\n",
            "    \"lingo\",\n",
            "    \"lino\",\n",
            "    \"livedo\",\n",
            "    \"loco\",\n",
            "    \"logo\",\n",
            "    \"lumbago\",\n",
            "    \"macho\",\n",
            "    \"macro\",\n",
            "    \"mafioso\",\n",
            "    \"magneto\",\n",
            "    \"magnifico\",\n",
            "    \"Majuro\",\n",
            "    \"Malabo\",\n",
            "    \"manifesto\",\n",
            "    \"Maputo\",\n",
            "    \"Maracaibo\",\n",
            "    \"medico\",\n",
            "    \"memo\",\n",
            "    \"metro\",\n",
            "    \"Mexico\",\n",
            "    \"micro\",\n",
            "    \"Milano\",\n",
            "    \"Monaco\",\n",
            "    \"mono\",\n",
            "    \"Montenegro\",\n",
            "    \"Morocco\",\n",
            "    \"Muqdisho\",\n",
            "    \"myo\",\n",
            "    \"neutrino\",\n",
            "    \"Ningbo\",\n",
            "    \"octavo\",\n",
            "    \"oregano\",\n",
            "    \"Orinoco\",\n",
            "    \"Orlando\",\n",
            "    \"Oslo\",\n",
            "    \"panto\",\n",
            "    \"Paramaribo\",\n",
            "    \"Pardusco\",\n",
            "    \"pedalo\",\n",
            "    \"photo\",\n",
            "    \"pimento\",\n",
            "    \"pinto\",\n",
            "    \"pleco\",\n",
            "    \"Pluto\",\n",
            "    \"pogo\",\n",
            "    \"polo\",\n",
            "    \"poncho\",\n",
            "    \"Porto-Novo\",\n",
            "    \"Porto\",\n",
            "    \"pro\",\n",
            "    \"psycho\",\n",
            "    \"pueblo\",\n",
            "    \"quarto\",\n",
            "    \"Quito\",\n",
            "    \"rhino\",\n",
            "    \"risotto\",\n",
            "    \"rococo\",\n",
            "    \"rondo\",\n",
            "    \"Sacramento\",\n",
            "    \"saddo\",\n",
            "    \"sago\",\n",
            "    \"salvo\",\n",
            "    \"Santiago\",\n",
            "    \"Sapporo\",\n",
            "    \"Sarajevo\",\n",
            "    \"scherzando\",\n",
            "    \"scherzo\",\n",
            "    \"silo\",\n",
            "    \"sirocco\",\n",
            "    \"sombrero\",\n",
            "    \"staccato\",\n",
            "    \"sterno\",\n",
            "    \"stucco\",\n",
            "    \"stylo\",\n",
            "    \"sumo\",\n",
            "    \"Taiko\",\n",
            "    \"techno\",\n",
            "    \"terrazzo\",\n",
            "    \"testudo\",\n",
            "    \"timpano\",\n",
            "    \"tiro\",\n",
            "    \"tobacco\",\n",
            "    \"Togo\",\n",
            "    \"Tokyo\",\n",
            "    \"torero\",\n",
            "    \"Torino\",\n",
            "    \"Toronto\",\n",
            "    \"torso\",\n",
            "    \"tremolo\",\n",
            "    \"typo\",\n",
            "    \"tyro\",\n",
            "    \"ufo\",\n",
            "    \"UNESCO\",\n",
            "    \"vaquero\",\n",
            "    \"vermicello\",\n",
            "    \"verso\",\n",
            "    \"vibrato\",\n",
            "    \"violoncello\",\n",
            "    \"Virgo\",\n",
            "    \"weirdo\",\n",
            "    \"WHO\",\n",
            "    \"WTO\",\n",
            "    \"Yamoussoukro\",\n",
            "    \"yo-yo\",\n",
            "    \"zero\",\n",
            "    \"Zibo\",\n",
            "] + pl_sb_C_o_i\n",
            "\n",
            "pl_sb_U_o_os_bysize = bysize(pl_sb_U_o_os_endings)\n",
            "si_sb_U_o_os_bysize = bysize([\"%ss\" % w for w in pl_sb_U_o_os_endings])\n",
            "\n",
            "\n",
            "# UNCONDITIONAL \"..ch\" -> \"..chs\"\n",
            "\n",
            "pl_sb_U_ch_chs_list = (\"czech\", \"eunuch\", \"stomach\")\n",
            "\n",
            "(\n",
            "    si_sb_U_ch_chs_list,\n",
            "    si_sb_U_ch_chs_bysize,\n",
            "    pl_sb_U_ch_chs_bysize,\n",
            "    pl_sb_U_ch_chs,\n",
            ") = make_pl_si_lists(pl_sb_U_ch_chs_list, \"s\", None)\n",
            "\n",
            "\n",
            "# UNCONDITIONAL \"..[ei]x\" -> \"..ices\"\n",
            "\n",
            "pl_sb_U_ex_ices_list = (\"codex\", \"murex\", \"silex\")\n",
            "(\n",
            "    si_sb_U_ex_ices_list,\n",
            "    si_sb_U_ex_ices_bysize,\n",
            "    pl_sb_U_ex_ices_bysize,\n",
            "    pl_sb_U_ex_ices,\n",
            ") = make_pl_si_lists(pl_sb_U_ex_ices_list, \"ices\", 2)\n",
            "\n",
            "pl_sb_U_ix_ices_list = (\"radix\", \"helix\")\n",
            "(\n",
            "    si_sb_U_ix_ices_list,\n",
            "    si_sb_U_ix_ices_bysize,\n",
            "    pl_sb_U_ix_ices_bysize,\n",
            "    pl_sb_U_ix_ices,\n",
            ") = make_pl_si_lists(pl_sb_U_ix_ices_list, \"ices\", 2)\n",
            "\n",
            "# CLASSICAL \"..[ei]x\" -> \"..ices\"\n",
            "\n",
            "pl_sb_C_ex_ices_list = (\n",
            "    \"vortex\",\n",
            "    \"vertex\",\n",
            "    \"cortex\",\n",
            "    \"latex\",\n",
            "    \"pontifex\",\n",
            "    \"apex\",\n",
            "    \"index\",\n",
            "    \"simplex\",\n",
            ")\n",
            "\n",
            "(\n",
            "    si_sb_C_ex_ices_list,\n",
            "    si_sb_C_ex_ices_bysize,\n",
            "    pl_sb_C_ex_ices_bysize,\n",
            "    pl_sb_C_ex_ices,\n",
            ") = make_pl_si_lists(pl_sb_C_ex_ices_list, \"ices\", 2)\n",
            "\n",
            "\n",
            "pl_sb_C_ix_ices_list = (\"appendix\",)\n",
            "\n",
            "(\n",
            "    si_sb_C_ix_ices_list,\n",
            "    si_sb_C_ix_ices_bysize,\n",
            "    pl_sb_C_ix_ices_bysize,\n",
            "    pl_sb_C_ix_ices,\n",
            ") = make_pl_si_lists(pl_sb_C_ix_ices_list, \"ices\", 2)\n",
            "\n",
            "\n",
            "# ARABIC: \"..\" -> \"..i\"\n",
            "\n",
            "pl_sb_C_i_list = (\"afrit\", \"afreet\", \"efreet\")\n",
            "\n",
            "(si_sb_C_i_list, si_sb_C_i_bysize, pl_sb_C_i_bysize, pl_sb_C_i) = make_pl_si_lists(\n",
            "    pl_sb_C_i_list, \"i\", None\n",
            ")\n",
            "\n",
            "\n",
            "# HEBREW: \"..\" -> \"..im\"\n",
            "\n",
            "pl_sb_C_im_list = (\"goy\", \"seraph\", \"cherub\")\n",
            "\n",
            "(si_sb_C_im_list, si_sb_C_im_bysize, pl_sb_C_im_bysize, pl_sb_C_im) = make_pl_si_lists(\n",
            "    pl_sb_C_im_list, \"im\", None\n",
            ")\n",
            "\n",
            "\n",
            "# UNCONDITIONAL \"..man\" -> \"..mans\"\n",
            "\n",
            "pl_sb_U_man_mans_list = \"\"\"\n",
            "    ataman caiman cayman ceriman\n",
            "    desman dolman farman harman hetman\n",
            "    human leman ottoman shaman talisman\n",
            "\"\"\".split()\n",
            "pl_sb_U_man_mans_caps_list = \"\"\"\n",
            "    Alabaman Bahaman Burman German\n",
            "    Hiroshiman Liman Nakayaman Norman Oklahoman\n",
            "    Panaman Roman Selman Sonaman Tacoman Yakiman\n",
            "    Yokohaman Yuman\n",
            "\"\"\".split()\n",
            "\n",
            "(\n",
            "    si_sb_U_man_mans_list,\n",
            "    si_sb_U_man_mans_bysize,\n",
            "    pl_sb_U_man_mans_bysize,\n",
            ") = make_pl_si_lists(pl_sb_U_man_mans_list, \"s\", None, dojoinstem=False)\n",
            "(\n",
            "    si_sb_U_man_mans_caps_list,\n",
            "    si_sb_U_man_mans_caps_bysize,\n",
            "    pl_sb_U_man_mans_caps_bysize,\n",
            ") = make_pl_si_lists(pl_sb_U_man_mans_caps_list, \"s\", None, dojoinstem=False)\n",
            "\n",
            "\n",
            "pl_sb_uninflected_s_complete = [\n",
            "    # PAIRS OR GROUPS SUBSUMED TO A SINGULAR...\n",
            "    \"breeches\",\n",
            "    \"britches\",\n",
            "    \"pajamas\",\n",
            "    \"pyjamas\",\n",
            "    \"clippers\",\n",
            "    \"gallows\",\n",
            "    \"hijinks\",\n",
            "    \"headquarters\",\n",
            "    \"pliers\",\n",
            "    \"scissors\",\n",
            "    \"testes\",\n",
            "    \"herpes\",\n",
            "    \"pincers\",\n",
            "    \"shears\",\n",
            "    \"proceedings\",\n",
            "    \"trousers\",\n",
            "    # UNASSIMILATED LATIN 4th DECLENSION\n",
            "    \"cantus\",\n",
            "    \"coitus\",\n",
            "    \"nexus\",\n",
            "    # RECENT IMPORTS...\n",
            "    \"contretemps\",\n",
            "    \"corps\",\n",
            "    \"debris\",\n",
            "    \"siemens\",\n",
            "    # DISEASES\n",
            "    \"mumps\",\n",
            "    # MISCELLANEOUS OTHERS...\n",
            "    \"diabetes\",\n",
            "    \"jackanapes\",\n",
            "    \"series\",\n",
            "    \"species\",\n",
            "    \"subspecies\",\n",
            "    \"rabies\",\n",
            "    \"chassis\",\n",
            "    \"innings\",\n",
            "    \"news\",\n",
            "    \"mews\",\n",
            "    \"haggis\",\n",
            "]\n",
            "\n",
            "pl_sb_uninflected_s_endings = [\n",
            "    # RECENT IMPORTS...\n",
            "    \"ois\",\n",
            "    # DISEASES\n",
            "    \"measles\",\n",
            "]\n",
            "\n",
            "pl_sb_uninflected_s = pl_sb_uninflected_s_complete + [\n",
            "    \".*%s\" % w for w in pl_sb_uninflected_s_endings\n",
            "]\n",
            "\n",
            "pl_sb_uninflected_herd = (\n",
            "    # DON'T INFLECT IN CLASSICAL MODE, OTHERWISE NORMAL INFLECTION\n",
            "    \"wildebeest\",\n",
            "    \"swine\",\n",
            "    \"eland\",\n",
            "    \"bison\",\n",
            "    \"buffalo\",\n",
            "    \"elk\",\n",
            "    \"rhinoceros\",\n",
            "    \"zucchini\",\n",
            "    \"caribou\",\n",
            "    \"dace\",\n",
            "    \"grouse\",\n",
            "    \"guinea fowl\",\n",
            "    \"guinea-fowl\",\n",
            "    \"haddock\",\n",
            "    \"hake\",\n",
            "    \"halibut\",\n",
            "    \"herring\",\n",
            "    \"mackerel\",\n",
            "    \"pickerel\",\n",
            "    \"pike\",\n",
            "    \"roe\",\n",
            "    \"seed\",\n",
            "    \"shad\",\n",
            "    \"snipe\",\n",
            "    \"teal\",\n",
            "    \"turbot\",\n",
            "    \"water fowl\",\n",
            "    \"water-fowl\",\n",
            ")\n",
            "\n",
            "pl_sb_uninflected_complete = [\n",
            "    # SOME FISH AND HERD ANIMALS\n",
            "    \"tuna\",\n",
            "    \"salmon\",\n",
            "    \"mackerel\",\n",
            "    \"trout\",\n",
            "    \"bream\",\n",
            "    \"sea-bass\",\n",
            "    \"sea bass\",\n",
            "    \"carp\",\n",
            "    \"cod\",\n",
            "    \"flounder\",\n",
            "    \"whiting\",\n",
            "    \"moose\",\n",
            "    # OTHER ODDITIES\n",
            "    \"graffiti\",\n",
            "    \"djinn\",\n",
            "    \"samuri\",\n",
            "    \"offspring\",\n",
            "    \"pence\",\n",
            "    \"quid\",\n",
            "    \"hertz\",\n",
            "] + pl_sb_uninflected_s_complete\n",
            "# SOME WORDS ENDING IN ...s (OFTEN PAIRS TAKEN AS A WHOLE)\n",
            "\n",
            "pl_sb_uninflected_caps = [\n",
            "    # ALL NATIONALS ENDING IN -ese\n",
            "    \"Portuguese\",\n",
            "    \"Amoyese\",\n",
            "    \"Borghese\",\n",
            "    \"Congoese\",\n",
            "    \"Faroese\",\n",
            "    \"Foochowese\",\n",
            "    \"Genevese\",\n",
            "    \"Genoese\",\n",
            "    \"Gilbertese\",\n",
            "    \"Hottentotese\",\n",
            "    \"Kiplingese\",\n",
            "    \"Kongoese\",\n",
            "    \"Lucchese\",\n",
            "    \"Maltese\",\n",
            "    \"Nankingese\",\n",
            "    \"Niasese\",\n",
            "    \"Pekingese\",\n",
            "    \"Piedmontese\",\n",
            "    \"Pistoiese\",\n",
            "    \"Sarawakese\",\n",
            "    \"Shavese\",\n",
            "    \"Vermontese\",\n",
            "    \"Wenchowese\",\n",
            "    \"Yengeese\",\n",
            "]\n",
            "\n",
            "\n",
            "pl_sb_uninflected_endings = [\n",
            "    # SOME FISH AND HERD ANIMALS\n",
            "    \"fish\",\n",
            "    \"deer\",\n",
            "    \"sheep\",\n",
            "    # ALL NATIONALS ENDING IN -ese\n",
            "    \"nese\",\n",
            "    \"rese\",\n",
            "    \"lese\",\n",
            "    \"mese\",\n",
            "    # DISEASES\n",
            "    \"pox\",\n",
            "    # OTHER ODDITIES\n",
            "    \"craft\",\n",
            "] + pl_sb_uninflected_s_endings\n",
            "# SOME WORDS ENDING IN ...s (OFTEN PAIRS TAKEN AS A WHOLE)\n",
            "\n",
            "\n",
            "pl_sb_uninflected_bysize = bysize(pl_sb_uninflected_endings)\n",
            "\n",
            "\n",
            "# SINGULAR WORDS ENDING IN ...s (ALL INFLECT WITH ...es)\n",
            "\n",
            "pl_sb_singular_s_complete = [\n",
            "    \"acropolis\",\n",
            "    \"aegis\",\n",
            "    \"alias\",\n",
            "    \"asbestos\",\n",
            "    \"bathos\",\n",
            "    \"bias\",\n",
            "    \"bronchitis\",\n",
            "    \"bursitis\",\n",
            "    \"caddis\",\n",
            "    \"cannabis\",\n",
            "    \"canvas\",\n",
            "    \"chaos\",\n",
            "    \"cosmos\",\n",
            "    \"dais\",\n",
            "    \"digitalis\",\n",
            "    \"epidermis\",\n",
            "    \"ethos\",\n",
            "    \"eyas\",\n",
            "    \"gas\",\n",
            "    \"glottis\",\n",
            "    \"hubris\",\n",
            "    \"ibis\",\n",
            "    \"lens\",\n",
            "    \"mantis\",\n",
            "    \"marquis\",\n",
            "    \"metropolis\",\n",
            "    \"pathos\",\n",
            "    \"pelvis\",\n",
            "    \"polis\",\n",
            "    \"rhinoceros\",\n",
            "    \"sassafras\",\n",
            "    \"trellis\",\n",
            "] + pl_sb_C_is_ides_complete\n",
            "\n",
            "\n",
            "pl_sb_singular_s_endings = [\"ss\", \"us\"] + pl_sb_C_is_ides_endings\n",
            "\n",
            "pl_sb_singular_s_bysize = bysize(pl_sb_singular_s_endings)\n",
            "\n",
            "si_sb_singular_s_complete = [\"%ses\" % w for w in pl_sb_singular_s_complete]\n",
            "si_sb_singular_s_endings = [\"%ses\" % w for w in pl_sb_singular_s_endings]\n",
            "si_sb_singular_s_bysize = bysize(si_sb_singular_s_endings)\n",
            "\n",
            "pl_sb_singular_s_es = [\"[A-Z].*es\"]\n",
            "\n",
            "pl_sb_singular_s = enclose(\n",
            "    \"|\".join(\n",
            "        pl_sb_singular_s_complete\n",
            "        + [\".*%s\" % w for w in pl_sb_singular_s_endings]\n",
            "        + pl_sb_singular_s_es\n",
            "    )\n",
            ")\n",
            "\n",
            "\n",
            "# PLURALS ENDING IN uses -> use\n",
            "\n",
            "\n",
            "si_sb_ois_oi_case = (\"Bolshois\", \"Hanois\")\n",
            "\n",
            "si_sb_uses_use_case = (\"Betelgeuses\", \"Duses\", \"Meuses\", \"Syracuses\", \"Toulouses\")\n",
            "\n",
            "si_sb_uses_use = (\n",
            "    \"abuses\",\n",
            "    \"applauses\",\n",
            "    \"blouses\",\n",
            "    \"carouses\",\n",
            "    \"causes\",\n",
            "    \"chartreuses\",\n",
            "    \"clauses\",\n",
            "    \"contuses\",\n",
            "    \"douses\",\n",
            "    \"excuses\",\n",
            "    \"fuses\",\n",
            "    \"grouses\",\n",
            "    \"hypotenuses\",\n",
            "    \"masseuses\",\n",
            "    \"menopauses\",\n",
            "    \"misuses\",\n",
            "    \"muses\",\n",
            "    \"overuses\",\n",
            "    \"pauses\",\n",
            "    \"peruses\",\n",
            "    \"profuses\",\n",
            "    \"recluses\",\n",
            "    \"reuses\",\n",
            "    \"ruses\",\n",
            "    \"souses\",\n",
            "    \"spouses\",\n",
            "    \"suffuses\",\n",
            "    \"transfuses\",\n",
            "    \"uses\",\n",
            ")\n",
            "\n",
            "si_sb_ies_ie_case = (\n",
            "    \"Addies\",\n",
            "    \"Aggies\",\n",
            "    \"Allies\",\n",
            "    \"Amies\",\n",
            "    \"Angies\",\n",
            "    \"Annies\",\n",
            "    \"Annmaries\",\n",
            "    \"Archies\",\n",
            "    \"Arties\",\n",
            "    \"Aussies\",\n",
            "    \"Barbies\",\n",
            "    \"Barries\",\n",
            "    \"Basies\",\n",
            "    \"Bennies\",\n",
            "    \"Bernies\",\n",
            "    \"Berties\",\n",
            "    \"Bessies\",\n",
            "    \"Betties\",\n",
            "    \"Billies\",\n",
            "    \"Blondies\",\n",
            "    \"Bobbies\",\n",
            "    \"Bonnies\",\n",
            "    \"Bowies\",\n",
            "    \"Brandies\",\n",
            "    \"Bries\",\n",
            "    \"Brownies\",\n",
            "    \"Callies\",\n",
            "    \"Carnegies\",\n",
            "    \"Carries\",\n",
            "    \"Cassies\",\n",
            "    \"Charlies\",\n",
            "    \"Cheries\",\n",
            "    \"Christies\",\n",
            "    \"Connies\",\n",
            "    \"Curies\",\n",
            "    \"Dannies\",\n",
            "    \"Debbies\",\n",
            "    \"Dixies\",\n",
            "    \"Dollies\",\n",
            "    \"Donnies\",\n",
            "    \"Drambuies\",\n",
            "    \"Eddies\",\n",
            "    \"Effies\",\n",
            "    \"Ellies\",\n",
            "    \"Elsies\",\n",
            "    \"Eries\",\n",
            "    \"Ernies\",\n",
            "    \"Essies\",\n",
            "    \"Eugenies\",\n",
            "    \"Fannies\",\n",
            "    \"Flossies\",\n",
            "    \"Frankies\",\n",
            "    \"Freddies\",\n",
            "    \"Gillespies\",\n",
            "    \"Goldies\",\n",
            "    \"Gracies\",\n",
            "    \"Guthries\",\n",
            "    \"Hallies\",\n",
            "    \"Hatties\",\n",
            "    \"Hetties\",\n",
            "    \"Hollies\",\n",
            "    \"Jackies\",\n",
            "    \"Jamies\",\n",
            "    \"Janies\",\n",
            "    \"Jannies\",\n",
            "    \"Jeanies\",\n",
            "    \"Jeannies\",\n",
            "    \"Jennies\",\n",
            "    \"Jessies\",\n",
            "    \"Jimmies\",\n",
            "    \"Jodies\",\n",
            "    \"Johnies\",\n",
            "    \"Johnnies\",\n",
            "    \"Josies\",\n",
            "    \"Julies\",\n",
            "    \"Kalgoorlies\",\n",
            "    \"Kathies\",\n",
            "    \"Katies\",\n",
            "    \"Kellies\",\n",
            "    \"Kewpies\",\n",
            "    \"Kristies\",\n",
            "    \"Laramies\",\n",
            "    \"Lassies\",\n",
            "    \"Lauries\",\n",
            "    \"Leslies\",\n",
            "    \"Lessies\",\n",
            "    \"Lillies\",\n",
            "    \"Lizzies\",\n",
            "    \"Lonnies\",\n",
            "    \"Lories\",\n",
            "    \"Lorries\",\n",
            "    \"Lotties\",\n",
            "    \"Louies\",\n",
            "    \"Mackenzies\",\n",
            "    \"Maggies\",\n",
            "    \"Maisies\",\n",
            "    \"Mamies\",\n",
            "    \"Marcies\",\n",
            "    \"Margies\",\n",
            "    \"Maries\",\n",
            "    \"Marjories\",\n",
            "    \"Matties\",\n",
            "    \"McKenzies\",\n",
            "    \"Melanies\",\n",
            "    \"Mickies\",\n",
            "    \"Millies\",\n",
            "    \"Minnies\",\n",
            "    \"Mollies\",\n",
            "    \"Mounties\",\n",
            "    \"Nannies\",\n",
            "    \"Natalies\",\n",
            "    \"Nellies\",\n",
            "    \"Netties\",\n",
            "    \"Ollies\",\n",
            "    \"Ozzies\",\n",
            "    \"Pearlies\",\n",
            "    \"Pottawatomies\",\n",
            "    \"Reggies\",\n",
            "    \"Richies\",\n",
            "    \"Rickies\",\n",
            "    \"Robbies\",\n",
            "    \"Ronnies\",\n",
            "    \"Rosalies\",\n",
            "    \"Rosemaries\",\n",
            "    \"Rosies\",\n",
            "    \"Roxies\",\n",
            "    \"Rushdies\",\n",
            "    \"Ruthies\",\n",
            "    \"Sadies\",\n",
            "    \"Sallies\",\n",
            "    \"Sammies\",\n",
            "    \"Scotties\",\n",
            "    \"Selassies\",\n",
            "    \"Sherries\",\n",
            "    \"Sophies\",\n",
            "    \"Stacies\",\n",
            "    \"Stefanies\",\n",
            "    \"Stephanies\",\n",
            "    \"Stevies\",\n",
            "    \"Susies\",\n",
            "    \"Sylvies\",\n",
            "    \"Tammies\",\n",
            "    \"Terries\",\n",
            "    \"Tessies\",\n",
            "    \"Tommies\",\n",
            "    \"Tracies\",\n",
            "    \"Trekkies\",\n",
            "    \"Valaries\",\n",
            "    \"Valeries\",\n",
            "    \"Valkyries\",\n",
            "    \"Vickies\",\n",
            "    \"Virgies\",\n",
            "    \"Willies\",\n",
            "    \"Winnies\",\n",
            "    \"Wylies\",\n",
            "    \"Yorkies\",\n",
            ")\n",
            "\n",
            "si_sb_ies_ie = (\n",
            "    \"aeries\",\n",
            "    \"baggies\",\n",
            "    \"belies\",\n",
            "    \"biggies\",\n",
            "    \"birdies\",\n",
            "    \"bogies\",\n",
            "    \"bonnies\",\n",
            "    \"boogies\",\n",
            "    \"bookies\",\n",
            "    \"bourgeoisies\",\n",
            "    \"brownies\",\n",
            "    \"budgies\",\n",
            "    \"caddies\",\n",
            "    \"calories\",\n",
            "    \"camaraderies\",\n",
            "    \"cockamamies\",\n",
            "    \"collies\",\n",
            "    \"cookies\",\n",
            "    \"coolies\",\n",
            "    \"cooties\",\n",
            "    \"coteries\",\n",
            "    \"crappies\",\n",
            "    \"curies\",\n",
            "    \"cutesies\",\n",
            "    \"dogies\",\n",
            "    \"eyrie\",\n",
            "    \"floozies\",\n",
            "    \"footsies\",\n",
            "    \"freebies\",\n",
            "    \"genies\",\n",
            "    \"goalies\",\n",
            "    \"groupies\",\n",
            "    \"hies\",\n",
            "    \"jalousies\",\n",
            "    \"junkies\",\n",
            "    \"kiddies\",\n",
            "    \"laddies\",\n",
            "    \"lassies\",\n",
            "    \"lies\",\n",
            "    \"lingeries\",\n",
            "    \"magpies\",\n",
            "    \"menageries\",\n",
            "    \"mommies\",\n",
            "    \"movies\",\n",
            "    \"neckties\",\n",
            "    \"newbies\",\n",
            "    \"nighties\",\n",
            "    \"oldies\",\n",
            "    \"organdies\",\n",
            "    \"overlies\",\n",
            "    \"pies\",\n",
            "    \"pinkies\",\n",
            "    \"pixies\",\n",
            "    \"potpies\",\n",
            "    \"prairies\",\n",
            "    \"quickies\",\n",
            "    \"reveries\",\n",
            "    \"rookies\",\n",
            "    \"rotisseries\",\n",
            "    \"softies\",\n",
            "    \"sorties\",\n",
            "    \"species\",\n",
            "    \"stymies\",\n",
            "    \"sweeties\",\n",
            "    \"ties\",\n",
            "    \"underlies\",\n",
            "    \"unties\",\n",
            "    \"veggies\",\n",
            "    \"vies\",\n",
            "    \"yuppies\",\n",
            "    \"zombies\",\n",
            ")\n",
            "\n",
            "\n",
            "si_sb_oes_oe_case = (\n",
            "    \"Chloes\",\n",
            "    \"Crusoes\",\n",
            "    \"Defoes\",\n",
            "    \"Faeroes\",\n",
            "    \"Ivanhoes\",\n",
            "    \"Joes\",\n",
            "    \"McEnroes\",\n",
            "    \"Moes\",\n",
            "    \"Monroes\",\n",
            "    \"Noes\",\n",
            "    \"Poes\",\n",
            "    \"Roscoes\",\n",
            "    \"Tahoes\",\n",
            "    \"Tippecanoes\",\n",
            "    \"Zoes\",\n",
            ")\n",
            "\n",
            "si_sb_oes_oe = (\n",
            "    \"aloes\",\n",
            "    \"backhoes\",\n",
            "    \"canoes\",\n",
            "    \"does\",\n",
            "    \"floes\",\n",
            "    \"foes\",\n",
            "    \"hoes\",\n",
            "    \"mistletoes\",\n",
            "    \"oboes\",\n",
            "    \"pekoes\",\n",
            "    \"roes\",\n",
            "    \"sloes\",\n",
            "    \"throes\",\n",
            "    \"tiptoes\",\n",
            "    \"toes\",\n",
            "    \"woes\",\n",
            ")\n",
            "\n",
            "si_sb_z_zes = (\"quartzes\", \"topazes\")\n",
            "\n",
            "si_sb_zzes_zz = (\"buzzes\", \"fizzes\", \"frizzes\", \"razzes\")\n",
            "\n",
            "si_sb_ches_che_case = (\n",
            "    \"Andromaches\",\n",
            "    \"Apaches\",\n",
            "    \"Blanches\",\n",
            "    \"Comanches\",\n",
            "    \"Nietzsches\",\n",
            "    \"Porsches\",\n",
            "    \"Roches\",\n",
            ")\n",
            "\n",
            "si_sb_ches_che = (\n",
            "    \"aches\",\n",
            "    \"avalanches\",\n",
            "    \"backaches\",\n",
            "    \"bellyaches\",\n",
            "    \"caches\",\n",
            "    \"cloches\",\n",
            "    \"creches\",\n",
            "    \"douches\",\n",
            "    \"earaches\",\n",
            "    \"fiches\",\n",
            "    \"headaches\",\n",
            "    \"heartaches\",\n",
            "    \"microfiches\",\n",
            "    \"niches\",\n",
            "    \"pastiches\",\n",
            "    \"psyches\",\n",
            "    \"quiches\",\n",
            "    \"stomachaches\",\n",
            "    \"toothaches\",\n",
            ")\n",
            "\n",
            "si_sb_xes_xe = (\"annexes\", \"axes\", \"deluxes\", \"pickaxes\")\n",
            "\n",
            "si_sb_sses_sse_case = (\"Hesses\", \"Jesses\", \"Larousses\", \"Matisses\")\n",
            "si_sb_sses_sse = (\n",
            "    \"bouillabaisses\",\n",
            "    \"crevasses\",\n",
            "    \"demitasses\",\n",
            "    \"impasses\",\n",
            "    \"mousses\",\n",
            "    \"posses\",\n",
            ")\n",
            "\n",
            "si_sb_ves_ve_case = (\n",
            "    # *[nwl]ives -> [nwl]live\n",
            "    \"Clives\",\n",
            "    \"Palmolives\",\n",
            ")\n",
            "si_sb_ves_ve = (\n",
            "    # *[^d]eaves -> eave\n",
            "    \"interweaves\",\n",
            "    \"weaves\",\n",
            "    # *[nwl]ives -> [nwl]live\n",
            "    \"olives\",\n",
            "    # *[eoa]lves -> [eoa]lve\n",
            "    \"bivalves\",\n",
            "    \"dissolves\",\n",
            "    \"resolves\",\n",
            "    \"salves\",\n",
            "    \"twelves\",\n",
            "    \"valves\",\n",
            ")\n",
            "\n",
            "\n",
            "plverb_special_s = enclose(\n",
            "    \"|\".join(\n",
            "        [pl_sb_singular_s]\n",
            "        + pl_sb_uninflected_s\n",
            "        + list(pl_sb_irregular_s.keys())\n",
            "        + [\"(.*[csx])is\", \"(.*)ceps\", \"[A-Z].*s\"]\n",
            "    )\n",
            ")\n",
            "\n",
            "pl_sb_postfix_adj = {\n",
            "    \"general\": [r\"(?!major|lieutenant|brigadier|adjutant|.*star)\\S+\"],\n",
            "    \"martial\": [\"court\"],\n",
            "    \"force\": [\"pound\"],\n",
            "}\n",
            "\n",
            "for k in list(pl_sb_postfix_adj.keys()):\n",
            "    pl_sb_postfix_adj[k] = enclose(\n",
            "        enclose(\"|\".join(pl_sb_postfix_adj[k])) + \"(?=(?:-|\\\\s+)%s)\" % k\n",
            "    )\n",
            "\n",
            "pl_sb_postfix_adj_stems = \"(\" + \"|\".join(list(pl_sb_postfix_adj.values())) + \")(.*)\"\n",
            "\n",
            "\n",
            "# PLURAL WORDS ENDING IS es GO TO SINGULAR is\n",
            "\n",
            "si_sb_es_is = (\n",
            "    \"amanuenses\",\n",
            "    \"amniocenteses\",\n",
            "    \"analyses\",\n",
            "    \"antitheses\",\n",
            "    \"apotheoses\",\n",
            "    \"arterioscleroses\",\n",
            "    \"atheroscleroses\",\n",
            "    \"axes\",\n",
            "    # 'bases', # bases -> basis\n",
            "    \"catalyses\",\n",
            "    \"catharses\",\n",
            "    \"chasses\",\n",
            "    \"cirrhoses\",\n",
            "    \"cocces\",\n",
            "    \"crises\",\n",
            "    \"diagnoses\",\n",
            "    \"dialyses\",\n",
            "    \"diereses\",\n",
            "    \"electrolyses\",\n",
            "    \"emphases\",\n",
            "    \"exegeses\",\n",
            "    \"geneses\",\n",
            "    \"halitoses\",\n",
            "    \"hydrolyses\",\n",
            "    \"hypnoses\",\n",
            "    \"hypotheses\",\n",
            "    \"hystereses\",\n",
            "    \"metamorphoses\",\n",
            "    \"metastases\",\n",
            "    \"misdiagnoses\",\n",
            "    \"mitoses\",\n",
            "    \"mononucleoses\",\n",
            "    \"narcoses\",\n",
            "    \"necroses\",\n",
            "    \"nemeses\",\n",
            "    \"neuroses\",\n",
            "    \"oases\",\n",
            "    \"osmoses\",\n",
            "    \"osteoporoses\",\n",
            "    \"paralyses\",\n",
            "    \"parentheses\",\n",
            "    \"parthenogeneses\",\n",
            "    \"periphrases\",\n",
            "    \"photosyntheses\",\n",
            "    \"probosces\",\n",
            "    \"prognoses\",\n",
            "    \"prophylaxes\",\n",
            "    \"prostheses\",\n",
            "    \"preces\",\n",
            "    \"psoriases\",\n",
            "    \"psychoanalyses\",\n",
            "    \"psychokineses\",\n",
            "    \"psychoses\",\n",
            "    \"scleroses\",\n",
            "    \"scolioses\",\n",
            "    \"sepses\",\n",
            "    \"silicoses\",\n",
            "    \"symbioses\",\n",
            "    \"synopses\",\n",
            "    \"syntheses\",\n",
            "    \"taxes\",\n",
            "    \"telekineses\",\n",
            "    \"theses\",\n",
            "    \"thromboses\",\n",
            "    \"tuberculoses\",\n",
            "    \"urinalyses\",\n",
            ")\n",
            "\n",
            "pl_prep_list = \"\"\"\n",
            "    about above across after among around at athwart before behind\n",
            "    below beneath beside besides between betwixt beyond but by\n",
            "    during except for from in into near of off on onto out over\n",
            "    since till to under until unto upon with\"\"\".split()\n",
            "\n",
            "pl_prep_list_da = pl_prep_list + [\"de\", \"du\", \"da\"]\n",
            "\n",
            "pl_prep_bysize = bysize(pl_prep_list_da)\n",
            "\n",
            "pl_prep = enclose(\"|\".join(pl_prep_list_da))\n",
            "\n",
            "pl_sb_prep_dual_compound = (\n",
            "    r\"(.*?)((?:-|\\s+)(?:\" + pl_prep + r\")(?:-|\\s+))a(?:-|\\s+)(.*)\"\n",
            ")\n",
            "\n",
            "\n",
            "singular_pronoun_genders = {\n",
            "    \"neuter\",\n",
            "    \"feminine\",\n",
            "    \"masculine\",\n",
            "    \"gender-neutral\",\n",
            "    \"feminine or masculine\",\n",
            "    \"masculine or feminine\",\n",
            "}\n",
            "\n",
            "pl_pron_nom = {\n",
            "    # NOMINATIVE    REFLEXIVE\n",
            "    \"i\": \"we\",\n",
            "    \"myself\": \"ourselves\",\n",
            "    \"you\": \"you\",\n",
            "    \"yourself\": \"yourselves\",\n",
            "    \"she\": \"they\",\n",
            "    \"herself\": \"themselves\",\n",
            "    \"he\": \"they\",\n",
            "    \"himself\": \"themselves\",\n",
            "    \"it\": \"they\",\n",
            "    \"itself\": \"themselves\",\n",
            "    \"they\": \"they\",\n",
            "    \"themself\": \"themselves\",\n",
            "    #   POSSESSIVE\n",
            "    \"mine\": \"ours\",\n",
            "    \"yours\": \"yours\",\n",
            "    \"hers\": \"theirs\",\n",
            "    \"his\": \"theirs\",\n",
            "    \"its\": \"theirs\",\n",
            "    \"theirs\": \"theirs\",\n",
            "}\n",
            "\n",
            "si_pron = {}\n",
            "si_pron[\"nom\"] = {v: k for (k, v) in pl_pron_nom.items()}\n",
            "si_pron[\"nom\"][\"we\"] = \"I\"\n",
            "\n",
            "\n",
            "pl_pron_acc = {\n",
            "    # ACCUSATIVE    REFLEXIVE\n",
            "    \"me\": \"us\",\n",
            "    \"myself\": \"ourselves\",\n",
            "    \"you\": \"you\",\n",
            "    \"yourself\": \"yourselves\",\n",
            "    \"her\": \"them\",\n",
            "    \"herself\": \"themselves\",\n",
            "    \"him\": \"them\",\n",
            "    \"himself\": \"themselves\",\n",
            "    \"it\": \"them\",\n",
            "    \"itself\": \"themselves\",\n",
            "    \"them\": \"them\",\n",
            "    \"themself\": \"themselves\",\n",
            "}\n",
            "\n",
            "pl_pron_acc_keys = enclose(\"|\".join(list(pl_pron_acc.keys())))\n",
            "pl_pron_acc_keys_bysize = bysize(list(pl_pron_acc.keys()))\n",
            "\n",
            "si_pron[\"acc\"] = {v: k for (k, v) in pl_pron_acc.items()}\n",
            "\n",
            "for thecase, plur, gend, sing in (\n",
            "    (\"nom\", \"they\", \"neuter\", \"it\"),\n",
            "    (\"nom\", \"they\", \"feminine\", \"she\"),\n",
            "    (\"nom\", \"they\", \"masculine\", \"he\"),\n",
            "    (\"nom\", \"they\", \"gender-neutral\", \"they\"),\n",
            "    (\"nom\", \"they\", \"feminine or masculine\", \"she or he\"),\n",
            "    (\"nom\", \"they\", \"masculine or feminine\", \"he or she\"),\n",
            "    (\"nom\", \"themselves\", \"neuter\", \"itself\"),\n",
            "    (\"nom\", \"themselves\", \"feminine\", \"herself\"),\n",
            "    (\"nom\", \"themselves\", \"masculine\", \"himself\"),\n",
            "    (\"nom\", \"themselves\", \"gender-neutral\", \"themself\"),\n",
            "    (\"nom\", \"themselves\", \"feminine or masculine\", \"herself or himself\"),\n",
            "    (\"nom\", \"themselves\", \"masculine or feminine\", \"himself or herself\"),\n",
            "    (\"nom\", \"theirs\", \"neuter\", \"its\"),\n",
            "    (\"nom\", \"theirs\", \"feminine\", \"hers\"),\n",
            "    (\"nom\", \"theirs\", \"masculine\", \"his\"),\n",
            "    (\"nom\", \"theirs\", \"gender-neutral\", \"theirs\"),\n",
            "    (\"nom\", \"theirs\", \"feminine or masculine\", \"hers or his\"),\n",
            "    (\"nom\", \"theirs\", \"masculine or feminine\", \"his or hers\"),\n",
            "    (\"acc\", \"them\", \"neuter\", \"it\"),\n",
            "    (\"acc\", \"them\", \"feminine\", \"her\"),\n",
            "    (\"acc\", \"them\", \"masculine\", \"him\"),\n",
            "    (\"acc\", \"them\", \"gender-neutral\", \"them\"),\n",
            "    (\"acc\", \"them\", \"feminine or masculine\", \"her or him\"),\n",
            "    (\"acc\", \"them\", \"masculine or feminine\", \"him or her\"),\n",
            "    (\"acc\", \"themselves\", \"neuter\", \"itself\"),\n",
            "    (\"acc\", \"themselves\", \"feminine\", \"herself\"),\n",
            "    (\"acc\", \"themselves\", \"masculine\", \"himself\"),\n",
            "    (\"acc\", \"themselves\", \"gender-neutral\", \"themself\"),\n",
            "    (\"acc\", \"themselves\", \"feminine or masculine\", \"herself or himself\"),\n",
            "    (\"acc\", \"themselves\", \"masculine or feminine\", \"himself or herself\"),\n",
            "):\n",
            "    try:\n",
            "        si_pron[thecase][plur][gend] = sing\n",
            "    except TypeError:\n",
            "        si_pron[thecase][plur] = {}\n",
            "        si_pron[thecase][plur][gend] = sing\n",
            "\n",
            "\n",
            "si_pron_acc_keys = enclose(\"|\".join(list(si_pron[\"acc\"].keys())))\n",
            "si_pron_acc_keys_bysize = bysize(list(si_pron[\"acc\"].keys()))\n",
            "\n",
            "\n",
            "def get_si_pron(thecase, word, gender):\n",
            "    try:\n",
            "        sing = si_pron[thecase][word]\n",
            "    except KeyError:\n",
            "        raise  # not a pronoun\n",
            "    try:\n",
            "        return sing[gender]  # has several types due to gender\n",
            "    except TypeError:\n",
            "        return sing  # answer independent of gender\n",
            "\n",
            "\n",
            "plverb_irregular_pres = {\n",
            "    # 1st PERS. SING.   2ND PERS. SING.   3RD PERS. SINGULAR\n",
            "    # 3RD PERS. (INDET.)\n",
            "    \"am\": \"are\",\n",
            "    \"are\": \"are\",\n",
            "    \"is\": \"are\",\n",
            "    \"was\": \"were\",\n",
            "    \"were\": \"were\",\n",
            "    \"was\": \"were\",\n",
            "    \"have\": \"have\",\n",
            "    \"have\": \"have\",\n",
            "    \"has\": \"have\",\n",
            "    \"do\": \"do\",\n",
            "    \"do\": \"do\",\n",
            "    \"does\": \"do\",\n",
            "}\n",
            "\n",
            "plverb_ambiguous_pres = {\n",
            "    # 1st PERS. SING.  2ND PERS. SING.   3RD PERS. SINGULAR\n",
            "    # 3RD PERS. (INDET.)\n",
            "    \"act\": \"act\",\n",
            "    \"act\": \"act\",\n",
            "    \"acts\": \"act\",\n",
            "    \"blame\": \"blame\",\n",
            "    \"blame\": \"blame\",\n",
            "    \"blames\": \"blame\",\n",
            "    \"can\": \"can\",\n",
            "    \"can\": \"can\",\n",
            "    \"can\": \"can\",\n",
            "    \"must\": \"must\",\n",
            "    \"must\": \"must\",\n",
            "    \"must\": \"must\",\n",
            "    \"fly\": \"fly\",\n",
            "    \"fly\": \"fly\",\n",
            "    \"flies\": \"fly\",\n",
            "    \"copy\": \"copy\",\n",
            "    \"copy\": \"copy\",\n",
            "    \"copies\": \"copy\",\n",
            "    \"drink\": \"drink\",\n",
            "    \"drink\": \"drink\",\n",
            "    \"drinks\": \"drink\",\n",
            "    \"fight\": \"fight\",\n",
            "    \"fight\": \"fight\",\n",
            "    \"fights\": \"fight\",\n",
            "    \"fire\": \"fire\",\n",
            "    \"fire\": \"fire\",\n",
            "    \"fires\": \"fire\",\n",
            "    \"like\": \"like\",\n",
            "    \"like\": \"like\",\n",
            "    \"likes\": \"like\",\n",
            "    \"look\": \"look\",\n",
            "    \"look\": \"look\",\n",
            "    \"looks\": \"look\",\n",
            "    \"make\": \"make\",\n",
            "    \"make\": \"make\",\n",
            "    \"makes\": \"make\",\n",
            "    \"reach\": \"reach\",\n",
            "    \"reach\": \"reach\",\n",
            "    \"reaches\": \"reach\",\n",
            "    \"run\": \"run\",\n",
            "    \"run\": \"run\",\n",
            "    \"runs\": \"run\",\n",
            "    \"sink\": \"sink\",\n",
            "    \"sink\": \"sink\",\n",
            "    \"sinks\": \"sink\",\n",
            "    \"sleep\": \"sleep\",\n",
            "    \"sleep\": \"sleep\",\n",
            "    \"sleeps\": \"sleep\",\n",
            "    \"view\": \"view\",\n",
            "    \"view\": \"view\",\n",
            "    \"views\": \"view\",\n",
            "}\n",
            "\n",
            "plverb_ambiguous_pres_keys = enclose(\"|\".join(list(plverb_ambiguous_pres.keys())))\n",
            "\n",
            "\n",
            "plverb_irregular_non_pres = (\n",
            "    \"did\",\n",
            "    \"had\",\n",
            "    \"ate\",\n",
            "    \"made\",\n",
            "    \"put\",\n",
            "    \"spent\",\n",
            "    \"fought\",\n",
            "    \"sank\",\n",
            "    \"gave\",\n",
            "    \"sought\",\n",
            "    \"shall\",\n",
            "    \"could\",\n",
            "    \"ought\",\n",
            "    \"should\",\n",
            ")\n",
            "\n",
            "plverb_ambiguous_non_pres = enclose(\n",
            "    \"|\".join((\"thought\", \"saw\", \"bent\", \"will\", \"might\", \"cut\"))\n",
            ")\n",
            "\n",
            "# \"..oes\" -> \"..oe\" (the rest are \"..oes\" -> \"o\")\n",
            "\n",
            "pl_v_oes_oe = (\"canoes\", \"floes\", \"oboes\", \"roes\", \"throes\", \"woes\")\n",
            "pl_v_oes_oe_endings_size4 = (\"hoes\", \"toes\")\n",
            "pl_v_oes_oe_endings_size5 = (\"shoes\",)\n",
            "\n",
            "\n",
            "pl_count_zero = (\"0\", \"no\", \"zero\", \"nil\")\n",
            "\n",
            "\n",
            "pl_count_one = (\"1\", \"a\", \"an\", \"one\", \"each\", \"every\", \"this\", \"that\")\n",
            "\n",
            "pl_adj_special = {\"a\": \"some\", \"an\": \"some\", \"this\": \"these\", \"that\": \"those\"}\n",
            "\n",
            "pl_adj_special_keys = enclose(\"|\".join(list(pl_adj_special.keys())))\n",
            "\n",
            "pl_adj_poss = {\n",
            "    \"my\": \"our\",\n",
            "    \"your\": \"your\",\n",
            "    \"its\": \"their\",\n",
            "    \"her\": \"their\",\n",
            "    \"his\": \"their\",\n",
            "    \"their\": \"their\",\n",
            "}\n",
            "\n",
            "pl_adj_poss_keys = enclose(\"|\".join(list(pl_adj_poss.keys())))\n",
            "\n",
            "\n",
            "# 2. INDEFINITE ARTICLES\n",
            "\n",
            "# THIS PATTERN MATCHES STRINGS OF CAPITALS STARTING WITH A \"VOWEL-SOUND\"\n",
            "# CONSONANT FOLLOWED BY ANOTHER CONSONANT, AND WHICH ARE NOT LIKELY\n",
            "# TO BE REAL WORDS (OH, ALL RIGHT THEN, IT'S JUST MAGIC!)\n",
            "\n",
            "A_abbrev = r\"\"\"\n",
            "(?! FJO | [HLMNS]Y.  | RY[EO] | SQU\n",
            "  | ( F[LR]? | [HL] | MN? | N | RH? | S[CHKLMNPTVW]? | X(YL)?) [AEIOU])\n",
            "[FHLMNRSX][A-Z]\n",
            "\"\"\"\n",
            "\n",
            "# THIS PATTERN CODES THE BEGINNINGS OF ALL ENGLISH WORDS BEGINING WITH A\n",
            "# 'y' FOLLOWED BY A CONSONANT. ANY OTHER Y-CONSONANT PREFIX THEREFORE\n",
            "# IMPLIES AN ABBREVIATION.\n",
            "\n",
            "A_y_cons = \"y(b[lor]|cl[ea]|fere|gg|p[ios]|rou|tt)\"\n",
            "\n",
            "# EXCEPTIONS TO EXCEPTIONS\n",
            "\n",
            "A_explicit_a = enclose(\"|\".join((\"unabomber\", \"unanimous\", \"US\")))\n",
            "\n",
            "A_explicit_an = enclose(\n",
            "    \"|\".join((\"euler\", \"hour(?!i)\", \"heir\", \"honest\", \"hono[ur]\", \"mpeg\"))\n",
            ")\n",
            "\n",
            "A_ordinal_an = enclose(\"|\".join((\"[aefhilmnorsx]-?th\",)))\n",
            "\n",
            "A_ordinal_a = enclose(\"|\".join((\"[bcdgjkpqtuvwyz]-?th\",)))\n",
            "\n",
            "\n",
            "# NUMERICAL INFLECTIONS\n",
            "\n",
            "nth = {\n",
            "    0: \"th\",\n",
            "    1: \"st\",\n",
            "    2: \"nd\",\n",
            "    3: \"rd\",\n",
            "    4: \"th\",\n",
            "    5: \"th\",\n",
            "    6: \"th\",\n",
            "    7: \"th\",\n",
            "    8: \"th\",\n",
            "    9: \"th\",\n",
            "    11: \"th\",\n",
            "    12: \"th\",\n",
            "    13: \"th\",\n",
            "}\n",
            "\n",
            "ordinal = dict(\n",
            "    ty=\"tieth\",\n",
            "    one=\"first\",\n",
            "    two=\"second\",\n",
            "    three=\"third\",\n",
            "    five=\"fifth\",\n",
            "    eight=\"eighth\",\n",
            "    nine=\"ninth\",\n",
            "    twelve=\"twelfth\",\n",
            ")\n",
            "\n",
            "ordinal_suff = \"|\".join(list(ordinal.keys()))\n",
            "\n",
            "\n",
            "# NUMBERS\n",
            "\n",
            "unit = [\"\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
            "teen = [\n",
            "    \"ten\",\n",
            "    \"eleven\",\n",
            "    \"twelve\",\n",
            "    \"thirteen\",\n",
            "    \"fourteen\",\n",
            "    \"fifteen\",\n",
            "    \"sixteen\",\n",
            "    \"seventeen\",\n",
            "    \"eighteen\",\n",
            "    \"nineteen\",\n",
            "]\n",
            "ten = [\n",
            "    \"\",\n",
            "    \"\",\n",
            "    \"twenty\",\n",
            "    \"thirty\",\n",
            "    \"forty\",\n",
            "    \"fifty\",\n",
            "    \"sixty\",\n",
            "    \"seventy\",\n",
            "    \"eighty\",\n",
            "    \"ninety\",\n",
            "]\n",
            "mill = [\n",
            "    \" \",\n",
            "    \" thousand\",\n",
            "    \" million\",\n",
            "    \" billion\",\n",
            "    \" trillion\",\n",
            "    \" quadrillion\",\n",
            "    \" quintillion\",\n",
            "    \" sextillion\",\n",
            "    \" septillion\",\n",
            "    \" octillion\",\n",
            "    \" nonillion\",\n",
            "    \" decillion\",\n",
            "]\n",
            "\n",
            "\n",
            "# SUPPORT CLASSICAL PLURALIZATIONS\n",
            "\n",
            "def_classical = dict(\n",
            "    all=False, zero=False, herd=False, names=True, persons=False, ancient=False\n",
            ")\n",
            "\n",
            "all_classical = {k: True for k in list(def_classical.keys())}\n",
            "no_classical = {k: False for k in list(def_classical.keys())}\n",
            "\n",
            "\n",
            "# Maps strings to built-in constant types\n",
            "string_to_constant = {\"True\": True, \"False\": False, \"None\": None}\n",
            "\n",
            "\n",
            "class engine:\n",
            "    def __init__(self):\n",
            "\n",
            "        self.classical_dict = def_classical.copy()\n",
            "        self.persistent_count = None\n",
            "        self.mill_count = 0\n",
            "        self.pl_sb_user_defined = []\n",
            "        self.pl_v_user_defined = []\n",
            "        self.pl_adj_user_defined = []\n",
            "        self.si_sb_user_defined = []\n",
            "        self.A_a_user_defined = []\n",
            "        self.thegender = \"neuter\"\n",
            "\n",
            "    deprecated_methods = dict(\n",
            "        pl=\"plural\",\n",
            "        plnoun=\"plural_noun\",\n",
            "        plverb=\"plural_verb\",\n",
            "        pladj=\"plural_adj\",\n",
            "        sinoun=\"single_noun\",\n",
            "        prespart=\"present_participle\",\n",
            "        numwords=\"number_to_words\",\n",
            "        plequal=\"compare\",\n",
            "        plnounequal=\"compare_nouns\",\n",
            "        plverbequal=\"compare_verbs\",\n",
            "        pladjequal=\"compare_adjs\",\n",
            "        wordlist=\"join\",\n",
            "    )\n",
            "\n",
            "    def __getattr__(self, meth):\n",
            "        if meth in self.deprecated_methods:\n",
            "            print3(\n",
            "                \"{}() deprecated, use {}()\".format(meth, self.deprecated_methods[meth])\n",
            "            )\n",
            "            raise DeprecationWarning\n",
            "        raise AttributeError\n",
            "\n",
            "    def defnoun(self, singular, plural):\n",
            "        \"\"\"\n",
            "        Set the noun plural of singular to plural.\n",
            "\n",
            "        \"\"\"\n",
            "        self.checkpat(singular)\n",
            "        self.checkpatplural(plural)\n",
            "        self.pl_sb_user_defined.extend((singular, plural))\n",
            "        self.si_sb_user_defined.extend((plural, singular))\n",
            "        return 1\n",
            "\n",
            "    def defverb(self, s1, p1, s2, p2, s3, p3):\n",
            "        \"\"\"\n",
            "        Set the verb plurals for s1, s2 and s3 to p1, p2 and p3 respectively.\n",
            "\n",
            "        Where 1, 2 and 3 represent the 1st, 2nd and 3rd person forms of the verb.\n",
            "\n",
            "        \"\"\"\n",
            "        self.checkpat(s1)\n",
            "        self.checkpat(s2)\n",
            "        self.checkpat(s3)\n",
            "        self.checkpatplural(p1)\n",
            "        self.checkpatplural(p2)\n",
            "        self.checkpatplural(p3)\n",
            "        self.pl_v_user_defined.extend((s1, p1, s2, p2, s3, p3))\n",
            "        return 1\n",
            "\n",
            "    def defadj(self, singular, plural):\n",
            "        \"\"\"\n",
            "        Set the adjective plural of singular to plural.\n",
            "\n",
            "        \"\"\"\n",
            "        self.checkpat(singular)\n",
            "        self.checkpatplural(plural)\n",
            "        self.pl_adj_user_defined.extend((singular, plural))\n",
            "        return 1\n",
            "\n",
            "    def defa(self, pattern):\n",
            "        \"\"\"\n",
            "        Define the indefinate article as 'a' for words matching pattern.\n",
            "\n",
            "        \"\"\"\n",
            "        self.checkpat(pattern)\n",
            "        self.A_a_user_defined.extend((pattern, \"a\"))\n",
            "        return 1\n",
            "\n",
            "    def defan(self, pattern):\n",
            "        \"\"\"\n",
            "        Define the indefinate article as 'an' for words matching pattern.\n",
            "\n",
            "        \"\"\"\n",
            "        self.checkpat(pattern)\n",
            "        self.A_a_user_defined.extend((pattern, \"an\"))\n",
            "        return 1\n",
            "\n",
            "    def checkpat(self, pattern):\n",
            "        \"\"\"\n",
            "        check for errors in a regex pattern\n",
            "        \"\"\"\n",
            "        if pattern is None:\n",
            "            return\n",
            "        try:\n",
            "            re.match(pattern, \"\")\n",
            "        except re.error:\n",
            "            print3(\"\\nBad user-defined singular pattern:\\n\\t%s\\n\" % pattern)\n",
            "            raise BadUserDefinedPatternError\n",
            "\n",
            "    def checkpatplural(self, pattern):\n",
            "        \"\"\"\n",
            "        check for errors in a regex replace pattern\n",
            "        \"\"\"\n",
            "        return\n",
            "\n",
            "    def ud_match(self, word, wordlist):\n",
            "        for i in range(len(wordlist) - 2, -2, -2):  # backwards through even elements\n",
            "            mo = re.search(r\"^%s$\" % wordlist[i], word, re.IGNORECASE)\n",
            "            if mo:\n",
            "                if wordlist[i + 1] is None:\n",
            "                    return None\n",
            "                pl = re.sub(\n",
            "                    r\"\\$(\\d+)\", r\"\\\\1\", wordlist[i + 1]\n",
            "                )  # change $n to \\n for expand\n",
            "                return mo.expand(pl)\n",
            "        return None\n",
            "\n",
            "    def classical(self, **kwargs):\n",
            "        \"\"\"\n",
            "        turn classical mode on and off for various categories\n",
            "\n",
            "        turn on all classical modes:\n",
            "        classical()\n",
            "        classical(all=True)\n",
            "\n",
            "        turn on or off specific claassical modes:\n",
            "        e.g.\n",
            "        classical(herd=True)\n",
            "        classical(names=False)\n",
            "\n",
            "        By default all classical modes are off except names.\n",
            "\n",
            "        unknown value in args or key in kwargs rasies\n",
            "        exception: UnknownClasicalModeError\n",
            "\n",
            "        \"\"\"\n",
            "        classical_mode = list(def_classical.keys())\n",
            "        if not kwargs:\n",
            "            self.classical_dict = all_classical.copy()\n",
            "            return\n",
            "        if \"all\" in kwargs:\n",
            "            if kwargs[\"all\"]:\n",
            "                self.classical_dict = all_classical.copy()\n",
            "            else:\n",
            "                self.classical_dict = no_classical.copy()\n",
            "\n",
            "        for k, v in list(kwargs.items()):\n",
            "            if k in classical_mode:\n",
            "                self.classical_dict[k] = v\n",
            "            else:\n",
            "                raise UnknownClassicalModeError\n",
            "\n",
            "    def num(self, count=None, show=None):  # (;$count,$show)\n",
            "        \"\"\"\n",
            "        Set the number to be used in other method calls.\n",
            "\n",
            "        Returns count.\n",
            "\n",
            "        Set show to False to return '' instead.\n",
            "\n",
            "        \"\"\"\n",
            "        if count is not None:\n",
            "            try:\n",
            "                self.persistent_count = int(count)\n",
            "            except ValueError:\n",
            "                raise BadNumValueError\n",
            "            if (show is None) or show:\n",
            "                return str(count)\n",
            "        else:\n",
            "            self.persistent_count = None\n",
            "        return \"\"\n",
            "\n",
            "    def gender(self, gender):\n",
            "        \"\"\"\n",
            "        set the gender for the singular of plural pronouns\n",
            "\n",
            "        can be one of:\n",
            "        'neuter'                ('they' -> 'it')\n",
            "        'feminine'              ('they' -> 'she')\n",
            "        'masculine'             ('they' -> 'he')\n",
            "        'gender-neutral'        ('they' -> 'they')\n",
            "        'feminine or masculine' ('they' -> 'she or he')\n",
            "        'masculine or feminine' ('they' -> 'he or she')\n",
            "        \"\"\"\n",
            "        if gender in singular_pronoun_genders:\n",
            "            self.thegender = gender\n",
            "        else:\n",
            "            raise BadGenderError\n",
            "\n",
            "    def _get_value_from_ast(self, obj):\n",
            "        \"\"\"\n",
            "        Return the value of the ast object.\n",
            "        \"\"\"\n",
            "        if isinstance(obj, ast.Num):\n",
            "            return obj.n\n",
            "        elif isinstance(obj, ast.Str):\n",
            "            return obj.s\n",
            "        elif isinstance(obj, ast.List):\n",
            "            return [self._get_value_from_ast(e) for e in obj.elts]\n",
            "        elif isinstance(obj, ast.Tuple):\n",
            "            return tuple([self._get_value_from_ast(e) for e in obj.elts])\n",
            "\n",
            "        # None, True and False are NameConstants in Py3.4 and above.\n",
            "        elif sys.version_info.major >= 3 and isinstance(obj, ast.NameConstant):\n",
            "            return obj.value\n",
            "\n",
            "        # For python versions below 3.4\n",
            "        elif isinstance(obj, ast.Name) and (obj.id in [\"True\", \"False\", \"None\"]):\n",
            "            return string_to_constant[obj.id]\n",
            "\n",
            "        # Probably passed a variable name.\n",
            "        # Or passed a single word without wrapping it in quotes as an argument\n",
            "        # ex: p.inflect(\"I plural(see)\") instead of p.inflect(\"I plural('see')\")\n",
            "        raise NameError(\"name '%s' is not defined\" % obj.id)\n",
            "\n",
            "    def _string_to_substitute(self, mo, methods_dict):\n",
            "        \"\"\"\n",
            "        Return the string to be substituted for the match.\n",
            "        \"\"\"\n",
            "        matched_text, f_name = mo.groups()\n",
            "        # matched_text is the complete match string. e.g. plural_noun(cat)\n",
            "        # f_name is the function name. e.g. plural_noun\n",
            "\n",
            "        # Return matched_text if function name is not in methods_dict\n",
            "        if f_name not in methods_dict:\n",
            "            return matched_text\n",
            "\n",
            "        # Parse the matched text\n",
            "        a_tree = ast.parse(matched_text)\n",
            "\n",
            "        # get the args and kwargs from ast objects\n",
            "        args_list = [self._get_value_from_ast(a) for a in a_tree.body[0].value.args]\n",
            "        kwargs_list = {\n",
            "            kw.arg: self._get_value_from_ast(kw.value)\n",
            "            for kw in a_tree.body[0].value.keywords\n",
            "        }\n",
            "\n",
            "        # Call the corresponding function\n",
            "        return methods_dict[f_name](*args_list, **kwargs_list)\n",
            "\n",
            "    # 0. PERFORM GENERAL INFLECTIONS IN A STRING\n",
            "\n",
            "    def inflect(self, text):\n",
            "        \"\"\"\n",
            "        Perform inflections in a string.\n",
            "\n",
            "        e.g. inflect('The plural of cat is plural(cat)') returns\n",
            "        'The plural of cat is cats'\n",
            "\n",
            "        can use plural, plural_noun, plural_verb, plural_adj,\n",
            "        singular_noun, a, an, no, ordinal, number_to_words,\n",
            "        and prespart\n",
            "\n",
            "        \"\"\"\n",
            "        save_persistent_count = self.persistent_count\n",
            "\n",
            "        # Dictionary of allowed methods\n",
            "        methods_dict = {\n",
            "            \"plural\": self.plural,\n",
            "            \"plural_adj\": self.plural_adj,\n",
            "            \"plural_noun\": self.plural_noun,\n",
            "            \"plural_verb\": self.plural_verb,\n",
            "            \"singular_noun\": self.singular_noun,\n",
            "            \"a\": self.a,\n",
            "            \"an\": self.a,\n",
            "            \"no\": self.no,\n",
            "            \"ordinal\": self.ordinal,\n",
            "            \"number_to_words\": self.number_to_words,\n",
            "            \"present_participle\": self.present_participle,\n",
            "            \"num\": self.num,\n",
            "        }\n",
            "\n",
            "        # Regular expression to find Python's function call syntax\n",
            "        functions_re = re.compile(r\"((\\w+)\\([^)]*\\)*)\", re.IGNORECASE)\n",
            "        output = functions_re.sub(\n",
            "            lambda mo: self._string_to_substitute(mo, methods_dict), text\n",
            "        )\n",
            "        self.persistent_count = save_persistent_count\n",
            "        return output\n",
            "\n",
            "    # ## PLURAL SUBROUTINES\n",
            "\n",
            "    def postprocess(self, orig, inflected):\n",
            "        if \"|\" in inflected:\n",
            "            inflected = inflected.split(\"|\")[self.classical_dict[\"all\"]]\n",
            "        result = inflected.split(\" \")\n",
            "        # Try to fix word wise capitalization\n",
            "        for index, word in enumerate(orig.split(\" \")):\n",
            "            if word == \"I\":\n",
            "                # Is this the only word for exceptions like this\n",
            "                # Where the original is fully capitalized\n",
            "                # without 'meaning' capitalization?\n",
            "                # Also this fails to handle a capitalizaion in context\n",
            "                continue\n",
            "            if word.capitalize() == word:\n",
            "                result[index] = result[index].capitalize()\n",
            "            if word == word.upper():\n",
            "                result[index] = result[index].upper()\n",
            "        return \" \".join(result)\n",
            "\n",
            "    def partition_word(self, text):\n",
            "        mo = re.search(r\"\\A(\\s*)(.+?)(\\s*)\\Z\", text)\n",
            "        try:\n",
            "            return mo.group(1), mo.group(2), mo.group(3)\n",
            "        except AttributeError:  # empty string\n",
            "            return \"\", \"\", \"\"\n",
            "\n",
            "    def plural(self, text, count=None):\n",
            "        \"\"\"\n",
            "        Return the plural of text.\n",
            "\n",
            "        If count supplied, then return text if count is one of:\n",
            "            1, a, an, one, each, every, this, that\n",
            "        otherwise return the plural.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        pre, word, post = self.partition_word(text)\n",
            "        if not word:\n",
            "            return text\n",
            "        plural = self.postprocess(\n",
            "            word,\n",
            "            self._pl_special_adjective(word, count)\n",
            "            or self._pl_special_verb(word, count)\n",
            "            or self._plnoun(word, count),\n",
            "        )\n",
            "        return \"{}{}{}\".format(pre, plural, post)\n",
            "\n",
            "    def plural_noun(self, text, count=None):\n",
            "        \"\"\"\n",
            "        Return the plural of text, where text is a noun.\n",
            "\n",
            "        If count supplied, then return text if count is one of:\n",
            "            1, a, an, one, each, every, this, that\n",
            "        otherwise return the plural.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        pre, word, post = self.partition_word(text)\n",
            "        if not word:\n",
            "            return text\n",
            "        plural = self.postprocess(word, self._plnoun(word, count))\n",
            "        return \"{}{}{}\".format(pre, plural, post)\n",
            "\n",
            "    def plural_verb(self, text, count=None):\n",
            "        \"\"\"\n",
            "        Return the plural of text, where text is a verb.\n",
            "\n",
            "        If count supplied, then return text if count is one of:\n",
            "            1, a, an, one, each, every, this, that\n",
            "        otherwise return the plural.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        pre, word, post = self.partition_word(text)\n",
            "        if not word:\n",
            "            return text\n",
            "        plural = self.postprocess(\n",
            "            word,\n",
            "            self._pl_special_verb(word, count) or self._pl_general_verb(word, count),\n",
            "        )\n",
            "        return \"{}{}{}\".format(pre, plural, post)\n",
            "\n",
            "    def plural_adj(self, text, count=None):\n",
            "        \"\"\"\n",
            "        Return the plural of text, where text is an adjective.\n",
            "\n",
            "        If count supplied, then return text if count is one of:\n",
            "            1, a, an, one, each, every, this, that\n",
            "        otherwise return the plural.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        pre, word, post = self.partition_word(text)\n",
            "        if not word:\n",
            "            return text\n",
            "        plural = self.postprocess(word, self._pl_special_adjective(word, count) or word)\n",
            "        return \"{}{}{}\".format(pre, plural, post)\n",
            "\n",
            "    def compare(self, word1, word2):\n",
            "        \"\"\"\n",
            "        compare word1 and word2 for equality regardless of plurality\n",
            "\n",
            "        return values:\n",
            "        eq - the strings are equal\n",
            "        p:s - word1 is the plural of word2\n",
            "        s:p - word2 is the plural of word1\n",
            "        p:p - word1 and word2 are two different plural forms of the one word\n",
            "        False - otherwise\n",
            "\n",
            "        \"\"\"\n",
            "        return (\n",
            "            self._plequal(word1, word2, self.plural_noun)\n",
            "            or self._plequal(word1, word2, self.plural_verb)\n",
            "            or self._plequal(word1, word2, self.plural_adj)\n",
            "        )\n",
            "\n",
            "    def compare_nouns(self, word1, word2):\n",
            "        \"\"\"\n",
            "        compare word1 and word2 for equality regardless of plurality\n",
            "        word1 and word2 are to be treated as nouns\n",
            "\n",
            "        return values:\n",
            "        eq - the strings are equal\n",
            "        p:s - word1 is the plural of word2\n",
            "        s:p - word2 is the plural of word1\n",
            "        p:p - word1 and word2 are two different plural forms of the one word\n",
            "        False - otherwise\n",
            "\n",
            "        \"\"\"\n",
            "        return self._plequal(word1, word2, self.plural_noun)\n",
            "\n",
            "    def compare_verbs(self, word1, word2):\n",
            "        \"\"\"\n",
            "        compare word1 and word2 for equality regardless of plurality\n",
            "        word1 and word2 are to be treated as verbs\n",
            "\n",
            "        return values:\n",
            "        eq - the strings are equal\n",
            "        p:s - word1 is the plural of word2\n",
            "        s:p - word2 is the plural of word1\n",
            "        p:p - word1 and word2 are two different plural forms of the one word\n",
            "        False - otherwise\n",
            "\n",
            "        \"\"\"\n",
            "        return self._plequal(word1, word2, self.plural_verb)\n",
            "\n",
            "    def compare_adjs(self, word1, word2):\n",
            "        \"\"\"\n",
            "        compare word1 and word2 for equality regardless of plurality\n",
            "        word1 and word2 are to be treated as adjectives\n",
            "\n",
            "        return values:\n",
            "        eq - the strings are equal\n",
            "        p:s - word1 is the plural of word2\n",
            "        s:p - word2 is the plural of word1\n",
            "        p:p - word1 and word2 are two different plural forms of the one word\n",
            "        False - otherwise\n",
            "\n",
            "        \"\"\"\n",
            "        return self._plequal(word1, word2, self.plural_adj)\n",
            "\n",
            "    def singular_noun(self, text, count=None, gender=None):\n",
            "        \"\"\"\n",
            "        Return the singular of text, where text is a plural noun.\n",
            "\n",
            "        If count supplied, then return the singular if count is one of:\n",
            "            1, a, an, one, each, every, this, that or if count is None\n",
            "        otherwise return text unchanged.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        pre, word, post = self.partition_word(text)\n",
            "        if not word:\n",
            "            return text\n",
            "        sing = self._sinoun(word, count=count, gender=gender)\n",
            "        if sing is not False:\n",
            "            plural = self.postprocess(\n",
            "                word, self._sinoun(word, count=count, gender=gender)\n",
            "            )\n",
            "            return \"{}{}{}\".format(pre, plural, post)\n",
            "        return False\n",
            "\n",
            "    def _plequal(self, word1, word2, pl):\n",
            "        classval = self.classical_dict.copy()\n",
            "        self.classical_dict = all_classical.copy()\n",
            "        if word1 == word2:\n",
            "            return \"eq\"\n",
            "        if word1 == pl(word2):\n",
            "            return \"p:s\"\n",
            "        if pl(word1) == word2:\n",
            "            return \"s:p\"\n",
            "        self.classical_dict = no_classical.copy()\n",
            "        if word1 == pl(word2):\n",
            "            return \"p:s\"\n",
            "        if pl(word1) == word2:\n",
            "            return \"s:p\"\n",
            "        self.classical_dict = classval.copy()\n",
            "\n",
            "        if pl == self.plural or pl == self.plural_noun:\n",
            "            if self._pl_check_plurals_N(word1, word2):\n",
            "                return \"p:p\"\n",
            "            if self._pl_check_plurals_N(word2, word1):\n",
            "                return \"p:p\"\n",
            "        if pl == self.plural or pl == self.plural_adj:\n",
            "            if self._pl_check_plurals_adj(word1, word2):\n",
            "                return \"p:p\"\n",
            "        return False\n",
            "\n",
            "    def _pl_reg_plurals(self, pair, stems, end1, end2):\n",
            "        pattern = r\"({})({}\\|\\1{}|{}\\|\\1{})\".format(stems, end1, end2, end2, end1)\n",
            "        return bool(re.search(pattern, pair))\n",
            "\n",
            "    def _pl_check_plurals_N(self, word1, word2):\n",
            "        stem_endings = (\n",
            "            (pl_sb_C_a_ata, \"as\", \"ata\"),\n",
            "            (pl_sb_C_is_ides, \"is\", \"ides\"),\n",
            "            (pl_sb_C_a_ae, \"s\", \"e\"),\n",
            "            (pl_sb_C_en_ina, \"ens\", \"ina\"),\n",
            "            (pl_sb_C_um_a, \"ums\", \"a\"),\n",
            "            (pl_sb_C_us_i, \"uses\", \"i\"),\n",
            "            (pl_sb_C_on_a, \"ons\", \"a\"),\n",
            "            (pl_sb_C_o_i_stems, \"os\", \"i\"),\n",
            "            (pl_sb_C_ex_ices, \"exes\", \"ices\"),\n",
            "            (pl_sb_C_ix_ices, \"ixes\", \"ices\"),\n",
            "            (pl_sb_C_i, \"s\", \"i\"),\n",
            "            (pl_sb_C_im, \"s\", \"im\"),\n",
            "            (\".*eau\", \"s\", \"x\"),\n",
            "            (\".*ieu\", \"s\", \"x\"),\n",
            "            (\".*tri\", \"xes\", \"ces\"),\n",
            "            (\".{2,}[yia]n\", \"xes\", \"ges\"),\n",
            "        )\n",
            "        pair = \"{}|{}\".format(word1, word2)\n",
            "\n",
            "        return (\n",
            "            pair in pl_sb_irregular_s.values()\n",
            "            or pair in pl_sb_irregular.values()\n",
            "            or pair in pl_sb_irregular_caps.values()\n",
            "            or any(\n",
            "                self._pl_reg_plurals(pair, stems, end1, end2)\n",
            "                for stems, end1, end2 in stem_endings\n",
            "            )\n",
            "        )\n",
            "\n",
            "    def _pl_check_plurals_adj(self, word1, word2):\n",
            "        word1a = word1[: word1.rfind(\"'\")] if word1.endswith((\"'s\", \"'\")) else \"\"\n",
            "        word2a = word2[: word2.rfind(\"'\")] if word2.endswith((\"'s\", \"'\")) else \"\"\n",
            "\n",
            "        return (\n",
            "            word1a\n",
            "            and word2a\n",
            "            and (\n",
            "                self._pl_check_plurals_N(word1a, word2a)\n",
            "                or self._pl_check_plurals_N(word2a, word1a)\n",
            "            )\n",
            "        )\n",
            "\n",
            "    def get_count(self, count=None):\n",
            "        if count is None and self.persistent_count is not None:\n",
            "            count = self.persistent_count\n",
            "\n",
            "        if count is not None:\n",
            "            count = (\n",
            "                1\n",
            "                if (\n",
            "                    (str(count) in pl_count_one)\n",
            "                    or (\n",
            "                        self.classical_dict[\"zero\"]\n",
            "                        and str(count).lower() in pl_count_zero\n",
            "                    )\n",
            "                )\n",
            "                else 2\n",
            "            )\n",
            "        else:\n",
            "            count = \"\"\n",
            "        return count\n",
            "\n",
            "    # @profile\n",
            "    def _plnoun(self, word, count=None):\n",
            "        count = self.get_count(count)\n",
            "\n",
            "        # DEFAULT TO PLURAL\n",
            "\n",
            "        if count == 1:\n",
            "            return word\n",
            "\n",
            "        # HANDLE USER-DEFINED NOUNS\n",
            "\n",
            "        value = self.ud_match(word, self.pl_sb_user_defined)\n",
            "        if value is not None:\n",
            "            return value\n",
            "\n",
            "        # HANDLE EMPTY WORD, SINGULAR COUNT AND UNINFLECTED PLURALS\n",
            "\n",
            "        if word == \"\":\n",
            "            return word\n",
            "\n",
            "        lowerword = word.lower()\n",
            "\n",
            "        if lowerword in pl_sb_uninflected_complete:\n",
            "            return word\n",
            "\n",
            "        if word in pl_sb_uninflected_caps:\n",
            "            return word\n",
            "\n",
            "        for k, v in pl_sb_uninflected_bysize.items():\n",
            "            if lowerword[-k:] in v:\n",
            "                return word\n",
            "\n",
            "        if self.classical_dict[\"herd\"] and lowerword in pl_sb_uninflected_herd:\n",
            "            return word\n",
            "\n",
            "        # HANDLE COMPOUNDS (\"Governor General\", \"mother-in-law\", \"aide-de-camp\", ETC.)\n",
            "\n",
            "        mo = re.search(r\"^(?:%s)$\" % pl_sb_postfix_adj_stems, word, re.IGNORECASE)\n",
            "        if mo and mo.group(2) != \"\":\n",
            "            return \"{}{}\".format(self._plnoun(mo.group(1), 2), mo.group(2))\n",
            "\n",
            "        if \" a \" in lowerword or \"-a-\" in lowerword:\n",
            "            mo = re.search(r\"^(?:%s)$\" % pl_sb_prep_dual_compound, word, re.IGNORECASE)\n",
            "            if mo and mo.group(2) != \"\" and mo.group(3) != \"\":\n",
            "                return \"{}{}{}\".format(\n",
            "                    self._plnoun(mo.group(1), 2), mo.group(2), self._plnoun(mo.group(3))\n",
            "                )\n",
            "\n",
            "        lowersplit = lowerword.split(\" \")\n",
            "        if len(lowersplit) >= 3:\n",
            "            for numword in range(1, len(lowersplit) - 1):\n",
            "                if lowersplit[numword] in pl_prep_list_da:\n",
            "                    return \" \".join(\n",
            "                        lowersplit[: numword - 1]\n",
            "                        + [self._plnoun(lowersplit[numword - 1], 2)]\n",
            "                        + lowersplit[numword:]\n",
            "                    )\n",
            "\n",
            "        # only pluralize denominators in units\n",
            "        mo = re.search(\n",
            "            r\"(?P<denominator>.+)( (%s) .+)\" % \"|\".join([\"per\", \"a\"]), lowerword\n",
            "        )\n",
            "        if mo:\n",
            "            index = len(mo.group(\"denominator\"))\n",
            "            return \"{}{}\".format(self._plnoun(word[:index]), word[index:])\n",
            "\n",
            "        # handle units given in degrees (only accept if\n",
            "        # there is no more than one word following)\n",
            "        # degree Celsius => degrees Celsius but degree\n",
            "        # fahrenheit hour => degree fahrenheit hours\n",
            "        if len(lowersplit) >= 2 and lowersplit[-2] in [\"degree\"]:\n",
            "            return \" \".join([self._plnoun(lowersplit[0])] + lowersplit[1:])\n",
            "\n",
            "        lowersplit = lowerword.split(\"-\")\n",
            "        if len(lowersplit) >= 3:\n",
            "            for numword in range(1, len(lowersplit) - 1):\n",
            "                if lowersplit[numword] in pl_prep_list_da:\n",
            "                    return \" \".join(\n",
            "                        lowersplit[: numword - 1]\n",
            "                        + [\n",
            "                            self._plnoun(lowersplit[numword - 1], 2)\n",
            "                            + \"-\"\n",
            "                            + lowersplit[numword]\n",
            "                            + \"-\"\n",
            "                        ]\n",
            "                    ) + \" \".join(lowersplit[(numword + 1) :])\n",
            "\n",
            "        # HANDLE PRONOUNS\n",
            "\n",
            "        for k, v in pl_pron_acc_keys_bysize.items():\n",
            "            if lowerword[-k:] in v:  # ends with accusivate pronoun\n",
            "                for pk, pv in pl_prep_bysize.items():\n",
            "                    if lowerword[:pk] in pv:  # starts with a prep\n",
            "                        if lowerword.split() == [lowerword[:pk], lowerword[-k:]]:\n",
            "                            # only whitespace in between\n",
            "                            return lowerword[:-k] + pl_pron_acc[lowerword[-k:]]\n",
            "\n",
            "        try:\n",
            "            return pl_pron_nom[word.lower()]\n",
            "        except KeyError:\n",
            "            pass\n",
            "\n",
            "        try:\n",
            "            return pl_pron_acc[word.lower()]\n",
            "        except KeyError:\n",
            "            pass\n",
            "\n",
            "        # HANDLE ISOLATED IRREGULAR PLURALS\n",
            "\n",
            "        wordsplit = word.split()\n",
            "        wordlast = wordsplit[-1]\n",
            "        lowerwordlast = wordlast.lower()\n",
            "\n",
            "        if wordlast in list(pl_sb_irregular_caps.keys()):\n",
            "            llen = len(wordlast)\n",
            "            return \"{}{}\".format(word[:-llen], pl_sb_irregular_caps[wordlast])\n",
            "\n",
            "        if lowerwordlast in list(pl_sb_irregular.keys()):\n",
            "            llen = len(lowerwordlast)\n",
            "            return \"{}{}\".format(word[:-llen], pl_sb_irregular[lowerwordlast])\n",
            "\n",
            "        if (\" \".join(wordsplit[-2:])).lower() in list(pl_sb_irregular_compound.keys()):\n",
            "            llen = len(\n",
            "                \" \".join(wordsplit[-2:])\n",
            "            )  # TODO: what if 2 spaces between these words?\n",
            "            return \"{}{}\".format(\n",
            "                word[:-llen],\n",
            "                pl_sb_irregular_compound[(\" \".join(wordsplit[-2:])).lower()],\n",
            "            )\n",
            "\n",
            "        if lowerword[-3:] == \"quy\":\n",
            "            return word[:-1] + \"ies\"\n",
            "\n",
            "        if lowerword[-6:] == \"person\":\n",
            "            if self.classical_dict[\"persons\"]:\n",
            "                return word + \"s\"\n",
            "            else:\n",
            "                return word[:-4] + \"ople\"\n",
            "\n",
            "        # HANDLE FAMILIES OF IRREGULAR PLURALS\n",
            "\n",
            "        if lowerword[-3:] == \"man\":\n",
            "            for k, v in pl_sb_U_man_mans_bysize.items():\n",
            "                if lowerword[-k:] in v:\n",
            "                    return word + \"s\"\n",
            "            for k, v in pl_sb_U_man_mans_caps_bysize.items():\n",
            "                if word[-k:] in v:\n",
            "                    return word + \"s\"\n",
            "            return word[:-3] + \"men\"\n",
            "        if lowerword[-5:] == \"mouse\":\n",
            "            return word[:-5] + \"mice\"\n",
            "        if lowerword[-5:] == \"louse\":\n",
            "            return word[:-5] + \"lice\"\n",
            "        if lowerword[-5:] == \"goose\":\n",
            "            return word[:-5] + \"geese\"\n",
            "        if lowerword[-5:] == \"tooth\":\n",
            "            return word[:-5] + \"teeth\"\n",
            "        if lowerword[-4:] == \"foot\":\n",
            "            return word[:-4] + \"feet\"\n",
            "        if lowerword[-4:] == \"taco\":\n",
            "            return word[:-5] + \"tacos\"\n",
            "\n",
            "        if lowerword == \"die\":\n",
            "            return \"dice\"\n",
            "\n",
            "        # HANDLE UNASSIMILATED IMPORTS\n",
            "\n",
            "        if lowerword[-4:] == \"ceps\":\n",
            "            return word\n",
            "        if lowerword[-4:] == \"zoon\":\n",
            "            return word[:-2] + \"a\"\n",
            "        if lowerword[-3:] in (\"cis\", \"sis\", \"xis\"):\n",
            "            return word[:-2] + \"es\"\n",
            "\n",
            "        for lastlet, d, numend, post in (\n",
            "            (\"h\", pl_sb_U_ch_chs_bysize, None, \"s\"),\n",
            "            (\"x\", pl_sb_U_ex_ices_bysize, -2, \"ices\"),\n",
            "            (\"x\", pl_sb_U_ix_ices_bysize, -2, \"ices\"),\n",
            "            (\"m\", pl_sb_U_um_a_bysize, -2, \"a\"),\n",
            "            (\"s\", pl_sb_U_us_i_bysize, -2, \"i\"),\n",
            "            (\"n\", pl_sb_U_on_a_bysize, -2, \"a\"),\n",
            "            (\"a\", pl_sb_U_a_ae_bysize, None, \"e\"),\n",
            "        ):\n",
            "            if lowerword[-1] == lastlet:  # this test to add speed\n",
            "                for k, v in d.items():\n",
            "                    if lowerword[-k:] in v:\n",
            "                        return word[:numend] + post\n",
            "\n",
            "        # HANDLE INCOMPLETELY ASSIMILATED IMPORTS\n",
            "\n",
            "        if self.classical_dict[\"ancient\"]:\n",
            "            if lowerword[-4:] == \"trix\":\n",
            "                return word[:-1] + \"ces\"\n",
            "            if lowerword[-3:] in (\"eau\", \"ieu\"):\n",
            "                return word + \"x\"\n",
            "            if lowerword[-3:] in (\"ynx\", \"inx\", \"anx\") and len(word) > 4:\n",
            "                return word[:-1] + \"ges\"\n",
            "\n",
            "            for lastlet, d, numend, post in (\n",
            "                (\"n\", pl_sb_C_en_ina_bysize, -2, \"ina\"),\n",
            "                (\"x\", pl_sb_C_ex_ices_bysize, -2, \"ices\"),\n",
            "                (\"x\", pl_sb_C_ix_ices_bysize, -2, \"ices\"),\n",
            "                (\"m\", pl_sb_C_um_a_bysize, -2, \"a\"),\n",
            "                (\"s\", pl_sb_C_us_i_bysize, -2, \"i\"),\n",
            "                (\"s\", pl_sb_C_us_us_bysize, None, \"\"),\n",
            "                (\"a\", pl_sb_C_a_ae_bysize, None, \"e\"),\n",
            "                (\"a\", pl_sb_C_a_ata_bysize, None, \"ta\"),\n",
            "                (\"s\", pl_sb_C_is_ides_bysize, -1, \"des\"),\n",
            "                (\"o\", pl_sb_C_o_i_bysize, -1, \"i\"),\n",
            "                (\"n\", pl_sb_C_on_a_bysize, -2, \"a\"),\n",
            "            ):\n",
            "                if lowerword[-1] == lastlet:  # this test to add speed\n",
            "                    for k, v in d.items():\n",
            "                        if lowerword[-k:] in v:\n",
            "                            return word[:numend] + post\n",
            "\n",
            "            for d, numend, post in (\n",
            "                (pl_sb_C_i_bysize, None, \"i\"),\n",
            "                (pl_sb_C_im_bysize, None, \"im\"),\n",
            "            ):\n",
            "                for k, v in d.items():\n",
            "                    if lowerword[-k:] in v:\n",
            "                        return word[:numend] + post\n",
            "\n",
            "        # HANDLE SINGULAR NOUNS ENDING IN ...s OR OTHER SILIBANTS\n",
            "\n",
            "        if lowerword in pl_sb_singular_s_complete:\n",
            "            return word + \"es\"\n",
            "\n",
            "        for k, v in pl_sb_singular_s_bysize.items():\n",
            "            if lowerword[-k:] in v:\n",
            "                return word + \"es\"\n",
            "\n",
            "        if lowerword[-2:] == \"es\" and word[0] == word[0].upper():\n",
            "            return word + \"es\"\n",
            "\n",
            "        if lowerword[-1] == \"z\":\n",
            "            for k, v in pl_sb_z_zes_bysize.items():\n",
            "                if lowerword[-k:] in v:\n",
            "                    return word + \"es\"\n",
            "\n",
            "            if lowerword[-2:-1] != \"z\":\n",
            "                return word + \"zes\"\n",
            "\n",
            "        if lowerword[-2:] == \"ze\":\n",
            "            for k, v in pl_sb_ze_zes_bysize.items():\n",
            "                if lowerword[-k:] in v:\n",
            "                    return word + \"s\"\n",
            "\n",
            "        if lowerword[-2:] in (\"ch\", \"sh\", \"zz\", \"ss\") or lowerword[-1] == \"x\":\n",
            "            return word + \"es\"\n",
            "\n",
            "        # HANDLE ...f -> ...ves\n",
            "\n",
            "        if lowerword[-3:] in (\"elf\", \"alf\", \"olf\"):\n",
            "            return word[:-1] + \"ves\"\n",
            "        if lowerword[-3:] == \"eaf\" and lowerword[-4:-3] != \"d\":\n",
            "            return word[:-1] + \"ves\"\n",
            "        if lowerword[-4:] in (\"nife\", \"life\", \"wife\"):\n",
            "            return word[:-2] + \"ves\"\n",
            "        if lowerword[-3:] == \"arf\":\n",
            "            return word[:-1] + \"ves\"\n",
            "\n",
            "        # HANDLE ...y\n",
            "\n",
            "        if lowerword[-1] == \"y\":\n",
            "            if lowerword[-2:-1] in \"aeiou\" or len(word) == 1:\n",
            "                return word + \"s\"\n",
            "\n",
            "            if self.classical_dict[\"names\"]:\n",
            "                if lowerword[-1] == \"y\" and word[0] == word[0].upper():\n",
            "                    return word + \"s\"\n",
            "\n",
            "            return word[:-1] + \"ies\"\n",
            "\n",
            "        # HANDLE ...o\n",
            "\n",
            "        if lowerword in pl_sb_U_o_os_complete:\n",
            "            return word + \"s\"\n",
            "\n",
            "        for k, v in pl_sb_U_o_os_bysize.items():\n",
            "            if lowerword[-k:] in v:\n",
            "                return word + \"s\"\n",
            "\n",
            "        if lowerword[-2:] in (\"ao\", \"eo\", \"io\", \"oo\", \"uo\"):\n",
            "            return word + \"s\"\n",
            "\n",
            "        if lowerword[-1] == \"o\":\n",
            "            return word + \"es\"\n",
            "\n",
            "        # OTHERWISE JUST ADD ...s\n",
            "\n",
            "        return \"%ss\" % word\n",
            "\n",
            "    def _pl_special_verb(self, word, count=None):\n",
            "        if self.classical_dict[\"zero\"] and str(count).lower() in pl_count_zero:\n",
            "            return False\n",
            "        count = self.get_count(count)\n",
            "\n",
            "        if count == 1:\n",
            "            return word\n",
            "\n",
            "        # HANDLE USER-DEFINED VERBS\n",
            "\n",
            "        value = self.ud_match(word, self.pl_v_user_defined)\n",
            "        if value is not None:\n",
            "            return value\n",
            "\n",
            "        # HANDLE IRREGULAR PRESENT TENSE (SIMPLE AND COMPOUND)\n",
            "\n",
            "        lowerword = word.lower()\n",
            "        try:\n",
            "            firstword = lowerword.split()[0]\n",
            "        except IndexError:\n",
            "            return False  # word is ''\n",
            "\n",
            "        if firstword in list(plverb_irregular_pres.keys()):\n",
            "            return \"{}{}\".format(\n",
            "                plverb_irregular_pres[firstword], word[len(firstword) :]\n",
            "            )\n",
            "\n",
            "        # HANDLE IRREGULAR FUTURE, PRETERITE AND PERFECT TENSES\n",
            "\n",
            "        if firstword in plverb_irregular_non_pres:\n",
            "            return word\n",
            "\n",
            "        # HANDLE PRESENT NEGATIONS (SIMPLE AND COMPOUND)\n",
            "\n",
            "        if firstword.endswith(\"n't\") and firstword[:-3] in list(\n",
            "            plverb_irregular_pres.keys()\n",
            "        ):\n",
            "            return \"{}n't{}\".format(\n",
            "                plverb_irregular_pres[firstword[:-3]], word[len(firstword) :]\n",
            "            )\n",
            "\n",
            "        if firstword.endswith(\"n't\"):\n",
            "            return word\n",
            "\n",
            "        # HANDLE SPECIAL CASES\n",
            "\n",
            "        mo = re.search(r\"^(%s)$\" % plverb_special_s, word)\n",
            "        if mo:\n",
            "            return False\n",
            "        if re.search(r\"\\s\", word):\n",
            "            return False\n",
            "        if lowerword == \"quizzes\":\n",
            "            return \"quiz\"\n",
            "\n",
            "        # HANDLE STANDARD 3RD PERSON (CHOP THE ...(e)s OFF SINGLE WORDS)\n",
            "\n",
            "        if (\n",
            "            lowerword[-4:] in (\"ches\", \"shes\", \"zzes\", \"sses\")\n",
            "            or lowerword[-3:] == \"xes\"\n",
            "        ):\n",
            "            return word[:-2]\n",
            "\n",
            "        if lowerword[-3:] == \"ies\" and len(word) > 3:\n",
            "            return lowerword[:-3] + \"y\"\n",
            "\n",
            "        if (\n",
            "            lowerword in pl_v_oes_oe\n",
            "            or lowerword[-4:] in pl_v_oes_oe_endings_size4\n",
            "            or lowerword[-5:] in pl_v_oes_oe_endings_size5\n",
            "        ):\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword.endswith(\"oes\") and len(word) > 3:\n",
            "            return lowerword[:-2]\n",
            "\n",
            "        mo = re.search(r\"^(.*[^s])s$\", word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return mo.group(1)\n",
            "\n",
            "        # OTHERWISE, A REGULAR VERB (HANDLE ELSEWHERE)\n",
            "\n",
            "        return False\n",
            "\n",
            "    def _pl_general_verb(self, word, count=None):\n",
            "        count = self.get_count(count)\n",
            "\n",
            "        if count == 1:\n",
            "            return word\n",
            "\n",
            "        # HANDLE AMBIGUOUS PRESENT TENSES  (SIMPLE AND COMPOUND)\n",
            "\n",
            "        mo = re.search(\n",
            "            r\"^(%s)((\\s.*)?)$\" % plverb_ambiguous_pres_keys, word, re.IGNORECASE\n",
            "        )\n",
            "        if mo:\n",
            "            return \"{}{}\".format(\n",
            "                plverb_ambiguous_pres[mo.group(1).lower()], mo.group(2)\n",
            "            )\n",
            "\n",
            "        # HANDLE AMBIGUOUS PRETERITE AND PERFECT TENSES\n",
            "\n",
            "        mo = re.search(\n",
            "            r\"^(%s)((\\s.*)?)$\" % plverb_ambiguous_non_pres, word, re.IGNORECASE\n",
            "        )\n",
            "        if mo:\n",
            "            return word\n",
            "\n",
            "        # OTHERWISE, 1st OR 2ND PERSON IS UNINFLECTED\n",
            "\n",
            "        return word\n",
            "\n",
            "    def _pl_special_adjective(self, word, count=None):\n",
            "        count = self.get_count(count)\n",
            "\n",
            "        if count == 1:\n",
            "            return word\n",
            "\n",
            "        # HANDLE USER-DEFINED ADJECTIVES\n",
            "\n",
            "        value = self.ud_match(word, self.pl_adj_user_defined)\n",
            "        if value is not None:\n",
            "            return value\n",
            "\n",
            "        # HANDLE KNOWN CASES\n",
            "\n",
            "        mo = re.search(r\"^(%s)$\" % pl_adj_special_keys, word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return \"%s\" % (pl_adj_special[mo.group(1).lower()])\n",
            "\n",
            "        # HANDLE POSSESSIVES\n",
            "\n",
            "        mo = re.search(r\"^(%s)$\" % pl_adj_poss_keys, word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return \"%s\" % (pl_adj_poss[mo.group(1).lower()])\n",
            "\n",
            "        mo = re.search(r\"^(.*)'s?$\", word)\n",
            "        if mo:\n",
            "            pl = self.plural_noun(mo.group(1))\n",
            "            trailing_s = \"\" if pl[-1] == \"s\" else \"s\"\n",
            "            return \"{}'{}\".format(pl, trailing_s)\n",
            "\n",
            "        # OTHERWISE, NO IDEA\n",
            "\n",
            "        return False\n",
            "\n",
            "    # @profile\n",
            "    def _sinoun(self, word, count=None, gender=None):\n",
            "        count = self.get_count(count)\n",
            "\n",
            "        # DEFAULT TO PLURAL\n",
            "\n",
            "        if count == 2:\n",
            "            return word\n",
            "\n",
            "        # SET THE GENDER\n",
            "\n",
            "        try:\n",
            "            if gender is None:\n",
            "                gender = self.thegender\n",
            "            elif gender not in singular_pronoun_genders:\n",
            "                raise BadGenderError\n",
            "        except (TypeError, IndexError):\n",
            "            raise BadGenderError\n",
            "\n",
            "        # HANDLE USER-DEFINED NOUNS\n",
            "\n",
            "        value = self.ud_match(word, self.si_sb_user_defined)\n",
            "        if value is not None:\n",
            "            return value\n",
            "\n",
            "        # HANDLE EMPTY WORD, SINGULAR COUNT AND UNINFLECTED PLURALS\n",
            "\n",
            "        if word == \"\":\n",
            "            return word\n",
            "\n",
            "        lowerword = word.lower()\n",
            "\n",
            "        if word in si_sb_ois_oi_case:\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword in pl_sb_uninflected_complete:\n",
            "            return word\n",
            "\n",
            "        if word in pl_sb_uninflected_caps:\n",
            "            return word\n",
            "\n",
            "        for k, v in pl_sb_uninflected_bysize.items():\n",
            "            if lowerword[-k:] in v:\n",
            "                return word\n",
            "\n",
            "        if self.classical_dict[\"herd\"] and lowerword in pl_sb_uninflected_herd:\n",
            "            return word\n",
            "\n",
            "        if lowerword in pl_sb_C_us_us:\n",
            "            return word\n",
            "\n",
            "        # HANDLE COMPOUNDS (\"Governor General\", \"mother-in-law\", \"aide-de-camp\", ETC.)\n",
            "\n",
            "        mo = re.search(r\"^(?:%s)$\" % pl_sb_postfix_adj_stems, word, re.IGNORECASE)\n",
            "        if mo and mo.group(2) != \"\":\n",
            "            return \"{}{}\".format(\n",
            "                self._sinoun(mo.group(1), 1, gender=gender), mo.group(2)\n",
            "            )\n",
            "\n",
            "        lowersplit = lowerword.split(\" \")\n",
            "        if len(lowersplit) >= 3:\n",
            "            for numword in range(1, len(lowersplit) - 1):\n",
            "                if lowersplit[numword] in pl_prep_list_da:\n",
            "                    return \" \".join(\n",
            "                        lowersplit[: numword - 1]\n",
            "                        + [\n",
            "                            self._sinoun(lowersplit[numword - 1], 1, gender=gender)\n",
            "                            or lowersplit[numword - 1]\n",
            "                        ]\n",
            "                        + lowersplit[numword:]\n",
            "                    )\n",
            "\n",
            "        lowersplit = lowerword.split(\"-\")\n",
            "        if len(lowersplit) >= 3:\n",
            "            for numword in range(1, len(lowersplit) - 1):\n",
            "                if lowersplit[numword] in pl_prep_list_da:\n",
            "                    return \" \".join(\n",
            "                        lowersplit[: numword - 1]\n",
            "                        + [\n",
            "                            (\n",
            "                                self._sinoun(lowersplit[numword - 1], 1, gender=gender)\n",
            "                                or lowersplit[numword - 1]\n",
            "                            )\n",
            "                            + \"-\"\n",
            "                            + lowersplit[numword]\n",
            "                            + \"-\"\n",
            "                        ]\n",
            "                    ) + \" \".join(lowersplit[(numword + 1) :])\n",
            "\n",
            "        # HANDLE PRONOUNS\n",
            "\n",
            "        for k, v in si_pron_acc_keys_bysize.items():\n",
            "            if lowerword[-k:] in v:  # ends with accusivate pronoun\n",
            "                for pk, pv in pl_prep_bysize.items():\n",
            "                    if lowerword[:pk] in pv:  # starts with a prep\n",
            "                        if lowerword.split() == [lowerword[:pk], lowerword[-k:]]:\n",
            "                            # only whitespace in between\n",
            "                            return lowerword[:-k] + get_si_pron(\n",
            "                                \"acc\", lowerword[-k:], gender\n",
            "                            )\n",
            "\n",
            "        try:\n",
            "            return get_si_pron(\"nom\", word.lower(), gender)\n",
            "        except KeyError:\n",
            "            pass\n",
            "\n",
            "        try:\n",
            "            return get_si_pron(\"acc\", word.lower(), gender)\n",
            "        except KeyError:\n",
            "            pass\n",
            "\n",
            "        # HANDLE ISOLATED IRREGULAR PLURALS\n",
            "\n",
            "        wordsplit = word.split()\n",
            "        wordlast = wordsplit[-1]\n",
            "        lowerwordlast = wordlast.lower()\n",
            "\n",
            "        if wordlast in list(si_sb_irregular_caps.keys()):\n",
            "            llen = len(wordlast)\n",
            "            return \"{}{}\".format(word[:-llen], si_sb_irregular_caps[wordlast])\n",
            "\n",
            "        if lowerwordlast in list(si_sb_irregular.keys()):\n",
            "            llen = len(lowerwordlast)\n",
            "            return \"{}{}\".format(word[:-llen], si_sb_irregular[lowerwordlast])\n",
            "\n",
            "        if (\" \".join(wordsplit[-2:])).lower() in list(si_sb_irregular_compound.keys()):\n",
            "            llen = len(\n",
            "                \" \".join(wordsplit[-2:])\n",
            "            )  # TODO: what if 2 spaces between these words?\n",
            "            return \"{}{}\".format(\n",
            "                word[:-llen],\n",
            "                si_sb_irregular_compound[(\" \".join(wordsplit[-2:])).lower()],\n",
            "            )\n",
            "\n",
            "        if lowerword[-5:] == \"quies\":\n",
            "            return word[:-3] + \"y\"\n",
            "\n",
            "        if lowerword[-7:] == \"persons\":\n",
            "            return word[:-1]\n",
            "        if lowerword[-6:] == \"people\":\n",
            "            return word[:-4] + \"rson\"\n",
            "\n",
            "        # HANDLE FAMILIES OF IRREGULAR PLURALS\n",
            "\n",
            "        if lowerword[-4:] == \"mans\":\n",
            "            for k, v in si_sb_U_man_mans_bysize.items():\n",
            "                if lowerword[-k:] in v:\n",
            "                    return word[:-1]\n",
            "            for k, v in si_sb_U_man_mans_caps_bysize.items():\n",
            "                if word[-k:] in v:\n",
            "                    return word[:-1]\n",
            "        if lowerword[-3:] == \"men\":\n",
            "            return word[:-3] + \"man\"\n",
            "        if lowerword[-4:] == \"mice\":\n",
            "            return word[:-4] + \"mouse\"\n",
            "        if lowerword[-4:] == \"lice\":\n",
            "            return word[:-4] + \"louse\"\n",
            "        if lowerword[-5:] == \"geese\":\n",
            "            return word[:-5] + \"goose\"\n",
            "        if lowerword[-5:] == \"teeth\":\n",
            "            return word[:-5] + \"tooth\"\n",
            "        if lowerword[-4:] == \"feet\":\n",
            "            return word[:-4] + \"foot\"\n",
            "\n",
            "        if lowerword == \"dice\":\n",
            "            return \"die\"\n",
            "\n",
            "        # HANDLE UNASSIMILATED IMPORTS\n",
            "\n",
            "        if lowerword[-4:] == \"ceps\":\n",
            "            return word\n",
            "        if lowerword[-3:] == \"zoa\":\n",
            "            return word[:-1] + \"on\"\n",
            "\n",
            "        for lastlet, d, numend, post in (\n",
            "            (\"s\", si_sb_U_ch_chs_bysize, -1, \"\"),\n",
            "            (\"s\", si_sb_U_ex_ices_bysize, -4, \"ex\"),\n",
            "            (\"s\", si_sb_U_ix_ices_bysize, -4, \"ix\"),\n",
            "            (\"a\", si_sb_U_um_a_bysize, -1, \"um\"),\n",
            "            (\"i\", si_sb_U_us_i_bysize, -1, \"us\"),\n",
            "            (\"a\", si_sb_U_on_a_bysize, -1, \"on\"),\n",
            "            (\"e\", si_sb_U_a_ae_bysize, -1, \"\"),\n",
            "        ):\n",
            "            if lowerword[-1] == lastlet:  # this test to add speed\n",
            "                for k, v in d.items():\n",
            "                    if lowerword[-k:] in v:\n",
            "                        return word[:numend] + post\n",
            "\n",
            "        # HANDLE INCOMPLETELY ASSIMILATED IMPORTS\n",
            "\n",
            "        if self.classical_dict[\"ancient\"]:\n",
            "\n",
            "            if lowerword[-6:] == \"trices\":\n",
            "                return word[:-3] + \"x\"\n",
            "            if lowerword[-4:] in (\"eaux\", \"ieux\"):\n",
            "                return word[:-1]\n",
            "            if lowerword[-5:] in (\"ynges\", \"inges\", \"anges\") and len(word) > 6:\n",
            "                return word[:-3] + \"x\"\n",
            "\n",
            "            for lastlet, d, numend, post in (\n",
            "                (\"a\", si_sb_C_en_ina_bysize, -3, \"en\"),\n",
            "                (\"s\", si_sb_C_ex_ices_bysize, -4, \"ex\"),\n",
            "                (\"s\", si_sb_C_ix_ices_bysize, -4, \"ix\"),\n",
            "                (\"a\", si_sb_C_um_a_bysize, -1, \"um\"),\n",
            "                (\"i\", si_sb_C_us_i_bysize, -1, \"us\"),\n",
            "                (\"s\", pl_sb_C_us_us_bysize, None, \"\"),\n",
            "                (\"e\", si_sb_C_a_ae_bysize, -1, \"\"),\n",
            "                (\"a\", si_sb_C_a_ata_bysize, -2, \"\"),\n",
            "                (\"s\", si_sb_C_is_ides_bysize, -3, \"s\"),\n",
            "                (\"i\", si_sb_C_o_i_bysize, -1, \"o\"),\n",
            "                (\"a\", si_sb_C_on_a_bysize, -1, \"on\"),\n",
            "                (\"m\", si_sb_C_im_bysize, -2, \"\"),\n",
            "                (\"i\", si_sb_C_i_bysize, -1, \"\"),\n",
            "            ):\n",
            "                if lowerword[-1] == lastlet:  # this test to add speed\n",
            "                    for k, v in d.items():\n",
            "                        if lowerword[-k:] in v:\n",
            "                            return word[:numend] + post\n",
            "\n",
            "        # HANDLE PLURLS ENDING IN uses -> use\n",
            "\n",
            "        if (\n",
            "            lowerword[-6:] == \"houses\"\n",
            "            or word in si_sb_uses_use_case\n",
            "            or lowerword in si_sb_uses_use\n",
            "        ):\n",
            "            return word[:-1]\n",
            "\n",
            "        # HANDLE PLURLS ENDING IN ies -> ie\n",
            "\n",
            "        if word in si_sb_ies_ie_case or lowerword in si_sb_ies_ie:\n",
            "            return word[:-1]\n",
            "\n",
            "        # HANDLE PLURLS ENDING IN oes -> oe\n",
            "\n",
            "        if (\n",
            "            lowerword[-5:] == \"shoes\"\n",
            "            or word in si_sb_oes_oe_case\n",
            "            or lowerword in si_sb_oes_oe\n",
            "        ):\n",
            "            return word[:-1]\n",
            "\n",
            "        # HANDLE SINGULAR NOUNS ENDING IN ...s OR OTHER SILIBANTS\n",
            "\n",
            "        if word in si_sb_sses_sse_case or lowerword in si_sb_sses_sse:\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword in si_sb_singular_s_complete:\n",
            "            return word[:-2]\n",
            "\n",
            "        for k, v in si_sb_singular_s_bysize.items():\n",
            "            if lowerword[-k:] in v:\n",
            "                return word[:-2]\n",
            "\n",
            "        if lowerword[-4:] == \"eses\" and word[0] == word[0].upper():\n",
            "            return word[:-2]\n",
            "\n",
            "        if lowerword in si_sb_z_zes:\n",
            "            return word[:-2]\n",
            "\n",
            "        if lowerword in si_sb_zzes_zz:\n",
            "            return word[:-2]\n",
            "\n",
            "        if lowerword[-4:] == \"zzes\":\n",
            "            return word[:-3]\n",
            "\n",
            "        if word in si_sb_ches_che_case or lowerword in si_sb_ches_che:\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword[-4:] in (\"ches\", \"shes\"):\n",
            "            return word[:-2]\n",
            "\n",
            "        if lowerword in si_sb_xes_xe:\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword[-3:] == \"xes\":\n",
            "            return word[:-2]\n",
            "\n",
            "        # HANDLE ...f -> ...ves\n",
            "\n",
            "        if word in si_sb_ves_ve_case or lowerword in si_sb_ves_ve:\n",
            "            return word[:-1]\n",
            "\n",
            "        if lowerword[-3:] == \"ves\":\n",
            "            if lowerword[-5:-3] in (\"el\", \"al\", \"ol\"):\n",
            "                return word[:-3] + \"f\"\n",
            "            if lowerword[-5:-3] == \"ea\" and word[-6:-5] != \"d\":\n",
            "                return word[:-3] + \"f\"\n",
            "            if lowerword[-5:-3] in (\"ni\", \"li\", \"wi\"):\n",
            "                return word[:-3] + \"fe\"\n",
            "            if lowerword[-5:-3] == \"ar\":\n",
            "                return word[:-3] + \"f\"\n",
            "\n",
            "        # HANDLE ...y\n",
            "\n",
            "        if lowerword[-2:] == \"ys\":\n",
            "            if len(lowerword) > 2 and lowerword[-3] in \"aeiou\":\n",
            "                return word[:-1]\n",
            "\n",
            "            if self.classical_dict[\"names\"]:\n",
            "                if lowerword[-2:] == \"ys\" and word[0] == word[0].upper():\n",
            "                    return word[:-1]\n",
            "\n",
            "        if lowerword[-3:] == \"ies\":\n",
            "            return word[:-3] + \"y\"\n",
            "\n",
            "        # HANDLE ...o\n",
            "\n",
            "        if lowerword[-2:] == \"os\":\n",
            "\n",
            "            if lowerword in si_sb_U_o_os_complete:\n",
            "                return word[:-1]\n",
            "\n",
            "            for k, v in si_sb_U_o_os_bysize.items():\n",
            "                if lowerword[-k:] in v:\n",
            "                    return word[:-1]\n",
            "\n",
            "            if lowerword[-3:] in (\"aos\", \"eos\", \"ios\", \"oos\", \"uos\"):\n",
            "                return word[:-1]\n",
            "\n",
            "        if lowerword[-3:] == \"oes\":\n",
            "            return word[:-2]\n",
            "\n",
            "        # UNASSIMILATED IMPORTS FINAL RULE\n",
            "\n",
            "        if word in si_sb_es_is:\n",
            "            return word[:-2] + \"is\"\n",
            "\n",
            "        # OTHERWISE JUST REMOVE ...s\n",
            "\n",
            "        if lowerword[-1] == \"s\":\n",
            "            return word[:-1]\n",
            "\n",
            "        # COULD NOT FIND SINGULAR\n",
            "\n",
            "        return False\n",
            "\n",
            "    # ADJECTIVES\n",
            "\n",
            "    def a(self, text, count=1):\n",
            "        \"\"\"\n",
            "        Return the appropriate indefinite article followed by text.\n",
            "\n",
            "        The indefinite article is either 'a' or 'an'.\n",
            "\n",
            "        If count is not one, then return count followed by text\n",
            "        instead of 'a' or 'an'.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        mo = re.search(r\"\\A(\\s*)(?:an?\\s+)?(.+?)(\\s*)\\Z\", text, re.IGNORECASE)\n",
            "        if mo:\n",
            "            word = mo.group(2)\n",
            "            if not word:\n",
            "                return text\n",
            "            pre = mo.group(1)\n",
            "            post = mo.group(3)\n",
            "            result = self._indef_article(word, count)\n",
            "            return \"{}{}{}\".format(pre, result, post)\n",
            "        return \"\"\n",
            "\n",
            "    an = a\n",
            "\n",
            "    def _indef_article(self, word, count):\n",
            "        mycount = self.get_count(count)\n",
            "\n",
            "        if mycount != 1:\n",
            "            return \"{} {}\".format(count, word)\n",
            "\n",
            "        # HANDLE USER-DEFINED VARIANTS\n",
            "\n",
            "        value = self.ud_match(word, self.A_a_user_defined)\n",
            "        if value is not None:\n",
            "            return \"{} {}\".format(value, word)\n",
            "\n",
            "        # HANDLE ORDINAL FORMS\n",
            "\n",
            "        for a in ((r\"^(%s)\" % A_ordinal_a, \"a\"), (r\"^(%s)\" % A_ordinal_an, \"an\")):\n",
            "            mo = re.search(a[0], word, re.IGNORECASE)\n",
            "            if mo:\n",
            "                return \"{} {}\".format(a[1], word)\n",
            "\n",
            "        # HANDLE SPECIAL CASES\n",
            "\n",
            "        for a in (\n",
            "            (r\"^(%s)\" % A_explicit_an, \"an\"),\n",
            "            (r\"^[aefhilmnorsx]$\", \"an\"),\n",
            "            (r\"^[bcdgjkpqtuvwyz]$\", \"a\"),\n",
            "        ):\n",
            "            mo = re.search(a[0], word, re.IGNORECASE)\n",
            "            if mo:\n",
            "                return \"{} {}\".format(a[1], word)\n",
            "\n",
            "        # HANDLE ABBREVIATIONS\n",
            "\n",
            "        for a in (\n",
            "            (r\"(%s)\" % A_abbrev, \"an\", re.VERBOSE),\n",
            "            (r\"^[aefhilmnorsx][.-]\", \"an\", re.IGNORECASE),\n",
            "            (r\"^[a-z][.-]\", \"a\", re.IGNORECASE),\n",
            "        ):\n",
            "            mo = re.search(a[0], word, a[2])\n",
            "            if mo:\n",
            "                return \"{} {}\".format(a[1], word)\n",
            "\n",
            "        # HANDLE CONSONANTS\n",
            "\n",
            "        mo = re.search(r\"^[^aeiouy]\", word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return \"a %s\" % word\n",
            "\n",
            "        # HANDLE SPECIAL VOWEL-FORMS\n",
            "\n",
            "        for a in (\n",
            "            (r\"^e[uw]\", \"a\"),\n",
            "            (r\"^onc?e\\b\", \"a\"),\n",
            "            (r\"^onetime\\b\", \"a\"),\n",
            "            (r\"^uni([^nmd]|mo)\", \"a\"),\n",
            "            (r\"^u[bcfghjkqrst][aeiou]\", \"a\"),\n",
            "            (r\"^ukr\", \"a\"),\n",
            "            (r\"^(%s)\" % A_explicit_a, \"a\"),\n",
            "        ):\n",
            "            mo = re.search(a[0], word, re.IGNORECASE)\n",
            "            if mo:\n",
            "                return \"{} {}\".format(a[1], word)\n",
            "\n",
            "        # HANDLE SPECIAL CAPITALS\n",
            "\n",
            "        mo = re.search(r\"^U[NK][AIEO]?\", word)\n",
            "        if mo:\n",
            "            return \"a %s\" % word\n",
            "\n",
            "        # HANDLE VOWELS\n",
            "\n",
            "        mo = re.search(r\"^[aeiou]\", word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return \"an %s\" % word\n",
            "\n",
            "        # HANDLE y... (BEFORE CERTAIN CONSONANTS IMPLIES (UNNATURALIZED) \"i..\" SOUND)\n",
            "\n",
            "        mo = re.search(r\"^(%s)\" % A_y_cons, word, re.IGNORECASE)\n",
            "        if mo:\n",
            "            return \"an %s\" % word\n",
            "\n",
            "        # OTHERWISE, GUESS \"a\"\n",
            "        return \"a %s\" % word\n",
            "\n",
            "    # 2. TRANSLATE ZERO-QUANTIFIED $word TO \"no plural($word)\"\n",
            "\n",
            "    def no(self, text, count=None):\n",
            "        \"\"\"\n",
            "        If count is 0, no, zero or nil, return 'no' followed by the plural\n",
            "        of text.\n",
            "\n",
            "        If count is one of:\n",
            "            1, a, an, one, each, every, this, that\n",
            "        return count followed by text.\n",
            "\n",
            "        Otherwise return count follow by the plural of text.\n",
            "\n",
            "        In the return value count is always followed by a space.\n",
            "\n",
            "        Whitespace at the start and end is preserved.\n",
            "\n",
            "        \"\"\"\n",
            "        if count is None and self.persistent_count is not None:\n",
            "            count = self.persistent_count\n",
            "\n",
            "        if count is None:\n",
            "            count = 0\n",
            "        mo = re.search(r\"\\A(\\s*)(.+?)(\\s*)\\Z\", text)\n",
            "        pre = mo.group(1)\n",
            "        word = mo.group(2)\n",
            "        post = mo.group(3)\n",
            "\n",
            "        if str(count).lower() in pl_count_zero:\n",
            "            return \"{}no {}{}\".format(pre, self.plural(word, 0), post)\n",
            "        else:\n",
            "            return \"{}{} {}{}\".format(pre, count, self.plural(word, count), post)\n",
            "\n",
            "    # PARTICIPLES\n",
            "\n",
            "    def present_participle(self, word):\n",
            "        \"\"\"\n",
            "        Return the present participle for word.\n",
            "\n",
            "        word is the 3rd person singular verb.\n",
            "\n",
            "        \"\"\"\n",
            "        plv = self.plural_verb(word, 2)\n",
            "\n",
            "        for pat, repl in (\n",
            "            (r\"ie$\", r\"y\"),\n",
            "            (r\"ue$\", r\"u\"),  # TODO: isn't ue$ -> u encompassed in the following rule?\n",
            "            (r\"([auy])e$\", r\"\\g<1>\"),\n",
            "            (r\"ski$\", r\"ski\"),\n",
            "            (r\"[^b]i$\", r\"\"),\n",
            "            (r\"^(are|were)$\", r\"be\"),\n",
            "            (r\"^(had)$\", r\"hav\"),\n",
            "            (r\"^(hoe)$\", r\"\\g<1>\"),\n",
            "            (r\"([^e])e$\", r\"\\g<1>\"),\n",
            "            (r\"er$\", r\"er\"),\n",
            "            (r\"([^aeiou][aeiouy]([bdgmnprst]))$\", r\"\\g<1>\\g<2>\"),\n",
            "        ):\n",
            "            (ans, num) = re.subn(pat, repl, plv)\n",
            "            if num:\n",
            "                return \"%sing\" % ans\n",
            "        return \"%sing\" % ans\n",
            "\n",
            "    # NUMERICAL INFLECTIONS\n",
            "\n",
            "    def ordinal(self, num):\n",
            "        \"\"\"\n",
            "        Return the ordinal of num.\n",
            "\n",
            "        num can be an integer or text\n",
            "\n",
            "        e.g. ordinal(1) returns '1st'\n",
            "        ordinal('one') returns 'first'\n",
            "\n",
            "        \"\"\"\n",
            "        if re.match(r\"\\d\", str(num)):\n",
            "            try:\n",
            "                num % 2\n",
            "                n = num\n",
            "            except TypeError:\n",
            "                if \".\" in str(num):\n",
            "                    try:\n",
            "                        # numbers after decimal,\n",
            "                        # so only need last one for ordinal\n",
            "                        n = int(num[-1])\n",
            "\n",
            "                    except ValueError:  # ends with '.', so need to use whole string\n",
            "                        n = int(num[:-1])\n",
            "                else:\n",
            "                    n = int(num)\n",
            "            try:\n",
            "                post = nth[n % 100]\n",
            "            except KeyError:\n",
            "                post = nth[n % 10]\n",
            "            return \"{}{}\".format(num, post)\n",
            "        else:\n",
            "            mo = re.search(r\"(%s)\\Z\" % ordinal_suff, num)\n",
            "            try:\n",
            "                post = ordinal[mo.group(1)]\n",
            "                return re.sub(r\"(%s)\\Z\" % ordinal_suff, post, num)\n",
            "            except AttributeError:\n",
            "                return \"%sth\" % num\n",
            "\n",
            "    def millfn(self, ind=0):\n",
            "        if ind > len(mill) - 1:\n",
            "            print3(\"number out of range\")\n",
            "            raise NumOutOfRangeError\n",
            "        return mill[ind]\n",
            "\n",
            "    def unitfn(self, units, mindex=0):\n",
            "        return \"{}{}\".format(unit[units], self.millfn(mindex))\n",
            "\n",
            "    def tenfn(self, tens, units, mindex=0):\n",
            "        if tens != 1:\n",
            "            return \"{}{}{}{}\".format(\n",
            "                ten[tens],\n",
            "                \"-\" if tens and units else \"\",\n",
            "                unit[units],\n",
            "                self.millfn(mindex),\n",
            "            )\n",
            "        return \"{}{}\".format(teen[units], mill[mindex])\n",
            "\n",
            "    def hundfn(self, hundreds, tens, units, mindex):\n",
            "        if hundreds:\n",
            "            andword = \" %s \" % self.number_args[\"andword\"] if tens or units else \"\"\n",
            "            return \"{} hundred{}{}{}, \".format(\n",
            "                unit[hundreds],  # use unit not unitfn as simpler\n",
            "                andword,\n",
            "                self.tenfn(tens, units),\n",
            "                self.millfn(mindex),\n",
            "            )\n",
            "        if tens or units:\n",
            "            return \"{}{}, \".format(self.tenfn(tens, units), self.millfn(mindex))\n",
            "        return \"\"\n",
            "\n",
            "    def group1sub(self, mo):\n",
            "        units = int(mo.group(1))\n",
            "        if units == 1:\n",
            "            return \" %s, \" % self.number_args[\"one\"]\n",
            "        elif units:\n",
            "            return \"%s, \" % unit[units]\n",
            "        else:\n",
            "            return \" %s, \" % self.number_args[\"zero\"]\n",
            "\n",
            "    def group1bsub(self, mo):\n",
            "        units = int(mo.group(1))\n",
            "        if units:\n",
            "            return \"%s, \" % unit[units]\n",
            "        else:\n",
            "            return \" %s, \" % self.number_args[\"zero\"]\n",
            "\n",
            "    def group2sub(self, mo):\n",
            "        tens = int(mo.group(1))\n",
            "        units = int(mo.group(2))\n",
            "        if tens:\n",
            "            return \"%s, \" % self.tenfn(tens, units)\n",
            "        if units:\n",
            "            return \" {} {}, \".format(self.number_args[\"zero\"], unit[units])\n",
            "        return \" {} {}, \".format(self.number_args[\"zero\"], self.number_args[\"zero\"])\n",
            "\n",
            "    def group3sub(self, mo):\n",
            "        hundreds = int(mo.group(1))\n",
            "        tens = int(mo.group(2))\n",
            "        units = int(mo.group(3))\n",
            "        if hundreds == 1:\n",
            "            hunword = \" %s\" % self.number_args[\"one\"]\n",
            "        elif hundreds:\n",
            "            hunword = \"%s\" % unit[hundreds]\n",
            "        else:\n",
            "            hunword = \" %s\" % self.number_args[\"zero\"]\n",
            "        if tens:\n",
            "            tenword = self.tenfn(tens, units)\n",
            "        elif units:\n",
            "            tenword = \" {} {}\".format(self.number_args[\"zero\"], unit[units])\n",
            "        else:\n",
            "            tenword = \" {} {}\".format(\n",
            "                self.number_args[\"zero\"], self.number_args[\"zero\"]\n",
            "            )\n",
            "        return \"{} {}, \".format(hunword, tenword)\n",
            "\n",
            "    def hundsub(self, mo):\n",
            "        ret = self.hundfn(\n",
            "            int(mo.group(1)), int(mo.group(2)), int(mo.group(3)), self.mill_count\n",
            "        )\n",
            "        self.mill_count += 1\n",
            "        return ret\n",
            "\n",
            "    def tensub(self, mo):\n",
            "        return \"%s, \" % self.tenfn(int(mo.group(1)), int(mo.group(2)), self.mill_count)\n",
            "\n",
            "    def unitsub(self, mo):\n",
            "        return \"%s, \" % self.unitfn(int(mo.group(1)), self.mill_count)\n",
            "\n",
            "    def enword(self, num, group):\n",
            "        # import pdb\n",
            "        # pdb.set_trace()\n",
            "\n",
            "        if group == 1:\n",
            "            num = re.sub(r\"(\\d)\", self.group1sub, num)\n",
            "        elif group == 2:\n",
            "            num = re.sub(r\"(\\d)(\\d)\", self.group2sub, num)\n",
            "            num = re.sub(r\"(\\d)\", self.group1bsub, num, 1)\n",
            "        elif group == 3:\n",
            "            num = re.sub(r\"(\\d)(\\d)(\\d)\", self.group3sub, num)\n",
            "            num = re.sub(r\"(\\d)(\\d)\", self.group2sub, num, 1)\n",
            "            num = re.sub(r\"(\\d)\", self.group1sub, num, 1)\n",
            "        elif int(num) == 0:\n",
            "            num = self.number_args[\"zero\"]\n",
            "        elif int(num) == 1:\n",
            "            num = self.number_args[\"one\"]\n",
            "        else:\n",
            "            num = num.lstrip().lstrip(\"0\")\n",
            "            self.mill_count = 0\n",
            "            # surely there's a better way to do the next bit\n",
            "            mo = re.search(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", num)\n",
            "            while mo:\n",
            "                num = re.sub(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", self.hundsub, num, 1)\n",
            "                mo = re.search(r\"(\\d)(\\d)(\\d)(?=\\D*\\Z)\", num)\n",
            "            num = re.sub(r\"(\\d)(\\d)(?=\\D*\\Z)\", self.tensub, num, 1)\n",
            "            num = re.sub(r\"(\\d)(?=\\D*\\Z)\", self.unitsub, num, 1)\n",
            "        return num\n",
            "\n",
            "    def blankfn(self, mo):\n",
            "        \"\"\" do a global blank replace\n",
            "        TODO: surely this can be done with an option to re.sub\n",
            "              rather than this fn\n",
            "        \"\"\"\n",
            "        return \"\"\n",
            "\n",
            "    def commafn(self, mo):\n",
            "        \"\"\" do a global ',' replace\n",
            "        TODO: surely this can be done with an option to re.sub\n",
            "              rather than this fn\n",
            "        \"\"\"\n",
            "        return \",\"\n",
            "\n",
            "    def spacefn(self, mo):\n",
            "        \"\"\" do a global ' ' replace\n",
            "        TODO: surely this can be done with an option to re.sub\n",
            "              rather than this fn\n",
            "        \"\"\"\n",
            "        return \" \"\n",
            "\n",
            "    def number_to_words(\n",
            "        self,\n",
            "        num,\n",
            "        wantlist=False,\n",
            "        group=0,\n",
            "        comma=\",\",\n",
            "        andword=\"and\",\n",
            "        zero=\"zero\",\n",
            "        one=\"one\",\n",
            "        decimal=\"point\",\n",
            "        threshold=None,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Return a number in words.\n",
            "\n",
            "        group = 1, 2 or 3 to group numbers before turning into words\n",
            "        comma: define comma\n",
            "        andword: word for 'and'. Can be set to ''.\n",
            "            e.g. \"one hundred and one\" vs \"one hundred one\"\n",
            "        zero: word for '0'\n",
            "        one: word for '1'\n",
            "        decimal: word for decimal point\n",
            "        threshold: numbers above threshold not turned into words\n",
            "\n",
            "        parameters not remembered from last call. Departure from Perl version.\n",
            "        \"\"\"\n",
            "        self.number_args = dict(andword=andword, zero=zero, one=one)\n",
            "        num = \"%s\" % num\n",
            "\n",
            "        # Handle \"stylistic\" conversions (up to a given threshold)...\n",
            "        if threshold is not None and float(num) > threshold:\n",
            "            spnum = num.split(\".\", 1)\n",
            "            while comma:\n",
            "                (spnum[0], n) = re.subn(r\"(\\d)(\\d{3}(?:,|\\Z))\", r\"\\1,\\2\", spnum[0])\n",
            "                if n == 0:\n",
            "                    break\n",
            "            try:\n",
            "                return \"{}.{}\".format(spnum[0], spnum[1])\n",
            "            except IndexError:\n",
            "                return \"%s\" % spnum[0]\n",
            "\n",
            "        if group < 0 or group > 3:\n",
            "            raise BadChunkingOptionError\n",
            "        nowhite = num.lstrip()\n",
            "        if nowhite[0] == \"+\":\n",
            "            sign = \"plus\"\n",
            "        elif nowhite[0] == \"-\":\n",
            "            sign = \"minus\"\n",
            "        else:\n",
            "            sign = \"\"\n",
            "\n",
            "        myord = num[-2:] in (\"st\", \"nd\", \"rd\", \"th\")\n",
            "        if myord:\n",
            "            num = num[:-2]\n",
            "        finalpoint = False\n",
            "        if decimal:\n",
            "            if group != 0:\n",
            "                chunks = num.split(\".\")\n",
            "            else:\n",
            "                chunks = num.split(\".\", 1)\n",
            "            if chunks[-1] == \"\":  # remove blank string if nothing after decimal\n",
            "                chunks = chunks[:-1]\n",
            "                finalpoint = True  # add 'point' to end of output\n",
            "        else:\n",
            "            chunks = [num]\n",
            "\n",
            "        first = 1\n",
            "        loopstart = 0\n",
            "\n",
            "        if chunks[0] == \"\":\n",
            "            first = 0\n",
            "            if len(chunks) > 1:\n",
            "                loopstart = 1\n",
            "\n",
            "        for i in range(loopstart, len(chunks)):\n",
            "            chunk = chunks[i]\n",
            "            # remove all non numeric \\D\n",
            "            chunk = re.sub(r\"\\D\", self.blankfn, chunk)\n",
            "            if chunk == \"\":\n",
            "                chunk = \"0\"\n",
            "\n",
            "            if group == 0 and (first == 0 or first == \"\"):\n",
            "                chunk = self.enword(chunk, 1)\n",
            "            else:\n",
            "                chunk = self.enword(chunk, group)\n",
            "\n",
            "            if chunk[-2:] == \", \":\n",
            "                chunk = chunk[:-2]\n",
            "            chunk = re.sub(r\"\\s+,\", self.commafn, chunk)\n",
            "\n",
            "            if group == 0 and first:\n",
            "                chunk = re.sub(r\", (\\S+)\\s+\\Z\", \" %s \\\\1\" % andword, chunk)\n",
            "            chunk = re.sub(r\"\\s+\", self.spacefn, chunk)\n",
            "            # chunk = re.sub(r\"(\\A\\s|\\s\\Z)\", self.blankfn, chunk)\n",
            "            chunk = chunk.strip()\n",
            "            if first:\n",
            "                first = \"\"\n",
            "            chunks[i] = chunk\n",
            "\n",
            "        numchunks = []\n",
            "        if first != 0:\n",
            "            numchunks = chunks[0].split(\"%s \" % comma)\n",
            "\n",
            "        if myord and numchunks:\n",
            "            # TODO: can this be just one re as it is in perl?\n",
            "            mo = re.search(r\"(%s)\\Z\" % ordinal_suff, numchunks[-1])\n",
            "            if mo:\n",
            "                numchunks[-1] = re.sub(\n",
            "                    r\"(%s)\\Z\" % ordinal_suff, ordinal[mo.group(1)], numchunks[-1]\n",
            "                )\n",
            "            else:\n",
            "                numchunks[-1] += \"th\"\n",
            "\n",
            "        for chunk in chunks[1:]:\n",
            "            numchunks.append(decimal)\n",
            "            numchunks.extend(chunk.split(\"%s \" % comma))\n",
            "\n",
            "        if finalpoint:\n",
            "            numchunks.append(decimal)\n",
            "\n",
            "        # wantlist: Perl list context. can explictly specify in Python\n",
            "        if wantlist:\n",
            "            if sign:\n",
            "                numchunks = [sign] + numchunks\n",
            "            return numchunks\n",
            "        elif group:\n",
            "            signout = \"%s \" % sign if sign else \"\"\n",
            "            return \"{}{}\".format(signout, \", \".join(numchunks))\n",
            "        else:\n",
            "            signout = \"%s \" % sign if sign else \"\"\n",
            "            num = \"{}{}\".format(signout, numchunks.pop(0))\n",
            "            if decimal is None:\n",
            "                first = True\n",
            "            else:\n",
            "                first = not num.endswith(decimal)\n",
            "            for nc in numchunks:\n",
            "                if nc == decimal:\n",
            "                    num += \" %s\" % nc\n",
            "                    first = 0\n",
            "                elif first:\n",
            "                    num += \"{} {}\".format(comma, nc)\n",
            "                else:\n",
            "                    num += \" %s\" % nc\n",
            "            return num\n",
            "\n",
            "    # Join words with commas and a trailing 'and' (when appropriate)...\n",
            "\n",
            "    def join(\n",
            "        self,\n",
            "        words,\n",
            "        sep=None,\n",
            "        sep_spaced=True,\n",
            "        final_sep=None,\n",
            "        conj=\"and\",\n",
            "        conj_spaced=True,\n",
            "    ):\n",
            "        \"\"\"\n",
            "        Join words into a list.\n",
            "\n",
            "        e.g. join(['ant', 'bee', 'fly']) returns 'ant, bee, and fly'\n",
            "\n",
            "        options:\n",
            "        conj: replacement for 'and'\n",
            "        sep: separator. default ',', unless ',' is in the list then ';'\n",
            "        final_sep: final separator. default ',', unless ',' is in the list then ';'\n",
            "        conj_spaced: boolean. Should conj have spaces around it\n",
            "\n",
            "        \"\"\"\n",
            "        if not words:\n",
            "            return \"\"\n",
            "        if len(words) == 1:\n",
            "            return words[0]\n",
            "\n",
            "        if conj_spaced:\n",
            "            if conj == \"\":\n",
            "                conj = \" \"\n",
            "            else:\n",
            "                conj = \" %s \" % conj\n",
            "\n",
            "        if len(words) == 2:\n",
            "            return \"{}{}{}\".format(words[0], conj, words[1])\n",
            "\n",
            "        if sep is None:\n",
            "            if \",\" in \"\".join(words):\n",
            "                sep = \";\"\n",
            "            else:\n",
            "                sep = \",\"\n",
            "        if final_sep is None:\n",
            "            final_sep = sep\n",
            "\n",
            "        final_sep = \"{}{}\".format(final_sep, conj)\n",
            "\n",
            "        if sep_spaced:\n",
            "            sep += \" \"\n",
            "\n",
            "        return \"{}{}{}\".format(sep.join(words[0:-1]), final_sep, words[-1])\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding:utf-8 -*-\n",
            "\"\"\"Functions for converting between Julian dates and calendar dates.\n",
            "\n",
            "A function for converting Gregorian calendar dates to Julian dates, and\n",
            "another function for converting Julian calendar dates to Julian dates\n",
            "are defined. Two functions for the reverse calculations are also\n",
            "defined.\n",
            "\n",
            "Different regions of the world switched to Gregorian calendar from\n",
            "Julian calendar on different dates. Having separate functions for Julian\n",
            "and Gregorian calendars allow maximum flexibility in choosing the\n",
            "relevant calendar.\n",
            "\n",
            "All the above functions are \"proleptic\". This means that they work for\n",
            "dates on which the concerned calendar is not valid. For example,\n",
            "Gregorian calendar was not used prior to around October 1582.\n",
            "\n",
            "Julian dates are stored in two floating point numbers (double).  Julian\n",
            "dates, and Modified Julian dates, are large numbers. If only one number\n",
            "is used, then the precision of the time stored is limited. Using two\n",
            "numbers, time can be split in a manner that will allow maximum\n",
            "precision. For example, the first number could be the Julian date for\n",
            "the beginning of a day and the second number could be the fractional\n",
            "day. Calculations that need the latter part can now work with maximum\n",
            "precision.\n",
            "\n",
            "A function to test if a given Gregorian calendar year is a leap year is\n",
            "defined.\n",
            "\n",
            "Zero point of Modified Julian Date (MJD) and the MJD of 2000/1/1\n",
            "12:00:00 are also given.\n",
            "\n",
            "This module is based on the TPM C library, by Jeffery W. Percival. The\n",
            "idea for splitting Julian date into two floating point numbers was\n",
            "inspired by the IAU SOFA C library.\n",
            "\n",
            ":author: Prasanth Nair\n",
            ":contact: prasanthhn@gmail.com\n",
            ":license: BSD (https://opensource.org/licenses/bsd-license.php)\n",
            "\"\"\"\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "import math\n",
            "\n",
            "__version__ = \"1.4.1\"\n",
            "\n",
            "MJD_0 = 2400000.5\n",
            "MJD_JD2000 = 51544.5\n",
            "\n",
            "\n",
            "def ipart(x):\n",
            "    \"\"\"Return integer part of given number.\"\"\"\n",
            "    return math.modf(x)[1]\n",
            "\n",
            "\n",
            "def is_leap(year):\n",
            "    \"\"\"Leap year or not in the Gregorian calendar.\"\"\"\n",
            "    x = math.fmod(year, 4)\n",
            "    y = math.fmod(year, 100)\n",
            "    z = math.fmod(year, 400)\n",
            "\n",
            "    # Divisible by 4 and,\n",
            "    # either not divisible by 100 or divisible by 400.\n",
            "    return not x and (y or not z)\n",
            "\n",
            "\n",
            "def gcal2jd(year, month, day):\n",
            "    \"\"\"Gregorian calendar date to Julian date.\n",
            "\n",
            "    The input and output are for the proleptic Gregorian calendar,\n",
            "    i.e., no consideration of historical usage of the calendar is\n",
            "    made.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    year : int\n",
            "        Year as an integer.\n",
            "    month : int\n",
            "        Month as an integer.\n",
            "    day : int\n",
            "        Day as an integer.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    jd1, jd2: 2-element tuple of floats\n",
            "        When added together, the numbers give the Julian date for the\n",
            "        given Gregorian calendar date. The first number is always\n",
            "        MJD_0 i.e., 2451545.5. So the second is the MJD.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> gcal2jd(2000,1,1)\n",
            "    (2400000.5, 51544.0)\n",
            "    >>> 2400000.5 + 51544.0 + 0.5\n",
            "    2451545.0\n",
            "    >>> year = [-4699, -2114, -1050, -123, -1, 0, 1, 123, 1678.0, 2000,\n",
            "    ....: 2012, 2245]\n",
            "    >>> month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "    >>> day = [1, 12, 23, 14, 25, 16, 27, 8, 9, 10, 11, 31]\n",
            "    >>> x = [gcal2jd(y, m, d) for y, m, d in zip(year, month, day)]\n",
            "    >>> for i in x: print i\n",
            "    (2400000.5, -2395215.0)\n",
            "    (2400000.5, -1451021.0)\n",
            "    (2400000.5, -1062364.0)\n",
            "    (2400000.5, -723762.0)\n",
            "    (2400000.5, -679162.0)\n",
            "    (2400000.5, -678774.0)\n",
            "    (2400000.5, -678368.0)\n",
            "    (2400000.5, -633797.0)\n",
            "    (2400000.5, -65812.0)\n",
            "    (2400000.5, 51827.0)\n",
            "    (2400000.5, 56242.0)\n",
            "    (2400000.5, 141393.0)\n",
            "\n",
            "    Negative months and days are valid. For example, 2000/-2/-4 =>\n",
            "    1999/+12-2/-4 => 1999/10/-4 => 1999/9/30-4 => 1999/9/26.\n",
            "\n",
            "    >>> gcal2jd(2000, -2, -4)\n",
            "    (2400000.5, 51447.0)\n",
            "    >>> gcal2jd(1999, 9, 26)\n",
            "    (2400000.5, 51447.0)\n",
            "\n",
            "    >>> gcal2jd(2000, 2, -1)\n",
            "    (2400000.5, 51573.0)\n",
            "    >>> gcal2jd(2000, 1, 30)\n",
            "    (2400000.5, 51573.0)\n",
            "\n",
            "    >>> gcal2jd(2000, 3, -1)\n",
            "    (2400000.5, 51602.0)\n",
            "    >>> gcal2jd(2000, 2, 28)\n",
            "    (2400000.5, 51602.0)\n",
            "\n",
            "    Month 0 becomes previous month.\n",
            "\n",
            "    >>> gcal2jd(2000, 0, 1)\n",
            "    (2400000.5, 51513.0)\n",
            "    >>> gcal2jd(1999, 12, 1)\n",
            "    (2400000.5, 51513.0)\n",
            "\n",
            "    Day number 0 becomes last day of previous month.\n",
            "\n",
            "    >>> gcal2jd(2000, 3, 0)\n",
            "    (2400000.5, 51603.0)\n",
            "    >>> gcal2jd(2000, 2, 29)\n",
            "    (2400000.5, 51603.0)\n",
            "\n",
            "    If `day` is greater than the number of days in `month`, then it\n",
            "    gets carried over to the next month.\n",
            "\n",
            "    >>> gcal2jd(2000,2,30)\n",
            "    (2400000.5, 51604.0)\n",
            "    >>> gcal2jd(2000,3,1)\n",
            "    (2400000.5, 51604.0)\n",
            "\n",
            "    >>> gcal2jd(2001,2,30)\n",
            "    (2400000.5, 51970.0)\n",
            "    >>> gcal2jd(2001,3,2)\n",
            "    (2400000.5, 51970.0)\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The returned Julian date is for mid-night of the given date. To\n",
            "    find the Julian date for any time of the day, simply add time as a\n",
            "    fraction of a day. For example Julian date for mid-day can be\n",
            "    obtained by adding 0.5 to either the first part or the second\n",
            "    part. The latter is preferable, since it will give the MJD for the\n",
            "    date and time.\n",
            "\n",
            "    BC dates should be given as -(BC - 1) where BC is the year. For\n",
            "    example 1 BC == 0, 2 BC == -1, and so on.\n",
            "\n",
            "    Negative numbers can be used for `month` and `day`. For example\n",
            "    2000, -1, 1 is the same as 1999, 11, 1.\n",
            "\n",
            "    The Julian dates are proleptic Julian dates, i.e., values are\n",
            "    returned without considering if Gregorian dates are valid for the\n",
            "    given date.\n",
            "\n",
            "    The input values are truncated to integers.\n",
            "\n",
            "    \"\"\"\n",
            "    year = int(year)\n",
            "    month = int(month)\n",
            "    day = int(day)\n",
            "\n",
            "    a = ipart((month - 14) / 12.0)\n",
            "    jd = ipart((1461 * (year + 4800 + a)) / 4.0)\n",
            "    jd += ipart((367 * (month - 2 - 12 * a)) / 12.0)\n",
            "    x = ipart((year + 4900 + a) / 100.0)\n",
            "    jd -= ipart((3 * x) / 4.0)\n",
            "    jd += day - 2432075.5  # was 32075; add 2400000.5\n",
            "\n",
            "    jd -= 0.5  # 0 hours; above JD is for midday, switch to midnight.\n",
            "\n",
            "    return MJD_0, jd\n",
            "\n",
            "\n",
            "def jd2gcal(jd1, jd2):\n",
            "    \"\"\"Julian date to Gregorian calendar date and time of day.\n",
            "\n",
            "    The input and output are for the proleptic Gregorian calendar,\n",
            "    i.e., no consideration of historical usage of the calendar is\n",
            "    made.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    jd1, jd2: int\n",
            "        Sum of the two numbers is taken as the given Julian date. For\n",
            "        example `jd1` can be the zero point of MJD (MJD_0) and `jd2`\n",
            "        can be the MJD of the date and time. But any combination will\n",
            "        work.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    y, m, d, f : int, int, int, float\n",
            "        Four element tuple containing year, month, day and the\n",
            "        fractional part of the day in the Gregorian calendar. The first\n",
            "        three are integers, and the last part is a float.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> jd2gcal(*gcal2jd(2000,1,1))\n",
            "    (2000, 1, 1, 0.0)\n",
            "    >>> jd2gcal(*gcal2jd(1950,1,1))\n",
            "    (1950, 1, 1, 0.0)\n",
            "\n",
            "    Out of range months and days are carried over to the next/previous\n",
            "    year or next/previous month. See gcal2jd for more examples.\n",
            "\n",
            "    >>> jd2gcal(*gcal2jd(1999,10,12))\n",
            "    (1999, 10, 12, 0.0)\n",
            "    >>> jd2gcal(*gcal2jd(2000,2,30))\n",
            "    (2000, 3, 1, 0.0)\n",
            "    >>> jd2gcal(*gcal2jd(-1999,10,12))\n",
            "    (-1999, 10, 12, 0.0)\n",
            "    >>> jd2gcal(*gcal2jd(2000, -2, -4))\n",
            "    (1999, 9, 26, 0.0)\n",
            "\n",
            "    >>> gcal2jd(2000,1,1)\n",
            "    (2400000.5, 51544.0)\n",
            "    >>> jd2gcal(2400000.5, 51544.0)\n",
            "    (2000, 1, 1, 0.0)\n",
            "    >>> jd2gcal(2400000.5, 51544.5)\n",
            "    (2000, 1, 1, 0.5)\n",
            "    >>> jd2gcal(2400000.5, 51544.245)\n",
            "    (2000, 1, 1, 0.24500000000261934)\n",
            "    >>> jd2gcal(2400000.5, 51544.1)\n",
            "    (2000, 1, 1, 0.099999999998544808)\n",
            "    >>> jd2gcal(2400000.5, 51544.75)\n",
            "    (2000, 1, 1, 0.75)\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    The last element of the tuple is the same as\n",
            "\n",
            "       (hh + mm / 60.0 + ss / 3600.0) / 24.0\n",
            "\n",
            "    where hh, mm, and ss are the hour, minute and second of the day.\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    gcal2jd\n",
            "\n",
            "    \"\"\"\n",
            "    from math import modf\n",
            "\n",
            "    jd1_f, jd1_i = modf(jd1)\n",
            "    jd2_f, jd2_i = modf(jd2)\n",
            "\n",
            "    jd_i = jd1_i + jd2_i\n",
            "\n",
            "    f = jd1_f + jd2_f\n",
            "\n",
            "    # Set JD to noon of the current date. Fractional part is the\n",
            "    # fraction from midnight of the current date.\n",
            "    if -0.5 < f < 0.5:\n",
            "        f += 0.5\n",
            "    elif f >= 0.5:\n",
            "        jd_i += 1\n",
            "        f -= 0.5\n",
            "    elif f <= -0.5:\n",
            "        jd_i -= 1\n",
            "        f += 1.5\n",
            "\n",
            "    l = jd_i + 68569\n",
            "    n = ipart((4 * l) / 146097.0)\n",
            "    l -= ipart(((146097 * n) + 3) / 4.0)\n",
            "    i = ipart((4000 * (l + 1)) / 1461001)\n",
            "    l -= ipart((1461 * i) / 4.0) - 31\n",
            "    j = ipart((80 * l) / 2447.0)\n",
            "    day = l - ipart((2447 * j) / 80.0)\n",
            "    l = ipart(j / 11.0)\n",
            "    month = j + 2 - (12 * l)\n",
            "    year = 100 * (n - 49) + i + l\n",
            "\n",
            "    return int(year), int(month), int(day), f\n",
            "\n",
            "\n",
            "def jcal2jd(year, month, day):\n",
            "    \"\"\"Julian calendar date to Julian date.\n",
            "\n",
            "    The input and output are for the proleptic Julian calendar,\n",
            "    i.e., no consideration of historical usage of the calendar is\n",
            "    made.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    year : int\n",
            "        Year as an integer.\n",
            "    month : int\n",
            "        Month as an integer.\n",
            "    day : int\n",
            "        Day as an integer.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    jd1, jd2: 2-element tuple of floats\n",
            "        When added together, the numbers give the Julian date for the\n",
            "        given Julian calendar date. The first number is always\n",
            "        MJD_0 i.e., 2451545.5. So the second is the MJD.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> jcal2jd(2000, 1, 1)\n",
            "    (2400000.5, 51557.0)\n",
            "    >>> year = [-4699, -2114, -1050, -123, -1, 0, 1, 123, 1678, 2000,\n",
            "       ...:  2012, 2245]\n",
            "    >>> month = [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]\n",
            "    >>> day = [1, 12, 23, 14, 25, 16, 27, 8, 9, 10, 11, 31]\n",
            "    >>> x = [jcal2jd(y, m, d) for y, m, d in zip(year, month, day)]\n",
            "    >>> for i in x: print i\n",
            "    (2400000.5, -2395252.0)\n",
            "    (2400000.5, -1451039.0)\n",
            "    (2400000.5, -1062374.0)\n",
            "    (2400000.5, -723765.0)\n",
            "    (2400000.5, -679164.0)\n",
            "    (2400000.5, -678776.0)\n",
            "    (2400000.5, -678370.0)\n",
            "    (2400000.5, -633798.0)\n",
            "    (2400000.5, -65772.0)\n",
            "    (2400000.5, 51871.0)\n",
            "    (2400000.5, 56285.0)\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    Unlike `gcal2jd`, negative months and days can result in incorrect\n",
            "    Julian dates.\n",
            "\n",
            "    \"\"\"\n",
            "    year = int(year)\n",
            "    month = int(month)\n",
            "    day = int(day)\n",
            "\n",
            "    jd = 367 * year\n",
            "    x = ipart((month - 9) / 7.0)\n",
            "    jd -= ipart((7 * (year + 5001 + x)) / 4.0)\n",
            "    jd += ipart((275 * month) / 9.0)\n",
            "    jd += day\n",
            "    jd += 1729777 - 2400000.5  # Return 240000.5 as first part of JD.\n",
            "\n",
            "    jd -= 0.5  # Convert midday to midnight.\n",
            "\n",
            "    return MJD_0, jd\n",
            "\n",
            "\n",
            "def jd2jcal(jd1, jd2):\n",
            "    \"\"\"Julian calendar date for the given Julian date.\n",
            "\n",
            "    The input and output are for the proleptic Julian calendar,\n",
            "    i.e., no consideration of historical usage of the calendar is\n",
            "    made.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    jd1, jd2: int\n",
            "        Sum of the two numbers is taken as the given Julian date. For\n",
            "        example `jd1` can be the zero point of MJD (MJD_0) and `jd2`\n",
            "        can be the MJD of the date and time. But any combination will\n",
            "        work.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    y, m, d, f : int, int, int, float\n",
            "        Four element tuple containing year, month, day and the\n",
            "        fractional part of the day in the Julian calendar. The first\n",
            "        three are integers, and the last part is a float.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> jd2jcal(*jcal2jd(2000, 1, 1))\n",
            "    (2000, 1, 1, 0.0)\n",
            "    >>> jd2jcal(*jcal2jd(-4000, 10, 11))\n",
            "    (-4000, 10, 11, 0.0)\n",
            "\n",
            "    >>> jcal2jd(2000, 1, 1)\n",
            "    (2400000.5, 51557.0)\n",
            "    >>> jd2jcal(2400000.5, 51557.0)\n",
            "    (2000, 1, 1, 0.0)\n",
            "    >>> jd2jcal(2400000.5, 51557.5)\n",
            "    (2000, 1, 1, 0.5)\n",
            "    >>> jd2jcal(2400000.5, 51557.245)\n",
            "    (2000, 1, 1, 0.24500000000261934)\n",
            "    >>> jd2jcal(2400000.5, 51557.1)\n",
            "    (2000, 1, 1, 0.099999999998544808)\n",
            "    >>> jd2jcal(2400000.5, 51557.75)\n",
            "    (2000, 1, 1, 0.75)\n",
            "\n",
            "    \"\"\"\n",
            "    from math import modf\n",
            "\n",
            "    jd1_f, jd1_i = modf(jd1)\n",
            "    jd2_f, jd2_i = modf(jd2)\n",
            "\n",
            "    jd_i = jd1_i + jd2_i\n",
            "\n",
            "    f = jd1_f + jd2_f\n",
            "\n",
            "    # Set JD to noon of the current date. Fractional part is the\n",
            "    # fraction from midnight of the current date.\n",
            "    if -0.5 < f < 0.5:\n",
            "        f += 0.5\n",
            "    elif f >= 0.5:\n",
            "        jd_i += 1\n",
            "        f -= 0.5\n",
            "    elif f <= -0.5:\n",
            "        jd_i -= 1\n",
            "        f += 1.5\n",
            "\n",
            "    j = jd_i + 1402.0\n",
            "    k = ipart((j - 1) / 1461.0)\n",
            "    l = j - (1461.0 * k)\n",
            "    n = ipart((l - 1) / 365.0) - ipart(l / 1461.0)\n",
            "    i = l - (365.0 * n) + 30.0\n",
            "    j = ipart((80.0 * i) / 2447.0)\n",
            "    day = i - ipart((2447.0 * j) / 80.0)\n",
            "    i = ipart(j / 11.0)\n",
            "    month = j + 2 - (12.0 * i)\n",
            "    year = (4 * k) + n + i - 4716.0\n",
            "\n",
            "    return int(year), int(month), int(day), f\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# PYTHON_ARGCOMPLETE_OK\n",
            "\"\"\"\n",
            "pytest: unit and functional testing with Python.\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# else we are imported\n",
            "\n",
            "from _pytest.config import main, UsageError, cmdline, hookspec, hookimpl\n",
            "from _pytest.fixtures import fixture, yield_fixture\n",
            "from _pytest.assertion import register_assert_rewrite\n",
            "from _pytest.freeze_support import freeze_includes\n",
            "from _pytest import __version__\n",
            "from _pytest.debugging import pytestPDB as __pytestPDB\n",
            "from _pytest.recwarn import warns, deprecated_call\n",
            "from _pytest.outcomes import fail, skip, importorskip, exit, xfail\n",
            "from _pytest.mark import MARK_GEN as mark, param\n",
            "from _pytest.main import Session\n",
            "from _pytest.nodes import Item, Collector, File\n",
            "from _pytest.fixtures import fillfixtures as _fillfuncargs\n",
            "from _pytest.python import Module, Class, Instance, Function, Generator\n",
            "\n",
            "from _pytest.python_api import approx, raises\n",
            "\n",
            "set_trace = __pytestPDB.set_trace\n",
            "\n",
            "__all__ = [\n",
            "    \"main\",\n",
            "    \"UsageError\",\n",
            "    \"cmdline\",\n",
            "    \"hookspec\",\n",
            "    \"hookimpl\",\n",
            "    \"__version__\",\n",
            "    \"register_assert_rewrite\",\n",
            "    \"freeze_includes\",\n",
            "    \"set_trace\",\n",
            "    \"warns\",\n",
            "    \"deprecated_call\",\n",
            "    \"fixture\",\n",
            "    \"yield_fixture\",\n",
            "    \"fail\",\n",
            "    \"skip\",\n",
            "    \"xfail\",\n",
            "    \"importorskip\",\n",
            "    \"exit\",\n",
            "    \"mark\",\n",
            "    \"param\",\n",
            "    \"approx\",\n",
            "    \"_fillfuncargs\",\n",
            "    \"Item\",\n",
            "    \"File\",\n",
            "    \"Collector\",\n",
            "    \"Session\",\n",
            "    \"Module\",\n",
            "    \"Class\",\n",
            "    \"Instance\",\n",
            "    \"Function\",\n",
            "    \"Generator\",\n",
            "    \"raises\",\n",
            "]\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    # if run as a script or by 'python -m pytest'\n",
            "    # we trigger the below \"else\" condition by the following import\n",
            "    import pytest\n",
            "\n",
            "    raise SystemExit(pytest.main())\n",
            "else:\n",
            "\n",
            "    from _pytest.compat import _setup_collect_fakemodule\n",
            "\n",
            "    _setup_collect_fakemodule()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# #########################     LICENCE     ###############################\n",
            "#\n",
            "#   Copyright (c) 2010-2016, Michele Simionato\n",
            "#   All rights reserved.\n",
            "#\n",
            "#   Redistributions of source code must retain the above copyright\n",
            "#   notice, this list of conditions and the following disclaimer.\n",
            "#   Redistributions in bytecode form must reproduce the above copyright\n",
            "#   notice, this list of conditions and the following disclaimer in\n",
            "#   the documentation and/or other materials provided with the\n",
            "#   distribution.\n",
            "\n",
            "#   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
            "#   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
            "#   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
            "#   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
            "#   HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n",
            "#   INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n",
            "#   BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS\n",
            "#   OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
            "#   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\n",
            "#   TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n",
            "#   USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
            "#   DAMAGE.\n",
            "\"\"\"\n",
            "See doc/plac.pdf, doc/plac_adv.pdf for the documentation.\n",
            "\"\"\"\n",
            "import sys\n",
            "from plac_core import *\n",
            "\n",
            "__version__ = '0.9.6'\n",
            "\n",
            "\n",
            "if sys.version >= '2.5':\n",
            "    from plac_ext import (Interpreter, import_main, ReadlineInput, \n",
            "                          stdout, runp, Monitor, default_help)\n",
            "    try:\n",
            "        from plac_tk import TkMonitor\n",
            "    except ImportError:\n",
            "        pass\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# coding: utf-8\n",
            "\"\"\"\n",
            "    mistune\n",
            "    ~~~~~~~\n",
            "\n",
            "    The fastest markdown parser in pure Python with renderer feature.\n",
            "\n",
            "    :copyright: (c) 2014 - 2018 by Hsiaoming Yang.\n",
            "\"\"\"\n",
            "\n",
            "import re\n",
            "import inspect\n",
            "\n",
            "__version__ = '0.8.4'\n",
            "__author__ = 'Hsiaoming Yang <me@lepture.com>'\n",
            "__all__ = [\n",
            "    'BlockGrammar', 'BlockLexer',\n",
            "    'InlineGrammar', 'InlineLexer',\n",
            "    'Renderer', 'Markdown',\n",
            "    'markdown', 'escape',\n",
            "]\n",
            "\n",
            "\n",
            "_key_pattern = re.compile(r'\\s+')\n",
            "_nonalpha_pattern = re.compile(r'\\W')\n",
            "_escape_pattern = re.compile(r'&(?!#?\\w+;)')\n",
            "_newline_pattern = re.compile(r'\\r\\n|\\r')\n",
            "_block_quote_leading_pattern = re.compile(r'^ *> ?', flags=re.M)\n",
            "_block_code_leading_pattern = re.compile(r'^ {4}', re.M)\n",
            "_inline_tags = [\n",
            "    'a', 'em', 'strong', 'small', 's', 'cite', 'q', 'dfn', 'abbr', 'data',\n",
            "    'time', 'code', 'var', 'samp', 'kbd', 'sub', 'sup', 'i', 'b', 'u', 'mark',\n",
            "    'ruby', 'rt', 'rp', 'bdi', 'bdo', 'span', 'br', 'wbr', 'ins', 'del',\n",
            "    'img', 'font',\n",
            "]\n",
            "_pre_tags = ['pre', 'script', 'style']\n",
            "_valid_end = r'(?!:/|[^\\w\\s@]*@)\\b'\n",
            "_valid_attr = r'''\\s*[a-zA-Z\\-](?:\\s*\\=\\s*(?:\"[^\"]*\"|'[^']*'|[^\\s'\">]+))?'''\n",
            "_block_tag = r'(?!(?:%s)\\b)\\w+%s' % ('|'.join(_inline_tags), _valid_end)\n",
            "_scheme_blacklist = ('javascript:', 'vbscript:')\n",
            "\n",
            "\n",
            "def _pure_pattern(regex):\n",
            "    pattern = regex.pattern\n",
            "    if pattern.startswith('^'):\n",
            "        pattern = pattern[1:]\n",
            "    return pattern\n",
            "\n",
            "\n",
            "def _keyify(key):\n",
            "    key = escape(key.lower(), quote=True)\n",
            "    return _key_pattern.sub(' ', key)\n",
            "\n",
            "\n",
            "def escape(text, quote=False, smart_amp=True):\n",
            "    \"\"\"Replace special characters \"&\", \"<\" and \">\" to HTML-safe sequences.\n",
            "\n",
            "    The original cgi.escape will always escape \"&\", but you can control\n",
            "    this one for a smart escape amp.\n",
            "\n",
            "    :param quote: if set to True, \" and ' will be escaped.\n",
            "    :param smart_amp: if set to False, & will always be escaped.\n",
            "    \"\"\"\n",
            "    if smart_amp:\n",
            "        text = _escape_pattern.sub('&amp;', text)\n",
            "    else:\n",
            "        text = text.replace('&', '&amp;')\n",
            "    text = text.replace('<', '&lt;')\n",
            "    text = text.replace('>', '&gt;')\n",
            "    if quote:\n",
            "        text = text.replace('\"', '&quot;')\n",
            "        text = text.replace(\"'\", '&#39;')\n",
            "    return text\n",
            "\n",
            "\n",
            "def escape_link(url):\n",
            "    \"\"\"Remove dangerous URL schemes like javascript: and escape afterwards.\"\"\"\n",
            "    lower_url = url.lower().strip('\\x00\\x1a \\n\\r\\t')\n",
            "\n",
            "    for scheme in _scheme_blacklist:\n",
            "        if re.sub(r'[^A-Za-z0-9\\/:]+', '', lower_url).startswith(scheme):\n",
            "            return ''\n",
            "    return escape(url, quote=True, smart_amp=False)\n",
            "\n",
            "\n",
            "def preprocessing(text, tab=4):\n",
            "    text = _newline_pattern.sub('\\n', text)\n",
            "    text = text.expandtabs(tab)\n",
            "    text = text.replace('\\u2424', '\\n')\n",
            "    pattern = re.compile(r'^ +$', re.M)\n",
            "    return pattern.sub('', text)\n",
            "\n",
            "\n",
            "class BlockGrammar(object):\n",
            "    \"\"\"Grammars for block level tokens.\"\"\"\n",
            "\n",
            "    def_links = re.compile(\n",
            "        r'^ *\\[([^^\\]]+)\\]: *'  # [key]:\n",
            "        r'<?([^\\s>]+)>?'  # <link> or link\n",
            "        r'(?: +[\"(]([^\\n]+)[\")])? *(?:\\n+|$)'\n",
            "    )\n",
            "    def_footnotes = re.compile(\n",
            "        r'^\\[\\^([^\\]]+)\\]: *('\n",
            "        r'[^\\n]*(?:\\n+|$)'  # [^key]:\n",
            "        r'(?: {1,}[^\\n]*(?:\\n+|$))*'\n",
            "        r')'\n",
            "    )\n",
            "\n",
            "    newline = re.compile(r'^\\n+')\n",
            "    block_code = re.compile(r'^( {4}[^\\n]+\\n*)+')\n",
            "    fences = re.compile(\n",
            "        r'^ *(`{3,}|~{3,}) *([^`\\s]+)? *\\n'  # ```lang\n",
            "        r'([\\s\\S]+?)\\s*'\n",
            "        r'\\1 *(?:\\n+|$)'  # ```\n",
            "    )\n",
            "    hrule = re.compile(r'^ {0,3}[-*_](?: *[-*_]){2,} *(?:\\n+|$)')\n",
            "    heading = re.compile(r'^ *(#{1,6}) *([^\\n]+?) *#* *(?:\\n+|$)')\n",
            "    lheading = re.compile(r'^([^\\n]+)\\n *(=|-)+ *(?:\\n+|$)')\n",
            "    block_quote = re.compile(r'^( *>[^\\n]+(\\n[^\\n]+)*\\n*)+')\n",
            "    list_block = re.compile(\n",
            "        r'^( *)(?=[*+-]|\\d+\\.)(([*+-])?(?:\\d+\\.)?) [\\s\\S]+?'\n",
            "        r'(?:'\n",
            "        r'\\n+(?=\\1?(?:[-*_] *){3,}(?:\\n+|$))'  # hrule\n",
            "        r'|\\n+(?=%s)'  # def links\n",
            "        r'|\\n+(?=%s)'  # def footnotes\\\n",
            "        r'|\\n+(?=\\1(?(3)\\d+\\.|[*+-]) )'   # heterogeneous bullet\n",
            "        r'|\\n{2,}'\n",
            "        r'(?! )'\n",
            "        r'(?!\\1(?:[*+-]|\\d+\\.) )\\n*'\n",
            "        r'|'\n",
            "        r'\\s*$)' % (\n",
            "            _pure_pattern(def_links),\n",
            "            _pure_pattern(def_footnotes),\n",
            "        )\n",
            "    )\n",
            "    list_item = re.compile(\n",
            "        r'^(( *)(?:[*+-]|\\d+\\.) [^\\n]*'\n",
            "        r'(?:\\n(?!\\2(?:[*+-]|\\d+\\.) )[^\\n]*)*)',\n",
            "        flags=re.M\n",
            "    )\n",
            "    list_bullet = re.compile(r'^ *(?:[*+-]|\\d+\\.) +')\n",
            "    paragraph = re.compile(\n",
            "        r'^((?:[^\\n]+\\n?(?!'\n",
            "        r'%s|%s|%s|%s|%s|%s|%s|%s|%s'\n",
            "        r'))+)\\n*' % (\n",
            "            _pure_pattern(fences).replace(r'\\1', r'\\2'),\n",
            "            _pure_pattern(list_block).replace(r'\\1', r'\\3'),\n",
            "            _pure_pattern(hrule),\n",
            "            _pure_pattern(heading),\n",
            "            _pure_pattern(lheading),\n",
            "            _pure_pattern(block_quote),\n",
            "            _pure_pattern(def_links),\n",
            "            _pure_pattern(def_footnotes),\n",
            "            '<' + _block_tag,\n",
            "        )\n",
            "    )\n",
            "    block_html = re.compile(\n",
            "        r'^ *(?:%s|%s|%s) *(?:\\n{2,}|\\s*$)' % (\n",
            "            r'<!--[\\s\\S]*?-->',\n",
            "            r'<(%s)((?:%s)*?)>([\\s\\S]*?)<\\/\\1>' % (_block_tag, _valid_attr),\n",
            "            r'<%s(?:%s)*?\\s*\\/?>' % (_block_tag, _valid_attr),\n",
            "        )\n",
            "    )\n",
            "    table = re.compile(\n",
            "        r'^ *\\|(.+)\\n *\\|( *[-:]+[-| :]*)\\n((?: *\\|.*(?:\\n|$))*)\\n*'\n",
            "    )\n",
            "    nptable = re.compile(\n",
            "        r'^ *(\\S.*\\|.*)\\n *([-:]+ *\\|[-| :]*)\\n((?:.*\\|.*(?:\\n|$))*)\\n*'\n",
            "    )\n",
            "    text = re.compile(r'^[^\\n]+')\n",
            "\n",
            "\n",
            "class BlockLexer(object):\n",
            "    \"\"\"Block level lexer for block grammars.\"\"\"\n",
            "    grammar_class = BlockGrammar\n",
            "\n",
            "    default_rules = [\n",
            "        'newline', 'hrule', 'block_code', 'fences', 'heading',\n",
            "        'nptable', 'lheading', 'block_quote',\n",
            "        'list_block', 'block_html', 'def_links',\n",
            "        'def_footnotes', 'table', 'paragraph', 'text'\n",
            "    ]\n",
            "\n",
            "    list_rules = (\n",
            "        'newline', 'block_code', 'fences', 'lheading', 'hrule',\n",
            "        'block_quote', 'list_block', 'block_html', 'text',\n",
            "    )\n",
            "\n",
            "    footnote_rules = (\n",
            "        'newline', 'block_code', 'fences', 'heading',\n",
            "        'nptable', 'lheading', 'hrule', 'block_quote',\n",
            "        'list_block', 'block_html', 'table', 'paragraph', 'text'\n",
            "    )\n",
            "\n",
            "    def __init__(self, rules=None, **kwargs):\n",
            "        self.tokens = []\n",
            "        self.def_links = {}\n",
            "        self.def_footnotes = {}\n",
            "\n",
            "        if not rules:\n",
            "            rules = self.grammar_class()\n",
            "\n",
            "        self.rules = rules\n",
            "        self._max_recursive_depth = kwargs.get('max_recursive_depth', 6)\n",
            "        self._list_depth = 0\n",
            "        self._blockquote_depth = 0\n",
            "\n",
            "    def __call__(self, text, rules=None):\n",
            "        return self.parse(text, rules)\n",
            "\n",
            "    def parse(self, text, rules=None):\n",
            "        text = text.rstrip('\\n')\n",
            "\n",
            "        if not rules:\n",
            "            rules = self.default_rules\n",
            "\n",
            "        def manipulate(text):\n",
            "            for key in rules:\n",
            "                rule = getattr(self.rules, key)\n",
            "                m = rule.match(text)\n",
            "                if not m:\n",
            "                    continue\n",
            "                getattr(self, 'parse_%s' % key)(m)\n",
            "                return m\n",
            "            return False  # pragma: no cover\n",
            "\n",
            "        while text:\n",
            "            m = manipulate(text)\n",
            "            if m is not False:\n",
            "                text = text[len(m.group(0)):]\n",
            "                continue\n",
            "            if text:  # pragma: no cover\n",
            "                raise RuntimeError('Infinite loop at: %s' % text)\n",
            "        return self.tokens\n",
            "\n",
            "    def parse_newline(self, m):\n",
            "        length = len(m.group(0))\n",
            "        if length > 1:\n",
            "            self.tokens.append({'type': 'newline'})\n",
            "\n",
            "    def parse_block_code(self, m):\n",
            "        # clean leading whitespace\n",
            "        code = _block_code_leading_pattern.sub('', m.group(0))\n",
            "        self.tokens.append({\n",
            "            'type': 'code',\n",
            "            'lang': None,\n",
            "            'text': code,\n",
            "        })\n",
            "\n",
            "    def parse_fences(self, m):\n",
            "        self.tokens.append({\n",
            "            'type': 'code',\n",
            "            'lang': m.group(2),\n",
            "            'text': m.group(3),\n",
            "        })\n",
            "\n",
            "    def parse_heading(self, m):\n",
            "        self.tokens.append({\n",
            "            'type': 'heading',\n",
            "            'level': len(m.group(1)),\n",
            "            'text': m.group(2),\n",
            "        })\n",
            "\n",
            "    def parse_lheading(self, m):\n",
            "        \"\"\"Parse setext heading.\"\"\"\n",
            "        self.tokens.append({\n",
            "            'type': 'heading',\n",
            "            'level': 1 if m.group(2) == '=' else 2,\n",
            "            'text': m.group(1),\n",
            "        })\n",
            "\n",
            "    def parse_hrule(self, m):\n",
            "        self.tokens.append({'type': 'hrule'})\n",
            "\n",
            "    def parse_list_block(self, m):\n",
            "        bull = m.group(2)\n",
            "        self.tokens.append({\n",
            "            'type': 'list_start',\n",
            "            'ordered': '.' in bull,\n",
            "        })\n",
            "        self._list_depth += 1\n",
            "        if self._list_depth > self._max_recursive_depth:\n",
            "            self.tokens.append({'type': 'list_item_start'})\n",
            "            self.parse_text(m)\n",
            "            self.tokens.append({'type': 'list_item_end'})\n",
            "        else:\n",
            "            cap = m.group(0)\n",
            "            self._process_list_item(cap, bull)\n",
            "        self.tokens.append({'type': 'list_end'})\n",
            "        self._list_depth -= 1\n",
            "\n",
            "    def _process_list_item(self, cap, bull):\n",
            "        cap = self.rules.list_item.findall(cap)\n",
            "\n",
            "        _next = False\n",
            "        length = len(cap)\n",
            "\n",
            "        for i in range(length):\n",
            "            item = cap[i][0]\n",
            "\n",
            "            # remove the bullet\n",
            "            space = len(item)\n",
            "            item = self.rules.list_bullet.sub('', item)\n",
            "\n",
            "            # outdent\n",
            "            if '\\n ' in item:\n",
            "                space = space - len(item)\n",
            "                pattern = re.compile(r'^ {1,%d}' % space, flags=re.M)\n",
            "                item = pattern.sub('', item)\n",
            "\n",
            "            # determine whether item is loose or not\n",
            "            loose = _next\n",
            "            if not loose and re.search(r'\\n\\n(?!\\s*$)', item):\n",
            "                loose = True\n",
            "\n",
            "            rest = len(item)\n",
            "            if i != length - 1 and rest:\n",
            "                _next = item[rest-1] == '\\n'\n",
            "                if not loose:\n",
            "                    loose = _next\n",
            "\n",
            "            if loose:\n",
            "                t = 'loose_item_start'\n",
            "            else:\n",
            "                t = 'list_item_start'\n",
            "\n",
            "            self.tokens.append({'type': t})\n",
            "            # recurse\n",
            "            self.parse(item, self.list_rules)\n",
            "            self.tokens.append({'type': 'list_item_end'})\n",
            "\n",
            "    def parse_block_quote(self, m):\n",
            "        self.tokens.append({'type': 'block_quote_start'})\n",
            "        self._blockquote_depth += 1\n",
            "        if self._blockquote_depth > self._max_recursive_depth:\n",
            "            self.parse_text(m)\n",
            "        else:\n",
            "            # clean leading >\n",
            "            cap = _block_quote_leading_pattern.sub('', m.group(0))\n",
            "            self.parse(cap)\n",
            "        self.tokens.append({'type': 'block_quote_end'})\n",
            "        self._blockquote_depth -= 1\n",
            "\n",
            "    def parse_def_links(self, m):\n",
            "        key = _keyify(m.group(1))\n",
            "        self.def_links[key] = {\n",
            "            'link': m.group(2),\n",
            "            'title': m.group(3),\n",
            "        }\n",
            "\n",
            "    def parse_def_footnotes(self, m):\n",
            "        key = _keyify(m.group(1))\n",
            "        if key in self.def_footnotes:\n",
            "            # footnote is already defined\n",
            "            return\n",
            "\n",
            "        self.def_footnotes[key] = 0\n",
            "\n",
            "        self.tokens.append({\n",
            "            'type': 'footnote_start',\n",
            "            'key': key,\n",
            "        })\n",
            "\n",
            "        text = m.group(2)\n",
            "\n",
            "        if '\\n' in text:\n",
            "            lines = text.split('\\n')\n",
            "            whitespace = None\n",
            "            for line in lines[1:]:\n",
            "                space = len(line) - len(line.lstrip())\n",
            "                if space and (not whitespace or space < whitespace):\n",
            "                    whitespace = space\n",
            "            newlines = [lines[0]]\n",
            "            for line in lines[1:]:\n",
            "                newlines.append(line[whitespace:])\n",
            "            text = '\\n'.join(newlines)\n",
            "\n",
            "        self.parse(text, self.footnote_rules)\n",
            "\n",
            "        self.tokens.append({\n",
            "            'type': 'footnote_end',\n",
            "            'key': key,\n",
            "        })\n",
            "\n",
            "    def parse_table(self, m):\n",
            "        item = self._process_table(m)\n",
            "\n",
            "        cells = re.sub(r'(?: *\\| *)?\\n$', '', m.group(3))\n",
            "        cells = cells.split('\\n')\n",
            "        for i, v in enumerate(cells):\n",
            "            v = re.sub(r'^ *\\| *| *\\| *$', '', v)\n",
            "            cells[i] = re.split(r' *(?<!\\\\)\\| *', v)\n",
            "\n",
            "        item['cells'] = self._process_cells(cells)\n",
            "        self.tokens.append(item)\n",
            "\n",
            "    def parse_nptable(self, m):\n",
            "        item = self._process_table(m)\n",
            "\n",
            "        cells = re.sub(r'\\n$', '', m.group(3))\n",
            "        cells = cells.split('\\n')\n",
            "        for i, v in enumerate(cells):\n",
            "            cells[i] = re.split(r' *(?<!\\\\)\\| *', v)\n",
            "\n",
            "        item['cells'] = self._process_cells(cells)\n",
            "        self.tokens.append(item)\n",
            "\n",
            "    def _process_table(self, m):\n",
            "        header = re.sub(r'^ *| *\\| *$', '', m.group(1))\n",
            "        header = re.split(r' *\\| *', header)\n",
            "        align = re.sub(r' *|\\| *$', '', m.group(2))\n",
            "        align = re.split(r' *\\| *', align)\n",
            "\n",
            "        for i, v in enumerate(align):\n",
            "            if re.search(r'^ *-+: *$', v):\n",
            "                align[i] = 'right'\n",
            "            elif re.search(r'^ *:-+: *$', v):\n",
            "                align[i] = 'center'\n",
            "            elif re.search(r'^ *:-+ *$', v):\n",
            "                align[i] = 'left'\n",
            "            else:\n",
            "                align[i] = None\n",
            "\n",
            "        item = {\n",
            "            'type': 'table',\n",
            "            'header': header,\n",
            "            'align': align,\n",
            "        }\n",
            "        return item\n",
            "\n",
            "    def _process_cells(self, cells):\n",
            "        for i, line in enumerate(cells):\n",
            "            for c, cell in enumerate(line):\n",
            "                # de-escape any pipe inside the cell here\n",
            "                cells[i][c] = re.sub('\\\\\\\\\\|', '|', cell)\n",
            "\n",
            "        return cells\n",
            "\n",
            "    def parse_block_html(self, m):\n",
            "        tag = m.group(1)\n",
            "        if not tag:\n",
            "            text = m.group(0)\n",
            "            self.tokens.append({\n",
            "                'type': 'close_html',\n",
            "                'text': text\n",
            "            })\n",
            "        else:\n",
            "            attr = m.group(2)\n",
            "            text = m.group(3)\n",
            "            self.tokens.append({\n",
            "                'type': 'open_html',\n",
            "                'tag': tag,\n",
            "                'extra': attr,\n",
            "                'text': text\n",
            "            })\n",
            "\n",
            "    def parse_paragraph(self, m):\n",
            "        text = m.group(1).rstrip('\\n')\n",
            "        self.tokens.append({'type': 'paragraph', 'text': text})\n",
            "\n",
            "    def parse_text(self, m):\n",
            "        text = m.group(0)\n",
            "        self.tokens.append({'type': 'text', 'text': text})\n",
            "\n",
            "\n",
            "class InlineGrammar(object):\n",
            "    \"\"\"Grammars for inline level tokens.\"\"\"\n",
            "\n",
            "    escape = re.compile(r'^\\\\([\\\\`*{}\\[\\]()#+\\-.!_>~|])')  # \\* \\+ \\! ....\n",
            "    inline_html = re.compile(\n",
            "        r'^(?:%s|%s|%s)' % (\n",
            "            r'<!--[\\s\\S]*?-->',\n",
            "            r'<(\\w+%s)((?:%s)*?)\\s*>([\\s\\S]*?)<\\/\\1>' % (\n",
            "                _valid_end, _valid_attr),\n",
            "            r'<\\w+%s(?:%s)*?\\s*\\/?>' % (_valid_end, _valid_attr),\n",
            "        )\n",
            "    )\n",
            "    autolink = re.compile(r'^<([^ >]+(@|:)[^ >]+)>')\n",
            "    link = re.compile(\n",
            "        r'^!?\\[('\n",
            "        r'(?:\\[[^^\\]]*\\]|[^\\[\\]]|\\](?=[^\\[]*\\]))*'\n",
            "        r')\\]\\('\n",
            "        r'''\\s*(<)?([\\s\\S]*?)(?(2)>)(?:\\s+['\"]([\\s\\S]*?)['\"])?\\s*'''\n",
            "        r'\\)'\n",
            "    )\n",
            "    reflink = re.compile(\n",
            "        r'^!?\\[('\n",
            "        r'(?:\\[[^^\\]]*\\]|[^\\[\\]]|\\](?=[^\\[]*\\]))*'\n",
            "        r')\\]\\s*\\[([^^\\]]*)\\]'\n",
            "    )\n",
            "    nolink = re.compile(r'^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]')\n",
            "    url = re.compile(r'''^(https?:\\/\\/[^\\s<]+[^<.,:;\"')\\]\\s])''')\n",
            "    double_emphasis = re.compile(\n",
            "        r'^_{2}([\\s\\S]+?)_{2}(?!_)'  # __word__\n",
            "        r'|'\n",
            "        r'^\\*{2}([\\s\\S]+?)\\*{2}(?!\\*)'  # **word**\n",
            "    )\n",
            "    emphasis = re.compile(\n",
            "        r'^\\b_((?:__|[^_])+?)_\\b'  # _word_\n",
            "        r'|'\n",
            "        r'^\\*((?:\\*\\*|[^\\*])+?)\\*(?!\\*)'  # *word*\n",
            "    )\n",
            "    code = re.compile(r'^(`+)\\s*([\\s\\S]*?[^`])\\s*\\1(?!`)')  # `code`\n",
            "    linebreak = re.compile(r'^ {2,}\\n(?!\\s*$)')\n",
            "    strikethrough = re.compile(r'^~~(?=\\S)([\\s\\S]*?\\S)~~')  # ~~word~~\n",
            "    footnote = re.compile(r'^\\[\\^([^\\]]+)\\]')\n",
            "    text = re.compile(r'^[\\s\\S]+?(?=[\\\\<!\\[_*`~]|https?://| {2,}\\n|$)')\n",
            "\n",
            "    def hard_wrap(self):\n",
            "        \"\"\"Grammar for hard wrap linebreak. You don't need to add two\n",
            "        spaces at the end of a line.\n",
            "        \"\"\"\n",
            "        self.linebreak = re.compile(r'^ *\\n(?!\\s*$)')\n",
            "        self.text = re.compile(\n",
            "            r'^[\\s\\S]+?(?=[\\\\<!\\[_*`~]|https?://| *\\n|$)'\n",
            "        )\n",
            "\n",
            "\n",
            "class InlineLexer(object):\n",
            "    \"\"\"Inline level lexer for inline grammars.\"\"\"\n",
            "    grammar_class = InlineGrammar\n",
            "\n",
            "    default_rules = [\n",
            "        'escape', 'inline_html', 'autolink', 'url',\n",
            "        'footnote', 'link', 'reflink', 'nolink',\n",
            "        'double_emphasis', 'emphasis', 'code',\n",
            "        'linebreak', 'strikethrough', 'text',\n",
            "    ]\n",
            "    inline_html_rules = [\n",
            "        'escape', 'inline_html', 'autolink', 'url', 'link', 'reflink',\n",
            "        'nolink', 'double_emphasis', 'emphasis', 'code',\n",
            "        'linebreak', 'strikethrough', 'text',\n",
            "    ]\n",
            "\n",
            "    def __init__(self, renderer, rules=None, **kwargs):\n",
            "        self.renderer = renderer\n",
            "        self.links = {}\n",
            "        self.footnotes = {}\n",
            "        self.footnote_index = 0\n",
            "\n",
            "        if not rules:\n",
            "            rules = self.grammar_class()\n",
            "\n",
            "        kwargs.update(self.renderer.options)\n",
            "        if kwargs.get('hard_wrap'):\n",
            "            rules.hard_wrap()\n",
            "\n",
            "        self.rules = rules\n",
            "\n",
            "        self._in_link = False\n",
            "        self._in_footnote = False\n",
            "        self._parse_inline_html = kwargs.get('parse_inline_html')\n",
            "\n",
            "    def __call__(self, text, rules=None):\n",
            "        return self.output(text, rules)\n",
            "\n",
            "    def setup(self, links, footnotes):\n",
            "        self.footnote_index = 0\n",
            "        self.links = links or {}\n",
            "        self.footnotes = footnotes or {}\n",
            "\n",
            "    def output(self, text, rules=None):\n",
            "        text = text.rstrip('\\n')\n",
            "        if not rules:\n",
            "            rules = list(self.default_rules)\n",
            "\n",
            "        if self._in_footnote and 'footnote' in rules:\n",
            "            rules.remove('footnote')\n",
            "\n",
            "        output = self.renderer.placeholder()\n",
            "\n",
            "        def manipulate(text):\n",
            "            for key in rules:\n",
            "                pattern = getattr(self.rules, key)\n",
            "                m = pattern.match(text)\n",
            "                if not m:\n",
            "                    continue\n",
            "                self.line_match = m\n",
            "                out = getattr(self, 'output_%s' % key)(m)\n",
            "                if out is not None:\n",
            "                    return m, out\n",
            "            return False  # pragma: no cover\n",
            "\n",
            "        while text:\n",
            "            ret = manipulate(text)\n",
            "            if ret is not False:\n",
            "                m, out = ret\n",
            "                output += out\n",
            "                text = text[len(m.group(0)):]\n",
            "                continue\n",
            "            if text:  # pragma: no cover\n",
            "                raise RuntimeError('Infinite loop at: %s' % text)\n",
            "\n",
            "        return output\n",
            "\n",
            "    def output_escape(self, m):\n",
            "        text = m.group(1)\n",
            "        return self.renderer.escape(text)\n",
            "\n",
            "    def output_autolink(self, m):\n",
            "        link = m.group(1)\n",
            "        if m.group(2) == '@':\n",
            "            is_email = True\n",
            "        else:\n",
            "            is_email = False\n",
            "        return self.renderer.autolink(link, is_email)\n",
            "\n",
            "    def output_url(self, m):\n",
            "        link = m.group(1)\n",
            "        if self._in_link:\n",
            "            return self.renderer.text(link)\n",
            "        return self.renderer.autolink(link, False)\n",
            "\n",
            "    def output_inline_html(self, m):\n",
            "        tag = m.group(1)\n",
            "        if self._parse_inline_html and tag in _inline_tags:\n",
            "            text = m.group(3)\n",
            "            if tag == 'a':\n",
            "                self._in_link = True\n",
            "                text = self.output(text, rules=self.inline_html_rules)\n",
            "                self._in_link = False\n",
            "            else:\n",
            "                text = self.output(text, rules=self.inline_html_rules)\n",
            "            extra = m.group(2) or ''\n",
            "            html = '<%s%s>%s</%s>' % (tag, extra, text, tag)\n",
            "        else:\n",
            "            html = m.group(0)\n",
            "        return self.renderer.inline_html(html)\n",
            "\n",
            "    def output_footnote(self, m):\n",
            "        key = _keyify(m.group(1))\n",
            "        if key not in self.footnotes:\n",
            "            return None\n",
            "        if self.footnotes[key]:\n",
            "            return None\n",
            "        self.footnote_index += 1\n",
            "        self.footnotes[key] = self.footnote_index\n",
            "        return self.renderer.footnote_ref(key, self.footnote_index)\n",
            "\n",
            "    def output_link(self, m):\n",
            "        return self._process_link(m, m.group(3), m.group(4))\n",
            "\n",
            "    def output_reflink(self, m):\n",
            "        key = _keyify(m.group(2) or m.group(1))\n",
            "        if key not in self.links:\n",
            "            return None\n",
            "        ret = self.links[key]\n",
            "        return self._process_link(m, ret['link'], ret['title'])\n",
            "\n",
            "    def output_nolink(self, m):\n",
            "        key = _keyify(m.group(1))\n",
            "        if key not in self.links:\n",
            "            return None\n",
            "        ret = self.links[key]\n",
            "        return self._process_link(m, ret['link'], ret['title'])\n",
            "\n",
            "    def _process_link(self, m, link, title=None):\n",
            "        line = m.group(0)\n",
            "        text = m.group(1)\n",
            "        if line[0] == '!':\n",
            "            return self.renderer.image(link, title, text)\n",
            "\n",
            "        self._in_link = True\n",
            "        text = self.output(text)\n",
            "        self._in_link = False\n",
            "        return self.renderer.link(link, title, text)\n",
            "\n",
            "    def output_double_emphasis(self, m):\n",
            "        text = m.group(2) or m.group(1)\n",
            "        text = self.output(text)\n",
            "        return self.renderer.double_emphasis(text)\n",
            "\n",
            "    def output_emphasis(self, m):\n",
            "        text = m.group(2) or m.group(1)\n",
            "        text = self.output(text)\n",
            "        return self.renderer.emphasis(text)\n",
            "\n",
            "    def output_code(self, m):\n",
            "        text = m.group(2)\n",
            "        return self.renderer.codespan(text)\n",
            "\n",
            "    def output_linebreak(self, m):\n",
            "        return self.renderer.linebreak()\n",
            "\n",
            "    def output_strikethrough(self, m):\n",
            "        text = self.output(m.group(1))\n",
            "        return self.renderer.strikethrough(text)\n",
            "\n",
            "    def output_text(self, m):\n",
            "        text = m.group(0)\n",
            "        return self.renderer.text(text)\n",
            "\n",
            "\n",
            "class Renderer(object):\n",
            "    \"\"\"The default HTML renderer for rendering Markdown.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.options = kwargs\n",
            "\n",
            "    def placeholder(self):\n",
            "        \"\"\"Returns the default, empty output value for the renderer.\n",
            "\n",
            "        All renderer methods use the '+=' operator to append to this value.\n",
            "        Default is a string so rendering HTML can build up a result string with\n",
            "        the rendered Markdown.\n",
            "\n",
            "        Can be overridden by Renderer subclasses to be types like an empty\n",
            "        list, allowing the renderer to create a tree-like structure to\n",
            "        represent the document (which can then be reprocessed later into a\n",
            "        separate format like docx or pdf).\n",
            "        \"\"\"\n",
            "        return ''\n",
            "\n",
            "    def block_code(self, code, lang=None):\n",
            "        \"\"\"Rendering block level code. ``pre > code``.\n",
            "\n",
            "        :param code: text content of the code block.\n",
            "        :param lang: language of the given code.\n",
            "        \"\"\"\n",
            "        code = code.rstrip('\\n')\n",
            "        if not lang:\n",
            "            code = escape(code, smart_amp=False)\n",
            "            return '<pre><code>%s\\n</code></pre>\\n' % code\n",
            "        code = escape(code, quote=True, smart_amp=False)\n",
            "        return '<pre><code class=\"lang-%s\">%s\\n</code></pre>\\n' % (lang, code)\n",
            "\n",
            "    def block_quote(self, text):\n",
            "        \"\"\"Rendering <blockquote> with the given text.\n",
            "\n",
            "        :param text: text content of the blockquote.\n",
            "        \"\"\"\n",
            "        return '<blockquote>%s\\n</blockquote>\\n' % text.rstrip('\\n')\n",
            "\n",
            "    def block_html(self, html):\n",
            "        \"\"\"Rendering block level pure html content.\n",
            "\n",
            "        :param html: text content of the html snippet.\n",
            "        \"\"\"\n",
            "        if self.options.get('skip_style') and \\\n",
            "           html.lower().startswith('<style'):\n",
            "            return ''\n",
            "        if self.options.get('escape'):\n",
            "            return escape(html)\n",
            "        return html\n",
            "\n",
            "    def header(self, text, level, raw=None):\n",
            "        \"\"\"Rendering header/heading tags like ``<h1>`` ``<h2>``.\n",
            "\n",
            "        :param text: rendered text content for the header.\n",
            "        :param level: a number for the header level, for example: 1.\n",
            "        :param raw: raw text content of the header.\n",
            "        \"\"\"\n",
            "        return '<h%d>%s</h%d>\\n' % (level, text, level)\n",
            "\n",
            "    def hrule(self):\n",
            "        \"\"\"Rendering method for ``<hr>`` tag.\"\"\"\n",
            "        if self.options.get('use_xhtml'):\n",
            "            return '<hr />\\n'\n",
            "        return '<hr>\\n'\n",
            "\n",
            "    def list(self, body, ordered=True):\n",
            "        \"\"\"Rendering list tags like ``<ul>`` and ``<ol>``.\n",
            "\n",
            "        :param body: body contents of the list.\n",
            "        :param ordered: whether this list is ordered or not.\n",
            "        \"\"\"\n",
            "        tag = 'ul'\n",
            "        if ordered:\n",
            "            tag = 'ol'\n",
            "        return '<%s>\\n%s</%s>\\n' % (tag, body, tag)\n",
            "\n",
            "    def list_item(self, text):\n",
            "        \"\"\"Rendering list item snippet. Like ``<li>``.\"\"\"\n",
            "        return '<li>%s</li>\\n' % text\n",
            "\n",
            "    def paragraph(self, text):\n",
            "        \"\"\"Rendering paragraph tags. Like ``<p>``.\"\"\"\n",
            "        return '<p>%s</p>\\n' % text.strip(' ')\n",
            "\n",
            "    def table(self, header, body):\n",
            "        \"\"\"Rendering table element. Wrap header and body in it.\n",
            "\n",
            "        :param header: header part of the table.\n",
            "        :param body: body part of the table.\n",
            "        \"\"\"\n",
            "        return (\n",
            "            '<table>\\n<thead>%s</thead>\\n'\n",
            "            '<tbody>\\n%s</tbody>\\n</table>\\n'\n",
            "        ) % (header, body)\n",
            "\n",
            "    def table_row(self, content):\n",
            "        \"\"\"Rendering a table row. Like ``<tr>``.\n",
            "\n",
            "        :param content: content of current table row.\n",
            "        \"\"\"\n",
            "        return '<tr>\\n%s</tr>\\n' % content\n",
            "\n",
            "    def table_cell(self, content, **flags):\n",
            "        \"\"\"Rendering a table cell. Like ``<th>`` ``<td>``.\n",
            "\n",
            "        :param content: content of current table cell.\n",
            "        :param header: whether this is header or not.\n",
            "        :param align: align of current table cell.\n",
            "        \"\"\"\n",
            "        if flags['header']:\n",
            "            tag = 'th'\n",
            "        else:\n",
            "            tag = 'td'\n",
            "        align = flags['align']\n",
            "        if not align:\n",
            "            return '<%s>%s</%s>\\n' % (tag, content, tag)\n",
            "        return '<%s style=\"text-align:%s\">%s</%s>\\n' % (\n",
            "            tag, align, content, tag\n",
            "        )\n",
            "\n",
            "    def double_emphasis(self, text):\n",
            "        \"\"\"Rendering **strong** text.\n",
            "\n",
            "        :param text: text content for emphasis.\n",
            "        \"\"\"\n",
            "        return '<strong>%s</strong>' % text\n",
            "\n",
            "    def emphasis(self, text):\n",
            "        \"\"\"Rendering *emphasis* text.\n",
            "\n",
            "        :param text: text content for emphasis.\n",
            "        \"\"\"\n",
            "        return '<em>%s</em>' % text\n",
            "\n",
            "    def codespan(self, text):\n",
            "        \"\"\"Rendering inline `code` text.\n",
            "\n",
            "        :param text: text content for inline code.\n",
            "        \"\"\"\n",
            "        text = escape(text.rstrip(), smart_amp=False)\n",
            "        return '<code>%s</code>' % text\n",
            "\n",
            "    def linebreak(self):\n",
            "        \"\"\"Rendering line break like ``<br>``.\"\"\"\n",
            "        if self.options.get('use_xhtml'):\n",
            "            return '<br />\\n'\n",
            "        return '<br>\\n'\n",
            "\n",
            "    def strikethrough(self, text):\n",
            "        \"\"\"Rendering ~~strikethrough~~ text.\n",
            "\n",
            "        :param text: text content for strikethrough.\n",
            "        \"\"\"\n",
            "        return '<del>%s</del>' % text\n",
            "\n",
            "    def text(self, text):\n",
            "        \"\"\"Rendering unformatted text.\n",
            "\n",
            "        :param text: text content.\n",
            "        \"\"\"\n",
            "        if self.options.get('parse_block_html'):\n",
            "            return text\n",
            "        return escape(text)\n",
            "\n",
            "    def escape(self, text):\n",
            "        \"\"\"Rendering escape sequence.\n",
            "\n",
            "        :param text: text content.\n",
            "        \"\"\"\n",
            "        return escape(text)\n",
            "\n",
            "    def autolink(self, link, is_email=False):\n",
            "        \"\"\"Rendering a given link or email address.\n",
            "\n",
            "        :param link: link content or email address.\n",
            "        :param is_email: whether this is an email or not.\n",
            "        \"\"\"\n",
            "        text = link = escape_link(link)\n",
            "        if is_email:\n",
            "            link = 'mailto:%s' % link\n",
            "        return '<a href=\"%s\">%s</a>' % (link, text)\n",
            "\n",
            "    def link(self, link, title, text):\n",
            "        \"\"\"Rendering a given link with content and title.\n",
            "\n",
            "        :param link: href link for ``<a>`` tag.\n",
            "        :param title: title content for `title` attribute.\n",
            "        :param text: text content for description.\n",
            "        \"\"\"\n",
            "        link = escape_link(link)\n",
            "        if not title:\n",
            "            return '<a href=\"%s\">%s</a>' % (link, text)\n",
            "        title = escape(title, quote=True)\n",
            "        return '<a href=\"%s\" title=\"%s\">%s</a>' % (link, title, text)\n",
            "\n",
            "    def image(self, src, title, text):\n",
            "        \"\"\"Rendering a image with title and text.\n",
            "\n",
            "        :param src: source link of the image.\n",
            "        :param title: title text of the image.\n",
            "        :param text: alt text of the image.\n",
            "        \"\"\"\n",
            "        src = escape_link(src)\n",
            "        text = escape(text, quote=True)\n",
            "        if title:\n",
            "            title = escape(title, quote=True)\n",
            "            html = '<img src=\"%s\" alt=\"%s\" title=\"%s\"' % (src, text, title)\n",
            "        else:\n",
            "            html = '<img src=\"%s\" alt=\"%s\"' % (src, text)\n",
            "        if self.options.get('use_xhtml'):\n",
            "            return '%s />' % html\n",
            "        return '%s>' % html\n",
            "\n",
            "    def inline_html(self, html):\n",
            "        \"\"\"Rendering span level pure html content.\n",
            "\n",
            "        :param html: text content of the html snippet.\n",
            "        \"\"\"\n",
            "        if self.options.get('escape'):\n",
            "            return escape(html)\n",
            "        return html\n",
            "\n",
            "    def newline(self):\n",
            "        \"\"\"Rendering newline element.\"\"\"\n",
            "        return ''\n",
            "\n",
            "    def footnote_ref(self, key, index):\n",
            "        \"\"\"Rendering the ref anchor of a footnote.\n",
            "\n",
            "        :param key: identity key for the footnote.\n",
            "        :param index: the index count of current footnote.\n",
            "        \"\"\"\n",
            "        html = (\n",
            "            '<sup class=\"footnote-ref\" id=\"fnref-%s\">'\n",
            "            '<a href=\"#fn-%s\">%d</a></sup>'\n",
            "        ) % (escape(key), escape(key), index)\n",
            "        return html\n",
            "\n",
            "    def footnote_item(self, key, text):\n",
            "        \"\"\"Rendering a footnote item.\n",
            "\n",
            "        :param key: identity key for the footnote.\n",
            "        :param text: text content of the footnote.\n",
            "        \"\"\"\n",
            "        back = (\n",
            "            '<a href=\"#fnref-%s\" class=\"footnote\">&#8617;</a>'\n",
            "        ) % escape(key)\n",
            "        text = text.rstrip()\n",
            "        if text.endswith('</p>'):\n",
            "            text = re.sub(r'<\\/p>$', r'%s</p>' % back, text)\n",
            "        else:\n",
            "            text = '%s<p>%s</p>' % (text, back)\n",
            "        html = '<li id=\"fn-%s\">%s</li>\\n' % (escape(key), text)\n",
            "        return html\n",
            "\n",
            "    def footnotes(self, text):\n",
            "        \"\"\"Wrapper for all footnotes.\n",
            "\n",
            "        :param text: contents of all footnotes.\n",
            "        \"\"\"\n",
            "        html = '<div class=\"footnotes\">\\n%s<ol>%s</ol>\\n</div>\\n'\n",
            "        return html % (self.hrule(), text)\n",
            "\n",
            "\n",
            "class Markdown(object):\n",
            "    \"\"\"The Markdown parser.\n",
            "\n",
            "    :param renderer: An instance of ``Renderer``.\n",
            "    :param inline: An inline lexer class or instance.\n",
            "    :param block: A block lexer class or instance.\n",
            "    \"\"\"\n",
            "    def __init__(self, renderer=None, inline=None, block=None, **kwargs):\n",
            "        if not renderer:\n",
            "            renderer = Renderer(**kwargs)\n",
            "        else:\n",
            "            kwargs.update(renderer.options)\n",
            "\n",
            "        self.renderer = renderer\n",
            "\n",
            "        if inline and inspect.isclass(inline):\n",
            "            inline = inline(renderer, **kwargs)\n",
            "        if block and inspect.isclass(block):\n",
            "            block = block(**kwargs)\n",
            "\n",
            "        if inline:\n",
            "            self.inline = inline\n",
            "        else:\n",
            "            self.inline = InlineLexer(renderer, **kwargs)\n",
            "\n",
            "        self.block = block or BlockLexer(BlockGrammar())\n",
            "        self.footnotes = []\n",
            "        self.tokens = []\n",
            "\n",
            "        # detect if it should parse text in block html\n",
            "        self._parse_block_html = kwargs.get('parse_block_html')\n",
            "\n",
            "    def __call__(self, text):\n",
            "        return self.parse(text)\n",
            "\n",
            "    def render(self, text):\n",
            "        \"\"\"Render the Markdown text.\n",
            "\n",
            "        :param text: markdown formatted text content.\n",
            "        \"\"\"\n",
            "        return self.parse(text)\n",
            "\n",
            "    def parse(self, text):\n",
            "        out = self.output(preprocessing(text))\n",
            "\n",
            "        keys = self.block.def_footnotes\n",
            "\n",
            "        # reset block\n",
            "        self.block.def_links = {}\n",
            "        self.block.def_footnotes = {}\n",
            "\n",
            "        # reset inline\n",
            "        self.inline.links = {}\n",
            "        self.inline.footnotes = {}\n",
            "\n",
            "        if not self.footnotes:\n",
            "            return out\n",
            "\n",
            "        footnotes = filter(lambda o: keys.get(o['key']), self.footnotes)\n",
            "        self.footnotes = sorted(\n",
            "            footnotes, key=lambda o: keys.get(o['key']), reverse=True\n",
            "        )\n",
            "\n",
            "        body = self.renderer.placeholder()\n",
            "        while self.footnotes:\n",
            "            note = self.footnotes.pop()\n",
            "            body += self.renderer.footnote_item(\n",
            "                note['key'], note['text']\n",
            "            )\n",
            "\n",
            "        out += self.renderer.footnotes(body)\n",
            "        return out\n",
            "\n",
            "    def pop(self):\n",
            "        if not self.tokens:\n",
            "            return None\n",
            "        self.token = self.tokens.pop()\n",
            "        return self.token\n",
            "\n",
            "    def peek(self):\n",
            "        if self.tokens:\n",
            "            return self.tokens[-1]\n",
            "        return None  # pragma: no cover\n",
            "\n",
            "    def output(self, text, rules=None):\n",
            "        self.tokens = self.block(text, rules)\n",
            "        self.tokens.reverse()\n",
            "\n",
            "        self.inline.setup(self.block.def_links, self.block.def_footnotes)\n",
            "\n",
            "        out = self.renderer.placeholder()\n",
            "        while self.pop():\n",
            "            out += self.tok()\n",
            "        return out\n",
            "\n",
            "    def tok(self):\n",
            "        t = self.token['type']\n",
            "\n",
            "        # sepcial cases\n",
            "        if t.endswith('_start'):\n",
            "            t = t[:-6]\n",
            "\n",
            "        return getattr(self, 'output_%s' % t)()\n",
            "\n",
            "    def tok_text(self):\n",
            "        text = self.token['text']\n",
            "        while self.peek()['type'] == 'text':\n",
            "            text += '\\n' + self.pop()['text']\n",
            "        return self.inline(text)\n",
            "\n",
            "    def output_newline(self):\n",
            "        return self.renderer.newline()\n",
            "\n",
            "    def output_hrule(self):\n",
            "        return self.renderer.hrule()\n",
            "\n",
            "    def output_heading(self):\n",
            "        return self.renderer.header(\n",
            "            self.inline(self.token['text']),\n",
            "            self.token['level'],\n",
            "            self.token['text'],\n",
            "        )\n",
            "\n",
            "    def output_code(self):\n",
            "        return self.renderer.block_code(\n",
            "            self.token['text'], self.token['lang']\n",
            "        )\n",
            "\n",
            "    def output_table(self):\n",
            "        aligns = self.token['align']\n",
            "        aligns_length = len(aligns)\n",
            "        cell = self.renderer.placeholder()\n",
            "\n",
            "        # header part\n",
            "        header = self.renderer.placeholder()\n",
            "        for i, value in enumerate(self.token['header']):\n",
            "            align = aligns[i] if i < aligns_length else None\n",
            "            flags = {'header': True, 'align': align}\n",
            "            cell += self.renderer.table_cell(self.inline(value), **flags)\n",
            "\n",
            "        header += self.renderer.table_row(cell)\n",
            "\n",
            "        # body part\n",
            "        body = self.renderer.placeholder()\n",
            "        for i, row in enumerate(self.token['cells']):\n",
            "            cell = self.renderer.placeholder()\n",
            "            for j, value in enumerate(row):\n",
            "                align = aligns[j] if j < aligns_length else None\n",
            "                flags = {'header': False, 'align': align}\n",
            "                cell += self.renderer.table_cell(self.inline(value), **flags)\n",
            "            body += self.renderer.table_row(cell)\n",
            "\n",
            "        return self.renderer.table(header, body)\n",
            "\n",
            "    def output_block_quote(self):\n",
            "        body = self.renderer.placeholder()\n",
            "        while self.pop()['type'] != 'block_quote_end':\n",
            "            body += self.tok()\n",
            "        return self.renderer.block_quote(body)\n",
            "\n",
            "    def output_list(self):\n",
            "        ordered = self.token['ordered']\n",
            "        body = self.renderer.placeholder()\n",
            "        while self.pop()['type'] != 'list_end':\n",
            "            body += self.tok()\n",
            "        return self.renderer.list(body, ordered)\n",
            "\n",
            "    def output_list_item(self):\n",
            "        body = self.renderer.placeholder()\n",
            "        while self.pop()['type'] != 'list_item_end':\n",
            "            if self.token['type'] == 'text':\n",
            "                body += self.tok_text()\n",
            "            else:\n",
            "                body += self.tok()\n",
            "\n",
            "        return self.renderer.list_item(body)\n",
            "\n",
            "    def output_loose_item(self):\n",
            "        body = self.renderer.placeholder()\n",
            "        while self.pop()['type'] != 'list_item_end':\n",
            "            body += self.tok()\n",
            "        return self.renderer.list_item(body)\n",
            "\n",
            "    def output_footnote(self):\n",
            "        self.inline._in_footnote = True\n",
            "        body = self.renderer.placeholder()\n",
            "        key = self.token['key']\n",
            "        while self.pop()['type'] != 'footnote_end':\n",
            "            body += self.tok()\n",
            "        self.footnotes.append({'key': key, 'text': body})\n",
            "        self.inline._in_footnote = False\n",
            "        return self.renderer.placeholder()\n",
            "\n",
            "    def output_close_html(self):\n",
            "        text = self.token['text']\n",
            "        return self.renderer.block_html(text)\n",
            "\n",
            "    def output_open_html(self):\n",
            "        text = self.token['text']\n",
            "        tag = self.token['tag']\n",
            "        if self._parse_block_html and tag not in _pre_tags:\n",
            "            text = self.inline(text, rules=self.inline.inline_html_rules)\n",
            "        extra = self.token.get('extra') or ''\n",
            "        html = '<%s%s>%s</%s>' % (tag, extra, text, tag)\n",
            "        return self.renderer.block_html(html)\n",
            "\n",
            "    def output_paragraph(self):\n",
            "        return self.renderer.paragraph(self.inline(self.token['text']))\n",
            "\n",
            "    def output_text(self):\n",
            "        return self.renderer.paragraph(self.tok_text())\n",
            "\n",
            "\n",
            "def markdown(text, escape=True, **kwargs):\n",
            "    \"\"\"Render markdown formatted text to html.\n",
            "\n",
            "    :param text: markdown formatted text content.\n",
            "    :param escape: if set to False, all html tags will not be escaped.\n",
            "    :param use_xhtml: output with xhtml tags.\n",
            "    :param hard_wrap: if set to True, it will use the GFM line breaks feature.\n",
            "    :param parse_block_html: parse text only in block level html.\n",
            "    :param parse_inline_html: parse text only in inline level html.\n",
            "    \"\"\"\n",
            "    return Markdown(escape=escape, **kwargs)(text)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# this module requires Python 2.6+\n",
            "from __future__ import with_statement\n",
            "from contextlib import contextmanager\n",
            "from operator import attrgetter\n",
            "from gettext import gettext as _\n",
            "import imp\n",
            "import inspect\n",
            "import os\n",
            "import sys\n",
            "import cmd\n",
            "import shlex\n",
            "import subprocess\n",
            "import argparse\n",
            "import itertools\n",
            "import traceback\n",
            "import multiprocessing\n",
            "import signal\n",
            "import threading\n",
            "import plac_core\n",
            "\n",
            "if sys.version < '3':\n",
            "    def exec_(_code_, _globs_=None, _locs_=None):\n",
            "        if _globs_ is None:\n",
            "            frame = sys._getframe(1)\n",
            "            _globs_ = frame.f_globals\n",
            "            if _locs_ is None:\n",
            "                _locs_ = frame.f_locals\n",
            "            del frame\n",
            "        elif _locs_ is None:\n",
            "            _locs_ = _globs_\n",
            "        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n",
            "\n",
            "    exec('''\n",
            "def raise_(tp, value=None, tb=None):\n",
            "    raise tp, value, tb\n",
            "''')\n",
            "else:\n",
            "    exec_ = eval('exec')\n",
            "\n",
            "    def raise_(tp, value=None, tb=None):\n",
            "        \"\"\"\n",
            "        A function that matches the Python 2.x ``raise`` statement. This\n",
            "        allows re-raising exceptions with the cls value and traceback on\n",
            "        Python 2 and 3.\n",
            "        \"\"\"\n",
            "        if value is not None and isinstance(tp, Exception):\n",
            "            raise TypeError(\"instance exception may not have a separate value\")\n",
            "        if value is not None:\n",
            "            exc = tp(value)\n",
            "        else:\n",
            "            exc = tp\n",
            "        if exc.__traceback__ is not tb:\n",
            "            raise exc.with_traceback(tb)\n",
            "        raise exc\n",
            "\n",
            "\n",
            "def decode(val):\n",
            "    \"\"\"\n",
            "    Decode an object assuming the encoding is UTF-8.\n",
            "    \"\"\"\n",
            "    try:\n",
            "        # assume it is an encoded bytes object\n",
            "        return val.decode('utf-8')\n",
            "    except AttributeError:\n",
            "        # it was an already decoded unicode object\n",
            "        return val\n",
            "\n",
            "# ############################ generic utils ############################### #\n",
            "\n",
            "\n",
            "@contextmanager\n",
            "def stdout(fileobj):\n",
            "    \"usage: with stdout(file('out.txt', 'a')): do_something()\"\n",
            "    orig_stdout = sys.stdout\n",
            "    sys.stdout = fileobj\n",
            "    try:\n",
            "        yield\n",
            "    finally:\n",
            "        sys.stdout = orig_stdout\n",
            "\n",
            "\n",
            "def write(x):\n",
            "    \"Write str(x) on stdout and flush, no newline added\"\n",
            "    sys.stdout.write(str(x))\n",
            "    sys.stdout.flush()\n",
            "\n",
            "\n",
            "def gen_val(value):\n",
            "    \"Return a generator object with a single element\"\n",
            "    yield value\n",
            "\n",
            "\n",
            "def gen_exc(etype, exc, tb):\n",
            "    \"Return a generator object raising an exception\"\n",
            "    raise_(etype, exc, tb)\n",
            "    yield\n",
            "\n",
            "\n",
            "def less(text):\n",
            "    \"Send a text to less via a pipe\"\n",
            "    # -c clear the screen before starting less\n",
            "    po = subprocess.Popen(['less', '-c'], stdin=subprocess.PIPE)\n",
            "    try:\n",
            "        po.stdin.write(text)\n",
            "    except IOError:\n",
            "        pass\n",
            "    po.stdin.close()\n",
            "    po.wait()\n",
            "\n",
            "use_less = (sys.platform != 'win32')  # unices\n",
            "\n",
            "\n",
            "class TerminatedProcess(Exception):\n",
            "    pass\n",
            "\n",
            "\n",
            "def terminatedProcess(signum, frame):\n",
            "    raise TerminatedProcess\n",
            "\n",
            "\n",
            "# ########################## readline support ############################ #\n",
            "\n",
            "def read_line(stdin, prompt=''):\n",
            "    \"Read a line from stdin, using readline when possible\"\n",
            "    if isinstance(stdin, ReadlineInput):\n",
            "        return stdin.readline(prompt)\n",
            "    else:\n",
            "        write(prompt)\n",
            "        return stdin.readline()\n",
            "\n",
            "\n",
            "def read_long_line(stdin, terminator):\n",
            "    \"\"\"\n",
            "    Read multiple lines from stdin until the terminator character is found,\n",
            "    then yield a single space-separated long line.\n",
            "    \"\"\"\n",
            "    while True:\n",
            "        lines = []\n",
            "        while True:\n",
            "            line = stdin.readline()  # ends with \\n\n",
            "            if not line:  # EOF\n",
            "                return\n",
            "            line = line.strip()\n",
            "            if not line:\n",
            "                continue\n",
            "            elif line[-1] == terminator:\n",
            "                lines.append(line[:-1])\n",
            "                break\n",
            "            else:\n",
            "                lines.append(line)\n",
            "        yield ' '.join(lines)\n",
            "\n",
            "\n",
            "class ReadlineInput(object):\n",
            "    \"\"\"\n",
            "    An iterable with a .readline method reading from stdin.\n",
            "    \"\"\"\n",
            "    def __init__(self, completions, case_sensitive=True, histfile=None):\n",
            "        self.completions = completions\n",
            "        self.case_sensitive = case_sensitive\n",
            "        self.histfile = histfile\n",
            "        if not case_sensitive:\n",
            "            self.completions = [c.upper() for c in completions]\n",
            "        import readline\n",
            "        self.rl = readline\n",
            "        readline.parse_and_bind(\"tab: complete\")\n",
            "        readline.set_completer(self.complete)\n",
            "\n",
            "    def __enter__(self):\n",
            "        self.old_completer = self.rl.get_completer()\n",
            "        try:\n",
            "            if self.histfile:\n",
            "                self.rl.read_history_file(self.histfile)\n",
            "        except IOError:  # the first time\n",
            "            pass\n",
            "        return self\n",
            "\n",
            "    def __exit__(self, etype, exc, tb):\n",
            "        self.rl.set_completer(self.old_completer)\n",
            "        if self.histfile:\n",
            "            self.rl.write_history_file(self.histfile)\n",
            "\n",
            "    def complete(self, kw, state):\n",
            "        # state is 0, 1, 2, ... and increases by hitting TAB\n",
            "        if not self.case_sensitive:\n",
            "            kw = kw.upper()\n",
            "        try:\n",
            "            return [k for k in self.completions if k.startswith(kw)][state]\n",
            "        except IndexError:  # no completions\n",
            "            return  # exit\n",
            "\n",
            "    def readline(self, prompt=''):\n",
            "        try:\n",
            "            return raw_input(prompt) + '\\n'\n",
            "        except EOFError:\n",
            "            return ''\n",
            "\n",
            "    def __iter__(self):\n",
            "        return iter(self.readline, '')\n",
            "\n",
            "# ################# help functionality in plac interpreters ################# #\n",
            "\n",
            "\n",
            "class HelpSummary(object):\n",
            "    \"Build the help summary consistently with the cmd module\"\n",
            "\n",
            "    @classmethod\n",
            "    def add(cls, obj, specialcommands):\n",
            "        p = plac_core.parser_from(obj)\n",
            "        c = cmd.Cmd(stdout=cls())\n",
            "        c.stdout.write('\\n')\n",
            "        c.print_topics('special commands',\n",
            "                       sorted(specialcommands), 15, 80)\n",
            "        c.print_topics('custom commands',\n",
            "                       sorted(obj.commands), 15, 80)\n",
            "        c.print_topics('commands run in external processes',\n",
            "                       sorted(obj.mpcommands), 15, 80)\n",
            "        c.print_topics('threaded commands',\n",
            "                       sorted(obj.thcommands), 15, 80)\n",
            "        p.helpsummary = str(c.stdout)\n",
            "\n",
            "    def __init__(self):\n",
            "        self._ls = []\n",
            "\n",
            "    def write(self, s):\n",
            "        self._ls.append(s)\n",
            "\n",
            "    def __str__(self):\n",
            "        return ''.join(self._ls)\n",
            "\n",
            "\n",
            "class PlacFormatter(argparse.RawDescriptionHelpFormatter):\n",
            "    def _metavar_formatter(self, action, default_metavar):\n",
            "        'Remove special commands from the usage message'\n",
            "        choices = action.choices or {}\n",
            "        action.choices = dict((n, c) for n, c in choices.items()\n",
            "                              if not n.startswith('.'))\n",
            "        return super(PlacFormatter, self)._metavar_formatter(\n",
            "            action, default_metavar)\n",
            "\n",
            "\n",
            "def format_help(self):\n",
            "    \"Attached to plac_core.ArgumentParser for plac interpreters\"\n",
            "    try:\n",
            "        return self.helpsummary\n",
            "    except AttributeError:\n",
            "        return super(plac_core.ArgumentParser, self).format_help()\n",
            "plac_core.ArgumentParser.format_help = format_help\n",
            "\n",
            "\n",
            "def default_help(obj, cmd=None):\n",
            "    \"The default help functionality in plac interpreters\"\n",
            "    parser = plac_core.parser_from(obj)\n",
            "    if cmd is None:\n",
            "        yield parser.format_help()\n",
            "        return\n",
            "    subp = parser.subparsers._name_parser_map.get(cmd)\n",
            "    if subp is None:\n",
            "        yield _('Unknown command %s' % cmd)\n",
            "    elif getattr(obj, '_interact_', False):  # in interactive mode\n",
            "        formatter = subp._get_formatter()\n",
            "        formatter._prog = cmd  # remove the program name from the usage\n",
            "        formatter.add_usage(\n",
            "            subp.usage, [a for a in subp._actions if a.dest != 'help'],\n",
            "            subp._mutually_exclusive_groups)\n",
            "        formatter.add_text(subp.description)\n",
            "        for action_group in subp._action_groups:\n",
            "            formatter.start_section(action_group.title)\n",
            "            formatter.add_text(action_group.description)\n",
            "            formatter.add_arguments(a for a in action_group._group_actions\n",
            "                                    if a.dest != 'help')\n",
            "            formatter.end_section()\n",
            "        yield formatter.format_help()\n",
            "    else:  # regular argparse help\n",
            "        yield subp.format_help()\n",
            "\n",
            "# ######################## import management ############################## #\n",
            "\n",
            "try:\n",
            "    PLACDIRS = os.environ.get('PLACPATH', '.').split(':')\n",
            "except:\n",
            "    raise ValueError(_('Ill-formed PLACPATH: got %PLACPATHs') % os.environ)\n",
            "\n",
            "\n",
            "def partial_call(factory, arglist):\n",
            "    \"Call a container factory with the arglist and return a plac object\"\n",
            "    a = plac_core.parser_from(factory).argspec\n",
            "    if a.defaults or a.varargs or a.varkw:\n",
            "        raise TypeError('Interpreter.call must be invoked on '\n",
            "                        'factories with required arguments only')\n",
            "    required_args = ', '.join(a.args)\n",
            "    if required_args:\n",
            "        required_args += ','  # trailing comma\n",
            "    code = '''def makeobj(interact, %s *args):\n",
            "    obj = factory(%s)\n",
            "    obj._interact_ = interact\n",
            "    obj._args_ = args\n",
            "    return obj\\n''' % (required_args, required_args)\n",
            "    dic = dict(factory=factory)\n",
            "    exec_(code, dic)\n",
            "    makeobj = dic['makeobj']\n",
            "    makeobj.add_help = False\n",
            "    if inspect.isclass(factory):\n",
            "        makeobj.__annotations__ = getattr(\n",
            "            factory.__init__, '__annotations__', {})\n",
            "    else:\n",
            "        makeobj.__annotations__ = getattr(\n",
            "            factory, '__annotations__', {})\n",
            "    makeobj.__annotations__['interact'] = (\n",
            "        'start interactive interpreter', 'flag', 'i')\n",
            "    return plac_core.call(makeobj, arglist)\n",
            "\n",
            "\n",
            "def import_main(path, *args):\n",
            "    \"\"\"\n",
            "    An utility to import the main function of a plac tool. It also\n",
            "    works with command container factories.\n",
            "    \"\"\"\n",
            "    if ':' in path:  # importing a factory\n",
            "        path, factory_name = path.split(':')\n",
            "    else:  # importing the main function\n",
            "        factory_name = None\n",
            "    if not os.path.isabs(path):  # relative path, look at PLACDIRS\n",
            "        for placdir in PLACDIRS:\n",
            "            fullpath = os.path.join(placdir, path)\n",
            "            if os.path.exists(fullpath):\n",
            "                break\n",
            "        else:  # no break\n",
            "            raise ImportError(_('Cannot find %s' % path))\n",
            "    else:\n",
            "        fullpath = path\n",
            "    name, ext = os.path.splitext(os.path.basename(fullpath))\n",
            "    module = imp.load_module(name, open(fullpath), fullpath, (ext, 'U', 1))\n",
            "    if factory_name:\n",
            "        tool = partial_call(getattr(module, factory_name), args)\n",
            "    else:\n",
            "        tool = module.main\n",
            "    return tool\n",
            "\n",
            "# ############################ Task classes ############################# #\n",
            "\n",
            "\n",
            "# base class not instantiated directly\n",
            "class BaseTask(object):\n",
            "    \"\"\"\n",
            "    A task is a wrapper over a generator object with signature\n",
            "    Task(no, arglist, genobj), attributes\n",
            "    .no\n",
            "    .arglist\n",
            "    .outlist\n",
            "    .str\n",
            "    .etype\n",
            "    .exc\n",
            "    .tb\n",
            "    .status\n",
            "    and methods .run and .kill.\n",
            "    \"\"\"\n",
            "    STATES = ('SUBMITTED', 'RUNNING', 'TOBEKILLED',  'KILLED', 'FINISHED',\n",
            "              'ABORTED')\n",
            "\n",
            "    def __init__(self, no, arglist, genobj):\n",
            "        self.no = no\n",
            "        self.arglist = arglist\n",
            "        self._genobj = self._wrap(genobj)\n",
            "        self.str, self.etype, self.exc, self.tb = '', None, None, None\n",
            "        self.status = 'SUBMITTED'\n",
            "        self.outlist = []\n",
            "\n",
            "    def notify(self, msg):\n",
            "        \"Notifies the underlying monitor. To be implemented\"\n",
            "\n",
            "    def _wrap(self, genobj, stringify_tb=False):\n",
            "        \"\"\"\n",
            "        Wrap the genobj into a generator managing the exceptions,\n",
            "        populating the .outlist, setting the .status and yielding None.\n",
            "        stringify_tb must be True if the traceback must be sent to a process.\n",
            "        \"\"\"\n",
            "        self.status = 'RUNNING'\n",
            "        try:\n",
            "            for value in genobj:\n",
            "                if self.status == 'TOBEKILLED':  # exit from the loop\n",
            "                    raise GeneratorExit\n",
            "                if value is not None:  # add output\n",
            "                    self.outlist.append(value)\n",
            "                    self.notify(decode(value))\n",
            "                yield\n",
            "        except Interpreter.Exit:  # wanted exit\n",
            "            self._regular_exit()\n",
            "            raise\n",
            "        except (GeneratorExit, TerminatedProcess, KeyboardInterrupt):\n",
            "            # soft termination\n",
            "            self.status = 'KILLED'\n",
            "        except:  # unexpected exception\n",
            "            self.etype, self.exc, tb = sys.exc_info()\n",
            "            self.tb = ''.join(traceback.format_tb(tb)) if stringify_tb else tb\n",
            "            self.status = 'ABORTED'\n",
            "        else:\n",
            "            self._regular_exit()\n",
            "\n",
            "    def _regular_exit(self):\n",
            "        self.status = 'FINISHED'\n",
            "        try:\n",
            "            self.str = '\\n'.join(map(decode, self.outlist))\n",
            "        except IndexError:\n",
            "            self.str = 'no result'\n",
            "\n",
            "    def run(self):\n",
            "        \"Run the inner generator\"\n",
            "        for none in self._genobj:\n",
            "            pass\n",
            "\n",
            "    def kill(self):\n",
            "        \"Set a TOBEKILLED status\"\n",
            "        self.status = 'TOBEKILLED'\n",
            "\n",
            "    def wait(self):\n",
            "        \"Wait for the task to finish: to be overridden\"\n",
            "\n",
            "    @property\n",
            "    def traceback(self):\n",
            "        \"Return the traceback as a (possibly empty) string\"\n",
            "        if self.tb is None:\n",
            "            return ''\n",
            "        elif isinstance(self.tb, (str, bytes)):\n",
            "            return self.tb\n",
            "        else:\n",
            "            return ''.join(traceback.format_tb(self.tb))\n",
            "\n",
            "    @property\n",
            "    def result(self):\n",
            "        self.wait()\n",
            "        if self.exc:\n",
            "            if isinstance(self.tb, (str, bytes)):\n",
            "                raise self.etype(self.tb)\n",
            "            else:\n",
            "                raise_(self.etype, self.exc, self.tb or None)\n",
            "        if not self.outlist:\n",
            "            return None\n",
            "        return self.outlist[-1]\n",
            "\n",
            "    def __repr__(self):\n",
            "        \"String representation containing class name, number, arglist, status\"\n",
            "        return '<%s %d [%s] %s>' % (\n",
            "            self.__class__.__name__, self.no,\n",
            "            ' '.join(self.arglist), self.status)\n",
            "\n",
            "nulltask = BaseTask(0, [], ('skip' for dummy in (1,)))\n",
            "\n",
            "# ######################## synchronous tasks ############################## #\n",
            "\n",
            "\n",
            "class SynTask(BaseTask):\n",
            "    \"\"\"\n",
            "    Synchronous task running in the interpreter loop and displaying its\n",
            "    output as soon as available.\n",
            "    \"\"\"\n",
            "    def __str__(self):\n",
            "        \"Return the output string or the error message\"\n",
            "        if self.etype:  # there was an error\n",
            "            return '%s: %s' % (self.etype.__name__, self.exc)\n",
            "        else:\n",
            "            return '\\n'.join(map(str, self.outlist))\n",
            "\n",
            "\n",
            "class ThreadedTask(BaseTask):\n",
            "    \"\"\"\n",
            "    A task running in a separated thread.\n",
            "    \"\"\"\n",
            "    def __init__(self, no, arglist, genobj):\n",
            "        BaseTask.__init__(self, no, arglist, genobj)\n",
            "        self.thread = threading.Thread(target=super(ThreadedTask, self).run)\n",
            "\n",
            "    def run(self):\n",
            "        \"Run the task into a thread\"\n",
            "        self.thread.start()\n",
            "\n",
            "    def wait(self):\n",
            "        \"Block until the thread ends\"\n",
            "        self.thread.join()\n",
            "\n",
            "\n",
            "# ######################## multiprocessing tasks ######################### #\n",
            "\n",
            "def sharedattr(name, on_error):\n",
            "    \"Return a property to be attached to an MPTask\"\n",
            "    def get(self):\n",
            "        try:\n",
            "            return getattr(self.ns, name)\n",
            "        except:  # the process was killed or died hard\n",
            "            return on_error\n",
            "\n",
            "    def set(self, value):\n",
            "        try:\n",
            "            setattr(self.ns, name, value)\n",
            "        except:  # the process was killed or died hard\n",
            "            pass\n",
            "    return property(get, set)\n",
            "\n",
            "\n",
            "class MPTask(BaseTask):\n",
            "    \"\"\"\n",
            "    A task running as an external process. The current implementation\n",
            "    only works on Unix-like systems, where multiprocessing use forks.\n",
            "    \"\"\"\n",
            "    str = sharedattr('str', '')\n",
            "    etype = sharedattr('etype', None)\n",
            "    exc = sharedattr('exc', None)\n",
            "    tb = sharedattr('tb', None)\n",
            "    status = sharedattr('status', 'ABORTED')\n",
            "\n",
            "    @property\n",
            "    def outlist(self):\n",
            "        try:\n",
            "            return self._outlist\n",
            "        except:  # the process died hard\n",
            "            return []\n",
            "\n",
            "    def notify(self, msg):\n",
            "        self.man.notify_listener(self.no, msg)\n",
            "\n",
            "    def __init__(self, no, arglist, genobj, manager):\n",
            "        \"\"\"\n",
            "        The monitor has a .send method and a .man multiprocessing.Manager\n",
            "        \"\"\"\n",
            "        self.no = no\n",
            "        self.arglist = arglist\n",
            "        self._genobj = self._wrap(genobj, stringify_tb=True)\n",
            "        self.man = manager\n",
            "        self._outlist = manager.mp.list()\n",
            "        self.ns = manager.mp.Namespace()\n",
            "        self.status = 'SUBMITTED'\n",
            "        self.etype, self.exc, self.tb = None, None, None\n",
            "        self.str = repr(self)\n",
            "        self.proc = multiprocessing.Process(target=super(MPTask, self).run)\n",
            "\n",
            "    def run(self):\n",
            "        \"Run the task into an external process\"\n",
            "        self.proc.start()\n",
            "\n",
            "    def wait(self):\n",
            "        \"Block until the external process ends or is killed\"\n",
            "        self.proc.join()\n",
            "\n",
            "    def kill(self):\n",
            "        \"\"\"Kill the process with a SIGTERM inducing a TerminatedProcess\n",
            "        exception in the children\"\"\"\n",
            "        self.proc.terminate()\n",
            "\n",
            "# ######################## Task Manager ###################### #\n",
            "\n",
            "\n",
            "class TaskManager(object):\n",
            "    \"\"\"\n",
            "    Store the given commands into a task registry. Provides methods to\n",
            "    manage the submitted tasks.\n",
            "    \"\"\"\n",
            "    cmdprefix = '.'\n",
            "    specialcommands = set(['.last_tb'])\n",
            "\n",
            "    def __init__(self, obj):\n",
            "        self.obj = obj\n",
            "        self.registry = {}  # {taskno : task}\n",
            "        if obj.mpcommands or obj.thcommands:\n",
            "            self.specialcommands.update(['.kill', '.list', '.output'])\n",
            "        interact = getattr(obj, '_interact_', False)\n",
            "        self.parser = plac_core.parser_from(\n",
            "            obj, prog='' if interact else None, formatter_class=PlacFormatter)\n",
            "        HelpSummary.add(obj, self.specialcommands)\n",
            "        self.man = Manager() if obj.mpcommands else None\n",
            "        signal.signal(signal.SIGTERM, terminatedProcess)\n",
            "\n",
            "    def close(self):\n",
            "        \"Kill all the running tasks\"\n",
            "        for task in self.registry.values():\n",
            "            try:\n",
            "                if task.status == 'RUNNING':\n",
            "                    task.kill()\n",
            "                    task.wait()\n",
            "            except:  # task killed, nothing to wait\n",
            "                pass\n",
            "        if self.man:\n",
            "            self.man.stop()\n",
            "\n",
            "    def _get_latest(self, taskno=-1, status=None):\n",
            "        \"Get the latest submitted task from the registry\"\n",
            "        assert taskno < 0, 'You must pass a negative number'\n",
            "        if status:\n",
            "            tasks = [t for t in self.registry.values()\n",
            "                     if t.status == status]\n",
            "        else:\n",
            "            tasks = [t for t in self.registry.values()]\n",
            "        tasks.sort(key=attrgetter('no'))\n",
            "        if len(tasks) >= abs(taskno):\n",
            "            return tasks[taskno]\n",
            "\n",
            "    # ########################## special commands ######################## #\n",
            "\n",
            "    @plac_core.annotations(\n",
            "        taskno=('task to kill', 'positional', None, int))\n",
            "    def kill(self, taskno=-1):\n",
            "        'kill the given task (-1 to kill the latest running task)'\n",
            "        if taskno < 0:\n",
            "            task = self._get_latest(taskno, status='RUNNING')\n",
            "            if task is None:\n",
            "                yield 'Nothing to kill'\n",
            "                return\n",
            "        elif taskno not in self.registry:\n",
            "            yield 'Unknown task %d' % taskno\n",
            "            return\n",
            "        else:\n",
            "            task = self.registry[taskno]\n",
            "        if task.status in ('ABORTED', 'KILLED', 'FINISHED'):\n",
            "            yield 'Already finished %s' % task\n",
            "            return\n",
            "        task.kill()\n",
            "        yield task\n",
            "\n",
            "    @plac_core.annotations(\n",
            "        status=('', 'positional', None, str, BaseTask.STATES))\n",
            "    def list(self, status='RUNNING'):\n",
            "        'list tasks with a given status'\n",
            "        for task in self.registry.values():\n",
            "            if task.status == status:\n",
            "                yield task\n",
            "\n",
            "    @plac_core.annotations(\n",
            "        taskno=('task number', 'positional', None, int))\n",
            "    def output(self, taskno=-1, fname=None):\n",
            "        'show the output of a given task (and optionally save it to a file)'\n",
            "        if taskno < 0:\n",
            "            task = self._get_latest(taskno)\n",
            "            if task is None:\n",
            "                yield 'Nothing to show'\n",
            "                return\n",
            "        elif taskno not in self.registry:\n",
            "            yield 'Unknown task %d' % taskno\n",
            "            return\n",
            "        else:\n",
            "            task = self.registry[taskno]\n",
            "        outstr = '\\n'.join(map(str, task.outlist))\n",
            "        if fname:\n",
            "            open(fname, 'w').write(outstr)\n",
            "            yield 'saved output of %d into %s' % (taskno, fname)\n",
            "            return\n",
            "        yield task\n",
            "        if len(task.outlist) > 20 and use_less:\n",
            "            less(outstr)  # has no meaning for a plac server\n",
            "        else:\n",
            "            yield outstr\n",
            "\n",
            "    @plac_core.annotations(\n",
            "        taskno=('task number', 'positional', None, int))\n",
            "    def last_tb(self, taskno=-1):\n",
            "        \"show the traceback of a given task, if any\"\n",
            "        task = self._get_latest(taskno)\n",
            "        if task:\n",
            "            yield task.traceback\n",
            "        else:\n",
            "            yield 'Nothing to show'\n",
            "\n",
            "# ########################## SyncProcess ############################# #\n",
            "\n",
            "\n",
            "class Process(subprocess.Popen):\n",
            "    \"Start the interpreter specified by the params in a subprocess\"\n",
            "\n",
            "    def __init__(self, params):\n",
            "        signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n",
            "        # to avoid broken pipe messages\n",
            "        code = '''import plac, sys\n",
            "sys.argv[0] = '<%s>'\n",
            "plac.Interpreter(plac.import_main(*%s)).interact(prompt='i>\\\\n')\n",
            "''' % (params[0], params)\n",
            "        subprocess.Popen.__init__(\n",
            "            self, [sys.executable, '-u', '-c', code],\n",
            "            stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
            "        self.man = multiprocessing.Manager()\n",
            "\n",
            "    def close(self):\n",
            "        \"Close stdin and stdout\"\n",
            "        self.stdin.close()\n",
            "        self.stdout.close()\n",
            "        self.man.shutdown()\n",
            "\n",
            "    def recv(self):  # char-by-char cannot work\n",
            "        \"Return the output of the subprocess, line-by-line until the prompt\"\n",
            "        lines = []\n",
            "        while True:\n",
            "            lines.append(self.stdout.readline())\n",
            "            if lines[-1] == 'i>\\n':\n",
            "                out = ''.join(lines)\n",
            "                return out[:-1] + ' '  # remove last newline\n",
            "\n",
            "    def send(self, line):\n",
            "        \"\"\"Send a line (adding a newline) to the underlying subprocess\n",
            "        and wait for the answer\"\"\"\n",
            "        self.stdin.write(line + os.linesep)\n",
            "        return self.recv()\n",
            "\n",
            "\n",
            "class StartStopObject(object):\n",
            "    started = False\n",
            "\n",
            "    def start(self):\n",
            "        pass\n",
            "\n",
            "    def stop(self):\n",
            "        pass\n",
            "\n",
            "\n",
            "class Monitor(StartStopObject):\n",
            "    \"\"\"\n",
            "    Base monitor class with methods add_listener/del_listener/notify_listener\n",
            "    read_queue and and start/stop.\n",
            "    \"\"\"\n",
            "    def __init__(self, name, queue=None):\n",
            "        self.name = name\n",
            "        self.queue = queue or multiprocessing.Queue()\n",
            "\n",
            "    def add_listener(self, taskno):\n",
            "        pass\n",
            "\n",
            "    def del_listener(self, taskno):\n",
            "        pass\n",
            "\n",
            "    def notify_listener(self, taskno, msg):\n",
            "        pass\n",
            "\n",
            "    def start(self):\n",
            "        pass\n",
            "\n",
            "    def stop(self):\n",
            "        pass\n",
            "\n",
            "    def read_queue(self):\n",
            "        pass\n",
            "\n",
            "\n",
            "class Manager(StartStopObject):\n",
            "    \"\"\"\n",
            "    The plac Manager contains a multiprocessing.Manager and a set\n",
            "    of slave monitor processes to which we can send commands. There\n",
            "    is a manager for each interpreter with mpcommands.\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        self.registry = {}\n",
            "        self.started = False\n",
            "        self.mp = None\n",
            "\n",
            "    def add(self, monitor):\n",
            "        'Add or replace a monitor in the registry'\n",
            "        proc = multiprocessing.Process(None, monitor.start, monitor.name)\n",
            "        proc.queue = monitor.queue\n",
            "        self.registry[monitor.name] = proc\n",
            "\n",
            "    def delete(self, name):\n",
            "        'Remove a named monitor from the registry'\n",
            "        del self.registry[name]\n",
            "\n",
            "    # can be called more than once\n",
            "    def start(self):\n",
            "        if self.mp is None:\n",
            "            self.mp = multiprocessing.Manager()\n",
            "        for monitor in self.registry.values():\n",
            "            monitor.start()\n",
            "        self.started = True\n",
            "\n",
            "    def stop(self):\n",
            "        for monitor in self.registry.values():\n",
            "            monitor.queue.close()\n",
            "            monitor.terminate()\n",
            "        if self.mp:\n",
            "            self.mp.shutdown()\n",
            "            self.mp = None\n",
            "        self.started = False\n",
            "\n",
            "    def notify_listener(self, taskno, msg):\n",
            "        for monitor in self.registry.values():\n",
            "            monitor.queue.put(('notify_listener', taskno, msg))\n",
            "\n",
            "    def add_listener(self, no):\n",
            "        for monitor in self.registry.values():\n",
            "            monitor.queue.put(('add_listener', no))\n",
            "\n",
            "# ######################### plac server ############################# #\n",
            "\n",
            "import asyncore\n",
            "import asynchat\n",
            "import socket\n",
            "\n",
            "\n",
            "class _AsynHandler(asynchat.async_chat):\n",
            "    \"asynchat handler starting a new interpreter loop for each connection\"\n",
            "\n",
            "    terminator = '\\r\\n'  # the standard one for telnet\n",
            "    prompt = 'i> '\n",
            "\n",
            "    def __init__(self, socket, interpreter):\n",
            "        asynchat.async_chat.__init__(self, socket)\n",
            "        self.set_terminator(self.terminator)\n",
            "        self.i = interpreter\n",
            "        self.i.__enter__()\n",
            "        self.data = []\n",
            "        self.write(self.prompt)\n",
            "\n",
            "    def write(self, data, *args):\n",
            "        \"Push a string back to the client\"\n",
            "        if args:\n",
            "            data %= args\n",
            "        if data.endswith('\\n') and not data.endswith(self.terminator):\n",
            "            data = data[:-1] + self.terminator  # fix newlines\n",
            "        self.push(data)\n",
            "    \n",
            "    def collect_incoming_data(self, data):\n",
            "        \"Collect one character at the time\"\n",
            "        self.data.append(data)\n",
            "\n",
            "    def found_terminator(self):\n",
            "        \"Put in the queue the line received from the client\"\n",
            "        line = ''.join(self.data)\n",
            "        self.log('Received line %r from %s' % (line, self.addr))\n",
            "        if line == 'EOF':\n",
            "            self.i.__exit__(None, None, None)\n",
            "            self.handle_close()\n",
            "        else:\n",
            "            task = self.i.submit(line)\n",
            "            task.run()  # synchronous or not\n",
            "            if task.etype:  # manage exception\n",
            "                error = '%s: %s\\nReceived: %s' % (\n",
            "                    task.etype.__name__, task.exc, ' '.join(task.arglist))\n",
            "                self.log_info(task.traceback + error)  # on the server\n",
            "                self.write(error + self.terminator)  # back to the client\n",
            "            else:  # no exception\n",
            "                self.write(task.str + self.terminator)\n",
            "            self.data = []\n",
            "            self.write(self.prompt)\n",
            "\n",
            "\n",
            "class _AsynServer(asyncore.dispatcher):\n",
            "    \"asyncore-based server spawning AsynHandlers\"\n",
            "\n",
            "    def __init__(self, interpreter, newhandler, port, listen=5):\n",
            "        self.interpreter = interpreter\n",
            "        self.newhandler = newhandler\n",
            "        self.port = port\n",
            "        asyncore.dispatcher.__init__(self)\n",
            "        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)\n",
            "        self.bind(('', port))\n",
            "        self.listen(listen)\n",
            "\n",
            "    def handle_accept(self):\n",
            "        clientsock, clientaddr = self.accept()\n",
            "        self.log('Connected from %s' % str(clientaddr))\n",
            "        i = self.interpreter.__class__(self.interpreter.obj)  # new interpreter\n",
            "        self.newhandler(clientsock, i)  # spawn a new handler\n",
            "\n",
            "\n",
            "# ########################## the Interpreter ############################ #\n",
            "\n",
            "class Interpreter(object):\n",
            "    \"\"\"\n",
            "    A context manager with a .send method and a few utility methods:\n",
            "    execute, test and doctest.\n",
            "    \"\"\"\n",
            "    class Exit(Exception):\n",
            "        pass\n",
            "\n",
            "    def __init__(self, obj, commentchar='#', split=shlex.split):\n",
            "        self.obj = obj\n",
            "        try:\n",
            "            self.name = obj.__module__\n",
            "        except AttributeError:\n",
            "            self.name = 'plac'\n",
            "        self.commentchar = commentchar\n",
            "        self.split = split\n",
            "        self._set_commands(obj)\n",
            "        self.tm = TaskManager(obj)\n",
            "        self.man = self.tm.man\n",
            "        self.parser = self.tm.parser\n",
            "        if self.commands:\n",
            "            self.parser.addsubcommands(\n",
            "                self.tm.specialcommands, self.tm, title='special commands')\n",
            "        if obj.mpcommands:\n",
            "            self.parser.addsubcommands(\n",
            "                obj.mpcommands, obj,\n",
            "                title='commands run in external processes')\n",
            "        if obj.thcommands:\n",
            "            self.parser.addsubcommands(\n",
            "                obj.thcommands, obj, title='threaded commands')\n",
            "        self.parser.error = lambda msg: sys.exit(msg)  # patch the parser\n",
            "        self._interpreter = None\n",
            "\n",
            "    def _set_commands(self, obj):\n",
            "        \"Make sure obj has the right command attributes as Python sets\"\n",
            "        for attrname in ('commands', 'mpcommands', 'thcommands'):\n",
            "            setattr(self, attrname, set(getattr(self.__class__, attrname, [])))\n",
            "            setattr(obj, attrname, set(getattr(obj, attrname, [])))\n",
            "        self.commands = obj.commands\n",
            "        self.mpcommands.update(obj.mpcommands)\n",
            "        self.thcommands.update(obj.thcommands)\n",
            "        if (obj.commands or obj.mpcommands or obj.thcommands) and \\\n",
            "           not hasattr(obj, 'help'):  # add default help\n",
            "            obj.help = default_help.__get__(obj, obj.__class__)\n",
            "            self.commands.add('help')\n",
            "\n",
            "    def __enter__(self):\n",
            "        \"Start the inner interpreter loop\"\n",
            "        self._interpreter = self._make_interpreter()\n",
            "        self._interpreter.send(None)\n",
            "        return self\n",
            "\n",
            "    def __exit__(self, exctype, exc, tb):\n",
            "        \"Close the inner interpreter and the task manager\"\n",
            "        self.close(exctype, exc, tb)\n",
            "\n",
            "    def submit(self, line):\n",
            "        \"Send a line to the underlying interpreter and return a task object\"\n",
            "        if self._interpreter is None:\n",
            "            raise RuntimeError(_('%r not initialized: probably you forgot to '\n",
            "                                 'use the with statement') % self)\n",
            "        if isinstance(line, (str, bytes)):\n",
            "            arglist = self.split(line, self.commentchar)\n",
            "        else:  # expects a list of strings\n",
            "            arglist = line\n",
            "        if not arglist:\n",
            "            return nulltask\n",
            "        m = self.tm.man  # manager\n",
            "        if m and not m.started:\n",
            "            m.start()\n",
            "        task = self._interpreter.send(arglist)  # nonblocking\n",
            "        if not plac_core._match_cmd(arglist[0], self.tm.specialcommands):\n",
            "            self.tm.registry[task.no] = task\n",
            "            if m:\n",
            "                m.add_listener(task.no)\n",
            "        return task\n",
            "\n",
            "    def send(self, line):\n",
            "        \"\"\"Send a line to the underlying interpreter and return\n",
            "        the finished task\"\"\"\n",
            "        task = self.submit(line)\n",
            "        BaseTask.run(task)  # blocking\n",
            "        return task\n",
            "\n",
            "    def tasks(self):\n",
            "        \"The full lists of the submitted tasks\"\n",
            "        return self.tm.registry.values()\n",
            "\n",
            "    def close(self, exctype=None, exc=None, tb=None):\n",
            "        \"Can be called to close the interpreter prematurely\"\n",
            "        self.tm.close()\n",
            "        if exctype is not None:\n",
            "            self._interpreter.throw(exctype, exc, tb)\n",
            "        else:\n",
            "            self._interpreter.close()\n",
            "\n",
            "    def _make_interpreter(self):\n",
            "        \"The interpreter main loop, from lists of arguments to task objects\"\n",
            "        enter = getattr(self.obj, '__enter__', lambda: None)\n",
            "        exit = getattr(self.obj, '__exit__', lambda et, ex, tb: None)\n",
            "        enter()\n",
            "        task = None\n",
            "        try:\n",
            "            for no in itertools.count(1):\n",
            "                arglist = yield task\n",
            "                try:\n",
            "                    cmd, result = self.parser.consume(arglist)\n",
            "                except SystemExit as e:  # for invalid commands\n",
            "                    if e.args == (0,):  # raised as sys.exit(0)\n",
            "                        errlist = []\n",
            "                    else:\n",
            "                        errlist = [str(e)]\n",
            "                    task = SynTask(no, arglist, iter(errlist))\n",
            "                    continue\n",
            "                except:  # anything else\n",
            "                    task = SynTask(no, arglist, gen_exc(*sys.exc_info()))\n",
            "                    continue\n",
            "                if not plac_core.iterable(result):  # atomic result\n",
            "                    task = SynTask(no, arglist, gen_val(result))\n",
            "                elif cmd in self.obj.mpcommands:\n",
            "                    task = MPTask(no, arglist, result, self.tm.man)\n",
            "                elif cmd in self.obj.thcommands:\n",
            "                    task = ThreadedTask(no, arglist, result)\n",
            "                else:  # blocking task\n",
            "                    task = SynTask(no, arglist, result)\n",
            "        except GeneratorExit:  # regular exit\n",
            "            exit(None, None, None)\n",
            "        except:  # exceptional exit\n",
            "            exit(*sys.exc_info())\n",
            "            raise\n",
            "\n",
            "    def check(self, given_input, expected_output):\n",
            "        \"Make sure you get the expected_output from the given_input\"\n",
            "        output = self.send(given_input).str  # blocking\n",
            "        ok = (output == expected_output)\n",
            "        if not ok:\n",
            "            # the message here is not internationalized on purpose\n",
            "            msg = 'input: %s\\noutput: %s\\nexpected: %s' % (\n",
            "                given_input, output, expected_output)\n",
            "            raise AssertionError(msg)\n",
            "\n",
            "    def _parse_doctest(self, lineiter):\n",
            "        \"Returns the lines of input, the lines of output, and the line number\"\n",
            "        lines = [line.strip() for line in lineiter]\n",
            "        inputs = []\n",
            "        positions = []\n",
            "        for i, line in enumerate(lines):\n",
            "            if line.startswith('i> '):\n",
            "                inputs.append(line[3:])\n",
            "                positions.append(i)\n",
            "        positions.append(len(lines) + 1)  # last position\n",
            "        outputs = []\n",
            "        for i, start in enumerate(positions[:-1]):\n",
            "            end = positions[i + 1]\n",
            "            outputs.append('\\n'.join(lines[start+1:end]))\n",
            "        return zip(inputs, outputs, positions)\n",
            "\n",
            "    def doctest(self, lineiter, verbose=False):\n",
            "        \"\"\"\n",
            "        Parse a text containing doctests in a context and tests of all them.\n",
            "        Raise an error even if a single doctest if broken. Use this for\n",
            "        sequential tests which are logically grouped.\n",
            "        \"\"\"\n",
            "        with self:\n",
            "            try:\n",
            "                for input, output, no in self._parse_doctest(lineiter):\n",
            "                    if verbose:\n",
            "                        write('i> %s\\n' % input)\n",
            "                        write('-> %s\\n' % output)\n",
            "                    task = self.send(input)  # blocking\n",
            "                    if not str(task) == output:\n",
            "                        msg = ('line %d: input: %s\\noutput: %s\\nexpected: %s\\n'\n",
            "                               % (no + 1, input, task, output))\n",
            "                        write(msg)\n",
            "                        if task.exc:\n",
            "                            raise_(task.etype, task.exc, task.tb)\n",
            "            except self.Exit:\n",
            "                pass\n",
            "\n",
            "    def execute(self, lineiter, verbose=False):\n",
            "        \"Execute a lineiter of commands in a context and print the output\"\n",
            "        with self:\n",
            "            try:\n",
            "                for line in lineiter:\n",
            "                    if verbose:\n",
            "                        write('i> ' + line)\n",
            "                    task = self.send(line)  # finished task\n",
            "                    if task.etype:  # there was an error\n",
            "                        raise_(task.etype, task.exc, task.tb)\n",
            "                    write('%s\\n' % task.str)\n",
            "            except self.Exit:\n",
            "                pass\n",
            "\n",
            "    def multiline(self, stdin=sys.stdin, terminator=';', verbose=False):\n",
            "        \"The multiline mode is especially suited for usage with emacs\"\n",
            "        with self:\n",
            "            try:\n",
            "                for line in read_long_line(stdin, terminator):\n",
            "                    task = self.submit(line)\n",
            "                    task.run()\n",
            "                    write('%s\\n' % task.str)\n",
            "                    if verbose and task.traceback:\n",
            "                        write(task.traceback)\n",
            "            except self.Exit:\n",
            "                pass\n",
            "\n",
            "    def interact(self, stdin=sys.stdin, prompt='i> ', verbose=False):\n",
            "        \"Starts an interactive command loop reading commands from the consolle\"\n",
            "        try:\n",
            "            import readline\n",
            "            readline_present = True\n",
            "        except ImportError:\n",
            "            readline_present = False\n",
            "        if stdin is sys.stdin and readline_present:  # use readline\n",
            "            histfile = os.path.expanduser('~/.%s.history' % self.name)\n",
            "            completions = list(self.commands) + list(self.mpcommands) + \\\n",
            "                list(self.thcommands) + list(self.tm.specialcommands)\n",
            "            self.stdin = ReadlineInput(completions, histfile=histfile)\n",
            "        else:\n",
            "            self.stdin = stdin\n",
            "        self.prompt = prompt\n",
            "        self.verbose = verbose\n",
            "        intro = self.obj.__doc__ or ''\n",
            "        write(intro + '\\n')\n",
            "        with self:\n",
            "            self.obj._interact_ = True\n",
            "            if self.stdin is sys.stdin:  # do not close stdin automatically\n",
            "                self._manage_input()\n",
            "            else:\n",
            "                with self.stdin:  # close stdin automatically\n",
            "                    self._manage_input()\n",
            "\n",
            "    def _manage_input(self):\n",
            "        \"Convert input lines into task which are then executed\"\n",
            "        try:\n",
            "            for line in iter(lambda: read_line(self.stdin, self.prompt), ''):\n",
            "                line = line.strip()\n",
            "                if not line:\n",
            "                    continue\n",
            "                task = self.submit(line)\n",
            "                task.run()  # synchronous or not\n",
            "                write(str(task) + '\\n')\n",
            "                if self.verbose and task.etype:\n",
            "                    write(task.traceback)\n",
            "        except self.Exit:\n",
            "            pass\n",
            "\n",
            "    def start_server(self, port=2199, **kw):\n",
            "        \"\"\"Starts an asyncore server reading commands for clients and opening\n",
            "        a new interpreter for each connection.\"\"\"\n",
            "        _AsynServer(self, _AsynHandler, port)  # register the server\n",
            "        try:\n",
            "            asyncore.loop(**kw)\n",
            "        except (KeyboardInterrupt, TerminatedProcess):\n",
            "            pass\n",
            "        finally:\n",
            "            asyncore.close_all()\n",
            "\n",
            "    def add_monitor(self, mon):\n",
            "        self.man.add(mon)\n",
            "\n",
            "    def del_monitor(self, name):\n",
            "        self.man.delete(name)\n",
            "\n",
            "    @classmethod\n",
            "    def call(cls, factory, arglist=sys.argv[1:],\n",
            "             commentchar='#', split=shlex.split,\n",
            "             stdin=sys.stdin, prompt='i> ', verbose=False):\n",
            "        \"\"\"\n",
            "        Call a container factory with the arglist and instantiate an\n",
            "        interpreter object. If there are remaining arguments, send them to the\n",
            "        interpreter, else start an interactive session.\n",
            "        \"\"\"\n",
            "        obj = partial_call(factory, arglist)\n",
            "        i = cls(obj, commentchar, split)\n",
            "        if i.obj._args_:\n",
            "            with i:\n",
            "                task = i.send(i.obj._args_)  # synchronous\n",
            "                if task.exc:\n",
            "                    raise_(task.etype, task.exc, task.tb)\n",
            "                out = str(task)\n",
            "                if out:\n",
            "                    print(out)\n",
            "        elif i.obj._interact_:\n",
            "            i.interact(stdin, prompt, verbose)\n",
            "        else:\n",
            "            i.parser.print_usage()\n",
            "\n",
            "# ################################## runp ################################### #\n",
            "\n",
            "\n",
            "class _TaskLauncher(object):\n",
            "    \"Helper for runp\"\n",
            "\n",
            "    def __init__(self, genseq, mode):\n",
            "        if mode == 'p':\n",
            "            self.mpcommands = ['rungen']\n",
            "        else:\n",
            "            self.thcommands = ['rungen']\n",
            "        self.genlist = list(genseq)\n",
            "\n",
            "    def rungen(self, i):\n",
            "        for out in self.genlist[int(i) - 1]:\n",
            "            yield out\n",
            "\n",
            "\n",
            "def runp(genseq, mode='p'):\n",
            "    \"\"\"Run a sequence of generators in parallel. Mode can be 'p' (use processes)\n",
            "    or 't' (use threads). After all of them are finished, return a list of\n",
            "    task objects.\n",
            "    \"\"\"\n",
            "    assert mode in 'pt', mode\n",
            "    launcher = _TaskLauncher(genseq, mode)\n",
            "    res = []\n",
            "    with Interpreter(launcher) as inter:\n",
            "        for i in range(len(launcher.genlist)):\n",
            "            inter.submit('rungen %d' % (i + 1)).run()\n",
            "        for task in inter.tasks():\n",
            "            try:\n",
            "                res.append(task.result)\n",
            "            except Exception as e:\n",
            "                res.append(e)\n",
            "    return res\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# #########################     LICENSE     ############################ #\n",
            "\n",
            "# Copyright (c) 2005-2018, Michele Simionato\n",
            "# All rights reserved.\n",
            "\n",
            "# Redistribution and use in source and binary forms, with or without\n",
            "# modification, are permitted provided that the following conditions are\n",
            "# met:\n",
            "\n",
            "#   Redistributions of source code must retain the above copyright\n",
            "#   notice, this list of conditions and the following disclaimer.\n",
            "#   Redistributions in bytecode form must reproduce the above copyright\n",
            "#   notice, this list of conditions and the following disclaimer in\n",
            "#   the documentation and/or other materials provided with the\n",
            "#   distribution.\n",
            "\n",
            "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
            "# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
            "# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
            "# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
            "# HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n",
            "# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n",
            "# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS\n",
            "# OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n",
            "# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\n",
            "# TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n",
            "# USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
            "# DAMAGE.\n",
            "\n",
            "\"\"\"\n",
            "Decorator module, see http://pypi.python.org/pypi/decorator\n",
            "for the documentation.\n",
            "\"\"\"\n",
            "from __future__ import print_function\n",
            "\n",
            "import re\n",
            "import sys\n",
            "import inspect\n",
            "import operator\n",
            "import itertools\n",
            "import collections\n",
            "\n",
            "__version__ = '4.4.0'\n",
            "\n",
            "if sys.version >= '3':\n",
            "    from inspect import getfullargspec\n",
            "\n",
            "    def get_init(cls):\n",
            "        return cls.__init__\n",
            "else:\n",
            "    FullArgSpec = collections.namedtuple(\n",
            "        'FullArgSpec', 'args varargs varkw defaults '\n",
            "        'kwonlyargs kwonlydefaults annotations')\n",
            "\n",
            "    def getfullargspec(f):\n",
            "        \"A quick and dirty replacement for getfullargspec for Python 2.X\"\n",
            "        return FullArgSpec._make(inspect.getargspec(f) + ([], None, {}))\n",
            "\n",
            "    def get_init(cls):\n",
            "        return cls.__init__.__func__\n",
            "\n",
            "try:\n",
            "    iscoroutinefunction = inspect.iscoroutinefunction\n",
            "except AttributeError:\n",
            "    # let's assume there are no coroutine functions in old Python\n",
            "    def iscoroutinefunction(f):\n",
            "        return False\n",
            "try:\n",
            "    from inspect import isgeneratorfunction\n",
            "except ImportError:\n",
            "    # assume no generator function in old Python versions\n",
            "    def isgeneratorfunction(caller):\n",
            "        return False\n",
            "\n",
            "\n",
            "DEF = re.compile(r'\\s*def\\s*([_\\w][_\\w\\d]*)\\s*\\(')\n",
            "\n",
            "\n",
            "# basic functionality\n",
            "class FunctionMaker(object):\n",
            "    \"\"\"\n",
            "    An object with the ability to create functions with a given signature.\n",
            "    It has attributes name, doc, module, signature, defaults, dict and\n",
            "    methods update and make.\n",
            "    \"\"\"\n",
            "\n",
            "    # Atomic get-and-increment provided by the GIL\n",
            "    _compile_count = itertools.count()\n",
            "\n",
            "    # make pylint happy\n",
            "    args = varargs = varkw = defaults = kwonlyargs = kwonlydefaults = ()\n",
            "\n",
            "    def __init__(self, func=None, name=None, signature=None,\n",
            "                 defaults=None, doc=None, module=None, funcdict=None):\n",
            "        self.shortsignature = signature\n",
            "        if func:\n",
            "            # func can be a class or a callable, but not an instance method\n",
            "            self.name = func.__name__\n",
            "            if self.name == '<lambda>':  # small hack for lambda functions\n",
            "                self.name = '_lambda_'\n",
            "            self.doc = func.__doc__\n",
            "            self.module = func.__module__\n",
            "            if inspect.isfunction(func):\n",
            "                argspec = getfullargspec(func)\n",
            "                self.annotations = getattr(func, '__annotations__', {})\n",
            "                for a in ('args', 'varargs', 'varkw', 'defaults', 'kwonlyargs',\n",
            "                          'kwonlydefaults'):\n",
            "                    setattr(self, a, getattr(argspec, a))\n",
            "                for i, arg in enumerate(self.args):\n",
            "                    setattr(self, 'arg%d' % i, arg)\n",
            "                allargs = list(self.args)\n",
            "                allshortargs = list(self.args)\n",
            "                if self.varargs:\n",
            "                    allargs.append('*' + self.varargs)\n",
            "                    allshortargs.append('*' + self.varargs)\n",
            "                elif self.kwonlyargs:\n",
            "                    allargs.append('*')  # single star syntax\n",
            "                for a in self.kwonlyargs:\n",
            "                    allargs.append('%s=None' % a)\n",
            "                    allshortargs.append('%s=%s' % (a, a))\n",
            "                if self.varkw:\n",
            "                    allargs.append('**' + self.varkw)\n",
            "                    allshortargs.append('**' + self.varkw)\n",
            "                self.signature = ', '.join(allargs)\n",
            "                self.shortsignature = ', '.join(allshortargs)\n",
            "                self.dict = func.__dict__.copy()\n",
            "        # func=None happens when decorating a caller\n",
            "        if name:\n",
            "            self.name = name\n",
            "        if signature is not None:\n",
            "            self.signature = signature\n",
            "        if defaults:\n",
            "            self.defaults = defaults\n",
            "        if doc:\n",
            "            self.doc = doc\n",
            "        if module:\n",
            "            self.module = module\n",
            "        if funcdict:\n",
            "            self.dict = funcdict\n",
            "        # check existence required attributes\n",
            "        assert hasattr(self, 'name')\n",
            "        if not hasattr(self, 'signature'):\n",
            "            raise TypeError('You are decorating a non function: %s' % func)\n",
            "\n",
            "    def update(self, func, **kw):\n",
            "        \"Update the signature of func with the data in self\"\n",
            "        func.__name__ = self.name\n",
            "        func.__doc__ = getattr(self, 'doc', None)\n",
            "        func.__dict__ = getattr(self, 'dict', {})\n",
            "        func.__defaults__ = self.defaults\n",
            "        func.__kwdefaults__ = self.kwonlydefaults or None\n",
            "        func.__annotations__ = getattr(self, 'annotations', None)\n",
            "        try:\n",
            "            frame = sys._getframe(3)\n",
            "        except AttributeError:  # for IronPython and similar implementations\n",
            "            callermodule = '?'\n",
            "        else:\n",
            "            callermodule = frame.f_globals.get('__name__', '?')\n",
            "        func.__module__ = getattr(self, 'module', callermodule)\n",
            "        func.__dict__.update(kw)\n",
            "\n",
            "    def make(self, src_templ, evaldict=None, addsource=False, **attrs):\n",
            "        \"Make a new function from a given template and update the signature\"\n",
            "        src = src_templ % vars(self)  # expand name and signature\n",
            "        evaldict = evaldict or {}\n",
            "        mo = DEF.search(src)\n",
            "        if mo is None:\n",
            "            raise SyntaxError('not a valid function template\\n%s' % src)\n",
            "        name = mo.group(1)  # extract the function name\n",
            "        names = set([name] + [arg.strip(' *') for arg in\n",
            "                              self.shortsignature.split(',')])\n",
            "        for n in names:\n",
            "            if n in ('_func_', '_call_'):\n",
            "                raise NameError('%s is overridden in\\n%s' % (n, src))\n",
            "\n",
            "        if not src.endswith('\\n'):  # add a newline for old Pythons\n",
            "            src += '\\n'\n",
            "\n",
            "        # Ensure each generated function has a unique filename for profilers\n",
            "        # (such as cProfile) that depend on the tuple of (<filename>,\n",
            "        # <definition line>, <function name>) being unique.\n",
            "        filename = '<%s:decorator-gen-%d>' % (\n",
            "            __file__, next(self._compile_count))\n",
            "        try:\n",
            "            code = compile(src, filename, 'single')\n",
            "            exec(code, evaldict)\n",
            "        except Exception:\n",
            "            print('Error in generated code:', file=sys.stderr)\n",
            "            print(src, file=sys.stderr)\n",
            "            raise\n",
            "        func = evaldict[name]\n",
            "        if addsource:\n",
            "            attrs['__source__'] = src\n",
            "        self.update(func, **attrs)\n",
            "        return func\n",
            "\n",
            "    @classmethod\n",
            "    def create(cls, obj, body, evaldict, defaults=None,\n",
            "               doc=None, module=None, addsource=True, **attrs):\n",
            "        \"\"\"\n",
            "        Create a function from the strings name, signature and body.\n",
            "        evaldict is the evaluation dictionary. If addsource is true an\n",
            "        attribute __source__ is added to the result. The attributes attrs\n",
            "        are added, if any.\n",
            "        \"\"\"\n",
            "        if isinstance(obj, str):  # \"name(signature)\"\n",
            "            name, rest = obj.strip().split('(', 1)\n",
            "            signature = rest[:-1]  # strip a right parens\n",
            "            func = None\n",
            "        else:  # a function\n",
            "            name = None\n",
            "            signature = None\n",
            "            func = obj\n",
            "        self = cls(func, name, signature, defaults, doc, module)\n",
            "        ibody = '\\n'.join('    ' + line for line in body.splitlines())\n",
            "        caller = evaldict.get('_call_')  # when called from `decorate`\n",
            "        if caller and iscoroutinefunction(caller):\n",
            "            body = ('async def %(name)s(%(signature)s):\\n' + ibody).replace(\n",
            "                'return', 'return await')\n",
            "        else:\n",
            "            body = 'def %(name)s(%(signature)s):\\n' + ibody\n",
            "        return self.make(body, evaldict, addsource, **attrs)\n",
            "\n",
            "\n",
            "def decorate(func, caller, extras=()):\n",
            "    \"\"\"\n",
            "    decorate(func, caller) decorates a function using a caller.\n",
            "    If the caller is a generator function, the resulting function\n",
            "    will be a generator function.\n",
            "    \"\"\"\n",
            "    evaldict = dict(_call_=caller, _func_=func)\n",
            "    es = ''\n",
            "    for i, extra in enumerate(extras):\n",
            "        ex = '_e%d_' % i\n",
            "        evaldict[ex] = extra\n",
            "        es += ex + ', '\n",
            "\n",
            "    if '3.5' <= sys.version < '3.6':\n",
            "        # with Python 3.5 isgeneratorfunction returns True for all coroutines\n",
            "        # however we know that it is NOT possible to have a generator\n",
            "        # coroutine in python 3.5: PEP525 was not there yet\n",
            "        generatorcaller = isgeneratorfunction(\n",
            "            caller) and not iscoroutinefunction(caller)\n",
            "    else:\n",
            "        generatorcaller = isgeneratorfunction(caller)\n",
            "    if generatorcaller:\n",
            "        fun = FunctionMaker.create(\n",
            "            func, \"for res in _call_(_func_, %s%%(shortsignature)s):\\n\"\n",
            "                  \"    yield res\" % es, evaldict, __wrapped__=func)\n",
            "    else:\n",
            "        fun = FunctionMaker.create(\n",
            "            func, \"return _call_(_func_, %s%%(shortsignature)s)\" % es,\n",
            "            evaldict, __wrapped__=func)\n",
            "    if hasattr(func, '__qualname__'):\n",
            "        fun.__qualname__ = func.__qualname__\n",
            "    return fun\n",
            "\n",
            "\n",
            "def decorator(caller, _func=None):\n",
            "    \"\"\"decorator(caller) converts a caller function into a decorator\"\"\"\n",
            "    if _func is not None:  # return a decorated function\n",
            "        # this is obsolete behavior; you should use decorate instead\n",
            "        return decorate(_func, caller)\n",
            "    # else return a decorator function\n",
            "    defaultargs, defaults = '', ()\n",
            "    if inspect.isclass(caller):\n",
            "        name = caller.__name__.lower()\n",
            "        doc = 'decorator(%s) converts functions/generators into ' \\\n",
            "            'factories of %s objects' % (caller.__name__, caller.__name__)\n",
            "    elif inspect.isfunction(caller):\n",
            "        if caller.__name__ == '<lambda>':\n",
            "            name = '_lambda_'\n",
            "        else:\n",
            "            name = caller.__name__\n",
            "        doc = caller.__doc__\n",
            "        nargs = caller.__code__.co_argcount\n",
            "        ndefs = len(caller.__defaults__ or ())\n",
            "        defaultargs = ', '.join(caller.__code__.co_varnames[nargs-ndefs:nargs])\n",
            "        if defaultargs:\n",
            "            defaultargs += ','\n",
            "        defaults = caller.__defaults__\n",
            "    else:  # assume caller is an object with a __call__ method\n",
            "        name = caller.__class__.__name__.lower()\n",
            "        doc = caller.__call__.__doc__\n",
            "    evaldict = dict(_call=caller, _decorate_=decorate)\n",
            "    dec = FunctionMaker.create(\n",
            "        '%s(func, %s)' % (name, defaultargs),\n",
            "        'if func is None: return lambda func:  _decorate_(func, _call, (%s))\\n'\n",
            "        'return _decorate_(func, _call, (%s))' % (defaultargs, defaultargs),\n",
            "        evaldict, doc=doc, module=caller.__module__, __wrapped__=caller)\n",
            "    if defaults:\n",
            "        dec.__defaults__ = (None,) + defaults\n",
            "    return dec\n",
            "\n",
            "\n",
            "# ####################### contextmanager ####################### #\n",
            "\n",
            "try:  # Python >= 3.2\n",
            "    from contextlib import _GeneratorContextManager\n",
            "except ImportError:  # Python >= 2.5\n",
            "    from contextlib import GeneratorContextManager as _GeneratorContextManager\n",
            "\n",
            "\n",
            "class ContextManager(_GeneratorContextManager):\n",
            "    def __call__(self, func):\n",
            "        \"\"\"Context manager decorator\"\"\"\n",
            "        return FunctionMaker.create(\n",
            "            func, \"with _self_: return _func_(%(shortsignature)s)\",\n",
            "            dict(_self_=self, _func_=func), __wrapped__=func)\n",
            "\n",
            "\n",
            "init = getfullargspec(_GeneratorContextManager.__init__)\n",
            "n_args = len(init.args)\n",
            "if n_args == 2 and not init.varargs:  # (self, genobj) Python 2.7\n",
            "    def __init__(self, g, *a, **k):\n",
            "        return _GeneratorContextManager.__init__(self, g(*a, **k))\n",
            "    ContextManager.__init__ = __init__\n",
            "elif n_args == 2 and init.varargs:  # (self, gen, *a, **k) Python 3.4\n",
            "    pass\n",
            "elif n_args == 4:  # (self, gen, args, kwds) Python 3.5\n",
            "    def __init__(self, g, *a, **k):\n",
            "        return _GeneratorContextManager.__init__(self, g, a, k)\n",
            "    ContextManager.__init__ = __init__\n",
            "\n",
            "_contextmanager = decorator(ContextManager)\n",
            "\n",
            "\n",
            "def contextmanager(func):\n",
            "    # Enable Pylint config: contextmanager-decorators=decorator.contextmanager\n",
            "    return _contextmanager(func)\n",
            "\n",
            "\n",
            "# ############################ dispatch_on ############################ #\n",
            "\n",
            "def append(a, vancestors):\n",
            "    \"\"\"\n",
            "    Append ``a`` to the list of the virtual ancestors, unless it is already\n",
            "    included.\n",
            "    \"\"\"\n",
            "    add = True\n",
            "    for j, va in enumerate(vancestors):\n",
            "        if issubclass(va, a):\n",
            "            add = False\n",
            "            break\n",
            "        if issubclass(a, va):\n",
            "            vancestors[j] = a\n",
            "            add = False\n",
            "    if add:\n",
            "        vancestors.append(a)\n",
            "\n",
            "\n",
            "# inspired from simplegeneric by P.J. Eby and functools.singledispatch\n",
            "def dispatch_on(*dispatch_args):\n",
            "    \"\"\"\n",
            "    Factory of decorators turning a function into a generic function\n",
            "    dispatching on the given arguments.\n",
            "    \"\"\"\n",
            "    assert dispatch_args, 'No dispatch args passed'\n",
            "    dispatch_str = '(%s,)' % ', '.join(dispatch_args)\n",
            "\n",
            "    def check(arguments, wrong=operator.ne, msg=''):\n",
            "        \"\"\"Make sure one passes the expected number of arguments\"\"\"\n",
            "        if wrong(len(arguments), len(dispatch_args)):\n",
            "            raise TypeError('Expected %d arguments, got %d%s' %\n",
            "                            (len(dispatch_args), len(arguments), msg))\n",
            "\n",
            "    def gen_func_dec(func):\n",
            "        \"\"\"Decorator turning a function into a generic function\"\"\"\n",
            "\n",
            "        # first check the dispatch arguments\n",
            "        argset = set(getfullargspec(func).args)\n",
            "        if not set(dispatch_args) <= argset:\n",
            "            raise NameError('Unknown dispatch arguments %s' % dispatch_str)\n",
            "\n",
            "        typemap = {}\n",
            "\n",
            "        def vancestors(*types):\n",
            "            \"\"\"\n",
            "            Get a list of sets of virtual ancestors for the given types\n",
            "            \"\"\"\n",
            "            check(types)\n",
            "            ras = [[] for _ in range(len(dispatch_args))]\n",
            "            for types_ in typemap:\n",
            "                for t, type_, ra in zip(types, types_, ras):\n",
            "                    if issubclass(t, type_) and type_ not in t.mro():\n",
            "                        append(type_, ra)\n",
            "            return [set(ra) for ra in ras]\n",
            "\n",
            "        def ancestors(*types):\n",
            "            \"\"\"\n",
            "            Get a list of virtual MROs, one for each type\n",
            "            \"\"\"\n",
            "            check(types)\n",
            "            lists = []\n",
            "            for t, vas in zip(types, vancestors(*types)):\n",
            "                n_vas = len(vas)\n",
            "                if n_vas > 1:\n",
            "                    raise RuntimeError(\n",
            "                        'Ambiguous dispatch for %s: %s' % (t, vas))\n",
            "                elif n_vas == 1:\n",
            "                    va, = vas\n",
            "                    mro = type('t', (t, va), {}).mro()[1:]\n",
            "                else:\n",
            "                    mro = t.mro()\n",
            "                lists.append(mro[:-1])  # discard t and object\n",
            "            return lists\n",
            "\n",
            "        def register(*types):\n",
            "            \"\"\"\n",
            "            Decorator to register an implementation for the given types\n",
            "            \"\"\"\n",
            "            check(types)\n",
            "\n",
            "            def dec(f):\n",
            "                check(getfullargspec(f).args, operator.lt, ' in ' + f.__name__)\n",
            "                typemap[types] = f\n",
            "                return f\n",
            "            return dec\n",
            "\n",
            "        def dispatch_info(*types):\n",
            "            \"\"\"\n",
            "            An utility to introspect the dispatch algorithm\n",
            "            \"\"\"\n",
            "            check(types)\n",
            "            lst = []\n",
            "            for anc in itertools.product(*ancestors(*types)):\n",
            "                lst.append(tuple(a.__name__ for a in anc))\n",
            "            return lst\n",
            "\n",
            "        def _dispatch(dispatch_args, *args, **kw):\n",
            "            types = tuple(type(arg) for arg in dispatch_args)\n",
            "            try:  # fast path\n",
            "                f = typemap[types]\n",
            "            except KeyError:\n",
            "                pass\n",
            "            else:\n",
            "                return f(*args, **kw)\n",
            "            combinations = itertools.product(*ancestors(*types))\n",
            "            next(combinations)  # the first one has been already tried\n",
            "            for types_ in combinations:\n",
            "                f = typemap.get(types_)\n",
            "                if f is not None:\n",
            "                    return f(*args, **kw)\n",
            "\n",
            "            # else call the default implementation\n",
            "            return func(*args, **kw)\n",
            "\n",
            "        return FunctionMaker.create(\n",
            "            func, 'return _f_(%s, %%(shortsignature)s)' % dispatch_str,\n",
            "            dict(_f_=_dispatch), register=register, default=func,\n",
            "            typemap=typemap, vancestors=vancestors, ancestors=ancestors,\n",
            "            dispatch_info=dispatch_info, __wrapped__=func)\n",
            "\n",
            "    gen_func_dec.__name__ = 'dispatch_on' + dispatch_str\n",
            "    return gen_func_dec\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#####\n",
            "# Copyright (c) 2011-2015, NVIDIA Corporation.  All rights reserved.\n",
            "#\n",
            "# Redistribution and use in source and binary forms, with or without\n",
            "# modification, are permitted provided that the following conditions are met:\n",
            "# \n",
            "#    * Redistributions of source code must retain the above copyright notice,\n",
            "#      this list of conditions and the following disclaimer.\n",
            "#    * Redistributions in binary form must reproduce the above copyright\n",
            "#      notice, this list of conditions and the following disclaimer in the\n",
            "#      documentation and/or other materials provided with the distribution.\n",
            "#    * Neither the name of the NVIDIA Corporation nor the names of its\n",
            "#      contributors may be used to endorse or promote products derived from\n",
            "#      this software without specific prior written permission.\n",
            "#\n",
            "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
            "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
            "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
            "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n",
            "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n",
            "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
            "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n",
            "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF \n",
            "# THE POSSIBILITY OF SUCH DAMAGE.\n",
            "#####\n",
            "\n",
            "#\n",
            "# nvidia_smi\n",
            "# nvml_bindings <at> nvidia <dot> com\n",
            "#\n",
            "# Sample code that attempts to reproduce the output of nvidia-smi -q -x\n",
            "# For many cases the output should match\n",
            "#\n",
            "# Can be used as a library or a command line script\n",
            "#\n",
            "# To Run:\n",
            "# $ python nvidia_smi.py\n",
            "#\n",
            "\n",
            "from pynvml import *\n",
            "import datetime\n",
            "\n",
            "#\n",
            "# Helper functions\n",
            "#\n",
            "def GetEccByType(handle, counterType, errorType):\n",
            "    strResult = ''\n",
            "    \n",
            "    try:\n",
            "        deviceMemory = nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType,\n",
            "                                                       NVML_MEMORY_LOCATION_DEVICE_MEMORY)\n",
            "    except NVMLError as err:\n",
            "        deviceMemory = handleError(err)\n",
            "    strResult += '          <device_memory>' + str(deviceMemory) + '</device_memory>\\n'\n",
            "    \n",
            "    try:\n",
            "        registerFile = nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType,\n",
            "                                                       NVML_MEMORY_LOCATION_REGISTER_FILE)\n",
            "    except NVMLError as err:\n",
            "        registerFile = handleError(err)\n",
            "    \n",
            "    strResult += '          <register_file>' + str(registerFile) + '</register_file>\\n'\n",
            "    \n",
            "    try:\n",
            "        l1Cache = nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType,\n",
            "                                                  NVML_MEMORY_LOCATION_L1_CACHE)\n",
            "    except NVMLError as err:\n",
            "        l1Cache = handleError(err)\n",
            "    strResult += '          <l1_cache>' + str(l1Cache) + '</l1_cache>\\n'\n",
            "    \n",
            "    try:\n",
            "        l2Cache = nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType,\n",
            "                                                  NVML_MEMORY_LOCATION_L2_CACHE)\n",
            "    except NVMLError as err:\n",
            "        l2Cache = handleError(err)\n",
            "    strResult += '          <l2_cache>' + str(l2Cache) + '</l2_cache>\\n'\n",
            "    \n",
            "    try:\n",
            "        textureMemory = nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType,\n",
            "                                                        NVML_MEMORY_LOCATION_TEXTURE_MEMORY)\n",
            "    except NVMLError as err:\n",
            "        textureMemory = handleError(err)\n",
            "    strResult += '          <texture_memory>' + str(textureMemory) + '</texture_memory>\\n'\n",
            "    \n",
            "    try:\n",
            "        count = str(nvmlDeviceGetTotalEccErrors(handle, errorType, counterType))\n",
            "    except NVMLError as err:\n",
            "        count = handleError(err)\n",
            "    strResult += '          <total>' + count + '</total>\\n'\n",
            "    \n",
            "    return strResult\n",
            "\n",
            "def GetEccByCounter(handle, counterType):\n",
            "    strResult = ''\n",
            "    strResult += '        <single_bit>\\n'\n",
            "    strResult += str(GetEccByType(handle, counterType, NVML_MEMORY_ERROR_TYPE_CORRECTED))\n",
            "    strResult += '        </single_bit>\\n'\n",
            "    strResult += '        <double_bit>\\n'\n",
            "    strResult += str(GetEccByType(handle, counterType, NVML_MEMORY_ERROR_TYPE_UNCORRECTED))\n",
            "    strResult += '        </double_bit>\\n'\n",
            "    return strResult\n",
            "\n",
            "def GetEccStr(handle):\n",
            "    strResult = ''\n",
            "    strResult += '      <volatile>\\n'\n",
            "    strResult += str(GetEccByCounter(handle, NVML_VOLATILE_ECC))\n",
            "    strResult += '      </volatile>\\n'\n",
            "    strResult += '      <aggregate>\\n'\n",
            "    strResult += str(GetEccByCounter(handle, NVML_AGGREGATE_ECC))\n",
            "    strResult += '      </aggregate>\\n'\n",
            "    return strResult\n",
            "\n",
            "def GetRetiredPagesByCause(handle, cause):\n",
            "    strResult = ''\n",
            "    try:\n",
            "        pages = nvmlDeviceGetRetiredPages(handle, cause)   \n",
            "        count = str(len(pages))\n",
            "    except NVMLError as err:\n",
            "        error = handleError(err)\n",
            "        pages = None\n",
            "        count = error\n",
            "    strResult += '        <retired_count>' + count + '</retired_count>\\n'\n",
            "    if pages is not None:\n",
            "        strResult += '        <retired_page_addresses>\\n'\n",
            "        for page in pages:\n",
            "            strResult += '          <retired_page_address>' + \"0x%016x\" % page + '</retired_page_address>\\n'\n",
            "        strResult += '        </retired_page_addresses>\\n'\n",
            "    else:\n",
            "        strResult += '        <retired_page_addresses>' + error + '</retired_page_addresses>\\n'\n",
            "    return strResult\n",
            "\n",
            "def GetRetiredPagesStr(handle):\n",
            "    strResult = ''\n",
            "    causes = [ \"multiple_single_bit_retirement\", \"double_bit_retirement\" ]\n",
            "    for idx in range(NVML_PAGE_RETIREMENT_CAUSE_COUNT):\n",
            "        strResult += '      <' + causes[idx] + '>\\n'\n",
            "        strResult += GetRetiredPagesByCause(handle, idx)\n",
            "        strResult += '      </' + causes[idx] + '>\\n'\n",
            "\n",
            "    strResult += '      <pending_retirement>'\n",
            "    try:\n",
            "        if NVML_FEATURE_DISABLED == nvmlDeviceGetRetiredPagesPendingStatus(handle):\n",
            "            strResult += \"No\"\n",
            "        else:\n",
            "            strResult += \"Yes\"\n",
            "    except NVMLError as err:\n",
            "        strResult += handleError(err)\n",
            "    strResult += '</pending_retirement>\\n'\n",
            "    return strResult\n",
            "    \n",
            "def StrGOM(mode):\n",
            "    if mode == NVML_GOM_ALL_ON:\n",
            "        return \"All On\";\n",
            "    elif mode == NVML_GOM_COMPUTE:\n",
            "        return \"Compute\";\n",
            "    elif mode == NVML_GOM_LOW_DP:\n",
            "        return \"Low Double Precision\";\n",
            "    else:\n",
            "        return \"Unknown\";\n",
            "\n",
            "def GetClocksThrottleReasons(handle):\n",
            "    throttleReasons = [\n",
            "            [nvmlClocksThrottleReasonGpuIdle,           \"clocks_throttle_reason_gpu_idle\"],\n",
            "            [nvmlClocksThrottleReasonUserDefinedClocks, \"clocks_throttle_reason_user_defined_clocks\"],\n",
            "            [nvmlClocksThrottleReasonApplicationsClocksSetting, \"clocks_throttle_reason_applications_clocks_setting\"],\n",
            "            [nvmlClocksThrottleReasonSwPowerCap,        \"clocks_throttle_reason_sw_power_cap\"],\n",
            "            [nvmlClocksThrottleReasonHwSlowdown,        \"clocks_throttle_reason_hw_slowdown\"],\n",
            "            [nvmlClocksThrottleReasonUnknown,           \"clocks_throttle_reason_unknown\"]\n",
            "            ];\n",
            "\n",
            "    strResult = ''\n",
            "\n",
            "    try:\n",
            "        supportedClocksThrottleReasons = nvmlDeviceGetSupportedClocksThrottleReasons(handle);\n",
            "        clocksThrottleReasons = nvmlDeviceGetCurrentClocksThrottleReasons(handle);\n",
            "        strResult += '    <clocks_throttle_reasons>\\n'\n",
            "        for (mask, name) in throttleReasons:\n",
            "            if (name != \"clocks_throttle_reason_user_defined_clocks\"):\n",
            "                if (mask & supportedClocksThrottleReasons):\n",
            "                    val = \"Active\" if mask & clocksThrottleReasons else \"Not Active\";\n",
            "                else:\n",
            "                    val = handleError(NVML_ERROR_NOT_SUPPORTED);\n",
            "                strResult += \"      <%s>%s</%s>\\n\" % (name, val, name);\n",
            "        strResult += '    </clocks_throttle_reasons>\\n'\n",
            "    except NVMLError as err:\n",
            "        strResult += '    <clocks_throttle_reasons>%s</clocks_throttle_reasons>\\n' % (handleError(err));\n",
            "\n",
            "    return strResult;\n",
            "        \n",
            "#\n",
            "# Converts errors into string messages\n",
            "#\n",
            "def handleError(err):\n",
            "    if (err.value == NVML_ERROR_NOT_SUPPORTED):\n",
            "        return \"N/A\"\n",
            "    else:\n",
            "        return err.__str__()\n",
            "\n",
            "#######\n",
            "def XmlDeviceQuery():\n",
            "\n",
            "    strResult = ''\n",
            "    try:\n",
            "        #\n",
            "        # Initialize NVML\n",
            "        #\n",
            "        nvmlInit()\n",
            "\n",
            "        strResult += '<?xml version=\"1.0\" ?>\\n'\n",
            "        strResult += '<!DOCTYPE nvidia_smi_log SYSTEM \"nvsmi_device_v4.dtd\">\\n'\n",
            "        strResult += '<nvidia_smi_log>\\n'\n",
            "\n",
            "        strResult += '  <timestamp>' + str(datetime.date.today()) + '</timestamp>\\n'\n",
            "        strResult += '  <driver_version>' + str(nvmlSystemGetDriverVersion()) + '</driver_version>\\n'\n",
            "\n",
            "        deviceCount = nvmlDeviceGetCount()\n",
            "        strResult += '  <attached_gpus>' + str(deviceCount) + '</attached_gpus>\\n'\n",
            "\n",
            "        for i in range(0, deviceCount):\n",
            "            handle = nvmlDeviceGetHandleByIndex(i)\n",
            "            \n",
            "            pciInfo = nvmlDeviceGetPciInfo(handle)    \n",
            "            \n",
            "            strResult += '  <gpu id=\"%s\">\\n' % pciInfo.busId\n",
            "            \n",
            "            strResult += '    <product_name>' + nvmlDeviceGetName(handle) + '</product_name>\\n'\n",
            "            \n",
            "            brandNames = {NVML_BRAND_UNKNOWN :  \"Unknown\",\n",
            "                          NVML_BRAND_QUADRO  :  \"Quadro\",\n",
            "                          NVML_BRAND_TESLA   :  \"Tesla\",\n",
            "                          NVML_BRAND_NVS     :  \"NVS\",\n",
            "                          NVML_BRAND_GRID    :  \"Grid\",\n",
            "                          NVML_BRAND_GEFORCE :  \"GeForce\",\n",
            "            }\n",
            "\n",
            "            try:\n",
            "                # if nvmlDeviceGetBrand() succeeds it is guaranteed to be in the dictionary\n",
            "                brandName = brandNames[nvmlDeviceGetBrand(handle)]\n",
            "            except NVMLError as err:\n",
            "                brandName = handleError(err)\n",
            "\n",
            "\n",
            "            strResult += '    <product_brand>' + brandName + '</product_brand>\\n'\n",
            "                \n",
            "            try:\n",
            "                state = ('Enabled' if (nvmlDeviceGetDisplayMode(handle) != 0) else 'Disabled')\n",
            "            except NVMLError as err:\n",
            "                state = handleError(err)\n",
            "            \n",
            "            strResult += '    <display_mode>' + state + '</display_mode>\\n'\n",
            "            \n",
            "            try:\n",
            "                state = ('Enabled' if (nvmlDeviceGetDisplayActive(handle) != 0) else 'Disabled')\n",
            "            except NVMLError as err:\n",
            "                state = handleError(err)\n",
            "            \n",
            "            strResult += '    <display_active>' + state + '</display_active>\\n'\n",
            "            \n",
            "            try:\n",
            "                mode = 'Enabled' if (nvmlDeviceGetPersistenceMode(handle) != 0) else 'Disabled'\n",
            "            except NVMLError as err:\n",
            "                mode = handleError(err)\n",
            "            \n",
            "            strResult += '    <persistence_mode>' + mode + '</persistence_mode>\\n'\n",
            "            \n",
            "            try:\n",
            "                mode = 'Enabled' if (nvmlDeviceGetAccountingMode(handle) != 0) else 'Disabled'\n",
            "            except NVMLError as err:\n",
            "                mode = handleError(err)\n",
            "            \n",
            "            strResult += '    <accounting_mode>' + mode + '</accounting_mode>\\n'\n",
            "           \n",
            "            try:\n",
            "                bufferSize = str(nvmlDeviceGetAccountingBufferSize(handle))\n",
            "            except NVMLError as err:\n",
            "                bufferSize = handleError(err)\n",
            "            \n",
            "            strResult += '    <accounting_mode_buffer_size>' + bufferSize + '</accounting_mode_buffer_size>\\n'\n",
            "                \n",
            "            strResult += '    <driver_model>\\n'\n",
            "\n",
            "            try:\n",
            "                current = 'WDDM' if (nvmlDeviceGetCurrentDriverModel(handle) == NVML_DRIVER_WDDM) else 'TCC' \n",
            "            except NVMLError as err:\n",
            "                current = handleError(err)\n",
            "            strResult += '      <current_dm>' + current + '</current_dm>\\n'\n",
            "\n",
            "            try:\n",
            "                pending = 'WDDM' if (nvmlDeviceGetPendingDriverModel(handle) == NVML_DRIVER_WDDM) else 'TCC' \n",
            "            except NVMLError as err:\n",
            "                pending = handleError(err)\n",
            "\n",
            "            strResult += '      <pending_dm>' + pending + '</pending_dm>\\n'\n",
            "\n",
            "            strResult += '    </driver_model>\\n'\n",
            "\n",
            "            try:\n",
            "                serial = nvmlDeviceGetSerial(handle)\n",
            "            except NVMLError as err:\n",
            "                serial = handleError(err)\n",
            "\n",
            "            strResult += '    <serial>' + serial + '</serial>\\n'\n",
            "\n",
            "            try:\n",
            "                uuid = nvmlDeviceGetUUID(handle)\n",
            "            except NVMLError as err:\n",
            "                uuid = handleError(err)\n",
            "\n",
            "            strResult += '    <uuid>' + uuid + '</uuid>\\n'\n",
            "            \n",
            "            try:\n",
            "                minor_number = nvmlDeviceGetMinorNumber(handle)\n",
            "            except NVMLError as err:\n",
            "                minor_number = handleError(err)\n",
            "\n",
            "            strResult += '    <minor_number>' + str(minor_number) + '</minor_number>\\n'\n",
            "            \n",
            "            try:\n",
            "                vbios = nvmlDeviceGetVbiosVersion(handle)\n",
            "            except NVMLError as err:\n",
            "                vbios = handleError(err)\n",
            "\n",
            "            strResult += '    <vbios_version>' + vbios + '</vbios_version>\\n'\n",
            "\n",
            "            try:\n",
            "                multiGpuBool = nvmlDeviceGetMultiGpuBoard(handle)\n",
            "            except NVMLError as err:\n",
            "                multiGpuBool = handleError(err);\n",
            "\n",
            "            if multiGpuBool == \"N/A\":\n",
            "                strResult += '    <multigpu_board>' + 'N/A' + '</multigpu_board>\\n'\n",
            "            elif multiGpuBool:\n",
            "                strResult += '    <multigpu_board>' + 'Yes' + '</multigpu_board>\\n'\n",
            "            else:\n",
            "                strResult += '    <multigpu_board>' + 'No' + '</multigpu_board>\\n'\n",
            "\n",
            "            try:\n",
            "                boardId = nvmlDeviceGetBoardId(handle)\n",
            "            except NVMLError as err:\n",
            "                boardId = handleError(err)\n",
            "\n",
            "            try:\n",
            "                hexBID = \"0x%x\" % boardId\n",
            "            except: \n",
            "                hexBID = boardId\n",
            "\n",
            "            strResult += '    <board_id>' + hexBID + '</board_id>\\n'\n",
            "\n",
            "            strResult += '    <inforom_version>\\n'\n",
            "            \n",
            "            try:\n",
            "                img = nvmlDeviceGetInforomImageVersion(handle)\n",
            "            except NVMLError as err:\n",
            "                img = handleError(err)\n",
            "                \n",
            "            strResult += '      <img_version>' + img + '</img_version>\\n'\n",
            "\n",
            "            try:\n",
            "                oem = nvmlDeviceGetInforomVersion(handle, NVML_INFOROM_OEM)\n",
            "            except NVMLError as err:\n",
            "                oem = handleError(err)\n",
            "                \n",
            "            strResult += '      <oem_object>' + oem + '</oem_object>\\n'\n",
            "            \n",
            "            try:\n",
            "                ecc = nvmlDeviceGetInforomVersion(handle, NVML_INFOROM_ECC)\n",
            "            except NVMLError as err:\n",
            "                ecc = handleError(err)\n",
            "            \n",
            "            strResult += '      <ecc_object>' + ecc + '</ecc_object>\\n'\n",
            "\n",
            "            try:\n",
            "                pwr = nvmlDeviceGetInforomVersion(handle, NVML_INFOROM_POWER)\n",
            "            except NVMLError as err:\n",
            "                pwr = handleError(err)\n",
            "            \n",
            "            strResult += '      <pwr_object>' + pwr + '</pwr_object>\\n'\n",
            "                       \n",
            "            strResult += '    </inforom_version>\\n'\n",
            "\n",
            "            strResult += '    <gpu_operation_mode>\\n'\n",
            "\n",
            "            try:\n",
            "                current = StrGOM(nvmlDeviceGetCurrentGpuOperationMode(handle))\n",
            "            except NVMLError as err:\n",
            "                current = handleError(err)\n",
            "            strResult += '      <current_gom>' + current + '</current_gom>\\n'\n",
            "\n",
            "            try:\n",
            "                pending = StrGOM(nvmlDeviceGetPendingGpuOperationMode(handle))\n",
            "            except NVMLError as err:\n",
            "                pending = handleError(err)\n",
            "\n",
            "            strResult += '      <pending_gom>' + pending + '</pending_gom>\\n'\n",
            "\n",
            "            strResult += '    </gpu_operation_mode>\\n'\n",
            "\n",
            "            strResult += '    <pci>\\n'\n",
            "            strResult += '      <pci_bus>%02X</pci_bus>\\n' % pciInfo.bus\n",
            "            strResult += '      <pci_device>%02X</pci_device>\\n' % pciInfo.device\n",
            "            strResult += '      <pci_domain>%04X</pci_domain>\\n' % pciInfo.domain\n",
            "            strResult += '      <pci_device_id>%08X</pci_device_id>\\n' % (pciInfo.pciDeviceId)\n",
            "            strResult += '      <pci_bus_id>' + str(pciInfo.busId) + '</pci_bus_id>\\n'\n",
            "            strResult += '      <pci_sub_system_id>%08X</pci_sub_system_id>\\n' % (pciInfo.pciSubSystemId)\n",
            "            strResult += '      <pci_gpu_link_info>\\n'\n",
            "\n",
            "\n",
            "            strResult += '        <pcie_gen>\\n'\n",
            "\n",
            "            try:\n",
            "                gen = str(nvmlDeviceGetMaxPcieLinkGeneration(handle))\n",
            "            except NVMLError as err:\n",
            "                gen = handleError(err)\n",
            "\n",
            "            strResult += '          <max_link_gen>' + gen + '</max_link_gen>\\n'\n",
            "\n",
            "            try:\n",
            "                gen = str(nvmlDeviceGetCurrPcieLinkGeneration(handle))\n",
            "            except NVMLError as err:\n",
            "                gen = handleError(err)\n",
            "\n",
            "            strResult += '          <current_link_gen>' + gen + '</current_link_gen>\\n'\n",
            "            strResult += '        </pcie_gen>\\n'\n",
            "            strResult += '        <link_widths>\\n'\n",
            "\n",
            "            try:\n",
            "                width = str(nvmlDeviceGetMaxPcieLinkWidth(handle)) + 'x'\n",
            "            except NVMLError as err:\n",
            "                width = handleError(err)\n",
            "\n",
            "            strResult += '          <max_link_width>' + width + '</max_link_width>\\n'\n",
            "\n",
            "            try:\n",
            "                width = str(nvmlDeviceGetCurrPcieLinkWidth(handle)) + 'x'\n",
            "            except NVMLError as err:\n",
            "                width = handleError(err)\n",
            "\n",
            "            strResult += '          <current_link_width>' + width + '</current_link_width>\\n'\n",
            "\n",
            "            strResult += '        </link_widths>\\n'\n",
            "            strResult += '      </pci_gpu_link_info>\\n'\n",
            "\n",
            "\n",
            "            strResult += '      <pci_bridge_chip>\\n'\n",
            "\n",
            "            try:\n",
            "                bridgeHierarchy = nvmlDeviceGetBridgeChipInfo(handle)\n",
            "                bridge_type = ''\n",
            "                if bridgeHierarchy.bridgeChipInfo[0].type == 0:\n",
            "                    bridge_type += 'PLX'\n",
            "                else:\n",
            "                    bridge_type += 'BR04'                    \n",
            "                strResult += '        <bridge_chip_type>' + bridge_type + '</bridge_chip_type>\\n'\n",
            "\n",
            "                if bridgeHierarchy.bridgeChipInfo[0].fwVersion == 0:\n",
            "                    strFwVersion = 'N/A'\n",
            "                else:\n",
            "                    strFwVersion = '%08X' % (bridgeHierarchy.bridgeChipInfo[0].fwVersion)\n",
            "                strResult += '        <bridge_chip_fw>%s</bridge_chip_fw>\\n' % (strFwVersion)\n",
            "            except NVMLError as err:\n",
            "                strResult += '        <bridge_chip_type>' + handleError(err) + '</bridge_chip_type>\\n'\n",
            "                strResult += '        <bridge_chip_fw>' + handleError(err) + '</bridge_chip_fw>\\n'\n",
            "\n",
            "            # Add additional code for hierarchy of bridges for Bug # 1382323                \n",
            "            strResult += '      </pci_bridge_chip>\\n'\n",
            "\n",
            "            try:\n",
            "                replay = nvmlDeviceGetPcieReplayCounter(handle)\n",
            "                strResult += '      <replay_counter>' + str(replay) + '</replay_counter>'\n",
            "            except NVMLError as err:\n",
            "                strResult += '      <replay_counter>' + handleError(err) + '</replay_counter>'\n",
            "\n",
            "            try:\n",
            "                tx_bytes = nvmlDeviceGetPcieThroughput(handle, NVML_PCIE_UTIL_TX_BYTES)\n",
            "                strResult += '      <tx_util>' + str(tx_bytes) + ' KB/s' + '</tx_util>'\n",
            "            except NVMLError as err:\n",
            "                strResult += '      <tx_util>' + handleError(err) + '</tx_util>'\n",
            "\n",
            "            try:\n",
            "                rx_bytes = nvmlDeviceGetPcieThroughput(handle, NVML_PCIE_UTIL_RX_BYTES)\n",
            "                strResult += '      <rx_util>' + str(rx_bytes) + ' KB/s' + '</rx_util>'\n",
            "            except NVMLError as err:\n",
            "                strResult += '      <rx_util>' + handleError(err) + '</rx_util>'\n",
            "\n",
            "\n",
            "            strResult += '    </pci>\\n'\n",
            "\n",
            "            try:\n",
            "                fan = str(nvmlDeviceGetFanSpeed(handle)) + ' %'\n",
            "            except NVMLError as err:\n",
            "                fan = handleError(err)\n",
            "            strResult += '    <fan_speed>' + fan + '</fan_speed>\\n'\n",
            "\n",
            "            try:\n",
            "                perfState = nvmlDeviceGetPowerState(handle)\n",
            "                perfStateStr = 'P%s' % perfState\n",
            "            except NVMLError as err:\n",
            "                perfStateStr = handleError(err)\n",
            "            strResult += '    <performance_state>' + perfStateStr + '</performance_state>\\n'\n",
            "\n",
            "            strResult += GetClocksThrottleReasons(handle);\n",
            "\n",
            "            try:\n",
            "                memInfo = nvmlDeviceGetMemoryInfo(handle)\n",
            "                mem_total = str(memInfo.total / 1024 / 1024) + ' MiB'\n",
            "                mem_used = str(memInfo.used / 1024 / 1024) + ' MiB'\n",
            "                mem_free = str(memInfo.total / 1024 / 1024 - memInfo.used / 1024 / 1024) + ' MiB'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                mem_total = error\n",
            "                mem_used = error\n",
            "                mem_free = error\n",
            "\n",
            "            strResult += '    <fb_memory_usage>\\n'\n",
            "            strResult += '      <total>' + mem_total + '</total>\\n'\n",
            "            strResult += '      <used>' + mem_used + '</used>\\n'\n",
            "            strResult += '      <free>' + mem_free + '</free>\\n'\n",
            "            strResult += '    </fb_memory_usage>\\n'\n",
            "\n",
            "            try:\n",
            "                memInfo = nvmlDeviceGetBAR1MemoryInfo(handle)\n",
            "                mem_total = str(memInfo.bar1Total / 1024 / 1024) + ' MiB'\n",
            "                mem_used = str(memInfo.bar1Used / 1024 / 1024) + ' MiB'\n",
            "                mem_free = str(memInfo.bar1Total / 1024 / 1024 - memInfo.bar1Used / 1024 / 1024) + ' MiB'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                mem_total = error\n",
            "                mem_used = error\n",
            "                mem_free = error\n",
            "\n",
            "            strResult += '    <bar1_memory_usage>\\n'\n",
            "            strResult += '      <total>' + mem_total + '</total>\\n'\n",
            "            strResult += '      <used>' + mem_used + '</used>\\n'\n",
            "            strResult += '      <free>' + mem_free + '</free>\\n'\n",
            "            strResult += '    </bar1_memory_usage>\\n'\n",
            "            \n",
            "            try:\n",
            "                mode = nvmlDeviceGetComputeMode(handle)\n",
            "                if mode == NVML_COMPUTEMODE_DEFAULT:\n",
            "                    modeStr = 'Default'\n",
            "                elif mode == NVML_COMPUTEMODE_EXCLUSIVE_THREAD:\n",
            "                    modeStr = 'Exclusive Thread'\n",
            "                elif mode == NVML_COMPUTEMODE_PROHIBITED:\n",
            "                    modeStr = 'Prohibited'\n",
            "                elif mode == NVML_COMPUTEMODE_EXCLUSIVE_PROCESS:\n",
            "                    modeStr = 'Exclusive_Process'\n",
            "                else:\n",
            "                    modeStr = 'Unknown'\n",
            "            except NVMLError as err:\n",
            "                modeStr = handleError(err)\n",
            "\n",
            "            strResult += '    <compute_mode>' + modeStr + '</compute_mode>\\n'\n",
            "\n",
            "            try:\n",
            "                util = nvmlDeviceGetUtilizationRates(handle)\n",
            "                gpu_util = str(util.gpu) + ' %'\n",
            "                mem_util = str(util.memory) + ' %'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                gpu_util = error\n",
            "                mem_util = error\n",
            "            \n",
            "            strResult += '    <utilization>\\n'\n",
            "            strResult += '      <gpu_util>' + gpu_util + '</gpu_util>\\n'\n",
            "            strResult += '      <memory_util>' + mem_util + '</memory_util>\\n'\n",
            "\n",
            "            try:\n",
            "                (util_int, ssize) = nvmlDeviceGetEncoderUtilization(handle)\n",
            "                encoder_util = str(util_int) + ' %'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                encoder_util = error\n",
            "\n",
            "            strResult += '      <encoder_util>' + encoder_util + '</encoder_util>\\n'\n",
            "\n",
            "            try:\n",
            "                (util_int, ssize) = nvmlDeviceGetDecoderUtilization(handle)\n",
            "                decoder_util = str(util_int) + ' %'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                decoder_util = error\n",
            "\n",
            "            strResult += '      <decoder_util>' + decoder_util + '</decoder_util>\\n'\n",
            "\n",
            "            strResult += '    </utilization>\\n'\n",
            "            \n",
            "            try:\n",
            "                (current, pending) = nvmlDeviceGetEccMode(handle)\n",
            "                curr_str = 'Enabled' if (current != 0) else 'Disabled'\n",
            "                pend_str = 'Enabled' if (pending != 0) else 'Disabled'\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                curr_str = error\n",
            "                pend_str = error\n",
            "\n",
            "            strResult += '    <ecc_mode>\\n'\n",
            "            strResult += '      <current_ecc>' + curr_str + '</current_ecc>\\n'\n",
            "            strResult += '      <pending_ecc>' + pend_str + '</pending_ecc>\\n'\n",
            "            strResult += '    </ecc_mode>\\n'\n",
            "\n",
            "            strResult += '    <ecc_errors>\\n'\n",
            "            strResult += GetEccStr(handle)\n",
            "            strResult += '    </ecc_errors>\\n'\n",
            "\n",
            "            strResult += '    <retired_pages>\\n'\n",
            "            strResult += GetRetiredPagesStr(handle)\n",
            "            strResult += '    </retired_pages>\\n'\n",
            "            \n",
            "            try:\n",
            "                temp = str(nvmlDeviceGetTemperature(handle, NVML_TEMPERATURE_GPU)) + ' C'\n",
            "            except NVMLError as err:\n",
            "                temp = handleError(err)\n",
            "\n",
            "            strResult += '    <temperature>\\n'\n",
            "            strResult += '      <gpu_temp>' + temp + '</gpu_temp>\\n'\n",
            "\n",
            "            try:\n",
            "                temp = str(nvmlDeviceGetTemperatureThreshold(handle, NVML_TEMPERATURE_THRESHOLD_SHUTDOWN)) + ' C'\n",
            "            except NVMLError as err:\n",
            "                temp = handleError(err)\n",
            "\n",
            "            strResult += '      <gpu_temp_max_threshold>' + temp + '</gpu_temp_max_threshold>\\n'\n",
            "\n",
            "            try:\n",
            "                temp = str(nvmlDeviceGetTemperatureThreshold(handle, NVML_TEMPERATURE_THRESHOLD_SLOWDOWN)) + ' C'\n",
            "            except NVMLError as err:\n",
            "                temp = handleError(err)\n",
            "\n",
            "            strResult += '      <gpu_temp_slow_threshold>' + temp + '</gpu_temp_slow_threshold>\\n'\n",
            "            strResult += '    </temperature>\\n'\n",
            "\n",
            "            strResult += '    <power_readings>\\n'\n",
            "            try:\n",
            "                perfState = 'P' + str(nvmlDeviceGetPowerState(handle))\n",
            "            except NVMLError as err:\n",
            "                perfState = handleError(err)\n",
            "            strResult += '      <power_state>%s</power_state>\\n' % perfState\n",
            "            try:\n",
            "                powMan = nvmlDeviceGetPowerManagementMode(handle)\n",
            "                powManStr = 'Supported' if powMan != 0 else 'N/A'\n",
            "            except NVMLError as err:\n",
            "                powManStr = handleError(err)\n",
            "            strResult += '      <power_management>' + powManStr + '</power_management>\\n'\n",
            "            try:\n",
            "                powDraw = (nvmlDeviceGetPowerUsage(handle) / 1000.0)\n",
            "                powDrawStr = '%.2f W' % powDraw\n",
            "            except NVMLError as err:\n",
            "                powDrawStr = handleError(err)\n",
            "            strResult += '      <power_draw>' + powDrawStr + '</power_draw>\\n'\n",
            "            try:\n",
            "                powLimit = (nvmlDeviceGetPowerManagementLimit(handle) / 1000.0)\n",
            "                powLimitStr = '%.2f W' % powLimit\n",
            "            except NVMLError as err:\n",
            "                powLimitStr = handleError(err)\n",
            "            strResult += '      <power_limit>' + powLimitStr + '</power_limit>\\n'\n",
            "            try:\n",
            "                powLimit = (nvmlDeviceGetPowerManagementDefaultLimit(handle) / 1000.0)\n",
            "                powLimitStr = '%.2f W' % powLimit\n",
            "            except NVMLError as err:\n",
            "                powLimitStr = handleError(err)\n",
            "            strResult += '      <default_power_limit>' + powLimitStr + '</default_power_limit>\\n'\n",
            "\n",
            "            try:\n",
            "                enforcedPowLimit = (nvmlDeviceGetEnforcedPowerLimit(handle) / 1000.0)\n",
            "                enforcedPowLimitStr = '%.2f W' % enforcedPowLimit\n",
            "            except NVMLError as err:\n",
            "                enforcedPowLimitStr = handleError(err)\n",
            "\n",
            "            strResult += '      <enforced_power_limit>' + enforcedPowLimitStr + '</enforced_power_limit>\\n'\n",
            "\n",
            "            try:\n",
            "                powLimit = nvmlDeviceGetPowerManagementLimitConstraints(handle)\n",
            "                powLimitStrMin = '%.2f W' % (powLimit[0] / 1000.0)\n",
            "                powLimitStrMax = '%.2f W' % (powLimit[1] / 1000.0)\n",
            "            except NVMLError as err:\n",
            "                error = handleError(err)\n",
            "                powLimitStrMin = error\n",
            "                powLimitStrMax = error\n",
            "            strResult += '      <min_power_limit>' + powLimitStrMin + '</min_power_limit>\\n'\n",
            "            strResult += '      <max_power_limit>' + powLimitStrMax + '</max_power_limit>\\n'\n",
            "\n",
            "            strResult += '    </power_readings>\\n'\n",
            "\n",
            "            strResult += '    <clocks>\\n'\n",
            "            try:\n",
            "                graphics = str(nvmlDeviceGetClockInfo(handle, NVML_CLOCK_GRAPHICS)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                graphics = handleError(err)\n",
            "            strResult += '      <graphics_clock>' +graphics + '</graphics_clock>\\n'\n",
            "            try:\n",
            "                sm = str(nvmlDeviceGetClockInfo(handle, NVML_CLOCK_SM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                sm = handleError(err)\n",
            "            strResult += '      <sm_clock>' + sm + '</sm_clock>\\n'\n",
            "            try:\n",
            "                mem = str(nvmlDeviceGetClockInfo(handle, NVML_CLOCK_MEM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                mem = handleError(err)\n",
            "            strResult += '      <mem_clock>' + mem + '</mem_clock>\\n'\n",
            "            strResult += '    </clocks>\\n'\n",
            "\n",
            "            strResult += '    <applications_clocks>\\n'\n",
            "            try:\n",
            "                graphics = str(nvmlDeviceGetApplicationsClock(handle, NVML_CLOCK_GRAPHICS)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                graphics = handleError(err)\n",
            "            strResult += '      <graphics_clock>' +graphics + '</graphics_clock>\\n'\n",
            "            try:\n",
            "                mem = str(nvmlDeviceGetApplicationsClock(handle, NVML_CLOCK_MEM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                mem = handleError(err)\n",
            "            strResult += '      <mem_clock>' + mem + '</mem_clock>\\n'\n",
            "            strResult += '    </applications_clocks>\\n'\n",
            "            \n",
            "            strResult += '    <default_applications_clocks>\\n'\n",
            "            try:\n",
            "                graphics = str(nvmlDeviceGetDefaultApplicationsClock(handle, NVML_CLOCK_GRAPHICS)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                graphics = handleError(err)\n",
            "            strResult += '      <graphics_clock>' +graphics + '</graphics_clock>\\n'\n",
            "            try:\n",
            "                mem = str(nvmlDeviceGetDefaultApplicationsClock(handle, NVML_CLOCK_MEM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                mem = handleError(err)\n",
            "            strResult += '      <mem_clock>' + mem + '</mem_clock>\\n'\n",
            "            strResult += '    </default_applications_clocks>\\n'\n",
            "\n",
            "            strResult += '    <max_clocks>\\n'\n",
            "            try:\n",
            "                graphics = str(nvmlDeviceGetMaxClockInfo(handle, NVML_CLOCK_GRAPHICS)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                graphics = handleError(err)\n",
            "            strResult += '      <graphics_clock>' + graphics + '</graphics_clock>\\n'\n",
            "            try:\n",
            "                sm = str(nvmlDeviceGetMaxClockInfo(handle, NVML_CLOCK_SM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                sm = handleError(err)\n",
            "            strResult += '      <sm_clock>' + sm + '</sm_clock>\\n'\n",
            "            try:\n",
            "                mem = str(nvmlDeviceGetMaxClockInfo(handle, NVML_CLOCK_MEM)) + ' MHz'\n",
            "            except NVMLError as err:\n",
            "                mem = handleError(err)\n",
            "            strResult += '      <mem_clock>' + mem + '</mem_clock>\\n'\n",
            "            strResult += '    </max_clocks>\\n'\n",
            "            \n",
            "            strResult += '    <clock_policy>\\n'\n",
            "            try:\n",
            "                boostedState, boostedDefaultState = nvmlDeviceGetAutoBoostedClocksEnabled(handle)\n",
            "                if boostedState == NVML_FEATURE_DISABLED:\n",
            "                    autoBoostStr = \"Off\"\n",
            "                else:\n",
            "                    autoBoostStr = \"On\"\n",
            "                \n",
            "                if boostedDefaultState == NVML_FEATURE_DISABLED:\n",
            "                    autoBoostDefaultStr = \"Off\"\n",
            "                else:\n",
            "                    autoBoostDefaultStr = \"On\"\n",
            "                \n",
            "            except NVMLError_NotSupported:\n",
            "                autoBoostStr = \"N/A\"\n",
            "                autoBoostDefaultStr = \"N/A\"\n",
            "            except NVMLError as err:\n",
            "                autoBoostStr = handleError(err)\n",
            "                autoBoostDefaultStr = handleError(err)\n",
            "                pass\n",
            "            strResult += '      <auto_boost>' + autoBoostStr + '</auto_boost>\\n'\n",
            "            strResult += '      <auto_boost_default>' + autoBoostDefaultStr + '</auto_boost_default>\\n'\n",
            "            strResult += '    </clock_policy>\\n'\n",
            "\n",
            "            try:\n",
            "                memClocks = nvmlDeviceGetSupportedMemoryClocks(handle)\n",
            "                strResult += '    <supported_clocks>\\n'\n",
            "\n",
            "                for m in memClocks:\n",
            "                    strResult += '      <supported_mem_clock>\\n'\n",
            "                    strResult += '        <value>%d MHz</value>\\n' % m\n",
            "                    try:\n",
            "                        clocks = nvmlDeviceGetSupportedGraphicsClocks(handle, m)\n",
            "                        for c in clocks:\n",
            "                            strResult += '        <supported_graphics_clock>%d MHz</supported_graphics_clock>\\n' % c\n",
            "                    except NVMLError as err:\n",
            "                        strResult += '        <supported_graphics_clock>%s</supported_graphics_clock>\\n' % handleError(err)\n",
            "                    strResult += '      </supported_mem_clock>\\n'\n",
            "\n",
            "                strResult += '    </supported_clocks>\\n'\n",
            "            except NVMLError as err:\n",
            "                strResult += '    <supported_clocks>' + handleError(err) + '</supported_clocks>\\n'\n",
            "\n",
            "            try:\n",
            "                procs = nvmlDeviceGetComputeRunningProcesses(handle)\n",
            "                strResult += '    <processes>\\n'\n",
            "             \n",
            "                for p in procs:\n",
            "                    try:\n",
            "                        name = str(nvmlSystemGetProcessName(p.pid))\n",
            "                    except NVMLError as err:\n",
            "                        if (err.value == NVML_ERROR_NOT_FOUND):\n",
            "                            # probably went away\n",
            "                            continue\n",
            "                        else:\n",
            "                            name = handleError(err)\n",
            "                    \n",
            "                    strResult += '    <process_info>\\n'\n",
            "                    strResult += '      <pid>%d</pid>\\n' % p.pid\n",
            "                    strResult += '      <process_name>' + name + '</process_name>\\n'\n",
            "\n",
            "                    if (p.usedGpuMemory == None):\n",
            "                        mem = 'N\\A'\n",
            "                    else:\n",
            "                        mem = '%d MiB' % (p.usedGpuMemory / 1024 / 1024)\n",
            "                    strResult += '      <used_memory>' + mem + '</used_memory>\\n'\n",
            "                    strResult += '    </process_info>\\n'\n",
            "                \n",
            "                strResult += '    </processes>\\n'\n",
            "            except NVMLError as err:\n",
            "                strResult += '    <processes>' + handleError(err) + '</processes>\\n'\n",
            "            \n",
            "\n",
            "            try:\n",
            "                pids = nvmlDeviceGetAccountingPids(handle)\n",
            "                strResult += '    <accounted_processes>\\n'\n",
            "             \n",
            "                for pid in pids :\n",
            "                    try:\n",
            "                        stats = nvmlDeviceGetAccountingStats(handle, pid) \n",
            "                        gpuUtilization = \"%d %%\" % stats.gpuUtilization\n",
            "                        memoryUtilization = \"%d %%\" % stats.memoryUtilization\n",
            "                        if (stats.maxMemoryUsage == None):\n",
            "                            maxMemoryUsage = 'N\\A'\n",
            "                        else:\n",
            "                            maxMemoryUsage = '%d MiB' % (stats.maxMemoryUsage / 1024 / 1024)\n",
            "                        time = \"%d ms\" % stats.time\n",
            "                        is_running = \"%d\" % stats.isRunning\n",
            "                    except NVMLError as err:\n",
            "                        if (err.value == NVML_ERROR_NOT_FOUND):\n",
            "                            # probably went away\n",
            "                            continue\n",
            "                        err = handleError(err)\n",
            "                        gpuUtilization = err\n",
            "                        memoryUtilization = err\n",
            "                        maxMemoryUsage = err\n",
            "                        time = err\n",
            "                        is_running = err\n",
            "                    \n",
            "                    strResult += '    <accounted_process_info>\\n'\n",
            "                    strResult += '      <pid>%d</pid>\\n' % pid\n",
            "                    strResult += '      <gpu_util>' + gpuUtilization + '</gpu_util>\\n'\n",
            "                    strResult += '      <memory_util>' + memoryUtilization + '</memory_util>\\n'\n",
            "                    strResult += '      <max_memory_usage>' + maxMemoryUsage+ '</max_memory_usage>\\n'\n",
            "                    strResult += '      <time>' + time + '</time>\\n'\n",
            "                    strResult += '      <is_running>' + is_running + '</is_running>\\n'\n",
            "                    strResult += '    </accounted_process_info>\\n'\n",
            "                \n",
            "                strResult += '    </accounted_processes>\\n'\n",
            "            except NVMLError as err:\n",
            "                strResult += '    <accounted_processes>' + handleError(err) + '</accounted_processes>\\n'\n",
            "\n",
            "            strResult += '  </gpu>\\n'\n",
            "            \n",
            "        strResult += '</nvidia_smi_log>\\n'\n",
            "        \n",
            "    except NVMLError as err:\n",
            "        strResult += 'nvidia_smi.py: ' + err.__str__() + '\\n'\n",
            "    \n",
            "    nvmlShutdown()\n",
            "    \n",
            "    return strResult\n",
            "\n",
            "# this is not exectued when module is imported\n",
            "if __name__ == \"__main__\":\n",
            "    print(XmlDeviceQuery())\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# This is free and unencumbered software released into the public domain.\n",
            "#\n",
            "# Anyone is free to copy, modify, publish, use, compile, sell, or\n",
            "# distribute this software, either in source code form or as a compiled\n",
            "# binary, for any purpose, commercial or non-commercial, and by any\n",
            "# means.\n",
            "#\n",
            "# In jurisdictions that recognize copyright laws, the author or authors\n",
            "# of this software dedicate any and all copyright interest in the\n",
            "# software to the public domain. We make this dedication for the benefit\n",
            "# of the public at large and to the detriment of our heirs and\n",
            "# successors. We intend this dedication to be an overt act of\n",
            "# relinquishment in perpetuity of all present and future rights to this\n",
            "# software under copyright law.\n",
            "#\n",
            "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
            "# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
            "# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
            "# IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\n",
            "# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n",
            "# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n",
            "# OTHER DEALINGS IN THE SOFTWARE.\n",
            "#\n",
            "# For more information, please refer to <http://unlicense.org>\n",
            "\n",
            "\"\"\"\n",
            "A platform independent file lock that supports the with-statement.\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# Modules\n",
            "# ------------------------------------------------\n",
            "import logging\n",
            "import os\n",
            "import threading\n",
            "import time\n",
            "try:\n",
            "    import warnings\n",
            "except ImportError:\n",
            "    warnings = None\n",
            "\n",
            "try:\n",
            "    import msvcrt\n",
            "except ImportError:\n",
            "    msvcrt = None\n",
            "\n",
            "try:\n",
            "    import fcntl\n",
            "except ImportError:\n",
            "    fcntl = None\n",
            "\n",
            "\n",
            "# Backward compatibility\n",
            "# ------------------------------------------------\n",
            "try:\n",
            "    TimeoutError\n",
            "except NameError:\n",
            "    TimeoutError = OSError\n",
            "\n",
            "\n",
            "# Data\n",
            "# ------------------------------------------------\n",
            "__all__ = [\n",
            "    \"Timeout\",\n",
            "    \"BaseFileLock\",\n",
            "    \"WindowsFileLock\",\n",
            "    \"UnixFileLock\",\n",
            "    \"SoftFileLock\",\n",
            "    \"FileLock\"\n",
            "]\n",
            "\n",
            "__version__ = \"3.0.12\"\n",
            "\n",
            "\n",
            "_logger = None\n",
            "def logger():\n",
            "    \"\"\"Returns the logger instance used in this module.\"\"\"\n",
            "    global _logger\n",
            "    _logger = _logger or logging.getLogger(__name__)\n",
            "    return _logger\n",
            "\n",
            "\n",
            "# Exceptions\n",
            "# ------------------------------------------------\n",
            "class Timeout(TimeoutError):\n",
            "    \"\"\"\n",
            "    Raised when the lock could not be acquired in *timeout*\n",
            "    seconds.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, lock_file):\n",
            "        \"\"\"\n",
            "        \"\"\"\n",
            "        #: The path of the file lock.\n",
            "        self.lock_file = lock_file\n",
            "        return None\n",
            "\n",
            "    def __str__(self):\n",
            "        temp = \"The file lock '{}' could not be acquired.\"\\\n",
            "               .format(self.lock_file)\n",
            "        return temp\n",
            "\n",
            "\n",
            "# Classes\n",
            "# ------------------------------------------------\n",
            "\n",
            "# This is a helper class which is returned by :meth:`BaseFileLock.acquire`\n",
            "# and wraps the lock to make sure __enter__ is not called twice when entering\n",
            "# the with statement.\n",
            "# If we would simply return *self*, the lock would be acquired again\n",
            "# in the *__enter__* method of the BaseFileLock, but not released again\n",
            "# automatically.\n",
            "#\n",
            "# :seealso: issue #37 (memory leak)\n",
            "class _Acquire_ReturnProxy(object):\n",
            "\n",
            "    def __init__(self, lock):\n",
            "        self.lock = lock\n",
            "        return None\n",
            "\n",
            "    def __enter__(self):\n",
            "        return self.lock\n",
            "\n",
            "    def __exit__(self, exc_type, exc_value, traceback):\n",
            "        self.lock.release()\n",
            "        return None\n",
            "\n",
            "\n",
            "class BaseFileLock(object):\n",
            "    \"\"\"\n",
            "    Implements the base class of a file lock.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, lock_file, timeout = -1):\n",
            "        \"\"\"\n",
            "        \"\"\"\n",
            "        # The path to the lock file.\n",
            "        self._lock_file = lock_file\n",
            "\n",
            "        # The file descriptor for the *_lock_file* as it is returned by the\n",
            "        # os.open() function.\n",
            "        # This file lock is only NOT None, if the object currently holds the\n",
            "        # lock.\n",
            "        self._lock_file_fd = None\n",
            "\n",
            "        # The default timeout value.\n",
            "        self.timeout = timeout\n",
            "\n",
            "        # We use this lock primarily for the lock counter.\n",
            "        self._thread_lock = threading.Lock()\n",
            "\n",
            "        # The lock counter is used for implementing the nested locking\n",
            "        # mechanism. Whenever the lock is acquired, the counter is increased and\n",
            "        # the lock is only released, when this value is 0 again.\n",
            "        self._lock_counter = 0\n",
            "        return None\n",
            "\n",
            "    @property\n",
            "    def lock_file(self):\n",
            "        \"\"\"\n",
            "        The path to the lock file.\n",
            "        \"\"\"\n",
            "        return self._lock_file\n",
            "\n",
            "    @property\n",
            "    def timeout(self):\n",
            "        \"\"\"\n",
            "        You can set a default timeout for the filelock. It will be used as\n",
            "        fallback value in the acquire method, if no timeout value (*None*) is\n",
            "        given.\n",
            "\n",
            "        If you want to disable the timeout, set it to a negative value.\n",
            "\n",
            "        A timeout of 0 means, that there is exactly one attempt to acquire the\n",
            "        file lock.\n",
            "\n",
            "        .. versionadded:: 2.0.0\n",
            "        \"\"\"\n",
            "        return self._timeout\n",
            "\n",
            "    @timeout.setter\n",
            "    def timeout(self, value):\n",
            "        \"\"\"\n",
            "        \"\"\"\n",
            "        self._timeout = float(value)\n",
            "        return None\n",
            "\n",
            "    # Platform dependent locking\n",
            "    # --------------------------------------------\n",
            "\n",
            "    def _acquire(self):\n",
            "        \"\"\"\n",
            "        Platform dependent. If the file lock could be\n",
            "        acquired, self._lock_file_fd holds the file descriptor\n",
            "        of the lock file.\n",
            "        \"\"\"\n",
            "        raise NotImplementedError()\n",
            "\n",
            "    def _release(self):\n",
            "        \"\"\"\n",
            "        Releases the lock and sets self._lock_file_fd to None.\n",
            "        \"\"\"\n",
            "        raise NotImplementedError()\n",
            "\n",
            "    # Platform independent methods\n",
            "    # --------------------------------------------\n",
            "\n",
            "    @property\n",
            "    def is_locked(self):\n",
            "        \"\"\"\n",
            "        True, if the object holds the file lock.\n",
            "\n",
            "        .. versionchanged:: 2.0.0\n",
            "\n",
            "            This was previously a method and is now a property.\n",
            "        \"\"\"\n",
            "        return self._lock_file_fd is not None\n",
            "\n",
            "    def acquire(self, timeout=None, poll_intervall=0.05):\n",
            "        \"\"\"\n",
            "        Acquires the file lock or fails with a :exc:`Timeout` error.\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # You can use this method in the context manager (recommended)\n",
            "            with lock.acquire():\n",
            "                pass\n",
            "\n",
            "            # Or use an equivalent try-finally construct:\n",
            "            lock.acquire()\n",
            "            try:\n",
            "                pass\n",
            "            finally:\n",
            "                lock.release()\n",
            "\n",
            "        :arg float timeout:\n",
            "            The maximum time waited for the file lock.\n",
            "            If ``timeout < 0``, there is no timeout and this method will\n",
            "            block until the lock could be acquired.\n",
            "            If ``timeout`` is None, the default :attr:`~timeout` is used.\n",
            "\n",
            "        :arg float poll_intervall:\n",
            "            We check once in *poll_intervall* seconds if we can acquire the\n",
            "            file lock.\n",
            "\n",
            "        :raises Timeout:\n",
            "            if the lock could not be acquired in *timeout* seconds.\n",
            "\n",
            "        .. versionchanged:: 2.0.0\n",
            "\n",
            "            This method returns now a *proxy* object instead of *self*,\n",
            "            so that it can be used in a with statement without side effects.\n",
            "        \"\"\"\n",
            "        # Use the default timeout, if no timeout is provided.\n",
            "        if timeout is None:\n",
            "            timeout = self.timeout\n",
            "\n",
            "        # Increment the number right at the beginning.\n",
            "        # We can still undo it, if something fails.\n",
            "        with self._thread_lock:\n",
            "            self._lock_counter += 1\n",
            "\n",
            "        lock_id = id(self)\n",
            "        lock_filename = self._lock_file\n",
            "        start_time = time.time()\n",
            "        try:\n",
            "            while True:\n",
            "                with self._thread_lock:\n",
            "                    if not self.is_locked:\n",
            "                        logger().debug('Attempting to acquire lock %s on %s', lock_id, lock_filename)\n",
            "                        self._acquire()\n",
            "\n",
            "                if self.is_locked:\n",
            "                    logger().info('Lock %s acquired on %s', lock_id, lock_filename)\n",
            "                    break\n",
            "                elif timeout >= 0 and time.time() - start_time > timeout:\n",
            "                    logger().debug('Timeout on acquiring lock %s on %s', lock_id, lock_filename)\n",
            "                    raise Timeout(self._lock_file)\n",
            "                else:\n",
            "                    logger().debug(\n",
            "                        'Lock %s not acquired on %s, waiting %s seconds ...',\n",
            "                        lock_id, lock_filename, poll_intervall\n",
            "                    )\n",
            "                    time.sleep(poll_intervall)\n",
            "        except:\n",
            "            # Something did go wrong, so decrement the counter.\n",
            "            with self._thread_lock:\n",
            "                self._lock_counter = max(0, self._lock_counter - 1)\n",
            "\n",
            "            raise\n",
            "        return _Acquire_ReturnProxy(lock = self)\n",
            "\n",
            "    def release(self, force = False):\n",
            "        \"\"\"\n",
            "        Releases the file lock.\n",
            "\n",
            "        Please note, that the lock is only completly released, if the lock\n",
            "        counter is 0.\n",
            "\n",
            "        Also note, that the lock file itself is not automatically deleted.\n",
            "\n",
            "        :arg bool force:\n",
            "            If true, the lock counter is ignored and the lock is released in\n",
            "            every case.\n",
            "        \"\"\"\n",
            "        with self._thread_lock:\n",
            "\n",
            "            if self.is_locked:\n",
            "                self._lock_counter -= 1\n",
            "\n",
            "                if self._lock_counter == 0 or force:\n",
            "                    lock_id = id(self)\n",
            "                    lock_filename = self._lock_file\n",
            "\n",
            "                    logger().debug('Attempting to release lock %s on %s', lock_id, lock_filename)\n",
            "                    self._release()\n",
            "                    self._lock_counter = 0\n",
            "                    logger().info('Lock %s released on %s', lock_id, lock_filename)\n",
            "\n",
            "        return None\n",
            "\n",
            "    def __enter__(self):\n",
            "        self.acquire()\n",
            "        return self\n",
            "\n",
            "    def __exit__(self, exc_type, exc_value, traceback):\n",
            "        self.release()\n",
            "        return None\n",
            "\n",
            "    def __del__(self):\n",
            "        self.release(force = True)\n",
            "        return None\n",
            "\n",
            "\n",
            "# Windows locking mechanism\n",
            "# ~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "class WindowsFileLock(BaseFileLock):\n",
            "    \"\"\"\n",
            "    Uses the :func:`msvcrt.locking` function to hard lock the lock file on\n",
            "    windows systems.\n",
            "    \"\"\"\n",
            "\n",
            "    def _acquire(self):\n",
            "        open_mode = os.O_RDWR | os.O_CREAT | os.O_TRUNC\n",
            "\n",
            "        try:\n",
            "            fd = os.open(self._lock_file, open_mode)\n",
            "        except OSError:\n",
            "            pass\n",
            "        else:\n",
            "            try:\n",
            "                msvcrt.locking(fd, msvcrt.LK_NBLCK, 1)\n",
            "            except (IOError, OSError):\n",
            "                os.close(fd)\n",
            "            else:\n",
            "                self._lock_file_fd = fd\n",
            "        return None\n",
            "\n",
            "    def _release(self):\n",
            "        fd = self._lock_file_fd\n",
            "        self._lock_file_fd = None\n",
            "        msvcrt.locking(fd, msvcrt.LK_UNLCK, 1)\n",
            "        os.close(fd)\n",
            "\n",
            "        try:\n",
            "            os.remove(self._lock_file)\n",
            "        # Probably another instance of the application\n",
            "        # that acquired the file lock.\n",
            "        except OSError:\n",
            "            pass\n",
            "        return None\n",
            "\n",
            "# Unix locking mechanism\n",
            "# ~~~~~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "class UnixFileLock(BaseFileLock):\n",
            "    \"\"\"\n",
            "    Uses the :func:`fcntl.flock` to hard lock the lock file on unix systems.\n",
            "    \"\"\"\n",
            "\n",
            "    def _acquire(self):\n",
            "        open_mode = os.O_RDWR | os.O_CREAT | os.O_TRUNC\n",
            "        fd = os.open(self._lock_file, open_mode)\n",
            "\n",
            "        try:\n",
            "            fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n",
            "        except (IOError, OSError):\n",
            "            os.close(fd)\n",
            "        else:\n",
            "            self._lock_file_fd = fd\n",
            "        return None\n",
            "\n",
            "    def _release(self):\n",
            "        # Do not remove the lockfile:\n",
            "        #\n",
            "        #   https://github.com/benediktschmitt/py-filelock/issues/31\n",
            "        #   https://stackoverflow.com/questions/17708885/flock-removing-locked-file-without-race-condition\n",
            "        fd = self._lock_file_fd\n",
            "        self._lock_file_fd = None\n",
            "        fcntl.flock(fd, fcntl.LOCK_UN)\n",
            "        os.close(fd)\n",
            "        return None\n",
            "\n",
            "# Soft lock\n",
            "# ~~~~~~~~~\n",
            "\n",
            "class SoftFileLock(BaseFileLock):\n",
            "    \"\"\"\n",
            "    Simply watches the existence of the lock file.\n",
            "    \"\"\"\n",
            "\n",
            "    def _acquire(self):\n",
            "        open_mode = os.O_WRONLY | os.O_CREAT | os.O_EXCL | os.O_TRUNC\n",
            "        try:\n",
            "            fd = os.open(self._lock_file, open_mode)\n",
            "        except (IOError, OSError):\n",
            "            pass\n",
            "        else:\n",
            "            self._lock_file_fd = fd\n",
            "        return None\n",
            "\n",
            "    def _release(self):\n",
            "        os.close(self._lock_file_fd)\n",
            "        self._lock_file_fd = None\n",
            "\n",
            "        try:\n",
            "            os.remove(self._lock_file)\n",
            "        # The file is already deleted and that's what we want.\n",
            "        except OSError:\n",
            "            pass\n",
            "        return None\n",
            "\n",
            "\n",
            "# Platform filelock\n",
            "# ~~~~~~~~~~~~~~~~~\n",
            "\n",
            "#: Alias for the lock, which should be used for the current platform. On\n",
            "#: Windows, this is an alias for :class:`WindowsFileLock`, on Unix for\n",
            "#: :class:`UnixFileLock` and otherwise for :class:`SoftFileLock`.\n",
            "FileLock = None\n",
            "\n",
            "if msvcrt:\n",
            "    FileLock = WindowsFileLock\n",
            "elif fcntl:\n",
            "    FileLock = UnixFileLock\n",
            "else:\n",
            "    FileLock = SoftFileLock\n",
            "\n",
            "    if warnings is not None:\n",
            "        warnings.warn(\"only soft file lock is available\")\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "from matplotlib.pylab import *\n",
            "import matplotlib.pylab\n",
            "__doc__ = matplotlib.pylab.__doc__\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "from __future__ import print_function\n",
            "import os\n",
            "import sys\n",
            "import Queue\n",
            "import plac_core\n",
            "from Tkinter import Tk\n",
            "from ScrolledText import ScrolledText\n",
            "from plac_ext import Monitor, TerminatedProcess\n",
            "\n",
            "\n",
            "class TkMonitor(Monitor):\n",
            "    \"\"\"\n",
            "    An interface over a dictionary {taskno: scrolledtext widget}, with\n",
            "    methods add_listener, del_listener, notify_listener and start/stop.\n",
            "    \"\"\"\n",
            "    def __init__(self, name, queue=None):\n",
            "        Monitor.__init__(self, name, queue)\n",
            "        self.widgets = {}\n",
            "\n",
            "    @plac_core.annotations(taskno=('task number', 'positional', None, int))\n",
            "    def add_listener(self, taskno):\n",
            "        \"There is a ScrolledText for each task\"\n",
            "        st = ScrolledText(self.root, height=5)\n",
            "        st.insert('end', 'Output of task %d\\n' % taskno)\n",
            "        st.pack()\n",
            "        self.widgets[taskno] = st\n",
            "\n",
            "    @plac_core.annotations(taskno=('task number', 'positional', None, int))\n",
            "    def del_listener(self, taskno):\n",
            "        del self.widgets[taskno]\n",
            "\n",
            "    @plac_core.annotations(taskno=('task number', 'positional', None, int))\n",
            "    def notify_listener(self, taskno, msg):\n",
            "        w = self.widgets[taskno]\n",
            "        w.insert('end', msg + '\\n')\n",
            "        w.update()\n",
            "\n",
            "    def start(self):\n",
            "        'Start the mainloop'\n",
            "        self.root = Tk()\n",
            "        self.root.title(self.name)\n",
            "        self.root.wm_protocol(\"WM_DELETE_WINDOW\", self.stop)\n",
            "        self.root.after(0, self.read_queue)\n",
            "        try:\n",
            "            self.root.mainloop()\n",
            "        except KeyboardInterrupt:\n",
            "            print('Process %d killed by CTRL-C' % os.getpid(), file=sys.stderr)\n",
            "        except TerminatedProcess:\n",
            "            pass\n",
            "\n",
            "    def stop(self):\n",
            "        self.root.quit()\n",
            "\n",
            "    def read_queue(self):\n",
            "        try:\n",
            "            cmd_args = self.queue.get_nowait()\n",
            "        except Queue.Empty:\n",
            "            pass\n",
            "        else:\n",
            "            getattr(self, cmd_args[0])(*cmd_args[1:])\n",
            "        self.root.after(100, self.read_queue)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# coding: utf-8\n",
            "# Copyright (c) 2008-2011 Volvox Development Team\n",
            "#\n",
            "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "# of this software and associated documentation files (the \"Software\"), to deal\n",
            "# in the Software without restriction, including without limitation the rights\n",
            "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "# copies of the Software, and to permit persons to whom the Software is\n",
            "# furnished to do so, subject to the following conditions:\n",
            "#\n",
            "# The above copyright notice and this permission notice shall be included in\n",
            "# all copies or substantial portions of the Software.\n",
            "#\n",
            "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
            "# THE SOFTWARE.\n",
            "#\n",
            "# Author: Konstantin Lepa <konstantin.lepa@gmail.com>\n",
            "\n",
            "\"\"\"ANSII Color formatting for output in terminal.\"\"\"\n",
            "\n",
            "from __future__ import print_function\n",
            "import os\n",
            "\n",
            "\n",
            "__ALL__ = [ 'colored', 'cprint' ]\n",
            "\n",
            "VERSION = (1, 1, 0)\n",
            "\n",
            "ATTRIBUTES = dict(\n",
            "        list(zip([\n",
            "            'bold',\n",
            "            'dark',\n",
            "            '',\n",
            "            'underline',\n",
            "            'blink',\n",
            "            '',\n",
            "            'reverse',\n",
            "            'concealed'\n",
            "            ],\n",
            "            list(range(1, 9))\n",
            "            ))\n",
            "        )\n",
            "del ATTRIBUTES['']\n",
            "\n",
            "\n",
            "HIGHLIGHTS = dict(\n",
            "        list(zip([\n",
            "            'on_grey',\n",
            "            'on_red',\n",
            "            'on_green',\n",
            "            'on_yellow',\n",
            "            'on_blue',\n",
            "            'on_magenta',\n",
            "            'on_cyan',\n",
            "            'on_white'\n",
            "            ],\n",
            "            list(range(40, 48))\n",
            "            ))\n",
            "        )\n",
            "\n",
            "\n",
            "COLORS = dict(\n",
            "        list(zip([\n",
            "            'grey',\n",
            "            'red',\n",
            "            'green',\n",
            "            'yellow',\n",
            "            'blue',\n",
            "            'magenta',\n",
            "            'cyan',\n",
            "            'white',\n",
            "            ],\n",
            "            list(range(30, 38))\n",
            "            ))\n",
            "        )\n",
            "\n",
            "\n",
            "RESET = '\\033[0m'\n",
            "\n",
            "\n",
            "def colored(text, color=None, on_color=None, attrs=None):\n",
            "    \"\"\"Colorize text.\n",
            "\n",
            "    Available text colors:\n",
            "        red, green, yellow, blue, magenta, cyan, white.\n",
            "\n",
            "    Available text highlights:\n",
            "        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.\n",
            "\n",
            "    Available attributes:\n",
            "        bold, dark, underline, blink, reverse, concealed.\n",
            "\n",
            "    Example:\n",
            "        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])\n",
            "        colored('Hello, World!', 'green')\n",
            "    \"\"\"\n",
            "    if os.getenv('ANSI_COLORS_DISABLED') is None:\n",
            "        fmt_str = '\\033[%dm%s'\n",
            "        if color is not None:\n",
            "            text = fmt_str % (COLORS[color], text)\n",
            "\n",
            "        if on_color is not None:\n",
            "            text = fmt_str % (HIGHLIGHTS[on_color], text)\n",
            "\n",
            "        if attrs is not None:\n",
            "            for attr in attrs:\n",
            "                text = fmt_str % (ATTRIBUTES[attr], text)\n",
            "\n",
            "        text += RESET\n",
            "    return text\n",
            "\n",
            "\n",
            "def cprint(text, color=None, on_color=None, attrs=None, **kwargs):\n",
            "    \"\"\"Print colorize text.\n",
            "\n",
            "    It accepts arguments of print function.\n",
            "    \"\"\"\n",
            "\n",
            "    print((colored(text, color, on_color, attrs)), **kwargs)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    print('Current terminal type: %s' % os.getenv('TERM'))\n",
            "    print('Test basic colors:')\n",
            "    cprint('Grey color', 'grey')\n",
            "    cprint('Red color', 'red')\n",
            "    cprint('Green color', 'green')\n",
            "    cprint('Yellow color', 'yellow')\n",
            "    cprint('Blue color', 'blue')\n",
            "    cprint('Magenta color', 'magenta')\n",
            "    cprint('Cyan color', 'cyan')\n",
            "    cprint('White color', 'white')\n",
            "    print(('-' * 78))\n",
            "\n",
            "    print('Test highlights:')\n",
            "    cprint('On grey color', on_color='on_grey')\n",
            "    cprint('On red color', on_color='on_red')\n",
            "    cprint('On green color', on_color='on_green')\n",
            "    cprint('On yellow color', on_color='on_yellow')\n",
            "    cprint('On blue color', on_color='on_blue')\n",
            "    cprint('On magenta color', on_color='on_magenta')\n",
            "    cprint('On cyan color', on_color='on_cyan')\n",
            "    cprint('On white color', color='grey', on_color='on_white')\n",
            "    print('-' * 78)\n",
            "\n",
            "    print('Test attributes:')\n",
            "    cprint('Bold grey color', 'grey', attrs=['bold'])\n",
            "    cprint('Dark red color', 'red', attrs=['dark'])\n",
            "    cprint('Underline green color', 'green', attrs=['underline'])\n",
            "    cprint('Blink yellow color', 'yellow', attrs=['blink'])\n",
            "    cprint('Reversed blue color', 'blue', attrs=['reverse'])\n",
            "    cprint('Concealed Magenta color', 'magenta', attrs=['concealed'])\n",
            "    cprint('Bold underline reverse cyan color', 'cyan',\n",
            "            attrs=['bold', 'underline', 'reverse'])\n",
            "    cprint('Dark blink concealed white color', 'white',\n",
            "            attrs=['dark', 'blink', 'concealed'])\n",
            "    print(('-' * 78))\n",
            "\n",
            "    print('Test mixing:')\n",
            "    cprint('Underline red on grey color', 'red', 'on_grey',\n",
            "            ['underline'])\n",
            "    cprint('Reversed green on red color', 'green', 'on_red', ['reverse'])\n",
            "\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#####\n",
            "# Copyright (c) 2011-2015, NVIDIA Corporation.  All rights reserved.\n",
            "#\n",
            "# Redistribution and use in source and binary forms, with or without\n",
            "# modification, are permitted provided that the following conditions are met:\n",
            "#\n",
            "#    * Redistributions of source code must retain the above copyright notice,\n",
            "#      this list of conditions and the following disclaimer.\n",
            "#    * Redistributions in binary form must reproduce the above copyright\n",
            "#      notice, this list of conditions and the following disclaimer in the\n",
            "#      documentation and/or other materials provided with the distribution.\n",
            "#    * Neither the name of the NVIDIA Corporation nor the names of its\n",
            "#      contributors may be used to endorse or promote products derived from\n",
            "#      this software without specific prior written permission.\n",
            "#\n",
            "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
            "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n",
            "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
            "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
            "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
            "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
            "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
            "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n",
            "# THE POSSIBILITY OF SUCH DAMAGE.\n",
            "#####\n",
            "\n",
            "##\n",
            "# Python bindings for the NVML library\n",
            "##\n",
            "from ctypes import *\n",
            "from ctypes.util import find_library\n",
            "import sys\n",
            "import os\n",
            "import threading\n",
            "import string\n",
            "\n",
            "## C Type mappings ##\n",
            "## Enums\n",
            "_nvmlEnableState_t = c_uint\n",
            "NVML_FEATURE_DISABLED    = 0\n",
            "NVML_FEATURE_ENABLED     = 1\n",
            "\n",
            "_nvmlBrandType_t = c_uint\n",
            "NVML_BRAND_UNKNOWN = 0\n",
            "NVML_BRAND_QUADRO  = 1\n",
            "NVML_BRAND_TESLA   = 2\n",
            "NVML_BRAND_NVS     = 3\n",
            "NVML_BRAND_GRID    = 4\n",
            "NVML_BRAND_GEFORCE = 5\n",
            "NVML_BRAND_COUNT   = 6\n",
            "\n",
            "_nvmlTemperatureThresholds_t = c_uint\n",
            "NVML_TEMPERATURE_THRESHOLD_SHUTDOWN = 0\n",
            "NVML_TEMPERATURE_THRESHOLD_SLOWDOWN = 1\n",
            "NVML_TEMPERATURE_THRESHOLD_COUNT = 1\n",
            "\n",
            "_nvmlTemperatureSensors_t = c_uint\n",
            "NVML_TEMPERATURE_GPU     = 0\n",
            "NVML_TEMPERATURE_COUNT   = 1\n",
            "\n",
            "_nvmlComputeMode_t = c_uint\n",
            "NVML_COMPUTEMODE_DEFAULT           = 0\n",
            "NVML_COMPUTEMODE_EXCLUSIVE_THREAD  = 1\n",
            "NVML_COMPUTEMODE_PROHIBITED        = 2\n",
            "NVML_COMPUTEMODE_EXCLUSIVE_PROCESS = 3\n",
            "NVML_COMPUTEMODE_COUNT             = 4\n",
            "\n",
            "_nvmlMemoryLocation_t = c_uint\n",
            "NVML_MEMORY_LOCATION_L1_CACHE = 0\n",
            "NVML_MEMORY_LOCATION_L2_CACHE = 1\n",
            "NVML_MEMORY_LOCATION_DEVICE_MEMORY = 2\n",
            "NVML_MEMORY_LOCATION_REGISTER_FILE = 3\n",
            "NVML_MEMORY_LOCATION_TEXTURE_MEMORY = 4\n",
            "NVML_MEMORY_LOCATION_COUNT = 5\n",
            "\n",
            "# These are deprecated, instead use _nvmlMemoryErrorType_t\n",
            "_nvmlEccBitType_t = c_uint\n",
            "NVML_SINGLE_BIT_ECC    = 0\n",
            "NVML_DOUBLE_BIT_ECC    = 1\n",
            "NVML_ECC_ERROR_TYPE_COUNT = 2\n",
            "\n",
            "_nvmlEccCounterType_t = c_uint\n",
            "NVML_VOLATILE_ECC      = 0\n",
            "NVML_AGGREGATE_ECC     = 1\n",
            "NVML_ECC_COUNTER_TYPE_COUNT = 2\n",
            "\n",
            "_nvmlMemoryErrorType_t = c_uint\n",
            "NVML_MEMORY_ERROR_TYPE_CORRECTED   = 0\n",
            "NVML_MEMORY_ERROR_TYPE_UNCORRECTED = 1\n",
            "NVML_MEMORY_ERROR_TYPE_COUNT       = 2\n",
            "\n",
            "_nvmlClockType_t = c_uint\n",
            "NVML_CLOCK_GRAPHICS  = 0\n",
            "NVML_CLOCK_SM        = 1\n",
            "NVML_CLOCK_MEM       = 2\n",
            "NVML_CLOCK_COUNT     = 3\n",
            "\n",
            "_nvmlDriverModel_t = c_uint\n",
            "NVML_DRIVER_WDDM       = 0\n",
            "NVML_DRIVER_WDM        = 1\n",
            "\n",
            "_nvmlPstates_t = c_uint\n",
            "NVML_PSTATE_0               = 0\n",
            "NVML_PSTATE_1               = 1\n",
            "NVML_PSTATE_2               = 2\n",
            "NVML_PSTATE_3               = 3\n",
            "NVML_PSTATE_4               = 4\n",
            "NVML_PSTATE_5               = 5\n",
            "NVML_PSTATE_6               = 6\n",
            "NVML_PSTATE_7               = 7\n",
            "NVML_PSTATE_8               = 8\n",
            "NVML_PSTATE_9               = 9\n",
            "NVML_PSTATE_10              = 10\n",
            "NVML_PSTATE_11              = 11\n",
            "NVML_PSTATE_12              = 12\n",
            "NVML_PSTATE_13              = 13\n",
            "NVML_PSTATE_14              = 14\n",
            "NVML_PSTATE_15              = 15\n",
            "NVML_PSTATE_UNKNOWN         = 32\n",
            "\n",
            "_nvmlInforomObject_t = c_uint\n",
            "NVML_INFOROM_OEM            = 0\n",
            "NVML_INFOROM_ECC            = 1\n",
            "NVML_INFOROM_POWER          = 2\n",
            "NVML_INFOROM_COUNT          = 3\n",
            "\n",
            "_nvmlReturn_t = c_uint\n",
            "NVML_SUCCESS                   = 0\n",
            "NVML_ERROR_UNINITIALIZED       = 1\n",
            "NVML_ERROR_INVALID_ARGUMENT    = 2\n",
            "NVML_ERROR_NOT_SUPPORTED       = 3\n",
            "NVML_ERROR_NO_PERMISSION       = 4\n",
            "NVML_ERROR_ALREADY_INITIALIZED = 5\n",
            "NVML_ERROR_NOT_FOUND           = 6\n",
            "NVML_ERROR_INSUFFICIENT_SIZE   = 7\n",
            "NVML_ERROR_INSUFFICIENT_POWER  = 8\n",
            "NVML_ERROR_DRIVER_NOT_LOADED   = 9\n",
            "NVML_ERROR_TIMEOUT             = 10\n",
            "NVML_ERROR_IRQ_ISSUE           = 11\n",
            "NVML_ERROR_LIBRARY_NOT_FOUND   = 12\n",
            "NVML_ERROR_FUNCTION_NOT_FOUND  = 13\n",
            "NVML_ERROR_CORRUPTED_INFOROM   = 14\n",
            "NVML_ERROR_GPU_IS_LOST         = 15\n",
            "NVML_ERROR_RESET_REQUIRED      = 16\n",
            "NVML_ERROR_OPERATING_SYSTEM    = 17\n",
            "NVML_ERROR_LIB_RM_VERSION_MISMATCH = 18\n",
            "NVML_ERROR_UNKNOWN             = 999\n",
            "\n",
            "_nvmlFanState_t = c_uint\n",
            "NVML_FAN_NORMAL             = 0\n",
            "NVML_FAN_FAILED             = 1\n",
            "\n",
            "_nvmlLedColor_t = c_uint\n",
            "NVML_LED_COLOR_GREEN        = 0\n",
            "NVML_LED_COLOR_AMBER        = 1\n",
            "\n",
            "_nvmlGpuOperationMode_t = c_uint\n",
            "NVML_GOM_ALL_ON                 = 0\n",
            "NVML_GOM_COMPUTE                = 1\n",
            "NVML_GOM_LOW_DP                 = 2\n",
            "\n",
            "_nvmlPageRetirementCause_t = c_uint\n",
            "NVML_PAGE_RETIREMENT_CAUSE_DOUBLE_BIT_ECC_ERROR           = 0\n",
            "NVML_PAGE_RETIREMENT_CAUSE_MULTIPLE_SINGLE_BIT_ECC_ERRORS = 1\n",
            "NVML_PAGE_RETIREMENT_CAUSE_COUNT                          = 2\n",
            "\n",
            "_nvmlRestrictedAPI_t = c_uint\n",
            "NVML_RESTRICTED_API_SET_APPLICATION_CLOCKS                = 0\n",
            "NVML_RESTRICTED_API_SET_AUTO_BOOSTED_CLOCKS               = 1\n",
            "NVML_RESTRICTED_API_COUNT                                 = 2\n",
            "\n",
            "_nvmlBridgeChipType_t = c_uint\n",
            "NVML_BRIDGE_CHIP_PLX = 0\n",
            "NVML_BRIDGE_CHIP_BRO4 = 1\n",
            "NVML_MAX_PHYSICAL_BRIDGE = 128\n",
            "\n",
            "_nvmlValueType_t = c_uint\n",
            "NVML_VALUE_TYPE_DOUBLE = 0\n",
            "NVML_VALUE_TYPE_UNSIGNED_INT = 1\n",
            "NVML_VALUE_TYPE_UNSIGNED_LONG = 2\n",
            "NVML_VALUE_TYPE_UNSIGNED_LONG_LONG = 3\n",
            "NVML_VALUE_TYPE_COUNT = 4\n",
            "\n",
            "_nvmlPerfPolicyType_t = c_uint\n",
            "NVML_PERF_POLICY_POWER = 0\n",
            "NVML_PERF_POLICY_THERMAL = 1\n",
            "NVML_PERF_POLICY_COUNT = 2\n",
            "\n",
            "_nvmlSamplingType_t = c_uint\n",
            "NVML_TOTAL_POWER_SAMPLES = 0\n",
            "NVML_GPU_UTILIZATION_SAMPLES = 1\n",
            "NVML_MEMORY_UTILIZATION_SAMPLES = 2\n",
            "NVML_ENC_UTILIZATION_SAMPLES = 3\n",
            "NVML_DEC_UTILIZATION_SAMPLES = 4\n",
            "NVML_PROCESSOR_CLK_SAMPLES = 5\n",
            "NVML_MEMORY_CLK_SAMPLES = 6\n",
            "NVML_SAMPLINGTYPE_COUNT = 7\n",
            "\n",
            "_nvmlPcieUtilCounter_t = c_uint\n",
            "NVML_PCIE_UTIL_TX_BYTES = 0\n",
            "NVML_PCIE_UTIL_RX_BYTES = 1\n",
            "NVML_PCIE_UTIL_COUNT = 2\n",
            "\n",
            "_nvmlGpuTopologyLevel_t = c_uint\n",
            "NVML_TOPOLOGY_INTERNAL = 0\n",
            "NVML_TOPOLOGY_SINGLE = 10\n",
            "NVML_TOPOLOGY_MULTIPLE = 20\n",
            "NVML_TOPOLOGY_HOSTBRIDGE = 30\n",
            "NVML_TOPOLOGY_CPU = 40\n",
            "NVML_TOPOLOGY_SYSTEM = 50\n",
            "\n",
            "# C preprocessor defined values\n",
            "nvmlFlagDefault             = 0\n",
            "nvmlFlagForce               = 1\n",
            "\n",
            "# buffer size\n",
            "NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE      = 16\n",
            "NVML_DEVICE_UUID_BUFFER_SIZE                 = 80\n",
            "NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE       = 81\n",
            "NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE         = 80\n",
            "NVML_DEVICE_NAME_BUFFER_SIZE                 = 64\n",
            "NVML_DEVICE_SERIAL_BUFFER_SIZE               = 30\n",
            "NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE        = 32\n",
            "NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE           = 16\n",
            "\n",
            "NVML_VALUE_NOT_AVAILABLE_ulonglong = c_ulonglong(-1)\n",
            "NVML_VALUE_NOT_AVAILABLE_uint = c_uint(-1)\n",
            "\n",
            "## Lib loading ##\n",
            "nvmlLib = None\n",
            "libLoadLock = threading.Lock()\n",
            "_nvmlLib_refcount = 0 # Incremented on each nvmlInit and decremented on nvmlShutdown\n",
            "\n",
            "## Error Checking ##\n",
            "class NVMLError(Exception):\n",
            "    _valClassMapping = dict()\n",
            "    # List of currently known error codes\n",
            "    _errcode_to_string = {\n",
            "        NVML_ERROR_UNINITIALIZED:       \"Uninitialized\",\n",
            "        NVML_ERROR_INVALID_ARGUMENT:    \"Invalid Argument\",\n",
            "        NVML_ERROR_NOT_SUPPORTED:       \"Not Supported\",\n",
            "        NVML_ERROR_NO_PERMISSION:       \"Insufficient Permissions\",\n",
            "        NVML_ERROR_ALREADY_INITIALIZED: \"Already Initialized\",\n",
            "        NVML_ERROR_NOT_FOUND:           \"Not Found\",\n",
            "        NVML_ERROR_INSUFFICIENT_SIZE:   \"Insufficient Size\",\n",
            "        NVML_ERROR_INSUFFICIENT_POWER:  \"Insufficient External Power\",\n",
            "        NVML_ERROR_DRIVER_NOT_LOADED:   \"Driver Not Loaded\",\n",
            "        NVML_ERROR_TIMEOUT:             \"Timeout\",\n",
            "        NVML_ERROR_IRQ_ISSUE:           \"Interrupt Request Issue\",\n",
            "        NVML_ERROR_LIBRARY_NOT_FOUND:   \"NVML Shared Library Not Found\",\n",
            "        NVML_ERROR_FUNCTION_NOT_FOUND:  \"Function Not Found\",\n",
            "        NVML_ERROR_CORRUPTED_INFOROM:   \"Corrupted infoROM\",\n",
            "        NVML_ERROR_GPU_IS_LOST:         \"GPU is lost\",\n",
            "        NVML_ERROR_RESET_REQUIRED:      \"GPU requires restart\",\n",
            "        NVML_ERROR_OPERATING_SYSTEM:    \"The operating system has blocked the request.\",\n",
            "        NVML_ERROR_LIB_RM_VERSION_MISMATCH: \"RM has detected an NVML/RM version mismatch.\",\n",
            "        NVML_ERROR_UNKNOWN:             \"Unknown Error\",\n",
            "        }\n",
            "    def __new__(typ, value):\n",
            "        '''\n",
            "        Maps value to a proper subclass of NVMLError.\n",
            "        See _extractNVMLErrorsAsClasses function for more details\n",
            "        '''\n",
            "        if typ == NVMLError:\n",
            "            typ = NVMLError._valClassMapping.get(value, typ)\n",
            "        obj = Exception.__new__(typ)\n",
            "        obj.value = value\n",
            "        return obj\n",
            "    def __str__(self):\n",
            "        try:\n",
            "            if self.value not in NVMLError._errcode_to_string:\n",
            "                NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))\n",
            "            return NVMLError._errcode_to_string[self.value]\n",
            "        except NVMLError_Uninitialized:\n",
            "            return \"NVML Error with code %d\" % self.value\n",
            "    def __eq__(self, other):\n",
            "        return self.value == other.value\n",
            "\n",
            "def _extractNVMLErrorsAsClasses():\n",
            "    '''\n",
            "    Generates a hierarchy of classes on top of NVMLError class.\n",
            "\n",
            "    Each NVML Error gets a new NVMLError subclass. This way try,except blocks can filter appropriate\n",
            "    exceptions more easily.\n",
            "\n",
            "    NVMLError is a parent class. Each NVML_ERROR_* gets it's own subclass.\n",
            "    e.g. NVML_ERROR_ALREADY_INITIALIZED will be turned into NVMLError_AlreadyInitialized\n",
            "    '''\n",
            "    this_module = sys.modules[__name__]\n",
            "    nvmlErrorsNames = filter(lambda x: x.startswith(\"NVML_ERROR_\"), dir(this_module))\n",
            "    for err_name in nvmlErrorsNames:\n",
            "        # e.g. Turn NVML_ERROR_ALREADY_INITIALIZED into NVMLError_AlreadyInitialized\n",
            "        class_name = \"NVMLError_\" + string.capwords(err_name.replace(\"NVML_ERROR_\", \"\"), \"_\").replace(\"_\", \"\")\n",
            "        err_val = getattr(this_module, err_name)\n",
            "        def gen_new(val):\n",
            "            def new(typ):\n",
            "                obj = NVMLError.__new__(typ, val)\n",
            "                return obj\n",
            "            return new\n",
            "        new_error_class = type(class_name, (NVMLError,), {'__new__': gen_new(err_val)})\n",
            "        new_error_class.__module__ = __name__\n",
            "        setattr(this_module, class_name, new_error_class)\n",
            "        NVMLError._valClassMapping[err_val] = new_error_class\n",
            "_extractNVMLErrorsAsClasses()\n",
            "\n",
            "def _nvmlCheckReturn(ret):\n",
            "    if (ret != NVML_SUCCESS):\n",
            "        raise NVMLError(ret)\n",
            "    return ret\n",
            "\n",
            "## Function access ##\n",
            "_nvmlGetFunctionPointer_cache = dict() # function pointers are cached to prevent unnecessary libLoadLock locking\n",
            "def _nvmlGetFunctionPointer(name):\n",
            "    global nvmlLib\n",
            "\n",
            "    if name in _nvmlGetFunctionPointer_cache:\n",
            "        return _nvmlGetFunctionPointer_cache[name]\n",
            "\n",
            "    libLoadLock.acquire()\n",
            "    try:\n",
            "        # ensure library was loaded\n",
            "        if (nvmlLib == None):\n",
            "            raise NVMLError(NVML_ERROR_UNINITIALIZED)\n",
            "        try:\n",
            "            _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)\n",
            "            return _nvmlGetFunctionPointer_cache[name]\n",
            "        except AttributeError:\n",
            "            raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n",
            "    finally:\n",
            "        # lock is always freed\n",
            "        libLoadLock.release()\n",
            "\n",
            "## Alternative object\n",
            "# Allows the object to be printed\n",
            "# Allows mismatched types to be assigned\n",
            "#  - like None when the Structure variant requires c_uint\n",
            "class nvmlFriendlyObject(object):\n",
            "    def __init__(self, dictionary):\n",
            "        for x in dictionary:\n",
            "            setattr(self, x, dictionary[x])\n",
            "    def __str__(self):\n",
            "        return self.__dict__.__str__()\n",
            "\n",
            "def nvmlStructToFriendlyObject(struct):\n",
            "    d = {}\n",
            "    for x in struct._fields_:\n",
            "        key = x[0]\n",
            "        value = getattr(struct, key)\n",
            "        d[key] = value\n",
            "    obj = nvmlFriendlyObject(d)\n",
            "    return obj\n",
            "\n",
            "# pack the object so it can be passed to the NVML library\n",
            "def nvmlFriendlyObjectToStruct(obj, model):\n",
            "    for x in model._fields_:\n",
            "        key = x[0]\n",
            "        value = obj.__dict__[key]\n",
            "        setattr(model, key, value)\n",
            "    return model\n",
            "\n",
            "## Unit structures\n",
            "class struct_c_nvmlUnit_t(Structure):\n",
            "    pass # opaque handle\n",
            "c_nvmlUnit_t = POINTER(struct_c_nvmlUnit_t)\n",
            "\n",
            "class _PrintableStructure(Structure):\n",
            "    \"\"\"\n",
            "    Abstract class that produces nicer __str__ output than ctypes.Structure.\n",
            "    e.g. instead of:\n",
            "      >>> print str(obj)\n",
            "      <class_name object at 0x7fdf82fef9e0>\n",
            "    this class will print\n",
            "      class_name(field_name: formatted_value, field_name: formatted_value)\n",
            "\n",
            "    _fmt_ dictionary of <str _field_ name> -> <str format>\n",
            "    e.g. class that has _field_ 'hex_value', c_uint could be formatted with\n",
            "      _fmt_ = {\"hex_value\" : \"%08X\"}\n",
            "    to produce nicer output.\n",
            "    Default fomratting string for all fields can be set with key \"<default>\" like:\n",
            "      _fmt_ = {\"<default>\" : \"%d MHz\"} # e.g all values are numbers in MHz.\n",
            "    If not set it's assumed to be just \"%s\"\n",
            "\n",
            "    Exact format of returned str from this class is subject to change in the future.\n",
            "    \"\"\"\n",
            "    _fmt_ = {}\n",
            "    def __str__(self):\n",
            "        result = []\n",
            "        for x in self._fields_:\n",
            "            key = x[0]\n",
            "            value = getattr(self, key)\n",
            "            fmt = \"%s\"\n",
            "            if key in self._fmt_:\n",
            "                fmt = self._fmt_[key]\n",
            "            elif \"<default>\" in self._fmt_:\n",
            "                fmt = self._fmt_[\"<default>\"]\n",
            "            result.append((\"%s: \" + fmt) % (key, value))\n",
            "        return self.__class__.__name__ + \"(\" + string.join(result, \", \") + \")\"\n",
            "\n",
            "class c_nvmlUnitInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('name', c_char * 96),\n",
            "        ('id', c_char * 96),\n",
            "        ('serial', c_char * 96),\n",
            "        ('firmwareVersion', c_char * 96),\n",
            "    ]\n",
            "\n",
            "class c_nvmlLedState_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('cause', c_char * 256),\n",
            "        ('color', _nvmlLedColor_t),\n",
            "    ]\n",
            "\n",
            "class c_nvmlPSUInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('state', c_char * 256),\n",
            "        ('current', c_uint),\n",
            "        ('voltage', c_uint),\n",
            "        ('power', c_uint),\n",
            "    ]\n",
            "\n",
            "class c_nvmlUnitFanInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('speed', c_uint),\n",
            "        ('state', _nvmlFanState_t),\n",
            "    ]\n",
            "\n",
            "class c_nvmlUnitFanSpeeds_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('fans', c_nvmlUnitFanInfo_t * 24),\n",
            "        ('count', c_uint)\n",
            "    ]\n",
            "\n",
            "## Device structures\n",
            "class struct_c_nvmlDevice_t(Structure):\n",
            "    pass # opaque handle\n",
            "c_nvmlDevice_t = POINTER(struct_c_nvmlDevice_t)\n",
            "\n",
            "class nvmlPciInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('busId', c_char * 16),\n",
            "        ('domain', c_uint),\n",
            "        ('bus', c_uint),\n",
            "        ('device', c_uint),\n",
            "        ('pciDeviceId', c_uint),\n",
            "\n",
            "        # Added in 2.285\n",
            "        ('pciSubSystemId', c_uint),\n",
            "        ('reserved0', c_uint),\n",
            "        ('reserved1', c_uint),\n",
            "        ('reserved2', c_uint),\n",
            "        ('reserved3', c_uint),\n",
            "    ]\n",
            "    _fmt_ = {\n",
            "            'domain'         : \"0x%04X\",\n",
            "            'bus'            : \"0x%02X\",\n",
            "            'device'         : \"0x%02X\",\n",
            "            'pciDeviceId'    : \"0x%08X\",\n",
            "            'pciSubSystemId' : \"0x%08X\",\n",
            "            }\n",
            "\n",
            "class c_nvmlMemory_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('total', c_ulonglong),\n",
            "        ('free', c_ulonglong),\n",
            "        ('used', c_ulonglong),\n",
            "    ]\n",
            "    _fmt_ = {'<default>': \"%d B\"}\n",
            "\n",
            "class c_nvmlBAR1Memory_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('bar1Total', c_ulonglong),\n",
            "        ('bar1Free', c_ulonglong),\n",
            "        ('bar1Used', c_ulonglong),\n",
            "    ]\n",
            "    _fmt_ = {'<default>': \"%d B\"}\n",
            "\n",
            "# On Windows with the WDDM driver, usedGpuMemory is reported as None\n",
            "# Code that processes this structure should check for None, I.E.\n",
            "#\n",
            "# if (info.usedGpuMemory == None):\n",
            "#     # TODO handle the error\n",
            "#     pass\n",
            "# else:\n",
            "#    print(\"Using %d MiB of memory\" % (info.usedGpuMemory / 1024 / 1024))\n",
            "#\n",
            "# See NVML documentation for more information\n",
            "class c_nvmlProcessInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('pid', c_uint),\n",
            "        ('usedGpuMemory', c_ulonglong),\n",
            "    ]\n",
            "    _fmt_ = {'usedGpuMemory': \"%d B\"}\n",
            "\n",
            "class c_nvmlBridgeChipInfo_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('type', _nvmlBridgeChipType_t),\n",
            "        ('fwVersion', c_uint),\n",
            "    ]\n",
            "\n",
            "class c_nvmlBridgeChipHierarchy_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('bridgeCount', c_uint),\n",
            "        ('bridgeChipInfo', c_nvmlBridgeChipInfo_t * 128),\n",
            "    ]\n",
            "\n",
            "class c_nvmlEccErrorCounts_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('l1Cache', c_ulonglong),\n",
            "        ('l2Cache', c_ulonglong),\n",
            "        ('deviceMemory', c_ulonglong),\n",
            "        ('registerFile', c_ulonglong),\n",
            "    ]\n",
            "\n",
            "class c_nvmlUtilization_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('gpu', c_uint),\n",
            "        ('memory', c_uint),\n",
            "    ]\n",
            "    _fmt_ = {'<default>': \"%d %%\"}\n",
            "\n",
            "# Added in 2.285\n",
            "class c_nvmlHwbcEntry_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('hwbcId', c_uint),\n",
            "        ('firmwareVersion', c_char * 32),\n",
            "    ]\n",
            "\n",
            "class c_nvmlValue_t(Union):\n",
            "    _fields_ = [\n",
            "        ('dVal', c_double),\n",
            "        ('uiVal', c_uint),\n",
            "        ('ulVal', c_ulong),\n",
            "        ('ullVal', c_ulonglong),\n",
            "    ]\n",
            "\n",
            "class c_nvmlSample_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('timeStamp', c_ulonglong),\n",
            "        ('sampleValue', c_nvmlValue_t),\n",
            "    ]\n",
            "\n",
            "class c_nvmlViolationTime_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('referenceTime', c_ulonglong),\n",
            "        ('violationTime', c_ulonglong),\n",
            "    ]\n",
            "\n",
            "## Event structures\n",
            "class struct_c_nvmlEventSet_t(Structure):\n",
            "    pass # opaque handle\n",
            "c_nvmlEventSet_t = POINTER(struct_c_nvmlEventSet_t)\n",
            "\n",
            "nvmlEventTypeSingleBitEccError     = 0x0000000000000001\n",
            "nvmlEventTypeDoubleBitEccError     = 0x0000000000000002\n",
            "nvmlEventTypePState                = 0x0000000000000004\n",
            "nvmlEventTypeXidCriticalError      = 0x0000000000000008\n",
            "nvmlEventTypeClock                 = 0x0000000000000010\n",
            "nvmlEventTypeNone                  = 0x0000000000000000\n",
            "nvmlEventTypeAll                   = (\n",
            "                                        nvmlEventTypeNone |\n",
            "                                        nvmlEventTypeSingleBitEccError |\n",
            "                                        nvmlEventTypeDoubleBitEccError |\n",
            "                                        nvmlEventTypePState |\n",
            "                                        nvmlEventTypeClock |\n",
            "                                        nvmlEventTypeXidCriticalError\n",
            "                                     )\n",
            "\n",
            "## Clock Throttle Reasons defines\n",
            "nvmlClocksThrottleReasonGpuIdle           = 0x0000000000000001\n",
            "nvmlClocksThrottleReasonApplicationsClocksSetting = 0x0000000000000002\n",
            "nvmlClocksThrottleReasonUserDefinedClocks         = nvmlClocksThrottleReasonApplicationsClocksSetting # deprecated, use nvmlClocksThrottleReasonApplicationsClocksSetting\n",
            "nvmlClocksThrottleReasonSwPowerCap        = 0x0000000000000004\n",
            "nvmlClocksThrottleReasonHwSlowdown        = 0x0000000000000008\n",
            "nvmlClocksThrottleReasonUnknown           = 0x8000000000000000\n",
            "nvmlClocksThrottleReasonNone              = 0x0000000000000000\n",
            "nvmlClocksThrottleReasonAll               = (\n",
            "                                               nvmlClocksThrottleReasonNone |\n",
            "                                               nvmlClocksThrottleReasonGpuIdle |\n",
            "                                               nvmlClocksThrottleReasonApplicationsClocksSetting |\n",
            "                                               nvmlClocksThrottleReasonSwPowerCap |\n",
            "                                               nvmlClocksThrottleReasonHwSlowdown |\n",
            "                                               nvmlClocksThrottleReasonUnknown\n",
            "                                            )\n",
            "\n",
            "class c_nvmlEventData_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('device', c_nvmlDevice_t),\n",
            "        ('eventType', c_ulonglong),\n",
            "        ('eventData', c_ulonglong)\n",
            "    ]\n",
            "    _fmt_ = {'eventType': \"0x%08X\"}\n",
            "\n",
            "class c_nvmlAccountingStats_t(_PrintableStructure):\n",
            "    _fields_ = [\n",
            "        ('gpuUtilization', c_uint),\n",
            "        ('memoryUtilization', c_uint),\n",
            "        ('maxMemoryUsage', c_ulonglong),\n",
            "        ('time', c_ulonglong),\n",
            "        ('startTime', c_ulonglong),\n",
            "        ('isRunning', c_uint),\n",
            "        ('reserved', c_uint * 5)\n",
            "    ]\n",
            "\n",
            "## C function wrappers ##\n",
            "def nvmlInit():\n",
            "    _LoadNvmlLibrary()\n",
            "\n",
            "    #\n",
            "    # Initialize the library\n",
            "    #\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlInit_v2\")\n",
            "    ret = fn()\n",
            "    _nvmlCheckReturn(ret)\n",
            "\n",
            "    # Atomically update refcount\n",
            "    global _nvmlLib_refcount\n",
            "    libLoadLock.acquire()\n",
            "    _nvmlLib_refcount += 1\n",
            "    libLoadLock.release()\n",
            "    return None\n",
            "\n",
            "def _LoadNvmlLibrary():\n",
            "    '''\n",
            "    Load the library if it isn't loaded already\n",
            "    '''\n",
            "    global nvmlLib\n",
            "\n",
            "    if (nvmlLib == None):\n",
            "        # lock to ensure only one caller loads the library\n",
            "        libLoadLock.acquire()\n",
            "\n",
            "        try:\n",
            "            # ensure the library still isn't loaded\n",
            "            if (nvmlLib == None):\n",
            "                try:\n",
            "                    if (sys.platform[:3] == \"win\"):\n",
            "                        # cdecl calling convention\n",
            "                        # load nvml.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/nvml.dll\n",
            "                        nvmlLib = CDLL(os.path.join(os.getenv(\"ProgramFiles\", \"C:/Program Files\"), \"NVIDIA Corporation/NVSMI/nvml.dll\"))\n",
            "                    else:\n",
            "                        # assume linux\n",
            "                        nvmlLib = CDLL(\"libnvidia-ml.so.1\")\n",
            "                except OSError as ose:\n",
            "                    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n",
            "                if (nvmlLib == None):\n",
            "                    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n",
            "        finally:\n",
            "            # lock is always freed\n",
            "            libLoadLock.release()\n",
            "\n",
            "def nvmlShutdown():\n",
            "    #\n",
            "    # Leave the library loaded, but shutdown the interface\n",
            "    #\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlShutdown\")\n",
            "    ret = fn()\n",
            "    _nvmlCheckReturn(ret)\n",
            "\n",
            "    # Atomically update refcount\n",
            "    global _nvmlLib_refcount\n",
            "    libLoadLock.acquire()\n",
            "    if (0 < _nvmlLib_refcount):\n",
            "        _nvmlLib_refcount -= 1\n",
            "    libLoadLock.release()\n",
            "    return None\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlErrorString(result):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlErrorString\")\n",
            "    fn.restype = c_char_p # otherwise return is an int\n",
            "    ret = fn(result)\n",
            "    return ret\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlSystemGetNVMLVersion():\n",
            "    c_version = create_string_buffer(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetNVMLVersion\")\n",
            "    ret = fn(c_version, c_uint(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_version.value\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlSystemGetProcessName(pid):\n",
            "    c_name = create_string_buffer(1024)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetProcessName\")\n",
            "    ret = fn(c_uint(pid), c_name, c_uint(1024))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_name.value\n",
            "\n",
            "def nvmlSystemGetDriverVersion():\n",
            "    c_version = create_string_buffer(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetDriverVersion\")\n",
            "    ret = fn(c_version, c_uint(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_version.value\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlSystemGetHicVersion():\n",
            "    c_count = c_uint(0)\n",
            "    hics = None\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetHicVersion\")\n",
            "\n",
            "    # get the count\n",
            "    ret = fn(byref(c_count), None)\n",
            "\n",
            "    # this should only fail with insufficient size\n",
            "    if ((ret != NVML_SUCCESS) and\n",
            "        (ret != NVML_ERROR_INSUFFICIENT_SIZE)):\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "    # if there are no hics\n",
            "    if (c_count.value == 0):\n",
            "        return []\n",
            "\n",
            "    hic_array = c_nvmlHwbcEntry_t * c_count.value\n",
            "    hics = hic_array()\n",
            "    ret = fn(byref(c_count), hics)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return hics\n",
            "\n",
            "## Unit get functions\n",
            "def nvmlUnitGetCount():\n",
            "    c_count = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetCount\")\n",
            "    ret = fn(byref(c_count))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_count.value\n",
            "\n",
            "def nvmlUnitGetHandleByIndex(index):\n",
            "    c_index = c_uint(index)\n",
            "    unit = c_nvmlUnit_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetHandleByIndex\")\n",
            "    ret = fn(c_index, byref(unit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return unit\n",
            "\n",
            "def nvmlUnitGetUnitInfo(unit):\n",
            "    c_info = c_nvmlUnitInfo_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetUnitInfo\")\n",
            "    ret = fn(unit, byref(c_info))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_info\n",
            "\n",
            "def nvmlUnitGetLedState(unit):\n",
            "    c_state =  c_nvmlLedState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetLedState\")\n",
            "    ret = fn(unit, byref(c_state))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_state\n",
            "\n",
            "def nvmlUnitGetPsuInfo(unit):\n",
            "    c_info = c_nvmlPSUInfo_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetPsuInfo\")\n",
            "    ret = fn(unit, byref(c_info))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_info\n",
            "\n",
            "def nvmlUnitGetTemperature(unit, type):\n",
            "    c_temp = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetTemperature\")\n",
            "    ret = fn(unit, c_uint(type), byref(c_temp))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_temp.value\n",
            "\n",
            "def nvmlUnitGetFanSpeedInfo(unit):\n",
            "    c_speeds = c_nvmlUnitFanSpeeds_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetFanSpeedInfo\")\n",
            "    ret = fn(unit, byref(c_speeds))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_speeds\n",
            "\n",
            "# added to API\n",
            "def nvmlUnitGetDeviceCount(unit):\n",
            "    c_count = c_uint(0)\n",
            "    # query the unit to determine device count\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetDevices\")\n",
            "    ret = fn(unit, byref(c_count), None)\n",
            "    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n",
            "        ret = NVML_SUCCESS\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_count.value\n",
            "\n",
            "def nvmlUnitGetDevices(unit):\n",
            "    c_count = c_uint(nvmlUnitGetDeviceCount(unit))\n",
            "    device_array = c_nvmlDevice_t * c_count.value\n",
            "    c_devices = device_array()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetDevices\")\n",
            "    ret = fn(unit, byref(c_count), c_devices)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_devices\n",
            "\n",
            "## Device get functions\n",
            "def nvmlDeviceGetCount():\n",
            "    c_count = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCount_v2\")\n",
            "    ret = fn(byref(c_count))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_count.value\n",
            "\n",
            "def nvmlDeviceGetHandleByIndex(index):\n",
            "    c_index = c_uint(index)\n",
            "    device = c_nvmlDevice_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByIndex_v2\")\n",
            "    ret = fn(c_index, byref(device))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return device\n",
            "\n",
            "def nvmlDeviceGetHandleBySerial(serial):\n",
            "    c_serial = c_char_p(serial)\n",
            "    device = c_nvmlDevice_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleBySerial\")\n",
            "    ret = fn(c_serial, byref(device))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return device\n",
            "\n",
            "def nvmlDeviceGetHandleByUUID(uuid):\n",
            "    c_uuid = c_char_p(uuid)\n",
            "    device = c_nvmlDevice_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByUUID\")\n",
            "    ret = fn(c_uuid, byref(device))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return device\n",
            "\n",
            "def nvmlDeviceGetHandleByPciBusId(pciBusId):\n",
            "    c_busId = c_char_p(pciBusId)\n",
            "    device = c_nvmlDevice_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByPciBusId_v2\")\n",
            "    ret = fn(c_busId, byref(device))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return device\n",
            "\n",
            "def nvmlDeviceGetName(handle):\n",
            "    c_name = create_string_buffer(NVML_DEVICE_NAME_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetName\")\n",
            "    ret = fn(handle, c_name, c_uint(NVML_DEVICE_NAME_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_name.value\n",
            "\n",
            "def nvmlDeviceGetBoardId(handle):\n",
            "    c_id = c_uint();\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBoardId\")\n",
            "    ret = fn(handle, byref(c_id))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_id.value\n",
            "\n",
            "def nvmlDeviceGetMultiGpuBoard(handle):\n",
            "    c_multiGpu = c_uint();\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMultiGpuBoard\")\n",
            "    ret = fn(handle, byref(c_multiGpu))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_multiGpu.value\n",
            "\n",
            "def nvmlDeviceGetBrand(handle):\n",
            "    c_type = _nvmlBrandType_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBrand\")\n",
            "    ret = fn(handle, byref(c_type))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_type.value\n",
            "\n",
            "def nvmlDeviceGetSerial(handle):\n",
            "    c_serial = create_string_buffer(NVML_DEVICE_SERIAL_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSerial\")\n",
            "    ret = fn(handle, c_serial, c_uint(NVML_DEVICE_SERIAL_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_serial.value\n",
            "\n",
            "def nvmlDeviceGetCpuAffinity(handle, cpuSetSize):\n",
            "    affinity_array = c_ulonglong * cpuSetSize\n",
            "    c_affinity = affinity_array()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCpuAffinity\")\n",
            "    ret = fn(handle, cpuSetSize, byref(c_affinity))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_affinity\n",
            "\n",
            "def nvmlDeviceSetCpuAffinity(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetCpuAffinity\")\n",
            "    ret = fn(handle)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceClearCpuAffinity(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearCpuAffinity\")\n",
            "    ret = fn(handle)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceGetMinorNumber(handle):\n",
            "    c_minor_number = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMinorNumber\")\n",
            "    ret = fn(handle, byref(c_minor_number))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_minor_number.value\n",
            "\n",
            "def nvmlDeviceGetUUID(handle):\n",
            "    c_uuid = create_string_buffer(NVML_DEVICE_UUID_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetUUID\")\n",
            "    ret = fn(handle, c_uuid, c_uint(NVML_DEVICE_UUID_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_uuid.value\n",
            "\n",
            "def nvmlDeviceGetInforomVersion(handle, infoRomObject):\n",
            "    c_version = create_string_buffer(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomVersion\")\n",
            "    ret = fn(handle, _nvmlInforomObject_t(infoRomObject),\n",
            "\t         c_version, c_uint(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_version.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetInforomImageVersion(handle):\n",
            "    c_version = create_string_buffer(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomImageVersion\")\n",
            "    ret = fn(handle, c_version, c_uint(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_version.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetInforomConfigurationChecksum(handle):\n",
            "    c_checksum = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomConfigurationChecksum\")\n",
            "    ret = fn(handle, byref(c_checksum))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_checksum.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceValidateInforom(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceValidateInforom\")\n",
            "    ret = fn(handle)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceGetDisplayMode(handle):\n",
            "    c_mode = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDisplayMode\")\n",
            "    ret = fn(handle, byref(c_mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_mode.value\n",
            "\n",
            "def nvmlDeviceGetDisplayActive(handle):\n",
            "    c_mode = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDisplayActive\")\n",
            "    ret = fn(handle, byref(c_mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_mode.value\n",
            "\n",
            "\n",
            "def nvmlDeviceGetPersistenceMode(handle):\n",
            "    c_state = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPersistenceMode\")\n",
            "    ret = fn(handle, byref(c_state))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_state.value\n",
            "\n",
            "def nvmlDeviceGetPciInfo(handle):\n",
            "    c_info = nvmlPciInfo_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPciInfo_v2\")\n",
            "    ret = fn(handle, byref(c_info))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_info\n",
            "\n",
            "def nvmlDeviceGetClockInfo(handle, type):\n",
            "    c_clock = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetClockInfo\")\n",
            "    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_clock.value\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlDeviceGetMaxClockInfo(handle, type):\n",
            "    c_clock = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxClockInfo\")\n",
            "    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_clock.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetApplicationsClock(handle, type):\n",
            "    c_clock = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetApplicationsClock\")\n",
            "    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_clock.value\n",
            "\n",
            "# Added in 5.319\n",
            "def nvmlDeviceGetDefaultApplicationsClock(handle, type):\n",
            "    c_clock = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDefaultApplicationsClock\")\n",
            "    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_clock.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetSupportedMemoryClocks(handle):\n",
            "    # first call to get the size\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedMemoryClocks\")\n",
            "    ret = fn(handle, byref(c_count), None)\n",
            "\n",
            "    if (ret == NVML_SUCCESS):\n",
            "        # special case, no clocks\n",
            "        return []\n",
            "    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n",
            "        # typical case\n",
            "        clocks_array = c_uint * c_count.value\n",
            "        c_clocks = clocks_array()\n",
            "\n",
            "        # make the call again\n",
            "        ret = fn(handle, byref(c_count), c_clocks)\n",
            "        _nvmlCheckReturn(ret)\n",
            "\n",
            "        procs = []\n",
            "        for i in range(c_count.value):\n",
            "            procs.append(c_clocks[i])\n",
            "\n",
            "        return procs\n",
            "    else:\n",
            "        # error case\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetSupportedGraphicsClocks(handle, memoryClockMHz):\n",
            "    # first call to get the size\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedGraphicsClocks\")\n",
            "    ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), None)\n",
            "\n",
            "    if (ret == NVML_SUCCESS):\n",
            "        # special case, no clocks\n",
            "        return []\n",
            "    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n",
            "        # typical case\n",
            "        clocks_array = c_uint * c_count.value\n",
            "        c_clocks = clocks_array()\n",
            "\n",
            "        # make the call again\n",
            "        ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), c_clocks)\n",
            "        _nvmlCheckReturn(ret)\n",
            "\n",
            "        procs = []\n",
            "        for i in range(c_count.value):\n",
            "            procs.append(c_clocks[i])\n",
            "\n",
            "        return procs\n",
            "    else:\n",
            "        # error case\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "def nvmlDeviceGetFanSpeed(handle):\n",
            "    c_speed = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFanSpeed\")\n",
            "    ret = fn(handle, byref(c_speed))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_speed.value\n",
            "\n",
            "def nvmlDeviceGetTemperature(handle, sensor):\n",
            "    c_temp = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTemperature\")\n",
            "    ret = fn(handle, _nvmlTemperatureSensors_t(sensor), byref(c_temp))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_temp.value\n",
            "\n",
            "def nvmlDeviceGetTemperatureThreshold(handle, threshold):\n",
            "    c_temp = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTemperatureThreshold\")\n",
            "    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_temp.value\n",
            "\n",
            "# DEPRECATED use nvmlDeviceGetPerformanceState\n",
            "def nvmlDeviceGetPowerState(handle):\n",
            "    c_pstate = _nvmlPstates_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerState\")\n",
            "    ret = fn(handle, byref(c_pstate))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_pstate.value\n",
            "\n",
            "def nvmlDeviceGetPerformanceState(handle):\n",
            "    c_pstate = _nvmlPstates_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPerformanceState\")\n",
            "    ret = fn(handle, byref(c_pstate))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_pstate.value\n",
            "\n",
            "def nvmlDeviceGetPowerManagementMode(handle):\n",
            "    c_pcapMode = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementMode\")\n",
            "    ret = fn(handle, byref(c_pcapMode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_pcapMode.value\n",
            "\n",
            "def nvmlDeviceGetPowerManagementLimit(handle):\n",
            "    c_limit = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementLimit\")\n",
            "    ret = fn(handle, byref(c_limit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_limit.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetPowerManagementLimitConstraints(handle):\n",
            "    c_minLimit = c_uint()\n",
            "    c_maxLimit = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementLimitConstraints\")\n",
            "    ret = fn(handle, byref(c_minLimit), byref(c_maxLimit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_minLimit.value, c_maxLimit.value]\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetPowerManagementDefaultLimit(handle):\n",
            "    c_limit = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementDefaultLimit\")\n",
            "    ret = fn(handle, byref(c_limit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_limit.value\n",
            "\n",
            "\n",
            "# Added in 331\n",
            "def nvmlDeviceGetEnforcedPowerLimit(handle):\n",
            "    c_limit = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEnforcedPowerLimit\")\n",
            "    ret = fn(handle, byref(c_limit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_limit.value\n",
            "\n",
            "def nvmlDeviceGetPowerUsage(handle):\n",
            "    c_watts = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerUsage\")\n",
            "    ret = fn(handle, byref(c_watts))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_watts.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetGpuOperationMode(handle):\n",
            "    c_currState = _nvmlGpuOperationMode_t()\n",
            "    c_pendingState = _nvmlGpuOperationMode_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuOperationMode\")\n",
            "    ret = fn(handle, byref(c_currState), byref(c_pendingState))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_currState.value, c_pendingState.value]\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetCurrentGpuOperationMode(handle):\n",
            "    return nvmlDeviceGetGpuOperationMode(handle)[0]\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetPendingGpuOperationMode(handle):\n",
            "    return nvmlDeviceGetGpuOperationMode(handle)[1]\n",
            "\n",
            "def nvmlDeviceGetMemoryInfo(handle):\n",
            "    c_memory = c_nvmlMemory_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryInfo\")\n",
            "    ret = fn(handle, byref(c_memory))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_memory\n",
            "\n",
            "def nvmlDeviceGetBAR1MemoryInfo(handle):\n",
            "    c_bar1_memory = c_nvmlBAR1Memory_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBAR1MemoryInfo\")\n",
            "    ret = fn(handle, byref(c_bar1_memory))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_bar1_memory\n",
            "\n",
            "def nvmlDeviceGetComputeMode(handle):\n",
            "    c_mode = _nvmlComputeMode_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeMode\")\n",
            "    ret = fn(handle, byref(c_mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_mode.value\n",
            "\n",
            "def nvmlDeviceGetEccMode(handle):\n",
            "    c_currState = _nvmlEnableState_t()\n",
            "    c_pendingState = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEccMode\")\n",
            "    ret = fn(handle, byref(c_currState), byref(c_pendingState))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_currState.value, c_pendingState.value]\n",
            "\n",
            "# added to API\n",
            "def nvmlDeviceGetCurrentEccMode(handle):\n",
            "    return nvmlDeviceGetEccMode(handle)[0]\n",
            "\n",
            "# added to API\n",
            "def nvmlDeviceGetPendingEccMode(handle):\n",
            "    return nvmlDeviceGetEccMode(handle)[1]\n",
            "\n",
            "def nvmlDeviceGetTotalEccErrors(handle, errorType, counterType):\n",
            "    c_count = c_ulonglong()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTotalEccErrors\")\n",
            "    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),\n",
            "\t         _nvmlEccCounterType_t(counterType), byref(c_count))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_count.value\n",
            "\n",
            "# This is deprecated, instead use nvmlDeviceGetMemoryErrorCounter\n",
            "def nvmlDeviceGetDetailedEccErrors(handle, errorType, counterType):\n",
            "    c_counts = c_nvmlEccErrorCounts_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDetailedEccErrors\")\n",
            "    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),\n",
            "\t         _nvmlEccCounterType_t(counterType), byref(c_counts))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_counts\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType, locationType):\n",
            "    c_count = c_ulonglong()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryErrorCounter\")\n",
            "    ret = fn(handle,\n",
            "             _nvmlMemoryErrorType_t(errorType),\n",
            "             _nvmlEccCounterType_t(counterType),\n",
            "             _nvmlMemoryLocation_t(locationType),\n",
            "             byref(c_count))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_count.value\n",
            "\n",
            "def nvmlDeviceGetUtilizationRates(handle):\n",
            "    c_util = c_nvmlUtilization_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetUtilizationRates\")\n",
            "    ret = fn(handle, byref(c_util))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_util\n",
            "\n",
            "def nvmlDeviceGetEncoderUtilization(handle):\n",
            "    c_util = c_uint()\n",
            "    c_samplingPeriod = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEncoderUtilization\")\n",
            "    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_util.value, c_samplingPeriod.value]\n",
            "\n",
            "def nvmlDeviceGetDecoderUtilization(handle):\n",
            "    c_util = c_uint()\n",
            "    c_samplingPeriod = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDecoderUtilization\")\n",
            "    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_util.value, c_samplingPeriod.value]\n",
            "\n",
            "def nvmlDeviceGetPcieReplayCounter(handle):\n",
            "    c_replay = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieReplayCounter\")\n",
            "    ret = fn(handle, byref(c_replay))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_replay.value\n",
            "\n",
            "def nvmlDeviceGetDriverModel(handle):\n",
            "    c_currModel = _nvmlDriverModel_t()\n",
            "    c_pendingModel = _nvmlDriverModel_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDriverModel\")\n",
            "    ret = fn(handle, byref(c_currModel), byref(c_pendingModel))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_currModel.value, c_pendingModel.value]\n",
            "\n",
            "# added to API\n",
            "def nvmlDeviceGetCurrentDriverModel(handle):\n",
            "    return nvmlDeviceGetDriverModel(handle)[0]\n",
            "\n",
            "# added to API\n",
            "def nvmlDeviceGetPendingDriverModel(handle):\n",
            "    return nvmlDeviceGetDriverModel(handle)[1]\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlDeviceGetVbiosVersion(handle):\n",
            "    c_version = create_string_buffer(NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVbiosVersion\")\n",
            "    ret = fn(handle, c_version, c_uint(NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_version.value\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlDeviceGetComputeRunningProcesses(handle):\n",
            "    # first call to get the size\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeRunningProcesses\")\n",
            "    ret = fn(handle, byref(c_count), None)\n",
            "\n",
            "    if (ret == NVML_SUCCESS):\n",
            "        # special case, no running processes\n",
            "        return []\n",
            "    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n",
            "        # typical case\n",
            "        # oversize the array incase more processes are created\n",
            "        c_count.value = c_count.value * 2 + 5\n",
            "        proc_array = c_nvmlProcessInfo_t * c_count.value\n",
            "        c_procs = proc_array()\n",
            "\n",
            "        # make the call again\n",
            "        ret = fn(handle, byref(c_count), c_procs)\n",
            "        _nvmlCheckReturn(ret)\n",
            "\n",
            "        procs = []\n",
            "        for i in range(c_count.value):\n",
            "            # use an alternative struct for this object\n",
            "            obj = nvmlStructToFriendlyObject(c_procs[i])\n",
            "            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n",
            "                # special case for WDDM on Windows, see comment above\n",
            "                obj.usedGpuMemory = None\n",
            "            procs.append(obj)\n",
            "\n",
            "        return procs\n",
            "    else:\n",
            "        # error case\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "def nvmlDeviceGetGraphicsRunningProcesses(handle):\n",
            "    # first call to get the size\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGraphicsRunningProcesses\")\n",
            "    ret = fn(handle, byref(c_count), None)\n",
            "\n",
            "    if (ret == NVML_SUCCESS):\n",
            "        # special case, no running processes\n",
            "        return []\n",
            "    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n",
            "        # typical case\n",
            "        # oversize the array incase more processes are created\n",
            "        c_count.value = c_count.value * 2 + 5\n",
            "        proc_array = c_nvmlProcessInfo_t * c_count.value\n",
            "        c_procs = proc_array()\n",
            "\n",
            "        # make the call again\n",
            "        ret = fn(handle, byref(c_count), c_procs)\n",
            "        _nvmlCheckReturn(ret)\n",
            "\n",
            "        procs = []\n",
            "        for i in range(c_count.value):\n",
            "            # use an alternative struct for this object\n",
            "            obj = nvmlStructToFriendlyObject(c_procs[i])\n",
            "            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n",
            "                # special case for WDDM on Windows, see comment above\n",
            "                obj.usedGpuMemory = None\n",
            "            procs.append(obj)\n",
            "\n",
            "        return procs\n",
            "    else:\n",
            "        # error case\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "def nvmlDeviceGetAutoBoostedClocksEnabled(handle):\n",
            "    c_isEnabled = _nvmlEnableState_t()\n",
            "    c_defaultIsEnabled = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAutoBoostedClocksEnabled\")\n",
            "    ret = fn(handle, byref(c_isEnabled), byref(c_defaultIsEnabled))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return [c_isEnabled.value, c_defaultIsEnabled.value]\n",
            "    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n",
            "\n",
            "## Set functions\n",
            "def nvmlUnitSetLedState(unit, color):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlUnitSetLedState\")\n",
            "    ret = fn(unit, _nvmlLedColor_t(color))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceSetPersistenceMode(handle, mode):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetPersistenceMode\")\n",
            "    ret = fn(handle, _nvmlEnableState_t(mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceSetComputeMode(handle, mode):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetComputeMode\")\n",
            "    ret = fn(handle, _nvmlComputeMode_t(mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceSetEccMode(handle, mode):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetEccMode\")\n",
            "    ret = fn(handle, _nvmlEnableState_t(mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceClearEccErrorCounts(handle, counterType):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearEccErrorCounts\")\n",
            "    ret = fn(handle, _nvmlEccCounterType_t(counterType))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceSetDriverModel(handle, model):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDriverModel\")\n",
            "    ret = fn(handle, _nvmlDriverModel_t(model))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceSetAutoBoostedClocksEnabled(handle, enabled):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAutoBoostedClocksEnabled\")\n",
            "    ret = fn(handle, _nvmlEnableState_t(enabled))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n",
            "\n",
            "def nvmlDeviceSetDefaultAutoBoostedClocksEnabled(handle, enabled, flags):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDefaultAutoBoostedClocksEnabled\")\n",
            "    ret = fn(handle, _nvmlEnableState_t(enabled), c_uint(flags))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceSetApplicationsClocks(handle, maxMemClockMHz, maxGraphicsClockMHz):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetApplicationsClocks\")\n",
            "    ret = fn(handle, c_uint(maxMemClockMHz), c_uint(maxGraphicsClockMHz))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceResetApplicationsClocks(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetApplicationsClocks\")\n",
            "    ret = fn(handle)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceSetPowerManagementLimit(handle, limit):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetPowerManagementLimit\")\n",
            "    ret = fn(handle, c_uint(limit))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceSetGpuOperationMode(handle, mode):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetGpuOperationMode\")\n",
            "    ret = fn(handle, _nvmlGpuOperationMode_t(mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlEventSetCreate():\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlEventSetCreate\")\n",
            "    eventSet = c_nvmlEventSet_t()\n",
            "    ret = fn(byref(eventSet))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return eventSet\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlDeviceRegisterEvents(handle, eventTypes, eventSet):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceRegisterEvents\")\n",
            "    ret = fn(handle, c_ulonglong(eventTypes), eventSet)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlDeviceGetSupportedEventTypes(handle):\n",
            "    c_eventTypes = c_ulonglong()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedEventTypes\")\n",
            "    ret = fn(handle, byref(c_eventTypes))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_eventTypes.value\n",
            "\n",
            "# Added in 2.285\n",
            "# raises NVML_ERROR_TIMEOUT exception on timeout\n",
            "def nvmlEventSetWait(eventSet, timeoutms):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlEventSetWait\")\n",
            "    data = c_nvmlEventData_t()\n",
            "    ret = fn(eventSet, byref(data), c_uint(timeoutms))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return data\n",
            "\n",
            "# Added in 2.285\n",
            "def nvmlEventSetFree(eventSet):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlEventSetFree\")\n",
            "    ret = fn(eventSet)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "# Added in 3.295\n",
            "def nvmlDeviceOnSameBoard(handle1, handle2):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceOnSameBoard\")\n",
            "    onSameBoard = c_int()\n",
            "    ret = fn(handle1, handle2, byref(onSameBoard))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return (onSameBoard.value != 0)\n",
            "\n",
            "# Added in 3.295\n",
            "def nvmlDeviceGetCurrPcieLinkGeneration(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrPcieLinkGeneration\")\n",
            "    gen = c_uint()\n",
            "    ret = fn(handle, byref(gen))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return gen.value\n",
            "\n",
            "# Added in 3.295\n",
            "def nvmlDeviceGetMaxPcieLinkGeneration(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxPcieLinkGeneration\")\n",
            "    gen = c_uint()\n",
            "    ret = fn(handle, byref(gen))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return gen.value\n",
            "\n",
            "# Added in 3.295\n",
            "def nvmlDeviceGetCurrPcieLinkWidth(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrPcieLinkWidth\")\n",
            "    width = c_uint()\n",
            "    ret = fn(handle, byref(width))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return width.value\n",
            "\n",
            "# Added in 3.295\n",
            "def nvmlDeviceGetMaxPcieLinkWidth(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxPcieLinkWidth\")\n",
            "    width = c_uint()\n",
            "    ret = fn(handle, byref(width))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return width.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetSupportedClocksThrottleReasons(handle):\n",
            "    c_reasons= c_ulonglong()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedClocksThrottleReasons\")\n",
            "    ret = fn(handle, byref(c_reasons))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_reasons.value\n",
            "\n",
            "# Added in 4.304\n",
            "def nvmlDeviceGetCurrentClocksThrottleReasons(handle):\n",
            "    c_reasons= c_ulonglong()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrentClocksThrottleReasons\")\n",
            "    ret = fn(handle, byref(c_reasons))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_reasons.value\n",
            "\n",
            "# Added in 5.319\n",
            "def nvmlDeviceGetIndex(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetIndex\")\n",
            "    c_index = c_uint()\n",
            "    ret = fn(handle, byref(c_index))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_index.value\n",
            "\n",
            "# Added in 5.319\n",
            "def nvmlDeviceGetAccountingMode(handle):\n",
            "    c_mode = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingMode\")\n",
            "    ret = fn(handle, byref(c_mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_mode.value\n",
            "\n",
            "def nvmlDeviceSetAccountingMode(handle, mode):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAccountingMode\")\n",
            "    ret = fn(handle, _nvmlEnableState_t(mode))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceClearAccountingPids(handle):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearAccountingPids\")\n",
            "    ret = fn(handle)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceGetAccountingStats(handle, pid):\n",
            "    stats = c_nvmlAccountingStats_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingStats\")\n",
            "    ret = fn(handle, c_uint(pid), byref(stats))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    if (stats.maxMemoryUsage == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n",
            "        # special case for WDDM on Windows, see comment above\n",
            "        stats.maxMemoryUsage = None\n",
            "    return stats\n",
            "\n",
            "def nvmlDeviceGetAccountingPids(handle):\n",
            "    count = c_uint(nvmlDeviceGetAccountingBufferSize(handle))\n",
            "    pids = (c_uint * count.value)()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingPids\")\n",
            "    ret = fn(handle, byref(count), pids)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return map(int, pids[0:count.value])\n",
            "\n",
            "def nvmlDeviceGetAccountingBufferSize(handle):\n",
            "    bufferSize = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingBufferSize\")\n",
            "    ret = fn(handle, byref(bufferSize))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return int(bufferSize.value)\n",
            "\n",
            "def nvmlDeviceGetRetiredPages(device, sourceFilter):\n",
            "    c_source = _nvmlPageRetirementCause_t(sourceFilter)\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRetiredPages\")\n",
            "\n",
            "    # First call will get the size\n",
            "    ret = fn(device, c_source, byref(c_count), None)\n",
            "\n",
            "    # this should only fail with insufficient size\n",
            "    if ((ret != NVML_SUCCESS) and\n",
            "        (ret != NVML_ERROR_INSUFFICIENT_SIZE)):\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "    # call again with a buffer\n",
            "    # oversize the array for the rare cases where additional pages\n",
            "    # are retired between NVML calls\n",
            "    c_count.value = c_count.value * 2 + 5\n",
            "    page_array = c_ulonglong * c_count.value\n",
            "    c_pages = page_array()\n",
            "    ret = fn(device, c_source, byref(c_count), c_pages)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return map(int, c_pages[0:c_count.value])\n",
            "\n",
            "def nvmlDeviceGetRetiredPagesPendingStatus(device):\n",
            "    c_pending = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRetiredPagesPendingStatus\")\n",
            "    ret = fn(device, byref(c_pending))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return int(c_pending.value)\n",
            "\n",
            "def nvmlDeviceGetAPIRestriction(device, apiType):\n",
            "    c_permission = _nvmlEnableState_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAPIRestriction\")\n",
            "    ret = fn(device, _nvmlRestrictedAPI_t(apiType), byref(c_permission))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return int(c_permission.value)\n",
            "\n",
            "def nvmlDeviceSetAPIRestriction(handle, apiType, isRestricted):\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAPIRestriction\")\n",
            "    ret = fn(handle, _nvmlRestrictedAPI_t(apiType), _nvmlEnableState_t(isRestricted))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return None\n",
            "\n",
            "def nvmlDeviceGetBridgeChipInfo(handle):\n",
            "    bridgeHierarchy = c_nvmlBridgeChipHierarchy_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBridgeChipInfo\")\n",
            "    ret = fn(handle, byref(bridgeHierarchy))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return bridgeHierarchy\n",
            "\n",
            "def nvmlDeviceGetSamples(device, sampling_type, timeStamp):\n",
            "    c_sampling_type = _nvmlSamplingType_t(sampling_type)\n",
            "    c_time_stamp = c_ulonglong(timeStamp)\n",
            "    c_sample_count = c_uint(0)\n",
            "    c_sample_value_type = _nvmlValueType_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSamples\")\n",
            "\n",
            "    ## First Call gets the size\n",
            "    ret = fn(device, c_sampling_type, c_time_stamp, byref(c_sample_value_type), byref(c_sample_count), None)\n",
            "\n",
            "    # Stop if this fails\n",
            "    if (ret != NVML_SUCCESS):\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "    sampleArray = c_sample_count.value * c_nvmlSample_t\n",
            "    c_samples = sampleArray()\n",
            "    ret = fn(device, c_sampling_type, c_time_stamp,  byref(c_sample_value_type), byref(c_sample_count), c_samples)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return (c_sample_value_type.value, c_samples[0:c_sample_count.value])\n",
            "\n",
            "def nvmlDeviceGetViolationStatus(device, perfPolicyType):\n",
            "    c_perfPolicy_type = _nvmlPerfPolicyType_t(perfPolicyType)\n",
            "    c_violTime = c_nvmlViolationTime_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetViolationStatus\")\n",
            "\n",
            "    ## Invoke the method to get violation time\n",
            "    ret = fn(device, c_perfPolicy_type, byref(c_violTime))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_violTime\n",
            "\n",
            "def nvmlDeviceGetPcieThroughput(device, counter):\n",
            "    c_util = c_uint()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieThroughput\")\n",
            "    ret = fn(device, _nvmlPcieUtilCounter_t(counter), byref(c_util))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_util.value\n",
            "\n",
            "def nvmlSystemGetTopologyGpuSet(cpuNumber):\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetTopologyGpuSet\")\n",
            "\n",
            "    # First call will get the size\n",
            "    ret = fn(cpuNumber, byref(c_count), None)\n",
            "\n",
            "    if ret != NVML_SUCCESS:\n",
            "        raise NVMLError(ret)\n",
            "    print(c_count.value)\n",
            "    # call again with a buffer\n",
            "    device_array = c_nvmlDevice_t * c_count.value\n",
            "    c_devices = device_array()\n",
            "    ret = fn(cpuNumber, byref(c_count), c_devices)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return map(None, c_devices[0:c_count.value])\n",
            "\n",
            "def nvmlDeviceGetTopologyNearestGpus(device, level):\n",
            "    c_count = c_uint(0)\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTopologyNearestGpus\")\n",
            "\n",
            "    # First call will get the size\n",
            "    ret = fn(device, level, byref(c_count), None)\n",
            "\n",
            "    if ret != NVML_SUCCESS:\n",
            "        raise NVMLError(ret)\n",
            "\n",
            "    # call again with a buffer\n",
            "    device_array = c_nvmlDevice_t * c_count.value\n",
            "    c_devices = device_array()\n",
            "    ret = fn(device, level, byref(c_count), c_devices)\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return map(None, c_devices[0:c_count.value])\n",
            "\n",
            "def nvmlDeviceGetTopologyCommonAncestor(device1, device2):\n",
            "    c_level = _nvmlGpuTopologyLevel_t()\n",
            "    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTopologyCommonAncestor\")\n",
            "    ret = fn(device1, device2, byref(c_level))\n",
            "    _nvmlCheckReturn(ret)\n",
            "    return c_level.value\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Graphviz's dot language parser.\n",
            "\n",
            "The dotparser parses GraphViz files in\n",
            "dot and dot files and transforms them\n",
            "into a class representation defined by `pydot`.\n",
            "\n",
            "Author: Michael Krause <michael@krause-software.de>\n",
            "Fixes by: Ero Carrera <ero@dkbza.org>\n",
            "\"\"\"\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "import sys\n",
            "\n",
            "from pyparsing import (\n",
            "    nestedExpr, Literal, CaselessLiteral,\n",
            "    Word, OneOrMore,\n",
            "    Forward,\n",
            "    Group, Optional, Combine,\n",
            "    restOfLine, cStyleComment, nums, alphanums,\n",
            "    printables,\n",
            "    ParseException, ParseResults, CharsNotIn,\n",
            "    QuotedString)\n",
            "\n",
            "import pydot\n",
            "\n",
            "__author__ = ['Michael Krause', 'Ero Carrera']\n",
            "__license__ = 'MIT'\n",
            "\n",
            "\n",
            "PY3 = sys.version_info >= (3, 0, 0)\n",
            "if PY3:\n",
            "    str_type = str\n",
            "else:\n",
            "    str_type = basestring\n",
            "\n",
            "\n",
            "class P_AttrList(object):\n",
            "\n",
            "    def __init__(self, toks):\n",
            "\n",
            "        self.attrs = {}\n",
            "        i = 0\n",
            "\n",
            "        while i < len(toks):\n",
            "            attrname = toks[i]\n",
            "            if i+2 < len(toks) and toks[i+1] == '=':\n",
            "                attrvalue = toks[i+2]\n",
            "                i += 3\n",
            "            else:\n",
            "                attrvalue = None\n",
            "                i += 1\n",
            "\n",
            "            self.attrs[attrname] = attrvalue\n",
            "\n",
            "\n",
            "    def __repr__(self):\n",
            "\n",
            "        return \"%s(%r)\" % (self.__class__.__name__, self.attrs)\n",
            "\n",
            "\n",
            "\n",
            "class DefaultStatement(P_AttrList):\n",
            "\n",
            "    def __init__(self, default_type, attrs):\n",
            "\n",
            "        self.default_type = default_type\n",
            "        self.attrs = attrs\n",
            "\n",
            "    def __repr__(self):\n",
            "\n",
            "        return \"%s(%s, %r)\" % (self.__class__.__name__,\n",
            "            self.default_type, self.attrs)\n",
            "\n",
            "\n",
            "top_graphs = list()\n",
            "\n",
            "def push_top_graph_stmt(str, loc, toks):\n",
            "\n",
            "    attrs = {}\n",
            "    g = None\n",
            "\n",
            "    for element in toks:\n",
            "\n",
            "        if (isinstance(element, (ParseResults, tuple, list)) and\n",
            "                len(element) == 1 and\n",
            "                isinstance(element[0], str_type)):\n",
            "\n",
            "            element = element[0]\n",
            "\n",
            "        if element == 'strict':\n",
            "            attrs['strict'] = True\n",
            "\n",
            "        elif element in ['graph', 'digraph']:\n",
            "\n",
            "            attrs = {}\n",
            "\n",
            "            g = pydot.Dot(graph_type=element, **attrs)\n",
            "            attrs['type'] = element\n",
            "\n",
            "            top_graphs.append( g )\n",
            "\n",
            "        elif isinstance( element, str_type):\n",
            "            g.set_name( element )\n",
            "\n",
            "        elif isinstance(element, pydot.Subgraph):\n",
            "\n",
            "            g.obj_dict['attributes'].update( element.obj_dict['attributes'] )\n",
            "            g.obj_dict['edges'].update( element.obj_dict['edges'] )\n",
            "            g.obj_dict['nodes'].update( element.obj_dict['nodes'] )\n",
            "            g.obj_dict['subgraphs'].update( element.obj_dict['subgraphs'] )\n",
            "\n",
            "            g.set_parent_graph(g)\n",
            "\n",
            "        elif isinstance(element, P_AttrList):\n",
            "            attrs.update(element.attrs)\n",
            "\n",
            "        elif isinstance(element, (ParseResults, list)):\n",
            "            add_elements(g, element)\n",
            "\n",
            "        else:\n",
            "            raise ValueError(\n",
            "                'Unknown element statement: {s}'.format(s=element))\n",
            "\n",
            "\n",
            "    for g in top_graphs:\n",
            "        update_parent_graph_hierarchy(g)\n",
            "\n",
            "    if len( top_graphs ) == 1:\n",
            "        return top_graphs[0]\n",
            "\n",
            "    return top_graphs\n",
            "\n",
            "\n",
            "def update_parent_graph_hierarchy(g, parent_graph=None, level=0):\n",
            "\n",
            "\n",
            "    if parent_graph is None:\n",
            "        parent_graph = g\n",
            "\n",
            "    for key_name in ('edges',):\n",
            "\n",
            "        if isinstance(g, pydot.frozendict):\n",
            "            item_dict = g\n",
            "        else:\n",
            "            item_dict = g.obj_dict\n",
            "\n",
            "        if key_name not in item_dict:\n",
            "            continue\n",
            "\n",
            "        for key, objs in item_dict[key_name].items():\n",
            "            for obj in objs:\n",
            "                if ('parent_graph' in obj and\n",
            "                        obj['parent_graph'].get_parent_graph()==g):\n",
            "                    if obj['parent_graph'] is g:\n",
            "                        pass\n",
            "                    else:\n",
            "                        obj['parent_graph'].set_parent_graph(parent_graph)\n",
            "\n",
            "                if key_name == 'edges' and len(key) == 2:\n",
            "                    for idx, vertex in enumerate( obj['points'] ):\n",
            "                        if isinstance( vertex,\n",
            "                                      (pydot.Graph,\n",
            "                                       pydot.Subgraph, pydot.Cluster)):\n",
            "                            vertex.set_parent_graph(parent_graph)\n",
            "                        if isinstance( vertex, pydot.frozendict):\n",
            "                            if vertex['parent_graph'] is g:\n",
            "                                pass\n",
            "                            else:\n",
            "                                vertex['parent_graph'].set_parent_graph(\n",
            "                                    parent_graph)\n",
            "\n",
            "\n",
            "\n",
            "def add_defaults(element, defaults):\n",
            "\n",
            "    d = element.__dict__\n",
            "    for key, value in defaults.items():\n",
            "        if not d.get(key):\n",
            "            d[key] = value\n",
            "\n",
            "\n",
            "\n",
            "def add_elements(g, toks, defaults_graph=None,\n",
            "                 defaults_node=None, defaults_edge=None):\n",
            "\n",
            "    if defaults_graph is None:\n",
            "        defaults_graph = {}\n",
            "    if defaults_node is None:\n",
            "        defaults_node = {}\n",
            "    if defaults_edge is None:\n",
            "        defaults_edge = {}\n",
            "\n",
            "    for elm_idx, element in enumerate(toks):\n",
            "\n",
            "        if isinstance(element, (pydot.Subgraph, pydot.Cluster)):\n",
            "\n",
            "            add_defaults(element, defaults_graph)\n",
            "            g.add_subgraph(element)\n",
            "\n",
            "        elif isinstance(element, pydot.Node):\n",
            "\n",
            "            add_defaults(element, defaults_node)\n",
            "            g.add_node(element)\n",
            "\n",
            "        elif isinstance(element, pydot.Edge):\n",
            "\n",
            "            add_defaults(element, defaults_edge)\n",
            "            g.add_edge(element)\n",
            "\n",
            "        elif isinstance(element, ParseResults):\n",
            "\n",
            "            for e in element:\n",
            "                add_elements(g, [e], defaults_graph,\n",
            "                             defaults_node, defaults_edge)\n",
            "\n",
            "        elif isinstance(element, DefaultStatement):\n",
            "\n",
            "            if element.default_type == 'graph':\n",
            "\n",
            "                default_graph_attrs = pydot.Node('graph', **element.attrs)\n",
            "                g.add_node(default_graph_attrs)\n",
            "\n",
            "            elif element.default_type == 'node':\n",
            "\n",
            "                default_node_attrs = pydot.Node('node', **element.attrs)\n",
            "                g.add_node(default_node_attrs)\n",
            "\n",
            "            elif element.default_type == 'edge':\n",
            "\n",
            "                default_edge_attrs = pydot.Node('edge', **element.attrs)\n",
            "                g.add_node(default_edge_attrs)\n",
            "                defaults_edge.update(element.attrs)\n",
            "\n",
            "            else:\n",
            "                raise ValueError(\n",
            "                    'Unknown DefaultStatement: {s}'.format(\n",
            "                         s=element.default_type))\n",
            "\n",
            "        elif isinstance(element, P_AttrList):\n",
            "\n",
            "            g.obj_dict['attributes'].update(element.attrs)\n",
            "\n",
            "        else:\n",
            "            raise ValueError(\n",
            "                'Unknown element statement: {s}'.format(s=element))\n",
            "\n",
            "\n",
            "def push_graph_stmt(str, loc, toks):\n",
            "\n",
            "    g = pydot.Subgraph('')\n",
            "    add_elements(g, toks)\n",
            "    return g\n",
            "\n",
            "\n",
            "def push_subgraph_stmt(str, loc, toks):\n",
            "\n",
            "    g = pydot.Subgraph('')\n",
            "    for e in toks:\n",
            "        if len(e)==3:\n",
            "            e[2].set_name(e[1])\n",
            "            if e[0] == 'subgraph':\n",
            "                e[2].obj_dict['show_keyword'] = True\n",
            "            return e[2]\n",
            "        else:\n",
            "            if e[0] == 'subgraph':\n",
            "                e[1].obj_dict['show_keyword'] = True\n",
            "            return e[1]\n",
            "\n",
            "    return g\n",
            "\n",
            "\n",
            "def push_default_stmt(str, loc, toks):\n",
            "\n",
            "    # The pydot class instances should be marked as\n",
            "    # default statements to be inherited by actual\n",
            "    # graphs, nodes and edges.\n",
            "    #\n",
            "    default_type = toks[0][0]\n",
            "    if len(toks) > 1:\n",
            "        attrs = toks[1].attrs\n",
            "    else:\n",
            "        attrs = {}\n",
            "\n",
            "    if default_type in ['graph', 'node', 'edge']:\n",
            "        return DefaultStatement(default_type, attrs)\n",
            "    else:\n",
            "        raise ValueError(\n",
            "            'Unknown default statement: {s}'.format(s=toks))\n",
            "\n",
            "\n",
            "def push_attr_list(str, loc, toks):\n",
            "\n",
            "    p = P_AttrList(toks)\n",
            "    return p\n",
            "\n",
            "\n",
            "def get_port(node):\n",
            "\n",
            "    if len(node)>1:\n",
            "        if isinstance(node[1], ParseResults):\n",
            "            if len(node[1][0])==2:\n",
            "                if node[1][0][0]==':':\n",
            "                    return node[1][0][1]\n",
            "\n",
            "    return None\n",
            "\n",
            "\n",
            "def do_node_ports(node):\n",
            "\n",
            "    node_port = ''\n",
            "    if len(node) > 1:\n",
            "        node_port = ''.join( [str(a)+str(b) for a,b in node[1] ] )\n",
            "\n",
            "    return node_port\n",
            "\n",
            "\n",
            "def push_edge_stmt(str, loc, toks):\n",
            "\n",
            "    tok_attrs = [a for a in toks if isinstance(a, P_AttrList)]\n",
            "    attrs = {}\n",
            "    for a in tok_attrs:\n",
            "        attrs.update(a.attrs)\n",
            "\n",
            "    e = []\n",
            "\n",
            "    if isinstance(toks[0][0], pydot.Graph):\n",
            "\n",
            "        n_prev = pydot.frozendict(toks[0][0].obj_dict)\n",
            "    else:\n",
            "        n_prev = toks[0][0] + do_node_ports( toks[0] )\n",
            "\n",
            "    if isinstance(toks[2][0], ParseResults):\n",
            "\n",
            "        n_next_list = [[n.get_name(),] for n in toks[2][0] ]\n",
            "        for n_next in [n for n in n_next_list]:\n",
            "            n_next_port = do_node_ports(n_next)\n",
            "            e.append(pydot.Edge(n_prev, n_next[0]+n_next_port, **attrs))\n",
            "\n",
            "    elif isinstance(toks[2][0], pydot.Graph):\n",
            "\n",
            "        e.append(pydot.Edge(n_prev,\n",
            "                            pydot.frozendict(toks[2][0].obj_dict),\n",
            "                            **attrs))\n",
            "\n",
            "    elif isinstance(toks[2][0], pydot.Node):\n",
            "\n",
            "        node = toks[2][0]\n",
            "\n",
            "        if node.get_port() is not None:\n",
            "            name_port = node.get_name() + \":\" + node.get_port()\n",
            "        else:\n",
            "            name_port = node.get_name()\n",
            "\n",
            "        e.append(pydot.Edge(n_prev, name_port, **attrs))\n",
            "\n",
            "    # if the target of this edge is the name of a node\n",
            "    elif isinstance(toks[2][0], str_type):\n",
            "\n",
            "        for n_next in [n for n in tuple(toks)[2::2]]:\n",
            "\n",
            "            if (isinstance(n_next, P_AttrList) or\n",
            "                    not isinstance(n_next[0], str_type)):\n",
            "                continue\n",
            "\n",
            "            n_next_port = do_node_ports( n_next )\n",
            "            e.append(pydot.Edge(n_prev, n_next[0]+n_next_port, **attrs))\n",
            "\n",
            "            n_prev = n_next[0]+n_next_port\n",
            "    else:\n",
            "        raise Exception(\n",
            "            'Edge target {r} with type {s} unsupported.'.format(\n",
            "                r=toks[2][0], s=type(toks[2][0])))\n",
            "\n",
            "    return e\n",
            "\n",
            "\n",
            "\n",
            "def push_node_stmt(s, loc, toks):\n",
            "\n",
            "    if len(toks) == 2:\n",
            "        attrs = toks[1].attrs\n",
            "    else:\n",
            "        attrs = {}\n",
            "\n",
            "    node_name = toks[0]\n",
            "    if isinstance(node_name, list) or isinstance(node_name, tuple):\n",
            "        if len(node_name)>0:\n",
            "            node_name = node_name[0]\n",
            "\n",
            "    n = pydot.Node(str(node_name), **attrs)\n",
            "    return n\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "graphparser = None\n",
            "\n",
            "def graph_definition():\n",
            "\n",
            "    global graphparser\n",
            "\n",
            "    if not graphparser:\n",
            "\n",
            "        # punctuation\n",
            "        colon  = Literal(\":\")\n",
            "        lbrace = Literal(\"{\")\n",
            "        rbrace = Literal(\"}\")\n",
            "        lbrack = Literal(\"[\")\n",
            "        rbrack = Literal(\"]\")\n",
            "        lparen = Literal(\"(\")\n",
            "        rparen = Literal(\")\")\n",
            "        equals = Literal(\"=\")\n",
            "        comma  = Literal(\",\")\n",
            "        dot    = Literal(\".\")\n",
            "        slash  = Literal(\"/\")\n",
            "        bslash = Literal(\"\\\\\")\n",
            "        star   = Literal(\"*\")\n",
            "        semi   = Literal(\";\")\n",
            "        at     = Literal(\"@\")\n",
            "        minus  = Literal(\"-\")\n",
            "\n",
            "        # keywords\n",
            "        strict_    = CaselessLiteral(\"strict\")\n",
            "        graph_     = CaselessLiteral(\"graph\")\n",
            "        digraph_   = CaselessLiteral(\"digraph\")\n",
            "        subgraph_  = CaselessLiteral(\"subgraph\")\n",
            "        node_      = CaselessLiteral(\"node\")\n",
            "        edge_      = CaselessLiteral(\"edge\")\n",
            "\n",
            "\n",
            "        # token definitions\n",
            "\n",
            "        identifier = Word(alphanums + \"_.\" ).setName(\"identifier\")\n",
            "\n",
            "        double_quoted_string = QuotedString(\n",
            "            '\"', multiline=True, unquoteResults=False, escChar='\\\\')  # dblQuotedString\n",
            "\n",
            "        noncomma = \"\".join([c for c in printables if c != \",\"])\n",
            "        alphastring_ = OneOrMore(CharsNotIn(noncomma + ' '))\n",
            "\n",
            "        def parse_html(s, loc, toks):\n",
            "            return '<%s>' % ''.join(toks[0])\n",
            "\n",
            "\n",
            "        opener = '<'\n",
            "        closer = '>'\n",
            "        html_text = nestedExpr( opener, closer,\n",
            "            ( CharsNotIn( opener + closer )  )\n",
            "                ).setParseAction(parse_html).leaveWhitespace()\n",
            "\n",
            "        ID = ( identifier | html_text |\n",
            "            double_quoted_string | #.setParseAction(strip_quotes) |\n",
            "            alphastring_ ).setName(\"ID\")\n",
            "\n",
            "\n",
            "        float_number = Combine(Optional(minus) +\n",
            "            OneOrMore(Word(nums + \".\"))).setName(\"float_number\")\n",
            "\n",
            "        righthand_id =  (float_number | ID ).setName(\"righthand_id\")\n",
            "\n",
            "        port_angle = (at + ID).setName(\"port_angle\")\n",
            "\n",
            "        port_location = (OneOrMore(Group(colon + ID)) |\n",
            "            Group(colon + lparen +\n",
            "                  ID + comma + ID + rparen)).setName(\"port_location\")\n",
            "\n",
            "        port = (Group(port_location + Optional(port_angle)) |\n",
            "            Group(port_angle + Optional(port_location))).setName(\"port\")\n",
            "\n",
            "        node_id = (ID + Optional(port))\n",
            "        a_list = OneOrMore(ID + Optional(equals + righthand_id) +\n",
            "            Optional(comma.suppress())).setName(\"a_list\")\n",
            "\n",
            "        attr_list = OneOrMore(lbrack.suppress() + Optional(a_list) +\n",
            "            rbrack.suppress()).setName(\"attr_list\")\n",
            "\n",
            "        attr_stmt = (Group(graph_ | node_ | edge_) +\n",
            "                     attr_list).setName(\"attr_stmt\")\n",
            "\n",
            "        edgeop = (Literal(\"--\") | Literal(\"->\")).setName(\"edgeop\")\n",
            "\n",
            "        stmt_list = Forward()\n",
            "        graph_stmt = Group(lbrace.suppress() + Optional(stmt_list) +\n",
            "            rbrace.suppress() +\n",
            "            Optional(semi.suppress())).setName(\"graph_stmt\")\n",
            "\n",
            "\n",
            "        edge_point = Forward()\n",
            "\n",
            "        edgeRHS = OneOrMore(edgeop + edge_point)\n",
            "        edge_stmt = edge_point + edgeRHS + Optional(attr_list)\n",
            "\n",
            "        subgraph = Group(\n",
            "            subgraph_ + Optional(ID) + graph_stmt).setName(\"subgraph\")\n",
            "\n",
            "        edge_point << Group(\n",
            "            subgraph | graph_stmt | node_id).setName('edge_point')\n",
            "\n",
            "        node_stmt = (\n",
            "            node_id + Optional(attr_list) +\n",
            "            Optional(semi.suppress())).setName(\"node_stmt\")\n",
            "\n",
            "        assignment = (ID + equals + righthand_id).setName(\"assignment\")\n",
            "        stmt = (assignment | edge_stmt | attr_stmt |\n",
            "                subgraph | graph_stmt | node_stmt).setName(\"stmt\")\n",
            "        stmt_list << OneOrMore(stmt + Optional(semi.suppress()))\n",
            "\n",
            "        graphparser = OneOrMore(\n",
            "            (Optional(strict_) + Group((graph_ | digraph_)) +\n",
            "             Optional(ID) + graph_stmt).setResultsName(\"graph\"))\n",
            "\n",
            "        singleLineComment = Group(\n",
            "            \"//\" + restOfLine) | Group(\"#\" + restOfLine)\n",
            "\n",
            "\n",
            "        # actions\n",
            "\n",
            "        graphparser.ignore(singleLineComment)\n",
            "        graphparser.ignore(cStyleComment)\n",
            "\n",
            "        assignment.setParseAction(push_attr_list)\n",
            "        a_list.setParseAction(push_attr_list)\n",
            "        edge_stmt.setParseAction(push_edge_stmt)\n",
            "        node_stmt.setParseAction(push_node_stmt)\n",
            "        attr_stmt.setParseAction(push_default_stmt)\n",
            "\n",
            "        subgraph.setParseAction(push_subgraph_stmt)\n",
            "        graph_stmt.setParseAction(push_graph_stmt)\n",
            "        graphparser.setParseAction(push_top_graph_stmt)\n",
            "\n",
            "\n",
            "    return graphparser\n",
            "\n",
            "\n",
            "def parse_dot_data(s):\n",
            "    \"\"\"Parse DOT description in (unicode) string `s`.\n",
            "\n",
            "    @return: Graphs that result from parsing.\n",
            "    @rtype: `list` of `pydot.Dot`\n",
            "    \"\"\"\n",
            "    global top_graphs\n",
            "    top_graphs = list()\n",
            "    try:\n",
            "        graphparser = graph_definition()\n",
            "        graphparser.parseWithTabs()\n",
            "        tokens = graphparser.parseString(s)\n",
            "        return list(tokens)\n",
            "    except ParseException as err:\n",
            "        print(\n",
            "            err.line +\n",
            "            \" \"*(err.column-1) + \"^\" +\n",
            "            err)\n",
            "        return None\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright (c) 2010-2018 Benjamin Peterson\n",
            "#\n",
            "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
            "# of this software and associated documentation files (the \"Software\"), to deal\n",
            "# in the Software without restriction, including without limitation the rights\n",
            "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
            "# copies of the Software, and to permit persons to whom the Software is\n",
            "# furnished to do so, subject to the following conditions:\n",
            "#\n",
            "# The above copyright notice and this permission notice shall be included in all\n",
            "# copies or substantial portions of the Software.\n",
            "#\n",
            "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
            "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
            "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
            "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
            "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
            "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
            "# SOFTWARE.\n",
            "\n",
            "\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "\n",
            "import functools\n",
            "import itertools\n",
            "import operator\n",
            "import sys\n",
            "import types\n",
            "\n",
            "__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n",
            "__version__ = \"1.12.0\"\n",
            "\n",
            "\n",
            "# Useful for very coarse version differentiation.\n",
            "PY2 = sys.version_info[0] == 2\n",
            "PY3 = sys.version_info[0] == 3\n",
            "PY34 = sys.version_info[0:2] >= (3, 4)\n",
            "\n",
            "if PY3:\n",
            "    string_types = str,\n",
            "    integer_types = int,\n",
            "    class_types = type,\n",
            "    text_type = str\n",
            "    binary_type = bytes\n",
            "\n",
            "    MAXSIZE = sys.maxsize\n",
            "else:\n",
            "    string_types = basestring,\n",
            "    integer_types = (int, long)\n",
            "    class_types = (type, types.ClassType)\n",
            "    text_type = unicode\n",
            "    binary_type = str\n",
            "\n",
            "    if sys.platform.startswith(\"java\"):\n",
            "        # Jython always uses 32 bits.\n",
            "        MAXSIZE = int((1 << 31) - 1)\n",
            "    else:\n",
            "        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n",
            "        class X(object):\n",
            "\n",
            "            def __len__(self):\n",
            "                return 1 << 31\n",
            "        try:\n",
            "            len(X())\n",
            "        except OverflowError:\n",
            "            # 32-bit\n",
            "            MAXSIZE = int((1 << 31) - 1)\n",
            "        else:\n",
            "            # 64-bit\n",
            "            MAXSIZE = int((1 << 63) - 1)\n",
            "        del X\n",
            "\n",
            "\n",
            "def _add_doc(func, doc):\n",
            "    \"\"\"Add documentation to a function.\"\"\"\n",
            "    func.__doc__ = doc\n",
            "\n",
            "\n",
            "def _import_module(name):\n",
            "    \"\"\"Import module, returning the module after the last dot.\"\"\"\n",
            "    __import__(name)\n",
            "    return sys.modules[name]\n",
            "\n",
            "\n",
            "class _LazyDescr(object):\n",
            "\n",
            "    def __init__(self, name):\n",
            "        self.name = name\n",
            "\n",
            "    def __get__(self, obj, tp):\n",
            "        result = self._resolve()\n",
            "        setattr(obj, self.name, result)  # Invokes __set__.\n",
            "        try:\n",
            "            # This is a bit ugly, but it avoids running this again by\n",
            "            # removing this descriptor.\n",
            "            delattr(obj.__class__, self.name)\n",
            "        except AttributeError:\n",
            "            pass\n",
            "        return result\n",
            "\n",
            "\n",
            "class MovedModule(_LazyDescr):\n",
            "\n",
            "    def __init__(self, name, old, new=None):\n",
            "        super(MovedModule, self).__init__(name)\n",
            "        if PY3:\n",
            "            if new is None:\n",
            "                new = name\n",
            "            self.mod = new\n",
            "        else:\n",
            "            self.mod = old\n",
            "\n",
            "    def _resolve(self):\n",
            "        return _import_module(self.mod)\n",
            "\n",
            "    def __getattr__(self, attr):\n",
            "        _module = self._resolve()\n",
            "        value = getattr(_module, attr)\n",
            "        setattr(self, attr, value)\n",
            "        return value\n",
            "\n",
            "\n",
            "class _LazyModule(types.ModuleType):\n",
            "\n",
            "    def __init__(self, name):\n",
            "        super(_LazyModule, self).__init__(name)\n",
            "        self.__doc__ = self.__class__.__doc__\n",
            "\n",
            "    def __dir__(self):\n",
            "        attrs = [\"__doc__\", \"__name__\"]\n",
            "        attrs += [attr.name for attr in self._moved_attributes]\n",
            "        return attrs\n",
            "\n",
            "    # Subclasses should override this\n",
            "    _moved_attributes = []\n",
            "\n",
            "\n",
            "class MovedAttribute(_LazyDescr):\n",
            "\n",
            "    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n",
            "        super(MovedAttribute, self).__init__(name)\n",
            "        if PY3:\n",
            "            if new_mod is None:\n",
            "                new_mod = name\n",
            "            self.mod = new_mod\n",
            "            if new_attr is None:\n",
            "                if old_attr is None:\n",
            "                    new_attr = name\n",
            "                else:\n",
            "                    new_attr = old_attr\n",
            "            self.attr = new_attr\n",
            "        else:\n",
            "            self.mod = old_mod\n",
            "            if old_attr is None:\n",
            "                old_attr = name\n",
            "            self.attr = old_attr\n",
            "\n",
            "    def _resolve(self):\n",
            "        module = _import_module(self.mod)\n",
            "        return getattr(module, self.attr)\n",
            "\n",
            "\n",
            "class _SixMetaPathImporter(object):\n",
            "\n",
            "    \"\"\"\n",
            "    A meta path importer to import six.moves and its submodules.\n",
            "\n",
            "    This class implements a PEP302 finder and loader. It should be compatible\n",
            "    with Python 2.5 and all existing versions of Python3\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, six_module_name):\n",
            "        self.name = six_module_name\n",
            "        self.known_modules = {}\n",
            "\n",
            "    def _add_module(self, mod, *fullnames):\n",
            "        for fullname in fullnames:\n",
            "            self.known_modules[self.name + \".\" + fullname] = mod\n",
            "\n",
            "    def _get_module(self, fullname):\n",
            "        return self.known_modules[self.name + \".\" + fullname]\n",
            "\n",
            "    def find_module(self, fullname, path=None):\n",
            "        if fullname in self.known_modules:\n",
            "            return self\n",
            "        return None\n",
            "\n",
            "    def __get_module(self, fullname):\n",
            "        try:\n",
            "            return self.known_modules[fullname]\n",
            "        except KeyError:\n",
            "            raise ImportError(\"This loader does not know module \" + fullname)\n",
            "\n",
            "    def load_module(self, fullname):\n",
            "        try:\n",
            "            # in case of a reload\n",
            "            return sys.modules[fullname]\n",
            "        except KeyError:\n",
            "            pass\n",
            "        mod = self.__get_module(fullname)\n",
            "        if isinstance(mod, MovedModule):\n",
            "            mod = mod._resolve()\n",
            "        else:\n",
            "            mod.__loader__ = self\n",
            "        sys.modules[fullname] = mod\n",
            "        return mod\n",
            "\n",
            "    def is_package(self, fullname):\n",
            "        \"\"\"\n",
            "        Return true, if the named module is a package.\n",
            "\n",
            "        We need this method to get correct spec objects with\n",
            "        Python 3.4 (see PEP451)\n",
            "        \"\"\"\n",
            "        return hasattr(self.__get_module(fullname), \"__path__\")\n",
            "\n",
            "    def get_code(self, fullname):\n",
            "        \"\"\"Return None\n",
            "\n",
            "        Required, if is_package is implemented\"\"\"\n",
            "        self.__get_module(fullname)  # eventually raises ImportError\n",
            "        return None\n",
            "    get_source = get_code  # same as get_code\n",
            "\n",
            "_importer = _SixMetaPathImporter(__name__)\n",
            "\n",
            "\n",
            "class _MovedItems(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects\"\"\"\n",
            "    __path__ = []  # mark as package\n",
            "\n",
            "\n",
            "_moved_attributes = [\n",
            "    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n",
            "    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n",
            "    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n",
            "    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n",
            "    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n",
            "    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n",
            "    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n",
            "    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n",
            "    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\n",
            "    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n",
            "    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n",
            "    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n",
            "    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n",
            "    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n",
            "    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n",
            "    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n",
            "    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n",
            "    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n",
            "    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n",
            "    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n",
            "    MovedModule(\"builtins\", \"__builtin__\"),\n",
            "    MovedModule(\"configparser\", \"ConfigParser\"),\n",
            "    MovedModule(\"copyreg\", \"copy_reg\"),\n",
            "    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n",
            "    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\"),\n",
            "    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n",
            "    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n",
            "    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n",
            "    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n",
            "    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n",
            "    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n",
            "    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\n",
            "    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n",
            "    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n",
            "    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n",
            "    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n",
            "    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n",
            "    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n",
            "    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n",
            "    MovedModule(\"queue\", \"Queue\"),\n",
            "    MovedModule(\"reprlib\", \"repr\"),\n",
            "    MovedModule(\"socketserver\", \"SocketServer\"),\n",
            "    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n",
            "    MovedModule(\"tkinter\", \"Tkinter\"),\n",
            "    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n",
            "    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n",
            "    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n",
            "    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n",
            "    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n",
            "    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n",
            "    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n",
            "    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n",
            "    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n",
            "                \"tkinter.colorchooser\"),\n",
            "    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n",
            "                \"tkinter.commondialog\"),\n",
            "    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n",
            "    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n",
            "    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n",
            "    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n",
            "                \"tkinter.simpledialog\"),\n",
            "    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n",
            "    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n",
            "    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n",
            "    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n",
            "    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n",
            "    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n",
            "]\n",
            "# Add windows specific modules.\n",
            "if sys.platform == \"win32\":\n",
            "    _moved_attributes += [\n",
            "        MovedModule(\"winreg\", \"_winreg\"),\n",
            "    ]\n",
            "\n",
            "for attr in _moved_attributes:\n",
            "    setattr(_MovedItems, attr.name, attr)\n",
            "    if isinstance(attr, MovedModule):\n",
            "        _importer._add_module(attr, \"moves.\" + attr.name)\n",
            "del attr\n",
            "\n",
            "_MovedItems._moved_attributes = _moved_attributes\n",
            "\n",
            "moves = _MovedItems(__name__ + \".moves\")\n",
            "_importer._add_module(moves, \"moves\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib_parse(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n",
            "\n",
            "\n",
            "_urllib_parse_moved_attributes = [\n",
            "    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"),\n",
            "    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n",
            "    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n",
            "]\n",
            "for attr in _urllib_parse_moved_attributes:\n",
            "    setattr(Module_six_moves_urllib_parse, attr.name, attr)\n",
            "del attr\n",
            "\n",
            "Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n",
            "                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib_error(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n",
            "\n",
            "\n",
            "_urllib_error_moved_attributes = [\n",
            "    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n",
            "    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n",
            "    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n",
            "]\n",
            "for attr in _urllib_error_moved_attributes:\n",
            "    setattr(Module_six_moves_urllib_error, attr.name, attr)\n",
            "del attr\n",
            "\n",
            "Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n",
            "                      \"moves.urllib_error\", \"moves.urllib.error\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib_request(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n",
            "\n",
            "\n",
            "_urllib_request_moved_attributes = [\n",
            "    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n",
            "    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\n",
            "    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\n",
            "]\n",
            "for attr in _urllib_request_moved_attributes:\n",
            "    setattr(Module_six_moves_urllib_request, attr.name, attr)\n",
            "del attr\n",
            "\n",
            "Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n",
            "                      \"moves.urllib_request\", \"moves.urllib.request\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib_response(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n",
            "\n",
            "\n",
            "_urllib_response_moved_attributes = [\n",
            "    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n",
            "    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n",
            "    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n",
            "    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n",
            "]\n",
            "for attr in _urllib_response_moved_attributes:\n",
            "    setattr(Module_six_moves_urllib_response, attr.name, attr)\n",
            "del attr\n",
            "\n",
            "Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n",
            "                      \"moves.urllib_response\", \"moves.urllib.response\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib_robotparser(_LazyModule):\n",
            "\n",
            "    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n",
            "\n",
            "\n",
            "_urllib_robotparser_moved_attributes = [\n",
            "    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n",
            "]\n",
            "for attr in _urllib_robotparser_moved_attributes:\n",
            "    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\n",
            "del attr\n",
            "\n",
            "Module_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n",
            "                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n",
            "\n",
            "\n",
            "class Module_six_moves_urllib(types.ModuleType):\n",
            "\n",
            "    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n",
            "    __path__ = []  # mark as package\n",
            "    parse = _importer._get_module(\"moves.urllib_parse\")\n",
            "    error = _importer._get_module(\"moves.urllib_error\")\n",
            "    request = _importer._get_module(\"moves.urllib_request\")\n",
            "    response = _importer._get_module(\"moves.urllib_response\")\n",
            "    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n",
            "\n",
            "    def __dir__(self):\n",
            "        return ['parse', 'error', 'request', 'response', 'robotparser']\n",
            "\n",
            "_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n",
            "                      \"moves.urllib\")\n",
            "\n",
            "\n",
            "def add_move(move):\n",
            "    \"\"\"Add an item to six.moves.\"\"\"\n",
            "    setattr(_MovedItems, move.name, move)\n",
            "\n",
            "\n",
            "def remove_move(name):\n",
            "    \"\"\"Remove item from six.moves.\"\"\"\n",
            "    try:\n",
            "        delattr(_MovedItems, name)\n",
            "    except AttributeError:\n",
            "        try:\n",
            "            del moves.__dict__[name]\n",
            "        except KeyError:\n",
            "            raise AttributeError(\"no such move, %r\" % (name,))\n",
            "\n",
            "\n",
            "if PY3:\n",
            "    _meth_func = \"__func__\"\n",
            "    _meth_self = \"__self__\"\n",
            "\n",
            "    _func_closure = \"__closure__\"\n",
            "    _func_code = \"__code__\"\n",
            "    _func_defaults = \"__defaults__\"\n",
            "    _func_globals = \"__globals__\"\n",
            "else:\n",
            "    _meth_func = \"im_func\"\n",
            "    _meth_self = \"im_self\"\n",
            "\n",
            "    _func_closure = \"func_closure\"\n",
            "    _func_code = \"func_code\"\n",
            "    _func_defaults = \"func_defaults\"\n",
            "    _func_globals = \"func_globals\"\n",
            "\n",
            "\n",
            "try:\n",
            "    advance_iterator = next\n",
            "except NameError:\n",
            "    def advance_iterator(it):\n",
            "        return it.next()\n",
            "next = advance_iterator\n",
            "\n",
            "\n",
            "try:\n",
            "    callable = callable\n",
            "except NameError:\n",
            "    def callable(obj):\n",
            "        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n",
            "\n",
            "\n",
            "if PY3:\n",
            "    def get_unbound_function(unbound):\n",
            "        return unbound\n",
            "\n",
            "    create_bound_method = types.MethodType\n",
            "\n",
            "    def create_unbound_method(func, cls):\n",
            "        return func\n",
            "\n",
            "    Iterator = object\n",
            "else:\n",
            "    def get_unbound_function(unbound):\n",
            "        return unbound.im_func\n",
            "\n",
            "    def create_bound_method(func, obj):\n",
            "        return types.MethodType(func, obj, obj.__class__)\n",
            "\n",
            "    def create_unbound_method(func, cls):\n",
            "        return types.MethodType(func, None, cls)\n",
            "\n",
            "    class Iterator(object):\n",
            "\n",
            "        def next(self):\n",
            "            return type(self).__next__(self)\n",
            "\n",
            "    callable = callable\n",
            "_add_doc(get_unbound_function,\n",
            "         \"\"\"Get the function out of a possibly unbound function\"\"\")\n",
            "\n",
            "\n",
            "get_method_function = operator.attrgetter(_meth_func)\n",
            "get_method_self = operator.attrgetter(_meth_self)\n",
            "get_function_closure = operator.attrgetter(_func_closure)\n",
            "get_function_code = operator.attrgetter(_func_code)\n",
            "get_function_defaults = operator.attrgetter(_func_defaults)\n",
            "get_function_globals = operator.attrgetter(_func_globals)\n",
            "\n",
            "\n",
            "if PY3:\n",
            "    def iterkeys(d, **kw):\n",
            "        return iter(d.keys(**kw))\n",
            "\n",
            "    def itervalues(d, **kw):\n",
            "        return iter(d.values(**kw))\n",
            "\n",
            "    def iteritems(d, **kw):\n",
            "        return iter(d.items(**kw))\n",
            "\n",
            "    def iterlists(d, **kw):\n",
            "        return iter(d.lists(**kw))\n",
            "\n",
            "    viewkeys = operator.methodcaller(\"keys\")\n",
            "\n",
            "    viewvalues = operator.methodcaller(\"values\")\n",
            "\n",
            "    viewitems = operator.methodcaller(\"items\")\n",
            "else:\n",
            "    def iterkeys(d, **kw):\n",
            "        return d.iterkeys(**kw)\n",
            "\n",
            "    def itervalues(d, **kw):\n",
            "        return d.itervalues(**kw)\n",
            "\n",
            "    def iteritems(d, **kw):\n",
            "        return d.iteritems(**kw)\n",
            "\n",
            "    def iterlists(d, **kw):\n",
            "        return d.iterlists(**kw)\n",
            "\n",
            "    viewkeys = operator.methodcaller(\"viewkeys\")\n",
            "\n",
            "    viewvalues = operator.methodcaller(\"viewvalues\")\n",
            "\n",
            "    viewitems = operator.methodcaller(\"viewitems\")\n",
            "\n",
            "_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n",
            "_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n",
            "_add_doc(iteritems,\n",
            "         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n",
            "_add_doc(iterlists,\n",
            "         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n",
            "\n",
            "\n",
            "if PY3:\n",
            "    def b(s):\n",
            "        return s.encode(\"latin-1\")\n",
            "\n",
            "    def u(s):\n",
            "        return s\n",
            "    unichr = chr\n",
            "    import struct\n",
            "    int2byte = struct.Struct(\">B\").pack\n",
            "    del struct\n",
            "    byte2int = operator.itemgetter(0)\n",
            "    indexbytes = operator.getitem\n",
            "    iterbytes = iter\n",
            "    import io\n",
            "    StringIO = io.StringIO\n",
            "    BytesIO = io.BytesIO\n",
            "    _assertCountEqual = \"assertCountEqual\"\n",
            "    if sys.version_info[1] <= 1:\n",
            "        _assertRaisesRegex = \"assertRaisesRegexp\"\n",
            "        _assertRegex = \"assertRegexpMatches\"\n",
            "    else:\n",
            "        _assertRaisesRegex = \"assertRaisesRegex\"\n",
            "        _assertRegex = \"assertRegex\"\n",
            "else:\n",
            "    def b(s):\n",
            "        return s\n",
            "    # Workaround for standalone backslash\n",
            "\n",
            "    def u(s):\n",
            "        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n",
            "    unichr = unichr\n",
            "    int2byte = chr\n",
            "\n",
            "    def byte2int(bs):\n",
            "        return ord(bs[0])\n",
            "\n",
            "    def indexbytes(buf, i):\n",
            "        return ord(buf[i])\n",
            "    iterbytes = functools.partial(itertools.imap, ord)\n",
            "    import StringIO\n",
            "    StringIO = BytesIO = StringIO.StringIO\n",
            "    _assertCountEqual = \"assertItemsEqual\"\n",
            "    _assertRaisesRegex = \"assertRaisesRegexp\"\n",
            "    _assertRegex = \"assertRegexpMatches\"\n",
            "_add_doc(b, \"\"\"Byte literal\"\"\")\n",
            "_add_doc(u, \"\"\"Text literal\"\"\")\n",
            "\n",
            "\n",
            "def assertCountEqual(self, *args, **kwargs):\n",
            "    return getattr(self, _assertCountEqual)(*args, **kwargs)\n",
            "\n",
            "\n",
            "def assertRaisesRegex(self, *args, **kwargs):\n",
            "    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n",
            "\n",
            "\n",
            "def assertRegex(self, *args, **kwargs):\n",
            "    return getattr(self, _assertRegex)(*args, **kwargs)\n",
            "\n",
            "\n",
            "if PY3:\n",
            "    exec_ = getattr(moves.builtins, \"exec\")\n",
            "\n",
            "    def reraise(tp, value, tb=None):\n",
            "        try:\n",
            "            if value is None:\n",
            "                value = tp()\n",
            "            if value.__traceback__ is not tb:\n",
            "                raise value.with_traceback(tb)\n",
            "            raise value\n",
            "        finally:\n",
            "            value = None\n",
            "            tb = None\n",
            "\n",
            "else:\n",
            "    def exec_(_code_, _globs_=None, _locs_=None):\n",
            "        \"\"\"Execute code in a namespace.\"\"\"\n",
            "        if _globs_ is None:\n",
            "            frame = sys._getframe(1)\n",
            "            _globs_ = frame.f_globals\n",
            "            if _locs_ is None:\n",
            "                _locs_ = frame.f_locals\n",
            "            del frame\n",
            "        elif _locs_ is None:\n",
            "            _locs_ = _globs_\n",
            "        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n",
            "\n",
            "    exec_(\"\"\"def reraise(tp, value, tb=None):\n",
            "    try:\n",
            "        raise tp, value, tb\n",
            "    finally:\n",
            "        tb = None\n",
            "\"\"\")\n",
            "\n",
            "\n",
            "if sys.version_info[:2] == (3, 2):\n",
            "    exec_(\"\"\"def raise_from(value, from_value):\n",
            "    try:\n",
            "        if from_value is None:\n",
            "            raise value\n",
            "        raise value from from_value\n",
            "    finally:\n",
            "        value = None\n",
            "\"\"\")\n",
            "elif sys.version_info[:2] > (3, 2):\n",
            "    exec_(\"\"\"def raise_from(value, from_value):\n",
            "    try:\n",
            "        raise value from from_value\n",
            "    finally:\n",
            "        value = None\n",
            "\"\"\")\n",
            "else:\n",
            "    def raise_from(value, from_value):\n",
            "        raise value\n",
            "\n",
            "\n",
            "print_ = getattr(moves.builtins, \"print\", None)\n",
            "if print_ is None:\n",
            "    def print_(*args, **kwargs):\n",
            "        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n",
            "        fp = kwargs.pop(\"file\", sys.stdout)\n",
            "        if fp is None:\n",
            "            return\n",
            "\n",
            "        def write(data):\n",
            "            if not isinstance(data, basestring):\n",
            "                data = str(data)\n",
            "            # If the file has an encoding, encode unicode with it.\n",
            "            if (isinstance(fp, file) and\n",
            "                    isinstance(data, unicode) and\n",
            "                    fp.encoding is not None):\n",
            "                errors = getattr(fp, \"errors\", None)\n",
            "                if errors is None:\n",
            "                    errors = \"strict\"\n",
            "                data = data.encode(fp.encoding, errors)\n",
            "            fp.write(data)\n",
            "        want_unicode = False\n",
            "        sep = kwargs.pop(\"sep\", None)\n",
            "        if sep is not None:\n",
            "            if isinstance(sep, unicode):\n",
            "                want_unicode = True\n",
            "            elif not isinstance(sep, str):\n",
            "                raise TypeError(\"sep must be None or a string\")\n",
            "        end = kwargs.pop(\"end\", None)\n",
            "        if end is not None:\n",
            "            if isinstance(end, unicode):\n",
            "                want_unicode = True\n",
            "            elif not isinstance(end, str):\n",
            "                raise TypeError(\"end must be None or a string\")\n",
            "        if kwargs:\n",
            "            raise TypeError(\"invalid keyword arguments to print()\")\n",
            "        if not want_unicode:\n",
            "            for arg in args:\n",
            "                if isinstance(arg, unicode):\n",
            "                    want_unicode = True\n",
            "                    break\n",
            "        if want_unicode:\n",
            "            newline = unicode(\"\\n\")\n",
            "            space = unicode(\" \")\n",
            "        else:\n",
            "            newline = \"\\n\"\n",
            "            space = \" \"\n",
            "        if sep is None:\n",
            "            sep = space\n",
            "        if end is None:\n",
            "            end = newline\n",
            "        for i, arg in enumerate(args):\n",
            "            if i:\n",
            "                write(sep)\n",
            "            write(arg)\n",
            "        write(end)\n",
            "if sys.version_info[:2] < (3, 3):\n",
            "    _print = print_\n",
            "\n",
            "    def print_(*args, **kwargs):\n",
            "        fp = kwargs.get(\"file\", sys.stdout)\n",
            "        flush = kwargs.pop(\"flush\", False)\n",
            "        _print(*args, **kwargs)\n",
            "        if flush and fp is not None:\n",
            "            fp.flush()\n",
            "\n",
            "_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n",
            "\n",
            "if sys.version_info[0:2] < (3, 4):\n",
            "    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n",
            "              updated=functools.WRAPPER_UPDATES):\n",
            "        def wrapper(f):\n",
            "            f = functools.wraps(wrapped, assigned, updated)(f)\n",
            "            f.__wrapped__ = wrapped\n",
            "            return f\n",
            "        return wrapper\n",
            "else:\n",
            "    wraps = functools.wraps\n",
            "\n",
            "\n",
            "def with_metaclass(meta, *bases):\n",
            "    \"\"\"Create a base class with a metaclass.\"\"\"\n",
            "    # This requires a bit of explanation: the basic idea is to make a dummy\n",
            "    # metaclass for one level of class instantiation that replaces itself with\n",
            "    # the actual metaclass.\n",
            "    class metaclass(type):\n",
            "\n",
            "        def __new__(cls, name, this_bases, d):\n",
            "            return meta(name, bases, d)\n",
            "\n",
            "        @classmethod\n",
            "        def __prepare__(cls, name, this_bases):\n",
            "            return meta.__prepare__(name, bases)\n",
            "    return type.__new__(metaclass, 'temporary_class', (), {})\n",
            "\n",
            "\n",
            "def add_metaclass(metaclass):\n",
            "    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n",
            "    def wrapper(cls):\n",
            "        orig_vars = cls.__dict__.copy()\n",
            "        slots = orig_vars.get('__slots__')\n",
            "        if slots is not None:\n",
            "            if isinstance(slots, str):\n",
            "                slots = [slots]\n",
            "            for slots_var in slots:\n",
            "                orig_vars.pop(slots_var)\n",
            "        orig_vars.pop('__dict__', None)\n",
            "        orig_vars.pop('__weakref__', None)\n",
            "        if hasattr(cls, '__qualname__'):\n",
            "            orig_vars['__qualname__'] = cls.__qualname__\n",
            "        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n",
            "    return wrapper\n",
            "\n",
            "\n",
            "def ensure_binary(s, encoding='utf-8', errors='strict'):\n",
            "    \"\"\"Coerce **s** to six.binary_type.\n",
            "\n",
            "    For Python 2:\n",
            "      - `unicode` -> encoded to `str`\n",
            "      - `str` -> `str`\n",
            "\n",
            "    For Python 3:\n",
            "      - `str` -> encoded to `bytes`\n",
            "      - `bytes` -> `bytes`\n",
            "    \"\"\"\n",
            "    if isinstance(s, text_type):\n",
            "        return s.encode(encoding, errors)\n",
            "    elif isinstance(s, binary_type):\n",
            "        return s\n",
            "    else:\n",
            "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
            "\n",
            "\n",
            "def ensure_str(s, encoding='utf-8', errors='strict'):\n",
            "    \"\"\"Coerce *s* to `str`.\n",
            "\n",
            "    For Python 2:\n",
            "      - `unicode` -> encoded to `str`\n",
            "      - `str` -> `str`\n",
            "\n",
            "    For Python 3:\n",
            "      - `str` -> `str`\n",
            "      - `bytes` -> decoded to `str`\n",
            "    \"\"\"\n",
            "    if not isinstance(s, (text_type, binary_type)):\n",
            "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
            "    if PY2 and isinstance(s, text_type):\n",
            "        s = s.encode(encoding, errors)\n",
            "    elif PY3 and isinstance(s, binary_type):\n",
            "        s = s.decode(encoding, errors)\n",
            "    return s\n",
            "\n",
            "\n",
            "def ensure_text(s, encoding='utf-8', errors='strict'):\n",
            "    \"\"\"Coerce *s* to six.text_type.\n",
            "\n",
            "    For Python 2:\n",
            "      - `unicode` -> `unicode`\n",
            "      - `str` -> `unicode`\n",
            "\n",
            "    For Python 3:\n",
            "      - `str` -> `str`\n",
            "      - `bytes` -> decoded to `str`\n",
            "    \"\"\"\n",
            "    if isinstance(s, binary_type):\n",
            "        return s.decode(encoding, errors)\n",
            "    elif isinstance(s, text_type):\n",
            "        return s\n",
            "    else:\n",
            "        raise TypeError(\"not expecting type '%s'\" % type(s))\n",
            "\n",
            "\n",
            "\n",
            "def python_2_unicode_compatible(klass):\n",
            "    \"\"\"\n",
            "    A decorator that defines __unicode__ and __str__ methods under Python 2.\n",
            "    Under Python 3 it does nothing.\n",
            "\n",
            "    To support Python 2 and 3 with a single code base, define a __str__ method\n",
            "    returning text and apply this decorator to the class.\n",
            "    \"\"\"\n",
            "    if PY2:\n",
            "        if '__str__' not in klass.__dict__:\n",
            "            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n",
            "                             \"to %s because it doesn't define __str__().\" %\n",
            "                             klass.__name__)\n",
            "        klass.__unicode__ = klass.__str__\n",
            "        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n",
            "    return klass\n",
            "\n",
            "\n",
            "# Complete the moves implementation.\n",
            "# This code is at the end of this module to speed up module loading.\n",
            "# Turn this module into a package.\n",
            "__path__ = []  # required for PEP 302 and PEP 451\n",
            "__package__ = __name__  # see PEP 366 @ReservedAssignment\n",
            "if globals().get(\"__spec__\") is not None:\n",
            "    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n",
            "# Remove other six meta path importers, since they cause problems. This can\n",
            "# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n",
            "# this for some reason.)\n",
            "if sys.meta_path:\n",
            "    for i, importer in enumerate(sys.meta_path):\n",
            "        # Here's some real nastiness: Another \"instance\" of the six module might\n",
            "        # be floating around. Therefore, we can't use isinstance() to check for\n",
            "        # the six meta path importer, since the other six instance will have\n",
            "        # inserted an importer with different class.\n",
            "        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n",
            "                importer.name == __name__):\n",
            "            del sys.meta_path[i]\n",
            "            break\n",
            "    del i, importer\n",
            "# Finally, add the importer to the meta path import hook.\n",
            "sys.meta_path.append(_importer)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"contextlib2 - backports and enhancements to the contextlib module\"\"\"\n",
            "\n",
            "import sys\n",
            "import warnings\n",
            "from collections import deque\n",
            "from functools import wraps\n",
            "\n",
            "__all__ = [\"contextmanager\", \"closing\", \"ContextDecorator\", \"ExitStack\",\n",
            "           \"redirect_stdout\", \"redirect_stderr\", \"suppress\"]\n",
            "\n",
            "# Backwards compatibility\n",
            "__all__ += [\"ContextStack\"]\n",
            "\n",
            "class ContextDecorator(object):\n",
            "    \"A base class or mixin that enables context managers to work as decorators.\"\n",
            "\n",
            "    def refresh_cm(self):\n",
            "        \"\"\"Returns the context manager used to actually wrap the call to the\n",
            "        decorated function.\n",
            "\n",
            "        The default implementation just returns *self*.\n",
            "\n",
            "        Overriding this method allows otherwise one-shot context managers\n",
            "        like _GeneratorContextManager to support use as decorators via\n",
            "        implicit recreation.\n",
            "\n",
            "        DEPRECATED: refresh_cm was never added to the standard library's\n",
            "                    ContextDecorator API\n",
            "        \"\"\"\n",
            "        warnings.warn(\"refresh_cm was never added to the standard library\",\n",
            "                      DeprecationWarning)\n",
            "        return self._recreate_cm()\n",
            "\n",
            "    def _recreate_cm(self):\n",
            "        \"\"\"Return a recreated instance of self.\n",
            "\n",
            "        Allows an otherwise one-shot context manager like\n",
            "        _GeneratorContextManager to support use as\n",
            "        a decorator via implicit recreation.\n",
            "\n",
            "        This is a private interface just for _GeneratorContextManager.\n",
            "        See issue #11647 for details.\n",
            "        \"\"\"\n",
            "        return self\n",
            "\n",
            "    def __call__(self, func):\n",
            "        @wraps(func)\n",
            "        def inner(*args, **kwds):\n",
            "            with self._recreate_cm():\n",
            "                return func(*args, **kwds)\n",
            "        return inner\n",
            "\n",
            "\n",
            "class _GeneratorContextManager(ContextDecorator):\n",
            "    \"\"\"Helper for @contextmanager decorator.\"\"\"\n",
            "\n",
            "    def __init__(self, func, args, kwds):\n",
            "        self.gen = func(*args, **kwds)\n",
            "        self.func, self.args, self.kwds = func, args, kwds\n",
            "        # Issue 19330: ensure context manager instances have good docstrings\n",
            "        doc = getattr(func, \"__doc__\", None)\n",
            "        if doc is None:\n",
            "            doc = type(self).__doc__\n",
            "        self.__doc__ = doc\n",
            "        # Unfortunately, this still doesn't provide good help output when\n",
            "        # inspecting the created context manager instances, since pydoc\n",
            "        # currently bypasses the instance docstring and shows the docstring\n",
            "        # for the class instead.\n",
            "        # See http://bugs.python.org/issue19404 for more details.\n",
            "\n",
            "    def _recreate_cm(self):\n",
            "        # _GCM instances are one-shot context managers, so the\n",
            "        # CM must be recreated each time a decorated function is\n",
            "        # called\n",
            "        return self.__class__(self.func, self.args, self.kwds)\n",
            "\n",
            "    def __enter__(self):\n",
            "        try:\n",
            "            return next(self.gen)\n",
            "        except StopIteration:\n",
            "            raise RuntimeError(\"generator didn't yield\")\n",
            "\n",
            "    def __exit__(self, type, value, traceback):\n",
            "        if type is None:\n",
            "            try:\n",
            "                next(self.gen)\n",
            "            except StopIteration:\n",
            "                return\n",
            "            else:\n",
            "                raise RuntimeError(\"generator didn't stop\")\n",
            "        else:\n",
            "            if value is None:\n",
            "                # Need to force instantiation so we can reliably\n",
            "                # tell if we get the same exception back\n",
            "                value = type()\n",
            "            try:\n",
            "                self.gen.throw(type, value, traceback)\n",
            "                raise RuntimeError(\"generator didn't stop after throw()\")\n",
            "            except StopIteration as exc:\n",
            "                # Suppress StopIteration *unless* it's the same exception that\n",
            "                # was passed to throw().  This prevents a StopIteration\n",
            "                # raised inside the \"with\" statement from being suppressed.\n",
            "                return exc is not value\n",
            "            except RuntimeError as exc:\n",
            "                # Don't re-raise the passed in exception\n",
            "                if exc is value:\n",
            "                    return False\n",
            "                # Likewise, avoid suppressing if a StopIteration exception\n",
            "                # was passed to throw() and later wrapped into a RuntimeError\n",
            "                # (see PEP 479).\n",
            "                if _HAVE_EXCEPTION_CHAINING and exc.__cause__ is value:\n",
            "                    return False\n",
            "                raise\n",
            "            except:\n",
            "                # only re-raise if it's *not* the exception that was\n",
            "                # passed to throw(), because __exit__() must not raise\n",
            "                # an exception unless __exit__() itself failed.  But throw()\n",
            "                # has to raise the exception to signal propagation, so this\n",
            "                # fixes the impedance mismatch between the throw() protocol\n",
            "                # and the __exit__() protocol.\n",
            "                #\n",
            "                if sys.exc_info()[1] is not value:\n",
            "                    raise\n",
            "\n",
            "\n",
            "def contextmanager(func):\n",
            "    \"\"\"@contextmanager decorator.\n",
            "\n",
            "    Typical usage:\n",
            "\n",
            "        @contextmanager\n",
            "        def some_generator(<arguments>):\n",
            "            <setup>\n",
            "            try:\n",
            "                yield <value>\n",
            "            finally:\n",
            "                <cleanup>\n",
            "\n",
            "    This makes this:\n",
            "\n",
            "        with some_generator(<arguments>) as <variable>:\n",
            "            <body>\n",
            "\n",
            "    equivalent to this:\n",
            "\n",
            "        <setup>\n",
            "        try:\n",
            "            <variable> = <value>\n",
            "            <body>\n",
            "        finally:\n",
            "            <cleanup>\n",
            "\n",
            "    \"\"\"\n",
            "    @wraps(func)\n",
            "    def helper(*args, **kwds):\n",
            "        return _GeneratorContextManager(func, args, kwds)\n",
            "    return helper\n",
            "\n",
            "\n",
            "class closing(object):\n",
            "    \"\"\"Context to automatically close something at the end of a block.\n",
            "\n",
            "    Code like this:\n",
            "\n",
            "        with closing(<module>.open(<arguments>)) as f:\n",
            "            <block>\n",
            "\n",
            "    is equivalent to this:\n",
            "\n",
            "        f = <module>.open(<arguments>)\n",
            "        try:\n",
            "            <block>\n",
            "        finally:\n",
            "            f.close()\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, thing):\n",
            "        self.thing = thing\n",
            "    def __enter__(self):\n",
            "        return self.thing\n",
            "    def __exit__(self, *exc_info):\n",
            "        self.thing.close()\n",
            "\n",
            "\n",
            "class _RedirectStream(object):\n",
            "\n",
            "    _stream = None\n",
            "\n",
            "    def __init__(self, new_target):\n",
            "        self._new_target = new_target\n",
            "        # We use a list of old targets to make this CM re-entrant\n",
            "        self._old_targets = []\n",
            "\n",
            "    def __enter__(self):\n",
            "        self._old_targets.append(getattr(sys, self._stream))\n",
            "        setattr(sys, self._stream, self._new_target)\n",
            "        return self._new_target\n",
            "\n",
            "    def __exit__(self, exctype, excinst, exctb):\n",
            "        setattr(sys, self._stream, self._old_targets.pop())\n",
            "\n",
            "\n",
            "class redirect_stdout(_RedirectStream):\n",
            "    \"\"\"Context manager for temporarily redirecting stdout to another file.\n",
            "\n",
            "        # How to send help() to stderr\n",
            "        with redirect_stdout(sys.stderr):\n",
            "            help(dir)\n",
            "\n",
            "        # How to write help() to a file\n",
            "        with open('help.txt', 'w') as f:\n",
            "            with redirect_stdout(f):\n",
            "                help(pow)\n",
            "    \"\"\"\n",
            "\n",
            "    _stream = \"stdout\"\n",
            "\n",
            "\n",
            "class redirect_stderr(_RedirectStream):\n",
            "    \"\"\"Context manager for temporarily redirecting stderr to another file.\"\"\"\n",
            "\n",
            "    _stream = \"stderr\"\n",
            "\n",
            "\n",
            "class suppress(object):\n",
            "    \"\"\"Context manager to suppress specified exceptions\n",
            "\n",
            "    After the exception is suppressed, execution proceeds with the next\n",
            "    statement following the with statement.\n",
            "\n",
            "         with suppress(FileNotFoundError):\n",
            "             os.remove(somefile)\n",
            "         # Execution still resumes here if the file was already removed\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, *exceptions):\n",
            "        self._exceptions = exceptions\n",
            "\n",
            "    def __enter__(self):\n",
            "        pass\n",
            "\n",
            "    def __exit__(self, exctype, excinst, exctb):\n",
            "        # Unlike isinstance and issubclass, CPython exception handling\n",
            "        # currently only looks at the concrete type hierarchy (ignoring\n",
            "        # the instance and subclass checking hooks). While Guido considers\n",
            "        # that a bug rather than a feature, it's a fairly hard one to fix\n",
            "        # due to various internal implementation details. suppress provides\n",
            "        # the simpler issubclass based semantics, rather than trying to\n",
            "        # exactly reproduce the limitations of the CPython interpreter.\n",
            "        #\n",
            "        # See http://bugs.python.org/issue12029 for more details\n",
            "        return exctype is not None and issubclass(exctype, self._exceptions)\n",
            "\n",
            "\n",
            "# Context manipulation is Python 3 only\n",
            "_HAVE_EXCEPTION_CHAINING = sys.version_info[0] >= 3\n",
            "if _HAVE_EXCEPTION_CHAINING:\n",
            "    def _make_context_fixer(frame_exc):\n",
            "        def _fix_exception_context(new_exc, old_exc):\n",
            "            # Context may not be correct, so find the end of the chain\n",
            "            while 1:\n",
            "                exc_context = new_exc.__context__\n",
            "                if exc_context is old_exc:\n",
            "                    # Context is already set correctly (see issue 20317)\n",
            "                    return\n",
            "                if exc_context is None or exc_context is frame_exc:\n",
            "                    break\n",
            "                new_exc = exc_context\n",
            "            # Change the end of the chain to point to the exception\n",
            "            # we expect it to reference\n",
            "            new_exc.__context__ = old_exc\n",
            "        return _fix_exception_context\n",
            "\n",
            "    def _reraise_with_existing_context(exc_details):\n",
            "        try:\n",
            "            # bare \"raise exc_details[1]\" replaces our carefully\n",
            "            # set-up context\n",
            "            fixed_ctx = exc_details[1].__context__\n",
            "            raise exc_details[1]\n",
            "        except BaseException:\n",
            "            exc_details[1].__context__ = fixed_ctx\n",
            "            raise\n",
            "else:\n",
            "    # No exception context in Python 2\n",
            "    def _make_context_fixer(frame_exc):\n",
            "        return lambda new_exc, old_exc: None\n",
            "\n",
            "    # Use 3 argument raise in Python 2,\n",
            "    # but use exec to avoid SyntaxError in Python 3\n",
            "    def _reraise_with_existing_context(exc_details):\n",
            "        exc_type, exc_value, exc_tb = exc_details\n",
            "        exec (\"raise exc_type, exc_value, exc_tb\")\n",
            "\n",
            "# Handle old-style classes if they exist\n",
            "try:\n",
            "    from types import InstanceType\n",
            "except ImportError:\n",
            "    # Python 3 doesn't have old-style classes\n",
            "    _get_type = type\n",
            "else:\n",
            "    # Need to handle old-style context managers on Python 2\n",
            "    def _get_type(obj):\n",
            "        obj_type = type(obj)\n",
            "        if obj_type is InstanceType:\n",
            "            return obj.__class__ # Old-style class\n",
            "        return obj_type # New-style class\n",
            "\n",
            "# Inspired by discussions on http://bugs.python.org/issue13585\n",
            "class ExitStack(object):\n",
            "    \"\"\"Context manager for dynamic management of a stack of exit callbacks\n",
            "\n",
            "    For example:\n",
            "\n",
            "        with ExitStack() as stack:\n",
            "            files = [stack.enter_context(open(fname)) for fname in filenames]\n",
            "            # All opened files will automatically be closed at the end of\n",
            "            # the with statement, even if attempts to open files later\n",
            "            # in the list raise an exception\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self):\n",
            "        self._exit_callbacks = deque()\n",
            "\n",
            "    def pop_all(self):\n",
            "        \"\"\"Preserve the context stack by transferring it to a new instance\"\"\"\n",
            "        new_stack = type(self)()\n",
            "        new_stack._exit_callbacks = self._exit_callbacks\n",
            "        self._exit_callbacks = deque()\n",
            "        return new_stack\n",
            "\n",
            "    def _push_cm_exit(self, cm, cm_exit):\n",
            "        \"\"\"Helper to correctly register callbacks to __exit__ methods\"\"\"\n",
            "        def _exit_wrapper(*exc_details):\n",
            "            return cm_exit(cm, *exc_details)\n",
            "        _exit_wrapper.__self__ = cm\n",
            "        self.push(_exit_wrapper)\n",
            "\n",
            "    def push(self, exit):\n",
            "        \"\"\"Registers a callback with the standard __exit__ method signature\n",
            "\n",
            "        Can suppress exceptions the same way __exit__ methods can.\n",
            "\n",
            "        Also accepts any object with an __exit__ method (registering a call\n",
            "        to the method instead of the object itself)\n",
            "        \"\"\"\n",
            "        # We use an unbound method rather than a bound method to follow\n",
            "        # the standard lookup behaviour for special methods\n",
            "        _cb_type = _get_type(exit)\n",
            "        try:\n",
            "            exit_method = _cb_type.__exit__\n",
            "        except AttributeError:\n",
            "            # Not a context manager, so assume its a callable\n",
            "            self._exit_callbacks.append(exit)\n",
            "        else:\n",
            "            self._push_cm_exit(exit, exit_method)\n",
            "        return exit # Allow use as a decorator\n",
            "\n",
            "    def callback(self, callback, *args, **kwds):\n",
            "        \"\"\"Registers an arbitrary callback and arguments.\n",
            "\n",
            "        Cannot suppress exceptions.\n",
            "        \"\"\"\n",
            "        def _exit_wrapper(exc_type, exc, tb):\n",
            "            callback(*args, **kwds)\n",
            "        # We changed the signature, so using @wraps is not appropriate, but\n",
            "        # setting __wrapped__ may still help with introspection\n",
            "        _exit_wrapper.__wrapped__ = callback\n",
            "        self.push(_exit_wrapper)\n",
            "        return callback # Allow use as a decorator\n",
            "\n",
            "    def enter_context(self, cm):\n",
            "        \"\"\"Enters the supplied context manager\n",
            "\n",
            "        If successful, also pushes its __exit__ method as a callback and\n",
            "        returns the result of the __enter__ method.\n",
            "        \"\"\"\n",
            "        # We look up the special methods on the type to match the with statement\n",
            "        _cm_type = _get_type(cm)\n",
            "        _exit = _cm_type.__exit__\n",
            "        result = _cm_type.__enter__(cm)\n",
            "        self._push_cm_exit(cm, _exit)\n",
            "        return result\n",
            "\n",
            "    def close(self):\n",
            "        \"\"\"Immediately unwind the context stack\"\"\"\n",
            "        self.__exit__(None, None, None)\n",
            "\n",
            "    def __enter__(self):\n",
            "        return self\n",
            "\n",
            "    def __exit__(self, *exc_details):\n",
            "        received_exc = exc_details[0] is not None\n",
            "\n",
            "        # We manipulate the exception state so it behaves as though\n",
            "        # we were actually nesting multiple with statements\n",
            "        frame_exc = sys.exc_info()[1]\n",
            "        _fix_exception_context = _make_context_fixer(frame_exc)\n",
            "\n",
            "        # Callbacks are invoked in LIFO order to match the behaviour of\n",
            "        # nested context managers\n",
            "        suppressed_exc = False\n",
            "        pending_raise = False\n",
            "        while self._exit_callbacks:\n",
            "            cb = self._exit_callbacks.pop()\n",
            "            try:\n",
            "                if cb(*exc_details):\n",
            "                    suppressed_exc = True\n",
            "                    pending_raise = False\n",
            "                    exc_details = (None, None, None)\n",
            "            except:\n",
            "                new_exc_details = sys.exc_info()\n",
            "                # simulate the stack of exceptions by setting the context\n",
            "                _fix_exception_context(new_exc_details[1], exc_details[1])\n",
            "                pending_raise = True\n",
            "                exc_details = new_exc_details\n",
            "        if pending_raise:\n",
            "            _reraise_with_existing_context(exc_details)\n",
            "        return received_exc and suppressed_exc\n",
            "\n",
            "# Preserve backwards compatibility\n",
            "class ContextStack(ExitStack):\n",
            "    \"\"\"Backwards compatibility alias for ExitStack\"\"\"\n",
            "\n",
            "    def __init__(self):\n",
            "        warnings.warn(\"ContextStack has been renamed to ExitStack\",\n",
            "                      DeprecationWarning)\n",
            "        super(ContextStack, self).__init__()\n",
            "\n",
            "    def register_exit(self, callback):\n",
            "        return self.push(callback)\n",
            "\n",
            "    def register(self, callback, *args, **kwds):\n",
            "        return self.callback(callback, *args, **kwds)\n",
            "\n",
            "    def preserve(self):\n",
            "        return self.pop_all()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"\n",
            "Cycler\n",
            "======\n",
            "\n",
            "Cycling through combinations of values, producing dictionaries.\n",
            "\n",
            "You can add cyclers::\n",
            "\n",
            "    from cycler import cycler\n",
            "    cc = (cycler(color=list('rgb')) +\n",
            "          cycler(linestyle=['-', '--', '-.']))\n",
            "    for d in cc:\n",
            "        print(d)\n",
            "\n",
            "Results in::\n",
            "\n",
            "    {'color': 'r', 'linestyle': '-'}\n",
            "    {'color': 'g', 'linestyle': '--'}\n",
            "    {'color': 'b', 'linestyle': '-.'}\n",
            "\n",
            "\n",
            "You can multiply cyclers::\n",
            "\n",
            "    from cycler import cycler\n",
            "    cc = (cycler(color=list('rgb')) *\n",
            "          cycler(linestyle=['-', '--', '-.']))\n",
            "    for d in cc:\n",
            "        print(d)\n",
            "\n",
            "Results in::\n",
            "\n",
            "    {'color': 'r', 'linestyle': '-'}\n",
            "    {'color': 'r', 'linestyle': '--'}\n",
            "    {'color': 'r', 'linestyle': '-.'}\n",
            "    {'color': 'g', 'linestyle': '-'}\n",
            "    {'color': 'g', 'linestyle': '--'}\n",
            "    {'color': 'g', 'linestyle': '-.'}\n",
            "    {'color': 'b', 'linestyle': '-'}\n",
            "    {'color': 'b', 'linestyle': '--'}\n",
            "    {'color': 'b', 'linestyle': '-.'}\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import (absolute_import, division, print_function,\n",
            "                        unicode_literals)\n",
            "\n",
            "import six\n",
            "from itertools import product, cycle\n",
            "from six.moves import zip, reduce\n",
            "from operator import mul, add\n",
            "import copy\n",
            "\n",
            "__version__ = '0.10.0'\n",
            "\n",
            "\n",
            "def _process_keys(left, right):\n",
            "    \"\"\"\n",
            "    Helper function to compose cycler keys\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    left, right : iterable of dictionaries or None\n",
            "        The cyclers to be composed\n",
            "    Returns\n",
            "    -------\n",
            "    keys : set\n",
            "        The keys in the composition of the two cyclers\n",
            "    \"\"\"\n",
            "    l_peek = next(iter(left)) if left is not None else {}\n",
            "    r_peek = next(iter(right)) if right is not None else {}\n",
            "    l_key = set(l_peek.keys())\n",
            "    r_key = set(r_peek.keys())\n",
            "    if l_key & r_key:\n",
            "        raise ValueError(\"Can not compose overlapping cycles\")\n",
            "    return l_key | r_key\n",
            "\n",
            "\n",
            "class Cycler(object):\n",
            "    \"\"\"\n",
            "    Composable cycles\n",
            "\n",
            "    This class has compositions methods:\n",
            "\n",
            "    ``+``\n",
            "      for 'inner' products (zip)\n",
            "\n",
            "    ``+=``\n",
            "      in-place ``+``\n",
            "\n",
            "    ``*``\n",
            "      for outer products (itertools.product) and integer multiplication\n",
            "\n",
            "    ``*=``\n",
            "      in-place ``*``\n",
            "\n",
            "    and supports basic slicing via ``[]``\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    left : Cycler or None\n",
            "        The 'left' cycler\n",
            "\n",
            "    right : Cycler or None\n",
            "        The 'right' cycler\n",
            "\n",
            "    op : func or None\n",
            "        Function which composes the 'left' and 'right' cyclers.\n",
            "\n",
            "    \"\"\"\n",
            "    def __call__(self):\n",
            "        return cycle(self)\n",
            "\n",
            "    def __init__(self, left, right=None, op=None):\n",
            "        \"\"\"Semi-private init\n",
            "\n",
            "        Do not use this directly, use `cycler` function instead.\n",
            "        \"\"\"\n",
            "        if isinstance(left, Cycler):\n",
            "            self._left = Cycler(left._left, left._right, left._op)\n",
            "        elif left is not None:\n",
            "            # Need to copy the dictionary or else that will be a residual\n",
            "            # mutable that could lead to strange errors\n",
            "            self._left = [copy.copy(v) for v in left]\n",
            "        else:\n",
            "            self._left = None\n",
            "\n",
            "        if isinstance(right, Cycler):\n",
            "            self._right = Cycler(right._left, right._right, right._op)\n",
            "        elif right is not None:\n",
            "            # Need to copy the dictionary or else that will be a residual\n",
            "            # mutable that could lead to strange errors\n",
            "            self._right = [copy.copy(v) for v in right]\n",
            "        else:\n",
            "            self._right = None\n",
            "\n",
            "        self._keys = _process_keys(self._left, self._right)\n",
            "        self._op = op\n",
            "\n",
            "    @property\n",
            "    def keys(self):\n",
            "        \"\"\"\n",
            "        The keys this Cycler knows about\n",
            "        \"\"\"\n",
            "        return set(self._keys)\n",
            "\n",
            "    def change_key(self, old, new):\n",
            "        \"\"\"\n",
            "        Change a key in this cycler to a new name.\n",
            "        Modification is performed in-place.\n",
            "\n",
            "        Does nothing if the old key is the same as the new key.\n",
            "        Raises a ValueError if the new key is already a key.\n",
            "        Raises a KeyError if the old key isn't a key.\n",
            "\n",
            "        \"\"\"\n",
            "        if old == new:\n",
            "            return\n",
            "        if new in self._keys:\n",
            "            raise ValueError(\"Can't replace %s with %s, %s is already a key\" %\n",
            "                             (old, new, new))\n",
            "        if old not in self._keys:\n",
            "            raise KeyError(\"Can't replace %s with %s, %s is not a key\" %\n",
            "                           (old, new, old))\n",
            "\n",
            "        self._keys.remove(old)\n",
            "        self._keys.add(new)\n",
            "\n",
            "        if self._right is not None and old in self._right.keys:\n",
            "            self._right.change_key(old, new)\n",
            "\n",
            "        # self._left should always be non-None\n",
            "        # if self._keys is non-empty.\n",
            "        elif isinstance(self._left, Cycler):\n",
            "            self._left.change_key(old, new)\n",
            "        else:\n",
            "            # It should be completely safe at this point to\n",
            "            # assume that the old key can be found in each\n",
            "            # iteration.\n",
            "            self._left = [{new: entry[old]} for entry in self._left]\n",
            "\n",
            "    def _compose(self):\n",
            "        \"\"\"\n",
            "        Compose the 'left' and 'right' components of this cycle\n",
            "        with the proper operation (zip or product as of now)\n",
            "        \"\"\"\n",
            "        for a, b in self._op(self._left, self._right):\n",
            "            out = dict()\n",
            "            out.update(a)\n",
            "            out.update(b)\n",
            "            yield out\n",
            "\n",
            "    @classmethod\n",
            "    def _from_iter(cls, label, itr):\n",
            "        \"\"\"\n",
            "        Class method to create 'base' Cycler objects\n",
            "        that do not have a 'right' or 'op' and for which\n",
            "        the 'left' object is not another Cycler.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        label : str\n",
            "            The property key.\n",
            "\n",
            "        itr : iterable\n",
            "            Finite length iterable of the property values.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        cycler : Cycler\n",
            "            New 'base' `Cycler`\n",
            "        \"\"\"\n",
            "        ret = cls(None)\n",
            "        ret._left = list({label: v} for v in itr)\n",
            "        ret._keys = set([label])\n",
            "        return ret\n",
            "\n",
            "    def __getitem__(self, key):\n",
            "        # TODO : maybe add numpy style fancy slicing\n",
            "        if isinstance(key, slice):\n",
            "            trans = self.by_key()\n",
            "            return reduce(add, (_cycler(k, v[key])\n",
            "                                for k, v in six.iteritems(trans)))\n",
            "        else:\n",
            "            raise ValueError(\"Can only use slices with Cycler.__getitem__\")\n",
            "\n",
            "    def __iter__(self):\n",
            "        if self._right is None:\n",
            "            return iter(dict(l) for l in self._left)\n",
            "\n",
            "        return self._compose()\n",
            "\n",
            "    def __add__(self, other):\n",
            "        \"\"\"\n",
            "        Pair-wise combine two equal length cycles (zip)\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        other : Cycler\n",
            "           The second Cycler\n",
            "        \"\"\"\n",
            "        if len(self) != len(other):\n",
            "            raise ValueError(\"Can only add equal length cycles, \"\n",
            "                             \"not {0} and {1}\".format(len(self), len(other)))\n",
            "        return Cycler(self, other, zip)\n",
            "\n",
            "    def __mul__(self, other):\n",
            "        \"\"\"\n",
            "        Outer product of two cycles (`itertools.product`) or integer\n",
            "        multiplication.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        other : Cycler or int\n",
            "           The second Cycler or integer\n",
            "        \"\"\"\n",
            "        if isinstance(other, Cycler):\n",
            "            return Cycler(self, other, product)\n",
            "        elif isinstance(other, int):\n",
            "            trans = self.by_key()\n",
            "            return reduce(add, (_cycler(k, v*other)\n",
            "                                for k, v in six.iteritems(trans)))\n",
            "        else:\n",
            "            return NotImplemented\n",
            "\n",
            "    def __rmul__(self, other):\n",
            "        return self * other\n",
            "\n",
            "    def __len__(self):\n",
            "        op_dict = {zip: min, product: mul}\n",
            "        if self._right is None:\n",
            "            return len(self._left)\n",
            "        l_len = len(self._left)\n",
            "        r_len = len(self._right)\n",
            "        return op_dict[self._op](l_len, r_len)\n",
            "\n",
            "    def __iadd__(self, other):\n",
            "        \"\"\"\n",
            "        In-place pair-wise combine two equal length cycles (zip)\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        other : Cycler\n",
            "           The second Cycler\n",
            "        \"\"\"\n",
            "        if not isinstance(other, Cycler):\n",
            "            raise TypeError(\"Cannot += with a non-Cycler object\")\n",
            "        # True shallow copy of self is fine since this is in-place\n",
            "        old_self = copy.copy(self)\n",
            "        self._keys = _process_keys(old_self, other)\n",
            "        self._left = old_self\n",
            "        self._op = zip\n",
            "        self._right = Cycler(other._left, other._right, other._op)\n",
            "        return self\n",
            "\n",
            "    def __imul__(self, other):\n",
            "        \"\"\"\n",
            "        In-place outer product of two cycles (`itertools.product`)\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        other : Cycler\n",
            "           The second Cycler\n",
            "        \"\"\"\n",
            "        if not isinstance(other, Cycler):\n",
            "            raise TypeError(\"Cannot *= with a non-Cycler object\")\n",
            "        # True shallow copy of self is fine since this is in-place\n",
            "        old_self = copy.copy(self)\n",
            "        self._keys = _process_keys(old_self, other)\n",
            "        self._left = old_self\n",
            "        self._op = product\n",
            "        self._right = Cycler(other._left, other._right, other._op)\n",
            "        return self\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        \"\"\"\n",
            "        Check equality\n",
            "        \"\"\"\n",
            "        if len(self) != len(other):\n",
            "            return False\n",
            "        if self.keys ^ other.keys:\n",
            "            return False\n",
            "\n",
            "        return all(a == b for a, b in zip(self, other))\n",
            "\n",
            "    def __repr__(self):\n",
            "        op_map = {zip: '+', product: '*'}\n",
            "        if self._right is None:\n",
            "            lab = self.keys.pop()\n",
            "            itr = list(v[lab] for v in self)\n",
            "            return \"cycler({lab!r}, {itr!r})\".format(lab=lab, itr=itr)\n",
            "        else:\n",
            "            op = op_map.get(self._op, '?')\n",
            "            msg = \"({left!r} {op} {right!r})\"\n",
            "            return msg.format(left=self._left, op=op, right=self._right)\n",
            "\n",
            "    def _repr_html_(self):\n",
            "        # an table showing the value of each key through a full cycle\n",
            "        output = \"<table>\"\n",
            "        sorted_keys = sorted(self.keys, key=repr)\n",
            "        for key in sorted_keys:\n",
            "            output += \"<th>{key!r}</th>\".format(key=key)\n",
            "        for d in iter(self):\n",
            "            output += \"<tr>\"\n",
            "            for k in sorted_keys:\n",
            "                output += \"<td>{val!r}</td>\".format(val=d[k])\n",
            "            output += \"</tr>\"\n",
            "        output += \"</table>\"\n",
            "        return output\n",
            "\n",
            "    def by_key(self):\n",
            "        \"\"\"Values by key\n",
            "\n",
            "        This returns the transposed values of the cycler.  Iterating\n",
            "        over a `Cycler` yields dicts with a single value for each key,\n",
            "        this method returns a `dict` of `list` which are the values\n",
            "        for the given key.\n",
            "\n",
            "        The returned value can be used to create an equivalent `Cycler`\n",
            "        using only `+`.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        transpose : dict\n",
            "            dict of lists of the values for each key.\n",
            "        \"\"\"\n",
            "\n",
            "        # TODO : sort out if this is a bottle neck, if there is a better way\n",
            "        # and if we care.\n",
            "\n",
            "        keys = self.keys\n",
            "        # change this to dict comprehension when drop 2.6\n",
            "        out = dict((k,  list()) for k in keys)\n",
            "\n",
            "        for d in self:\n",
            "            for k in keys:\n",
            "                out[k].append(d[k])\n",
            "        return out\n",
            "\n",
            "    # for back compatibility\n",
            "    _transpose = by_key\n",
            "\n",
            "    def simplify(self):\n",
            "        \"\"\"Simplify the Cycler\n",
            "\n",
            "        Returned as a composition using only sums (no multiplications)\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        simple : Cycler\n",
            "            An equivalent cycler using only summation\"\"\"\n",
            "        # TODO: sort out if it is worth the effort to make sure this is\n",
            "        # balanced.  Currently it is is\n",
            "        # (((a + b) + c) + d) vs\n",
            "        # ((a + b) + (c + d))\n",
            "        # I would believe that there is some performance implications\n",
            "\n",
            "        trans = self.by_key()\n",
            "        return reduce(add, (_cycler(k, v) for k, v in six.iteritems(trans)))\n",
            "\n",
            "    def concat(self, other):\n",
            "        \"\"\"Concatenate this cycler and an other.\n",
            "\n",
            "        The keys must match exactly.\n",
            "\n",
            "        This returns a single Cycler which is equivalent to\n",
            "        `itertools.chain(self, other)`\n",
            "\n",
            "        Examples\n",
            "        --------\n",
            "\n",
            "        >>> num = cycler('a', range(3))\n",
            "        >>> let = cycler('a', 'abc')\n",
            "        >>> num.concat(let)\n",
            "        cycler('a', [0, 1, 2, 'a', 'b', 'c'])\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        other : `Cycler`\n",
            "            The `Cycler` to concatenate to this one.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        ret : `Cycler`\n",
            "            The concatenated `Cycler`\n",
            "        \"\"\"\n",
            "        return concat(self, other)\n",
            "\n",
            "\n",
            "def concat(left, right):\n",
            "    \"\"\"Concatenate two cyclers.\n",
            "\n",
            "    The keys must match exactly.\n",
            "\n",
            "    This returns a single Cycler which is equivalent to\n",
            "    `itertools.chain(left, right)`\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "\n",
            "    >>> num = cycler('a', range(3))\n",
            "    >>> let = cycler('a', 'abc')\n",
            "    >>> num.concat(let)\n",
            "    cycler('a', [0, 1, 2, 'a', 'b', 'c'])\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    left, right : `Cycler`\n",
            "        The two `Cycler` instances to concatenate\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    ret : `Cycler`\n",
            "        The concatenated `Cycler`\n",
            "    \"\"\"\n",
            "    if left.keys != right.keys:\n",
            "        msg = '\\n\\t'.join([\"Keys do not match:\",\n",
            "                           \"Intersection: {both!r}\",\n",
            "                           \"Disjoint: {just_one!r}\"]).format(\n",
            "                               both=left.keys & right.keys,\n",
            "                               just_one=left.keys ^ right.keys)\n",
            "\n",
            "        raise ValueError(msg)\n",
            "\n",
            "    _l = left.by_key()\n",
            "    _r = right.by_key()\n",
            "    return reduce(add, (_cycler(k, _l[k] + _r[k]) for k in left.keys))\n",
            "\n",
            "\n",
            "def cycler(*args, **kwargs):\n",
            "    \"\"\"\n",
            "    Create a new `Cycler` object from a single positional argument,\n",
            "    a pair of positional arguments, or the combination of keyword arguments.\n",
            "\n",
            "    cycler(arg)\n",
            "    cycler(label1=itr1[, label2=iter2[, ...]])\n",
            "    cycler(label, itr)\n",
            "\n",
            "    Form 1 simply copies a given `Cycler` object.\n",
            "\n",
            "    Form 2 composes a `Cycler` as an inner product of the\n",
            "    pairs of keyword arguments. In other words, all of the\n",
            "    iterables are cycled simultaneously, as if through zip().\n",
            "\n",
            "    Form 3 creates a `Cycler` from a label and an iterable.\n",
            "    This is useful for when the label cannot be a keyword argument\n",
            "    (e.g., an integer or a name that has a space in it).\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    arg : Cycler\n",
            "        Copy constructor for Cycler (does a shallow copy of iterables).\n",
            "\n",
            "    label : name\n",
            "        The property key. In the 2-arg form of the function,\n",
            "        the label can be any hashable object. In the keyword argument\n",
            "        form of the function, it must be a valid python identifier.\n",
            "\n",
            "    itr : iterable\n",
            "        Finite length iterable of the property values.\n",
            "        Can be a single-property `Cycler` that would\n",
            "        be like a key change, but as a shallow copy.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    cycler : Cycler\n",
            "        New `Cycler` for the given property\n",
            "\n",
            "    \"\"\"\n",
            "    if args and kwargs:\n",
            "        raise TypeError(\"cyl() can only accept positional OR keyword \"\n",
            "                        \"arguments -- not both.\")\n",
            "\n",
            "    if len(args) == 1:\n",
            "        if not isinstance(args[0], Cycler):\n",
            "            raise TypeError(\"If only one positional argument given, it must \"\n",
            "                            \" be a Cycler instance.\")\n",
            "        return Cycler(args[0])\n",
            "    elif len(args) == 2:\n",
            "        return _cycler(*args)\n",
            "    elif len(args) > 2:\n",
            "        raise TypeError(\"Only a single Cycler can be accepted as the lone \"\n",
            "                        \"positional argument. Use keyword arguments instead.\")\n",
            "\n",
            "    if kwargs:\n",
            "        return reduce(add, (_cycler(k, v) for k, v in six.iteritems(kwargs)))\n",
            "\n",
            "    raise TypeError(\"Must have at least a positional OR keyword arguments\")\n",
            "\n",
            "\n",
            "def _cycler(label, itr):\n",
            "    \"\"\"\n",
            "    Create a new `Cycler` object from a property name and\n",
            "    iterable of values.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    label : hashable\n",
            "        The property key.\n",
            "\n",
            "    itr : iterable\n",
            "        Finite length iterable of the property values.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    cycler : Cycler\n",
            "        New `Cycler` for the given property\n",
            "    \"\"\"\n",
            "    if isinstance(itr, Cycler):\n",
            "        keys = itr.keys\n",
            "        if len(keys) != 1:\n",
            "            msg = \"Can not create Cycler from a multi-property Cycler\"\n",
            "            raise ValueError(msg)\n",
            "\n",
            "        lab = keys.pop()\n",
            "        # Doesn't need to be a new list because\n",
            "        # _from_iter() will be creating that new list anyway.\n",
            "        itr = (v[lab] for v in itr)\n",
            "\n",
            "    return Cycler._from_iter(label, itr)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "__version__ = '0.15.4'\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# this module should be kept Python 2.3 compatible\n",
            "import re\n",
            "import sys\n",
            "import inspect\n",
            "import argparse\n",
            "from gettext import gettext as _\n",
            "if sys.version >= '3':\n",
            "    from inspect import getfullargspec\n",
            "else:\n",
            "    class getfullargspec(object):\n",
            "        \"A quick and dirty replacement for getfullargspec for Python 2.X\"\n",
            "        def __init__(self, f):\n",
            "            self.args, self.varargs, self.varkw, self.defaults = \\\n",
            "                inspect.getargspec(f)\n",
            "            self.annotations = getattr(f, '__annotations__', {})\n",
            "\n",
            "\n",
            "def getargspec(callableobj):\n",
            "    \"\"\"Given a callable return an object with attributes .args, .varargs,\n",
            "    .varkw, .defaults. It tries to do the \"right thing\" with functions,\n",
            "    methods, classes and generic callables.\"\"\"\n",
            "    if inspect.isfunction(callableobj):\n",
            "        argspec = getfullargspec(callableobj)\n",
            "    elif inspect.ismethod(callableobj):\n",
            "        argspec = getfullargspec(callableobj)\n",
            "        del argspec.args[0]  # remove first argument\n",
            "    elif inspect.isclass(callableobj):\n",
            "        if callableobj.__init__ is object.__init__:  # to avoid an error\n",
            "            argspec = getfullargspec(lambda self: None)\n",
            "        else:\n",
            "            argspec = getfullargspec(callableobj.__init__)\n",
            "        del argspec.args[0]  # remove first argument\n",
            "    elif hasattr(callableobj, '__call__'):\n",
            "        argspec = getfullargspec(callableobj.__call__)\n",
            "        del argspec.args[0]  # remove first argument\n",
            "    else:\n",
            "        raise TypeError(_('Could not determine the signature of ') +\n",
            "                        str(callableobj))\n",
            "    return argspec\n",
            "\n",
            "\n",
            "def annotations(**ann):\n",
            "    \"\"\"\n",
            "    Returns a decorator annotating a function with the given annotations.\n",
            "    This is a trick to support function annotations in Python 2.X.\n",
            "    \"\"\"\n",
            "    def annotate(f):\n",
            "        fas = getfullargspec(f)\n",
            "        args = fas.args\n",
            "        if fas.varargs:\n",
            "            args.append(fas.varargs)\n",
            "        if fas.varkw:\n",
            "            args.append(fas.varkw)\n",
            "        for argname in ann:\n",
            "            if argname not in args:\n",
            "                raise NameError(\n",
            "                    _('Annotating non-existing argument: %s') % argname)\n",
            "        f.__annotations__ = ann\n",
            "        return f\n",
            "    return annotate\n",
            "\n",
            "\n",
            "def is_annotation(obj):\n",
            "    \"\"\"\n",
            "    An object is an annotation object if it has the attributes\n",
            "    help, kind, abbrev, type, choices, metavar.\n",
            "    \"\"\"\n",
            "    return (hasattr(obj, 'help') and hasattr(obj, 'kind')\n",
            "            and hasattr(obj, 'abbrev') and hasattr(obj, 'type')\n",
            "            and hasattr(obj, 'choices') and hasattr(obj, 'metavar'))\n",
            "\n",
            "\n",
            "class Annotation(object):\n",
            "    def __init__(self, help=None, kind=\"positional\", abbrev=None, type=None,\n",
            "                 choices=None, metavar=None):\n",
            "        assert kind in ('positional', 'option', 'flag'), kind\n",
            "        if kind == \"positional\":\n",
            "            assert abbrev is None, abbrev\n",
            "        self.help = help\n",
            "        self.kind = kind\n",
            "        self.abbrev = abbrev\n",
            "        self.type = type\n",
            "        self.choices = choices\n",
            "        self.metavar = metavar\n",
            "\n",
            "    def from_(cls, obj):\n",
            "        \"Helper to convert an object into an annotation, if needed\"\n",
            "        if is_annotation(obj):\n",
            "            return obj  # do nothing\n",
            "        elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):\n",
            "            return cls(*obj)\n",
            "        return cls(obj)\n",
            "    from_ = classmethod(from_)\n",
            "\n",
            "\n",
            "NONE = object()  # sentinel use to signal the absence of a default\n",
            "\n",
            "PARSER_CFG = getfullargspec(argparse.ArgumentParser.__init__).args[1:]\n",
            "# the default arguments accepted by an ArgumentParser object\n",
            "\n",
            "\n",
            "def pconf(obj):\n",
            "    \"Extracts the configuration of the underlying ArgumentParser from obj\"\n",
            "    cfg = dict(description=obj.__doc__,\n",
            "               formatter_class=argparse.RawDescriptionHelpFormatter)\n",
            "    for name in dir(obj):\n",
            "        if name in PARSER_CFG:  # argument of ArgumentParser\n",
            "            cfg[name] = getattr(obj, name)\n",
            "    return cfg\n",
            "\n",
            "_parser_registry = {}\n",
            "\n",
            "\n",
            "def parser_from(obj, **confparams):\n",
            "    \"\"\"\n",
            "    obj can be a callable or an object with a .commands attribute.\n",
            "    Returns an ArgumentParser.\n",
            "    \"\"\"\n",
            "    try:  # the underlying parser has been generated already\n",
            "        return _parser_registry[obj]\n",
            "    except KeyError:  # generate a new parser\n",
            "        pass\n",
            "    conf = pconf(obj).copy()\n",
            "    conf.update(confparams)\n",
            "    _parser_registry[obj] = parser = ArgumentParser(**conf)\n",
            "    parser.obj = obj\n",
            "    parser.case_sensitive = confparams.get(\n",
            "        'case_sensitive', getattr(obj, 'case_sensitive', True))\n",
            "    if hasattr(obj, 'commands') and not inspect.isclass(obj):\n",
            "        # a command container instance\n",
            "        parser.addsubcommands(obj.commands, obj, 'subcommands')\n",
            "    else:\n",
            "        parser.populate_from(obj)\n",
            "    return parser\n",
            "\n",
            "\n",
            "def _extract_kwargs(args):\n",
            "    \"Returns two lists: regular args and name=value args\"\n",
            "    arglist = []\n",
            "    kwargs = {}\n",
            "    for arg in args:\n",
            "        match = re.match(r'([a-zA-Z_]\\w*)=', arg)\n",
            "        if match:\n",
            "            name = match.group(1)\n",
            "            kwargs[name] = arg[len(name)+1:]\n",
            "        else:\n",
            "            arglist.append(arg)\n",
            "    return arglist, kwargs\n",
            "\n",
            "\n",
            "def _match_cmd(abbrev, commands, case_sensitive=True):\n",
            "    \"Extract the command name from an abbreviation or raise a NameError\"\n",
            "    if not case_sensitive:\n",
            "        abbrev = abbrev.upper()\n",
            "        commands = [c.upper() for c in commands]\n",
            "    perfect_matches = [name for name in commands if name == abbrev]\n",
            "    if len(perfect_matches) == 1:\n",
            "        return perfect_matches[0]\n",
            "    matches = [name for name in commands if name.startswith(abbrev)]\n",
            "    n = len(matches)\n",
            "    if n == 1:\n",
            "        return matches[0]\n",
            "    elif n > 1:\n",
            "        raise NameError(\n",
            "            _('Ambiguous command %r: matching %s' % (abbrev, matches)))\n",
            "\n",
            "\n",
            "class ArgumentParser(argparse.ArgumentParser):\n",
            "    \"\"\"\n",
            "    An ArgumentParser with .func and .argspec attributes, and possibly\n",
            "    .commands and .subparsers.\n",
            "    \"\"\"\n",
            "    case_sensitive = True\n",
            "\n",
            "    def alias(self, arg):\n",
            "        \"Can be overridden to preprocess command-line arguments\"\n",
            "        return arg\n",
            "\n",
            "    def consume(self, args):\n",
            "        \"\"\"Call the underlying function with the args. Works also for\n",
            "        command containers, by dispatching to the right subparser.\"\"\"\n",
            "        arglist = [self.alias(a) for a in args]\n",
            "        cmd = None\n",
            "        if hasattr(self, 'subparsers'):\n",
            "            subp, cmd = self._extract_subparser_cmd(arglist)\n",
            "            if subp is None and cmd is not None:\n",
            "                return cmd, self.missing(cmd)\n",
            "            elif subp is not None:  # use the subparser\n",
            "                self = subp\n",
            "        if hasattr(self, 'argspec') and self.argspec.varkw:\n",
            "            arglist, kwargs = _extract_kwargs(arglist)  # modify arglist!\n",
            "        else:\n",
            "            kwargs = {}\n",
            "        if hasattr(self, 'argspec') and self.argspec.varargs:\n",
            "            # ignore unrecognized arguments\n",
            "            ns, extraopts = self.parse_known_args(arglist)\n",
            "        else:\n",
            "            ns, extraopts = self.parse_args(arglist), []  # may raise an exit\n",
            "        if not hasattr(self, 'argspec'):\n",
            "            raise SystemExit\n",
            "        args = [getattr(ns, a) for a in self.argspec.args]\n",
            "        varargs = getattr(ns, self.argspec.varargs or '', [])\n",
            "        collision = set(self.argspec.args) & set(kwargs)\n",
            "        if collision:\n",
            "            self.error(\n",
            "                _('colliding keyword arguments: %s') % ' '.join(collision))\n",
            "        return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
            "\n",
            "    def _extract_subparser_cmd(self, arglist):\n",
            "        \"Extract the right subparser from the first recognized argument\"\n",
            "        optprefix = self.prefix_chars[0]\n",
            "        name_parser_map = self.subparsers._name_parser_map\n",
            "        for i, arg in enumerate(arglist):\n",
            "            if not arg.startswith(optprefix):\n",
            "                cmd = _match_cmd(arg, name_parser_map, self.case_sensitive)\n",
            "                del arglist[i]\n",
            "                return name_parser_map.get(cmd), cmd or arg\n",
            "        return None, None\n",
            "\n",
            "    def addsubcommands(self, commands, obj, title=None, cmdprefix=''):\n",
            "        \"Extract a list of subcommands from obj and add them to the parser\"\n",
            "        if hasattr(obj, cmdprefix) and obj.cmdprefix in self.prefix_chars:\n",
            "            raise ValueError(_('The prefix %r is already taken!' % cmdprefix))\n",
            "        if not hasattr(self, 'subparsers'):\n",
            "            self.subparsers = self.add_subparsers(title=title)\n",
            "        elif title:\n",
            "            self.add_argument_group(title=title)  # populate ._action_groups\n",
            "        prefixlen = len(getattr(obj, 'cmdprefix', ''))\n",
            "        add_help = getattr(obj, 'add_help', True)\n",
            "        for cmd in commands:\n",
            "            func = getattr(obj, cmd[prefixlen:])  # strip the prefix\n",
            "            self.subparsers.add_parser(\n",
            "                cmd, add_help=add_help, help=func.__doc__, **pconf(func)\n",
            "                ).populate_from(func)\n",
            "\n",
            "    def _set_func_argspec(self, obj):\n",
            "        \"\"\"Extracts the signature from a callable object and adds an .argspec\n",
            "        attribute to the parser. Also adds a .func reference to the object.\"\"\"\n",
            "        self.func = obj\n",
            "        self.argspec = getargspec(obj)\n",
            "        _parser_registry[obj] = self\n",
            "\n",
            "    def populate_from(self, func):\n",
            "        \"\"\"\n",
            "        Extract the arguments from the attributes of the passed function\n",
            "        and return a populated ArgumentParser instance.\n",
            "        \"\"\"\n",
            "        self._set_func_argspec(func)\n",
            "        f = self.argspec\n",
            "        defaults = f.defaults or ()\n",
            "        n_args = len(f.args)\n",
            "        n_defaults = len(defaults)\n",
            "        alldefaults = (NONE,) * (n_args - n_defaults) + defaults\n",
            "        prefix = self.prefix = getattr(func, 'prefix_chars', '-')[0]\n",
            "        for name, default in zip(f.args, alldefaults):\n",
            "            ann = f.annotations.get(name, ())\n",
            "            a = Annotation.from_(ann)\n",
            "            metavar = a.metavar\n",
            "            if default is NONE:\n",
            "                dflt = None\n",
            "            else:\n",
            "                dflt = default\n",
            "                if a.help is None:\n",
            "                    a.help = '[%s]' % str(dflt)  # dflt can be a tuple\n",
            "            if a.kind in ('option', 'flag'):\n",
            "                if a.abbrev:\n",
            "                    shortlong = (prefix + a.abbrev,\n",
            "                                 prefix*2 + name.replace('_', '-'))\n",
            "                else:\n",
            "                    shortlong = (prefix + name.replace('_', '-'),)\n",
            "            elif default is NONE:  # required argument\n",
            "                self.add_argument(name, help=a.help, type=a.type,\n",
            "                                  choices=a.choices, metavar=metavar)\n",
            "            else:  # default argument\n",
            "                self.add_argument(\n",
            "                    name, nargs='?', help=a.help, default=dflt,\n",
            "                    type=a.type, choices=a.choices, metavar=metavar)\n",
            "            if a.kind == 'option':\n",
            "                if default is not NONE:\n",
            "                    metavar = metavar or str(default)\n",
            "                self.add_argument(\n",
            "                    help=a.help, default=dflt, type=a.type,\n",
            "                    choices=a.choices, metavar=metavar, *shortlong)\n",
            "            elif a.kind == 'flag':\n",
            "                if default is not NONE and default is not False:\n",
            "                    raise TypeError(_('Flag %r wants default False, got %r') %\n",
            "                                    (name, default))\n",
            "                self.add_argument(action='store_true', help=a.help, *shortlong)\n",
            "        if f.varargs:\n",
            "            a = Annotation.from_(f.annotations.get(f.varargs, ()))\n",
            "            self.add_argument(f.varargs, nargs='*', help=a.help, default=[],\n",
            "                              type=a.type, metavar=a.metavar)\n",
            "        if f.varkw:\n",
            "            a = Annotation.from_(f.annotations.get(f.varkw, ()))\n",
            "            self.add_argument(f.varkw, nargs='*', help=a.help, default={},\n",
            "                              type=a.type, metavar=a.metavar)\n",
            "\n",
            "    def missing(self, name):\n",
            "        \"May raise a SystemExit\"\n",
            "        miss = getattr(self.obj, '__missing__', lambda name:\n",
            "                       self.error('No command %r' % name))\n",
            "        return miss(name)\n",
            "\n",
            "    def print_actions(self):\n",
            "        \"Useful for debugging\"\n",
            "        print(self)\n",
            "        for a in self._actions:\n",
            "            print(a)\n",
            "\n",
            "\n",
            "def iterable(obj):\n",
            "    \"Any object with an __iter__ method which is not a string\"\n",
            "    return hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes))\n",
            "\n",
            "\n",
            "def call(obj, arglist=sys.argv[1:], eager=True, version=None):\n",
            "    \"\"\"\n",
            "    If obj is a function or a bound method, parse the given arglist\n",
            "    by using the parser inferred from the annotations of obj\n",
            "    and call obj with the parsed arguments.\n",
            "    If obj is an object with attribute .commands, dispatch to the\n",
            "    associated subparser.\n",
            "    \"\"\"\n",
            "    parser = parser_from(obj)\n",
            "    if version:\n",
            "        parser.add_argument(\n",
            "            '--version', '-v', action='version', version=version)\n",
            "    cmd, result = parser.consume(arglist)\n",
            "    if iterable(result) and eager:  # listify the result\n",
            "        return list(result)\n",
            "    return result\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "#  python-holidays\n",
            "#  ---------------\n",
            "#  A fast, efficient Python library for generating country, province and state\n",
            "#  specific sets of holidays on the fly. It aims to make determining whether a\n",
            "#  specific date is a holiday as fast and flexible as possible.\n",
            "#\n",
            "#  Author:  ryanss <ryanssdev@icloud.com> (c) 2014-2017\n",
            "#           dr-prodigy <maurizio.montel@gmail.com> (c) 2017-2019\n",
            "#  Website: https://github.com/dr-prodigy/python-holidays\n",
            "#  License: MIT (see LICENSE file)\n",
            "\n",
            "from datetime import date, datetime, timedelta\n",
            "from dateutil.easter import easter, EASTER_ORTHODOX\n",
            "from dateutil.parser import parse\n",
            "from dateutil.relativedelta import relativedelta as rd\n",
            "from dateutil.relativedelta import MO, TU, WE, TH, FR, SA, SU\n",
            "import six\n",
            "import warnings\n",
            "\n",
            "\n",
            "__version__ = '0.9.11'\n",
            "\n",
            "MON, TUE, WED, THU, FRI, SAT, SUN = range(7)\n",
            "WEEKEND = (SAT, SUN)\n",
            "\n",
            "JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, \\\n",
            "    NOV, DEC = range(1, 13)\n",
            "\n",
            "\n",
            "class HolidayBase(dict):\n",
            "    PROVINCES = []\n",
            "\n",
            "    def __init__(self, years=[], expand=True, observed=True,\n",
            "                 prov=None, state=None):\n",
            "        self.observed = observed\n",
            "        self.expand = expand\n",
            "        if isinstance(years, int):\n",
            "            years = [years, ]\n",
            "        self.years = set(years)\n",
            "        if not getattr(self, 'prov', False):\n",
            "            self.prov = prov\n",
            "        self.state = state\n",
            "        for year in list(self.years):\n",
            "            self._populate(year)\n",
            "\n",
            "    def __setattr__(self, key, value):\n",
            "        if key == 'observed' and len(self) > 0:\n",
            "            dict.__setattr__(self, key, value)\n",
            "            if value is True:\n",
            "                # Add (Observed) dates\n",
            "                years = list(self.years)\n",
            "                self.years = set()\n",
            "                self.clear()\n",
            "                for year in years:\n",
            "                    self._populate(year)\n",
            "            else:\n",
            "                # Remove (Observed) dates\n",
            "                for k, v in list(self.items()):\n",
            "                    if v.find(\"Observed\") >= 0:\n",
            "                        del self[k]\n",
            "        else:\n",
            "            return dict.__setattr__(self, key, value)\n",
            "\n",
            "    def __keytransform__(self, key):\n",
            "        if isinstance(key, datetime):\n",
            "            key = key.date()\n",
            "        elif isinstance(key, date):\n",
            "            key = key\n",
            "        elif isinstance(key, int) or isinstance(key, float):\n",
            "            key = datetime.utcfromtimestamp(key).date()\n",
            "        elif isinstance(key, six.string_types):\n",
            "            try:\n",
            "                key = parse(key).date()\n",
            "            except (ValueError, OverflowError):\n",
            "                raise ValueError(\"Cannot parse date from string '%s'\" % key)\n",
            "        else:\n",
            "            raise TypeError(\"Cannot convert type '%s' to date.\" % type(key))\n",
            "\n",
            "        if self.expand and key.year not in self.years:\n",
            "            self.years.add(key.year)\n",
            "            self._populate(key.year)\n",
            "        return key\n",
            "\n",
            "    def __contains__(self, key):\n",
            "        return dict.__contains__(self, self.__keytransform__(key))\n",
            "\n",
            "    def __getitem__(self, key):\n",
            "        if isinstance(key, slice):\n",
            "            if not key.start or not key.stop:\n",
            "                raise ValueError(\"Both start and stop must be given.\")\n",
            "\n",
            "            start = self.__keytransform__(key.start)\n",
            "            stop = self.__keytransform__(key.stop)\n",
            "\n",
            "            if key.step is None:\n",
            "                step = 1\n",
            "            elif isinstance(key.step, timedelta):\n",
            "                step = key.step.days\n",
            "            elif isinstance(key.step, int):\n",
            "                step = key.step\n",
            "            else:\n",
            "                raise TypeError(\n",
            "                    \"Cannot convert type '%s' to int.\" % type(key.step)\n",
            "                )\n",
            "\n",
            "            if step == 0:\n",
            "                raise ValueError('Step value must not be zero.')\n",
            "\n",
            "            date_diff = stop - start\n",
            "            if date_diff.days < 0 <= step or date_diff.days >= 0 > step:\n",
            "                step *= -1\n",
            "\n",
            "            days_in_range = []\n",
            "            for delta_days in range(0, date_diff.days, step):\n",
            "                day = start + timedelta(days=delta_days)\n",
            "                try:\n",
            "                    dict.__getitem__(\n",
            "                        self,\n",
            "                        day\n",
            "                    )\n",
            "                    days_in_range.append(day)\n",
            "                except (KeyError):\n",
            "                    pass\n",
            "            return days_in_range\n",
            "        return dict.__getitem__(self, self.__keytransform__(key))\n",
            "\n",
            "    def __setitem__(self, key, value):\n",
            "        if key in self:\n",
            "            if self.get(key).find(value) < 0 \\\n",
            "                    and value.find(self.get(key)) < 0:\n",
            "                value = \"%s, %s\" % (value, self.get(key))\n",
            "            else:\n",
            "                value = self.get(key)\n",
            "        return dict.__setitem__(self, self.__keytransform__(key), value)\n",
            "\n",
            "    def update(self, *args):\n",
            "        args = list(args)\n",
            "        for arg in args:\n",
            "            if isinstance(arg, dict):\n",
            "                for key, value in list(arg.items()):\n",
            "                    self[key] = value\n",
            "            elif isinstance(arg, list):\n",
            "                for item in arg:\n",
            "                    self[item] = \"Holiday\"\n",
            "            else:\n",
            "                self[arg] = \"Holiday\"\n",
            "\n",
            "    def append(self, *args):\n",
            "        return self.update(*args)\n",
            "\n",
            "    def get(self, key, default=None):\n",
            "        return dict.get(self, self.__keytransform__(key), default)\n",
            "\n",
            "    def get_list(self, key):\n",
            "        return [h for h in self.get(key, \"\").split(\", \") if h]\n",
            "\n",
            "    def pop(self, key, default=None):\n",
            "        if default is None:\n",
            "            return dict.pop(self, self.__keytransform__(key))\n",
            "        return dict.pop(self, self.__keytransform__(key), default)\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        return dict.__eq__(self, other) and self.__dict__ == other.__dict__\n",
            "\n",
            "    def __ne__(self, other):\n",
            "        return dict.__ne__(self, other) or self.__dict__ != other.__dict__\n",
            "\n",
            "    def __add__(self, other):\n",
            "        if isinstance(other, int) and other == 0:\n",
            "            # Required to sum() list of holidays\n",
            "            # sum([h1, h2]) is equivalent to (0 + h1 + h2)\n",
            "            return self\n",
            "        elif not isinstance(other, HolidayBase):\n",
            "            raise TypeError()\n",
            "        HolidaySum = createHolidaySum(self, other)\n",
            "        country = (getattr(self, 'country', None) or\n",
            "                   getattr(other, 'country', None))\n",
            "        if self.country and other.country and self.country != other.country:\n",
            "            c1 = self.country\n",
            "            if not isinstance(c1, list):\n",
            "                c1 = [c1]\n",
            "            c2 = other.country\n",
            "            if not isinstance(c2, list):\n",
            "                c2 = [c2]\n",
            "            country = c1 + c2\n",
            "        prov = getattr(self, 'prov', None) or getattr(other, 'prov', None)\n",
            "        if self.prov and other.prov and self.prov != other.prov:\n",
            "            p1 = self.prov if isinstance(self.prov, list) else [self.prov]\n",
            "            p2 = other.prov if isinstance(other.prov, list) else [other.prov]\n",
            "            prov = p1 + p2\n",
            "        return HolidaySum(years=(self.years | other.years),\n",
            "                          expand=(self.expand or other.expand),\n",
            "                          observed=(self.observed or other.observed),\n",
            "                          country=country, prov=prov)\n",
            "\n",
            "    def __radd__(self, other):\n",
            "        return self.__add__(other)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        pass\n",
            "\n",
            "\n",
            "def createHolidaySum(h1, h2):\n",
            "    class HolidaySum(HolidayBase):\n",
            "\n",
            "        def __init__(self, country, **kwargs):\n",
            "            self.country = country\n",
            "            self.holidays = []\n",
            "            if getattr(h1, 'holidays', False):\n",
            "                for h in h1.holidays:\n",
            "                    self.holidays.append(h)\n",
            "            else:\n",
            "                self.holidays.append(h1)\n",
            "            if getattr(h2, 'holidays', False):\n",
            "                for h in h2.holidays:\n",
            "                    self.holidays.append(h)\n",
            "            else:\n",
            "                self.holidays.append(h2)\n",
            "            HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "        def _populate(self, year):\n",
            "            for h in self.holidays[::-1]:\n",
            "                h._populate(year)\n",
            "                self.update(h)\n",
            "\n",
            "    return HolidaySum\n",
            "\n",
            "\n",
            "def CountryHoliday(country, years=[], prov=None, state=None):\n",
            "    try:\n",
            "        country_holiday = globals()[country](years=years,\n",
            "                                             prov=prov, state=state)\n",
            "    except (KeyError):\n",
            "        raise KeyError(\"Country %s not available\" % country)\n",
            "    return country_holiday\n",
            "\n",
            "\n",
            "class Aruba(HolidayBase):\n",
            "    # http://www.gobierno.aw/informacion-tocante-servicio/vakantie-y-dia-di-fiesta_43437/item/dia-di-fiesta_14809.html\n",
            "    # https://www.visitaruba.com/about-aruba/national-holidays-and-celebrations/\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'AW'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \"Aa Nobo [New Year's Day]\"\n",
            "\n",
            "        # Dia di Betico\n",
            "        self[date(year, JAN, 25)] = \"Dia Di Betico [Betico Day]\"\n",
            "\n",
            "        # Carnaval Monday\n",
            "        self[easter(year) + rd(days=-48)] = \"Dialuna di Carnaval \\\n",
            "            [Carnaval Monday]\"\n",
            "\n",
            "        # Dia di Himno y Bandera\n",
            "        self[date(year, MAR, 18)] = \"Dia di Himno y Bandera \\\n",
            "            [National Anthem & Flag Day]\"\n",
            "\n",
            "        # Good Friday\n",
            "        self[easter(year) + rd(weekday=FR(-1))] = \"Bierna Santo [Good Friday]\"\n",
            "\n",
            "        # Easter Monday\n",
            "        self[easter(year) + rd(days=1)] = \"Di Dos Dia di Pasco di Resureccion \\\n",
            "            [Easter Monday]\"\n",
            "\n",
            "        # King's Day\n",
            "        if year >= 2014:\n",
            "            kings_day = date(year, APR, 27)\n",
            "            if kings_day.weekday() == 6:\n",
            "                kings_day = kings_day - rd(days=1)\n",
            "\n",
            "            self[kings_day] = \"Aa di Rey [King's Day]\"\n",
            "\n",
            "        # Queen's Day\n",
            "        if 1891 <= year <= 2013:\n",
            "            queens_day = date(year, APR, 30)\n",
            "            if year <= 1948:\n",
            "                queens_day = date(year, AUG, 31)\n",
            "\n",
            "            if queens_day.weekday() == 6:\n",
            "                if year < 1980:\n",
            "                    queens_day = queens_day + rd(days=1)\n",
            "                else:\n",
            "                    queens_day = queens_day - rd(days=1)\n",
            "\n",
            "            self[queens_day] = \"Aa di La Reina [Queen's Day]\"\n",
            "\n",
            "        # Labour Day\n",
            "        self[date(year, MAY, 1)] = \"Dia di Obrero [Labour Day]\"\n",
            "\n",
            "        # Ascension Day\n",
            "        self[easter(year) + rd(days=39)] = \"Dia di Asuncion [Ascension Day]\"\n",
            "\n",
            "        # Christmas Day\n",
            "        self[date(year, DEC, 25)] = \"Pasco di Nacemento [Christmas]\"\n",
            "\n",
            "        # Second Christmas\n",
            "        self[date(year, DEC, 26)] = \"Di Dos Dia di Pasco di \\\n",
            "            Nacemento [Second Christmas]\"\n",
            "\n",
            "\n",
            "class AW(Aruba):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Argentina(HolidayBase):\n",
            "    # https://www.argentina.gob.ar/interior/feriados\n",
            "    # https://es.wikipedia.org/wiki/Anexo:D%C3%ADas_feriados_en_Argentina\n",
            "    # http://servicios.lanacion.com.ar/feriados\n",
            "    # https://www.clarin.com/feriados/\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'AR'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        if not self.observed and date(year, JAN, 1).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JAN, 1)] = \"Ao Nuevo [New Year's Day]\"\n",
            "\n",
            "        # Carnival days\n",
            "        name = \"Da de Carnaval [Carnival's Day]\"\n",
            "        self[easter(year) - rd(days=48)] = name\n",
            "        self[easter(year) - rd(days=47)] = name\n",
            "\n",
            "        # Memory's National Day for the Truth and Justice\n",
            "        name = \"Da Nacional de la Memoria por la Verdad y la Justicia \" \\\n",
            "               \"[Memory's National Day for the Truth and Justice]\"\n",
            "\n",
            "        if not self.observed and date(year, MAR, 24).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, MAR, 24)] = name\n",
            "\n",
            "        # Holy Week\n",
            "        name_thu = \"Semana Santa (Jueves Santo)  [Holy day (Holy Thursday)]\"\n",
            "        name_fri = \"Semana Santa (Viernes Santo)  [Holy day (Holy Friday)]\"\n",
            "        name_easter = 'Da de Pascuas [Easter Day]'\n",
            "\n",
            "        self[easter(year) + rd(weekday=TH(-1))] = name_thu\n",
            "        self[easter(year) + rd(weekday=FR(-1))] = name_fri\n",
            "\n",
            "        if not self.observed and easter(year).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[easter(year)] = name_easter\n",
            "\n",
            "        # Veterans Day and the Fallen in the Malvinas War\n",
            "        if not self.observed and date(year, APR, 2).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, APR, 2)] = \"Da del Veterano y de los Caidos \" \\\n",
            "                \"en la Guerra de Malvinas [Veterans\" \\\n",
            "                \" Day and the Fallen in the\" \\\n",
            "                \" Malvinas War]\"\n",
            "\n",
            "        # Labor Day\n",
            "        name = \"Da del Trabajo [Labour Day]\"\n",
            "        if not self.observed and date(year, MAY, 1).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, MAY, 1)] = name\n",
            "\n",
            "        # May Revolution Day\n",
            "        name = \"Da de la Revolucion de Mayo [May Revolution Day]\"\n",
            "        if not self.observed and date(year, MAY, 25).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, MAY, 25)] = name\n",
            "\n",
            "        # Day Pass to the Immortality of General Martn Miguel de Gemes.\n",
            "        name = \"Da Pase a la Inmortalidad \" \\\n",
            "               \"del General Martn Miguel de Gemes [Day Pass \" \\\n",
            "               \"to the Immortality of General Martn Miguel de Gemes]\"\n",
            "        if not self.observed and date(year, JUN, 17).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JUN, 17)] = name\n",
            "\n",
            "        # Day Pass to the Immortality of General D. Manuel Belgrano.\n",
            "        name = \"Da Pase a la Inmortalidad \" \\\n",
            "               \"del General D. Manuel Belgrano [Day Pass \" \\\n",
            "               \"to the Immortality of General D. Manuel Belgrano]\"\n",
            "        if not self.observed and date(year, JUN, 20).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JUN, 20)] = name\n",
            "\n",
            "        # Independence Day\n",
            "        name = \"Da de la Independencia [Independence Day]\"\n",
            "        if not self.observed and date(year, JUL, 9).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JUL, 9)] = name\n",
            "\n",
            "        # Day Pass to the Immortality of General D. Jos de San Martin\n",
            "        name = \"Da Pase a la Inmortalidad \" \\\n",
            "               \"del General D. Jos de San Martin [Day Pass \" \\\n",
            "               \"to the Immortality of General D. Jos de San Martin]\"\n",
            "        if not self.observed and date(year, AUG, 17).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, AUG, 17)] = name\n",
            "\n",
            "        # Respect for Cultural Diversity Day or Columbus day\n",
            "        if not self.observed and date(year, OCT, 12).weekday() in WEEKEND:\n",
            "            pass\n",
            "        elif year < 2010:\n",
            "            self[date(year, OCT, 12)] = \"Da de la Raza [Columbus day]\"\n",
            "        else:\n",
            "            self[date(year, OCT, 12)] = \"Da del Respeto a la Diversidad\" \\\n",
            "                \" Cultural [Respect for\" \\\n",
            "                \" Cultural Diversity Day]\"\n",
            "        # National Sovereignty Day\n",
            "        name = \"Da Nacional de la Soberana [National Sovereignty Day]\"\n",
            "        if not self.observed and date(year, NOV, 20).weekday() in WEEKEND:\n",
            "            pass\n",
            "        elif year >= 2010:\n",
            "            self[date(year, NOV, 20)] = name\n",
            "\n",
            "        # Immaculate Conception\n",
            "        if not self.observed and date(year, DEC, 8).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, DEC, 8)] = \"La Inmaculada Concepcin\" \\\n",
            "                \" [Immaculate Conception]\"\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Navidad [Christmas]\"\n",
            "\n",
            "\n",
            "class AR(Argentina):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Belarus(HolidayBase):\n",
            "    \"\"\"\n",
            "    http://president.gov.by/en/holidays_en/\n",
            "    http://www.belarus.by/en/about-belarus/national-holidays\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"BY\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # The current set of holidays came into force in 1998\n",
            "        # http://laws.newsby.org/documents/ukazp/pos05/ukaz05806.htm\n",
            "        if year <= 1998:\n",
            "            return\n",
            "\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \" \"\n",
            "\n",
            "        # Christmas Day (Orthodox)\n",
            "        self[date(year, JAN, 7)] = \"  \" \\\n",
            "                                   \"( )\"\n",
            "\n",
            "        # Women's Day\n",
            "        self[date(year, MAR, 8)] = \" \"\n",
            "\n",
            "        # Radunitsa (\"Day of Rejoicing\")\n",
            "        self[easter(year, method=EASTER_ORTHODOX) + rd(days=9)] = \"\"\n",
            "\n",
            "        # Labour Day\n",
            "        self[date(year, MAY, 1)] = \" \"\n",
            "\n",
            "        # Victory Day\n",
            "        self[date(year, MAY, 9)] = \" \"\n",
            "\n",
            "        # Independence Day\n",
            "        self[date(year, JUL, 3)] = \"    \" \\\n",
            "                                   \"( )\"\n",
            "\n",
            "        # October Revolution Day\n",
            "        self[date(year, NOV, 7)] = \"  \"\n",
            "\n",
            "        # Christmas Day (Catholic)\n",
            "        self[date(year, DEC, 25)] = \"  \" \\\n",
            "                                    \"( )\"\n",
            "\n",
            "\n",
            "class BY(Belarus):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Brazil(HolidayBase):\n",
            "    \"\"\"\n",
            "    https://pt.wikipedia.org/wiki/Feriados_no_Brasil\n",
            "    \"\"\"\n",
            "\n",
            "    STATES = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT',\n",
            "              'MS', 'MG', 'PA', 'PB', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR',\n",
            "              'SC', 'SP', 'SE', 'TO']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'BR'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \"Ano novo\"\n",
            "\n",
            "        self[date(year, APR, 21)] = \"Tiradentes\"\n",
            "\n",
            "        self[date(year, MAY, 1)] = \"Dia Mundial do Trabalho\"\n",
            "\n",
            "        self[date(year, SEP, 7)] = \"Independncia do Brasil\"\n",
            "\n",
            "        self[date(year, OCT, 12)] = \"Nossa Senhora Aparecida\"\n",
            "\n",
            "        self[date(year, NOV, 2)] = \"Finados\"\n",
            "\n",
            "        self[date(year, NOV, 15)] = \"Proclamao da Repblica\"\n",
            "\n",
            "        # Christmas Day\n",
            "        self[date(year, DEC, 25)] = \"Natal\"\n",
            "\n",
            "        self[easter(year) - rd(days=2)] = \"Sexta-feira Santa\"\n",
            "\n",
            "        self[easter(year)] = \"Pscoa\"\n",
            "\n",
            "        self[easter(year) + rd(days=60)] = \"Corpus Christi\"\n",
            "\n",
            "        quaresma = easter(year) - rd(days=46)\n",
            "        self[quaresma] = \"Quarta-feira de cinzas (Incio da Quaresma)\"\n",
            "\n",
            "        self[quaresma - rd(weekday=TU(-1))] = \"Carnaval\"\n",
            "\n",
            "        if self.state == 'AC':\n",
            "            self[date(year, JAN, 23)] = \"Dia do evanglico\"\n",
            "            self[date(year, JUN, 15)] = \"Aniversrio do Acre\"\n",
            "            self[date(year, SEP, 5)] = \"Dia da Amaznia\"\n",
            "            self[date(year, NOV, 17)] = \"Assinatura do Tratado de\" \\\n",
            "                                        \" Petrpolis\"\n",
            "\n",
            "        if self.state == 'AL':\n",
            "            self[date(year, JUN, 24)] = \"So Joo\"\n",
            "            self[date(year, JUN, 29)] = \"So Pedro\"\n",
            "            self[date(year, SEP, 16)] = \"Emancipao poltica de Alagoas\"\n",
            "            self[date(year, NOV, 20)] = \"Conscincia Negra\"\n",
            "\n",
            "        if self.state == 'AP':\n",
            "            self[date(year, MAR, 19)] = \"Dia de So Jos\"\n",
            "            self[date(year, JUL, 25)] = \"So Tiago\"\n",
            "            self[date(year, OCT, 5)] = \"Criao do estado\"\n",
            "            self[date(year, NOV, 20)] = \"Conscincia Negra\"\n",
            "\n",
            "        if self.state == 'AM':\n",
            "            self[date(year, SEP, 5)] = \"Elevao do Amazonas\" \\\n",
            "                \"  categoria de provncia\"\n",
            "            self[date(year, NOV, 20)] = \"Conscincia Negra\"\n",
            "            self[date(year, DEC, 8)] = \"Dia de Nossa Senhora da Conceio\"\n",
            "\n",
            "        if self.state == 'BA':\n",
            "            self[date(year, JUL, 2)] = \"Independncia da Bahia\"\n",
            "\n",
            "        if self.state == 'CE':\n",
            "            self[date(year, MAR, 19)] = \"So Jos\"\n",
            "            self[date(year, MAR, 25)] = \"Data Magna do Cear\"\n",
            "\n",
            "        if self.state == 'DF':\n",
            "            self[date(year, APR, 21)] = \"Fundao de Braslia\"\n",
            "            self[date(year, NOV, 30)] = \"Dia do Evanglico\"\n",
            "\n",
            "        if self.state == 'ES':\n",
            "            self[date(year, OCT, 28)] = \"Dia do Servidor Pblico\"\n",
            "\n",
            "        if self.state == 'GO':\n",
            "            self[date(year, OCT, 28)] = \"Dia do Servidor Pblico\"\n",
            "\n",
            "        if self.state == 'MA':\n",
            "            self[date(year, JUL, 28)] = \"Adeso do Maranho\" \\\n",
            "                \"  independncia do Brasil\"\n",
            "            self[date(year, DEC, 8)] = \"Dia de Nossa Senhora da Conceio\"\n",
            "\n",
            "        if self.state == 'MT':\n",
            "            self[date(year, NOV, 20)] = \"Conscincia Negra\"\n",
            "\n",
            "        if self.state == 'MS':\n",
            "            self[date(year, OCT, 11)] = \"Criao do estado\"\n",
            "\n",
            "        if self.state == 'MG':\n",
            "            self[date(year, APR, 21)] = \"Data Magna de MG\"\n",
            "\n",
            "        if self.state == 'PA':\n",
            "            self[date(year, AUG, 15)] = \"Adeso do Gro-Par\" \\\n",
            "                \"  independncia do Brasil\"\n",
            "\n",
            "        if self.state == 'PB':\n",
            "            self[date(year, AUG, 5)] = \"Fundao do Estado\"\n",
            "\n",
            "        if self.state == 'PE':\n",
            "            self[date(year, MAR, 6)] = \"Revoluo Pernambucana (Data Magna)\"\n",
            "            self[date(year, JUN, 24)] = \"So Joo\"\n",
            "\n",
            "        if self.state == 'PI':\n",
            "            self[date(year, MAR, 13)] = \"Dia da Batalha do Jenipapo\"\n",
            "            self[date(year, OCT, 19)] = \"Dia do Piau\"\n",
            "\n",
            "        if self.state == 'RJ':\n",
            "            self[date(year, APR, 23)] = \"Dia de So Jorge\"\n",
            "            self[date(year, OCT, 28)] = \"Dia do Funcionrio Pblico\"\n",
            "            self[date(year, NOV, 20)] = \"Zumbi dos Palmares\"\n",
            "\n",
            "        if self.state == 'RN':\n",
            "            self[date(year, JUN, 29)] = \"Dia de So Pedro\"\n",
            "            self[date(year, OCT, 3)] = \"Mrtires de Cunha e Uruauu\"\n",
            "\n",
            "        if self.state == 'RS':\n",
            "            self[date(year, SEP, 20)] = \"Revoluo Farroupilha\"\n",
            "\n",
            "        if self.state == 'RO':\n",
            "            self[date(year, JAN, 4)] = \"Criao do estado\"\n",
            "            self[date(year, JUN, 18)] = \"Dia do Evanglico\"\n",
            "\n",
            "        if self.state == 'RR':\n",
            "            self[date(year, OCT, 5)] = \"Criao de Roraima\"\n",
            "\n",
            "        if self.state == 'SC':\n",
            "            self[date(year, AUG, 11)] = \"Criao da capitania,\" \\\n",
            "                \" separando-se de SP\"\n",
            "\n",
            "        if self.state == 'SP':\n",
            "            self[date(year, JUL, 9)] = \"Revoluo Constitucionalista de 1932\"\n",
            "\n",
            "        if self.state == 'SE':\n",
            "            self[date(year, JUL, 8)] = \"Autonomia poltica de Sergipe\"\n",
            "\n",
            "        if self.state == 'TO':\n",
            "            self[date(year, JAN, 1)] = \"Instalao de Tocantins\"\n",
            "            self[date(year, SEP, 8)] = \"Nossa Senhora da Natividade\"\n",
            "            self[date(year, OCT, 5)] = \"Criao de Tocantins\"\n",
            "\n",
            "\n",
            "class BR(Brazil):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Bulgaria(HolidayBase):\n",
            "    \"\"\"\n",
            "    Official holidays in Bulgaria in their current form. This class does not\n",
            "    any return holidays before 1990, as holidays in the People's Republic of\n",
            "    Bulgaria and earlier were different.\n",
            "\n",
            "    Most holidays are fixed and if the date falls on a Saturday or a Sunday,\n",
            "    the following Monday is a non-working day. The exceptions are (1) the\n",
            "    Easter holidays, which are always a consecutive Friday, Saturday, and\n",
            "    Sunday; and (2) the National Awakening Day which, while an official holiday\n",
            "    and a non-attendance day for schools, is still a working day.\n",
            "\n",
            "    Sources (Bulgarian):\n",
            "    - http://lex.bg/laws/ldoc/1594373121\n",
            "    - https://www.parliament.bg/bg/24\n",
            "\n",
            "    Sources (English):\n",
            "    - https://en.wikipedia.org/wiki/Public_holidays_in_Bulgaria\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'BG'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        if year < 1990:\n",
            "            return\n",
            "\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \" \"\n",
            "\n",
            "        # Liberation Day\n",
            "        self[date(year, MAR, 3)] = \\\n",
            "            \"       \"\n",
            "\n",
            "        # International Workers' Day\n",
            "        self[date(year, MAY, 1)] = \\\n",
            "            \"       \"\n",
            "\n",
            "        # Saint George's Day\n",
            "        self[date(year, MAY, 6)] = \\\n",
            "            \",      \"\n",
            "\n",
            "        # Bulgarian Education and Culture and Slavonic Literature Day\n",
            "        self[date(year, MAY, 24)] = \\\n",
            "            \"         \"\n",
            "\n",
            "        # Unification Day\n",
            "        self[date(year, SEP, 6)] = \"  \"\n",
            "\n",
            "        # Independence Day\n",
            "        self[date(year, SEP, 22)] = \"    \"\n",
            "\n",
            "        # National Awakening Day\n",
            "        self[date(year, NOV, 1)] = \"   \"\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 24)] = \" \"\n",
            "        self[date(year, DEC, 25)] = \" \"\n",
            "        self[date(year, DEC, 26)] = \" \"\n",
            "\n",
            "        # Easter\n",
            "        self[easter(year, method=EASTER_ORTHODOX)-rd(days=2)] = \" \"\n",
            "        self[easter(year, method=EASTER_ORTHODOX)-rd(days=1)] = \" \"\n",
            "        self[easter(year, method=EASTER_ORTHODOX)] = \"\"\n",
            "\n",
            "\n",
            "class BG(Bulgaria):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Canada(HolidayBase):\n",
            "    PROVINCES = ['AB', 'BC', 'MB', 'NB', 'NL', 'NS', 'NT', 'NU', 'ON', 'PE',\n",
            "                 'QC', 'SK', 'YU']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'CA'\n",
            "        self.prov = kwargs.pop('prov', 'ON')\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        if year >= 1867:\n",
            "            name = \"New Year's Day\"\n",
            "            self[date(year, JAN, 1)] = name\n",
            "            if self.observed and date(year, JAN, 1).weekday() == SUN:\n",
            "                self[date(year, JAN, 1) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, JAN, 1).weekday() == SAT:\n",
            "                # Add Dec 31st from the previous year without triggering\n",
            "                # the entire year to be added\n",
            "                expand = self.expand\n",
            "                self.expand = False\n",
            "                self[date(year, JAN, 1) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "                self.expand = expand\n",
            "            # The next year's observed New Year's Day can be in this year\n",
            "            # when it falls on a Friday (Jan 1st is a Saturday)\n",
            "            if self.observed and date(year, DEC, 31).weekday() == FRI:\n",
            "                self[date(year, DEC, 31)] = name + \" (Observed)\"\n",
            "\n",
            "        # Family Day / Louis Riel Day (MB) / Islander Day (PE)\n",
            "        # / Heritage Day (NS, YU)\n",
            "        if self.prov in ('AB', 'SK', 'ON') and year >= 2008:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Family Day\"\n",
            "        elif self.prov in ('AB', 'SK') and year >= 2007:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Family Day\"\n",
            "        elif self.prov == 'AB' and year >= 1990:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Family Day\"\n",
            "        elif self.prov == 'NB' and year >= 2018:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Family Day\"\n",
            "        elif self.prov == 'BC':\n",
            "            if year >= 2013 and year <= 2018:\n",
            "                self[date(year, FEB, 1) + rd(weekday=MO(+2))] = \\\n",
            "                    \"Family Day\"\n",
            "            elif year > 2018:\n",
            "                self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \\\n",
            "                    \"Family Day\"\n",
            "        elif self.prov == 'MB' and year >= 2008:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \\\n",
            "                \"Louis Riel Day\"\n",
            "        elif self.prov == 'PE' and year >= 2010:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Islander Day\"\n",
            "        elif self.prov == 'PE' and year == 2009:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+2))] = \"Islander Day\"\n",
            "        elif self.prov == 'NS' and year >= 2015:\n",
            "            # http://novascotia.ca/lae/employmentrights/NovaScotiaHeritageDay.asp\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = \"Heritage Day\"\n",
            "        elif self.prov == 'YU':\n",
            "            # start date?\n",
            "            # http://heritageyukon.ca/programs/heritage-day\n",
            "            # https://en.wikipedia.org/wiki/Family_Day_(Canada)#Yukon_Heritage_Day\n",
            "            # Friday before the last Sunday in February\n",
            "            dt = date(year, MAR, 1) + rd(weekday=SU(-1)) + rd(weekday=FR(-1))\n",
            "            self[dt] = \"Heritage Day\"\n",
            "\n",
            "        # St. Patrick's Day\n",
            "        if self.prov == 'NL' and year >= 1900:\n",
            "            dt = date(year, MAR, 17)\n",
            "            # Nearest Monday to March 17\n",
            "            dt1 = date(year, MAR, 17) + rd(weekday=MO(-1))\n",
            "            dt2 = date(year, MAR, 17) + rd(weekday=MO(+1))\n",
            "            if dt2 - dt <= dt - dt1:\n",
            "                self[dt2] = \"St. Patrick's Day\"\n",
            "            else:\n",
            "                self[dt1] = \"St. Patrick's Day\"\n",
            "\n",
            "        # Good Friday\n",
            "        if self.prov != 'QC' and year >= 1867:\n",
            "            self[easter(year) + rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "\n",
            "        # Easter Monday\n",
            "        if self.prov == 'QC' and year >= 1867:\n",
            "            self[easter(year) + rd(weekday=MO)] = \"Easter Monday\"\n",
            "\n",
            "        # St. George's Day\n",
            "        if self.prov == 'NL' and year == 2010:\n",
            "            # 4/26 is the Monday closer to 4/23 in 2010\n",
            "            # but the holiday was observed on 4/19? Crazy Newfies!\n",
            "            self[date(2010, 4, 19)] = \"St. George's Day\"\n",
            "        elif self.prov == 'NL' and year >= 1990:\n",
            "            dt = date(year, APR, 23)\n",
            "            # Nearest Monday to April 23\n",
            "            dt1 = dt + rd(weekday=MO(-1))\n",
            "            dt2 = dt + rd(weekday=MO(+1))\n",
            "            if dt2 - dt < dt - dt1:\n",
            "                self[dt2] = \"St. George's Day\"\n",
            "            else:\n",
            "                self[dt1] = \"St. George's Day\"\n",
            "\n",
            "        # Victoria Day / National Patriots' Day (QC)\n",
            "        if self.prov not in ('NB', 'NS', 'PE', 'NL', 'QC') and year >= 1953:\n",
            "            self[date(year, MAY, 24) + rd(weekday=MO(-1))] = \"Victoria Day\"\n",
            "        elif self.prov == 'QC' and year >= 1953:\n",
            "            name = \"National Patriots' Day\"\n",
            "            self[date(year, MAY, 24) + rd(weekday=MO(-1))] = name\n",
            "\n",
            "        # National Aboriginal Day\n",
            "        if self.prov == 'NT' and year >= 1996:\n",
            "            self[date(year, JUN, 21)] = \"National Aboriginal Day\"\n",
            "\n",
            "        # St. Jean Baptiste Day\n",
            "        if self.prov == 'QC' and year >= 1925:\n",
            "            self[date(year, JUN, 24)] = \"St. Jean Baptiste Day\"\n",
            "            if self.observed and date(year, JUN, 24).weekday() == SUN:\n",
            "                self[date(year, JUN, 25)] = \"St. Jean Baptiste Day (Observed)\"\n",
            "\n",
            "        # Discovery Day\n",
            "        if self.prov == 'NL' and year >= 1997:\n",
            "            dt = date(year, JUN, 24)\n",
            "            # Nearest Monday to June 24\n",
            "            dt1 = dt + rd(weekday=MO(-1))\n",
            "            dt2 = dt + rd(weekday=MO(+1))\n",
            "            if dt2 - dt <= dt - dt1:\n",
            "                self[dt2] = \"Discovery Day\"\n",
            "            else:\n",
            "                self[dt1] = \"Discovery Day\"\n",
            "        elif self.prov == 'YU' and year >= 1912:\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO(+3))] = \"Discovery Day\"\n",
            "\n",
            "        # Canada Day / Memorial Day (NL)\n",
            "        if self.prov != 'NL' and year >= 1867:\n",
            "            if year >= 1983:\n",
            "                name = \"Canada Day\"\n",
            "            else:\n",
            "                name = \"Dominion Day\"\n",
            "            self[date(year, JUL, 1)] = name\n",
            "            if year >= 1879 and self.observed \\\n",
            "                    and date(year, JUL, 1).weekday() in WEEKEND:\n",
            "                self[date(year, JUL, 1) + rd(weekday=MO)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "        elif year >= 1867:\n",
            "            if year >= 1983:\n",
            "                name = \"Memorial Day\"\n",
            "            else:\n",
            "                name = \"Dominion Day\"\n",
            "            self[date(year, JUL, 1)] = name\n",
            "            if year >= 1879 and self.observed \\\n",
            "                    and date(year, JUL, 1).weekday() in WEEKEND:\n",
            "                self[date(year, JUL, 1) + rd(weekday=MO)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Nunavut Day\n",
            "        if self.prov == 'NU' and year >= 2001:\n",
            "            self[date(year, JUL, 9)] = \"Nunavut Day\"\n",
            "            if self.observed and date(year, JUL, 9).weekday() == SUN:\n",
            "                self[date(year, JUL, 10)] = \"Nunavut Day (Observed)\"\n",
            "        elif self.prov == 'NU' and year == 2000:\n",
            "            self[date(2000, 4, 1)] = \"Nunavut Day\"\n",
            "\n",
            "        # Civic Holiday\n",
            "        if self.prov in ('ON', 'MB', 'NT') and year >= 1900:\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = \"Civic Holiday\"\n",
            "        elif self.prov == 'AB' and year >= 1974:\n",
            "            # https://en.wikipedia.org/wiki/Civic_Holiday#Alberta\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = \"Heritage Day\"\n",
            "        elif self.prov == 'BC' and year >= 1974:\n",
            "            # https://en.wikipedia.org/wiki/Civic_Holiday\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = \\\n",
            "                \"British Columbia Day\"\n",
            "        elif self.prov == 'NB' and year >= 1900:\n",
            "            # https://en.wikipedia.org/wiki/Civic_Holiday\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = \"New Brunswick Day\"\n",
            "        elif self.prov == 'SK' and year >= 1900:\n",
            "            # https://en.wikipedia.org/wiki/Civic_Holiday\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = \"Saskatchewan Day\"\n",
            "\n",
            "        # Labour Day\n",
            "        if year >= 1894:\n",
            "            self[date(year, SEP, 1) + rd(weekday=MO)] = \"Labour Day\"\n",
            "\n",
            "        # Thanksgiving\n",
            "        if self.prov not in ('NB', 'NS', 'PE', 'NL') and year >= 1931:\n",
            "            if year == 1935:\n",
            "                # in 1935, Canadian Thanksgiving was moved due to the General\n",
            "                # Election falling on the second Monday of October\n",
            "                # https://books.google.ca/books?id=KcwlQsmheG4C&pg=RA1-PA1940&lpg=RA1-PA1940&dq=canada+thanksgiving+1935&source=bl&ots=j4qYrcfGuY&sig=gxXeAQfXVsOF9fOwjSMswPHJPpM&hl=en&sa=X&ved=0ahUKEwjO0f3J2PjOAhVS4mMKHRzKBLAQ6AEIRDAG#v=onepage&q=canada%20thanksgiving%201935&f=false\n",
            "                self[date(1935, 10, 25)] = \"Thanksgiving\"\n",
            "            else:\n",
            "                self[date(year, OCT, 1) + rd(weekday=MO(+2))] = \\\n",
            "                    \"Thanksgiving\"\n",
            "\n",
            "        # Remembrance Day\n",
            "        name = \"Remembrance Day\"\n",
            "        provinces = ('ON', 'QC', 'NS', 'NL', 'NT', 'PE', 'SK')\n",
            "        if self.prov not in provinces and year >= 1931:\n",
            "            self[date(year, NOV, 11)] = name\n",
            "        elif self.prov in ('NS', 'NL', 'NT', 'PE', 'SK') and year >= 1931:\n",
            "            self[date(year, NOV, 11)] = name\n",
            "            if self.observed and date(year, NOV, 11).weekday() == SUN:\n",
            "                name = name + \" (Observed)\"\n",
            "                self[date(year, NOV, 11) + rd(weekday=MO)] = name\n",
            "\n",
            "        # Christmas Day\n",
            "        if year >= 1867:\n",
            "            self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "            if self.observed \\\n",
            "                    and date(year, DEC, 25).weekday() == SAT:\n",
            "                self[date(year, DEC, 24)] = \"Christmas Day (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, DEC, 25).weekday() == SUN:\n",
            "                self[date(year, DEC, 26)] = \"Christmas Day (Observed)\"\n",
            "\n",
            "        # Boxing Day\n",
            "        if year >= 1867:\n",
            "            name = \"Boxing Day\"\n",
            "            name_observed = name + \" (Observed)\"\n",
            "            if self.observed and date(year, DEC, 26).weekday() in WEEKEND:\n",
            "                self[date(year, DEC, 26) + rd(weekday=MO)] = name_observed\n",
            "            elif self.observed and date(year, DEC, 26).weekday() == 0:\n",
            "                self[date(year, DEC, 27)] = name_observed\n",
            "            else:\n",
            "                self[date(year, DEC, 26)] = name\n",
            "\n",
            "\n",
            "class CA(Canada):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Colombia(HolidayBase):\n",
            "    # https://es.wikipedia.org/wiki/Anexo:D%C3%ADas_festivos_en_Colombia\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'CO'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "\n",
            "        # Fixed date holidays!\n",
            "        # If observed=True and they fall on a weekend they are not observed.\n",
            "        # If observed=False there are 18 holidays\n",
            "\n",
            "        # New Year's Day\n",
            "        if self.observed and date(year, JAN, 1).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JAN, 1)] = \"Ao Nuevo [New Year's Day]\"\n",
            "\n",
            "        # Labor Day\n",
            "        self[date(year, MAY, 1)] = \"Da del Trabajo [Labour Day]\"\n",
            "\n",
            "        # Independence Day\n",
            "        name = \"Da de la Independencia [Independence Day]\"\n",
            "        if self.observed and date(year, JUL, 20).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, JUL, 20)] = name\n",
            "\n",
            "        # Battle of Boyaca\n",
            "        self[date(year, AUG, 7)] = \"Batalla de Boyac [Battle of Boyac]\"\n",
            "\n",
            "        # Immaculate Conception\n",
            "        if self.observed and date(year, DEC, 8).weekday() in WEEKEND:\n",
            "            pass\n",
            "        else:\n",
            "            self[date(year, DEC, 8)] = \"La Inmaculada Concepcin\" \\\n",
            "                \" [Immaculate Conception]\"\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Navidad [Christmas]\"\n",
            "\n",
            "        # Emiliani Law holidays!\n",
            "        # Unless they fall on a Monday they are observed the following monday\n",
            "\n",
            "        #  Epiphany\n",
            "        name = \"Da de los Reyes Magos [Epiphany]\"\n",
            "        if date(year, JAN, 6).weekday() == MON or not self.observed:\n",
            "            self[date(year, JAN, 6)] = name\n",
            "        else:\n",
            "            self[date(year, JAN, 6) + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Saint Joseph's Day\n",
            "        name = \"Da de San Jos [Saint Joseph's Day]\"\n",
            "        if date(year, MAR, 19).weekday() == MON or not self.observed:\n",
            "            self[date(year, MAR, 19)] = name\n",
            "        else:\n",
            "            self[date(year, MAR, 19) + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Saint Peter and Saint Paul's Day\n",
            "        name = \"San Pedro y San Pablo [Saint Peter and Saint Paul]\"\n",
            "        if date(year, JUN, 29).weekday() == MON or not self.observed:\n",
            "            self[date(year, JUN, 29)] = name\n",
            "        else:\n",
            "            self[date(year, JUN, 29) + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Assumption of Mary\n",
            "        name = \"La Asuncin [Assumption of Mary]\"\n",
            "        if date(year, AUG, 15).weekday() == MON or not self.observed:\n",
            "            self[date(year, AUG, 15)] = name\n",
            "        else:\n",
            "            self[date(year, AUG, 15) + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Discovery of America\n",
            "        name = \"Descubrimiento de Amrica [Discovery of America]\"\n",
            "        if date(year, OCT, 12).weekday() == MON or not self.observed:\n",
            "            self[date(year, OCT, 12)] = name\n",
            "        else:\n",
            "            self[date(year, OCT, 12) + rd(weekday=MO)] = name + \\\n",
            "                \"(Observed)\"\n",
            "\n",
            "        # All Saints Day\n",
            "        name = \"Dia de Todos los Santos [All Saint's Day]\"\n",
            "        if date(year, NOV, 1).weekday() == MON or not self.observed:\n",
            "            self[date(year, NOV, 1)] = name\n",
            "        else:\n",
            "            self[date(year, NOV, 1) + rd(weekday=MO)] = name + \\\n",
            "                \"(Observed)\"\n",
            "\n",
            "        # Independence of Cartagena\n",
            "        name = \"Independencia de Cartagena [Independence of Cartagena]\"\n",
            "        if date(year, NOV, 11).weekday() == MON or not self.observed:\n",
            "            self[date(year, NOV, 11)] = name\n",
            "        else:\n",
            "            self[date(year, NOV, 11) + rd(weekday=MO)] = name + \\\n",
            "                \"(Observed)\"\n",
            "\n",
            "        # Holidays based on Easter\n",
            "\n",
            "        # Maundy Thursday\n",
            "        self[easter(year) + rd(weekday=TH(-1))\n",
            "             ] = \"Jueves Santo [Maundy Thursday]\"\n",
            "\n",
            "        # Good Friday\n",
            "        self[easter(year) + rd(weekday=FR(-1))\n",
            "             ] = \"Viernes Santo [Good Friday]\"\n",
            "\n",
            "        # Holidays based on Easter but are observed the following monday\n",
            "        # (unless they occur on a monday)\n",
            "\n",
            "        # Ascension of Jesus\n",
            "        name = \"Ascensin del seor [Ascension of Jesus]\"\n",
            "        hdate = easter(year) + rd(days=+39)\n",
            "        if hdate.weekday() == MON or not self.observed:\n",
            "            self[hdate] = name\n",
            "        else:\n",
            "            self[hdate + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Corpus Christi\n",
            "        name = \"Corpus Christi [Corpus Christi]\"\n",
            "        hdate = easter(year) + rd(days=+60)\n",
            "        if hdate.weekday() == MON or not self.observed:\n",
            "            self[hdate] = name\n",
            "        else:\n",
            "            self[hdate + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "        # Sacred Heart\n",
            "        name = \"Sagrado Corazn [Sacred Heart]\"\n",
            "        hdate = easter(year) + rd(days=+68)\n",
            "        if hdate.weekday() == MON or not self.observed:\n",
            "            self[hdate] = name\n",
            "        else:\n",
            "            self[hdate + rd(weekday=MO)] = name + \"(Observed)\"\n",
            "\n",
            "\n",
            "class CO(Colombia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Mexico(HolidayBase):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'MX'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        name = \"Ao Nuevo [New Year's Day]\"\n",
            "        self[date(year, JAN, 1)] = name\n",
            "        if self.observed and date(year, JAN, 1).weekday() == SUN:\n",
            "            self[date(year, JAN, 1) + rd(days=+1)] = name + \" (Observed)\"\n",
            "        elif self.observed and date(year, JAN, 1).weekday() == SAT:\n",
            "            # Add Dec 31st from the previous year without triggering\n",
            "            # the entire year to be added\n",
            "            expand = self.expand\n",
            "            self.expand = False\n",
            "            self[date(year, JAN, 1) + rd(days=-1)] = name + \" (Observed)\"\n",
            "            self.expand = expand\n",
            "        # The next year's observed New Year's Day can be in this year\n",
            "        # when it falls on a Friday (Jan 1st is a Saturday)\n",
            "        if self.observed and date(year, DEC, 31).weekday() == FRI:\n",
            "            self[date(year, DEC, 31)] = name + \" (Observed)\"\n",
            "\n",
            "        # Constitution Day\n",
            "        name = \"Da de la Constitucin [Constitution Day]\"\n",
            "        if 2006 >= year >= 1917:\n",
            "            self[date(year, FEB, 5)] = name\n",
            "        elif year >= 2007:\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+1))] = name\n",
            "\n",
            "        # Benito Jurez's birthday\n",
            "        name = \"Natalicio de Benito Jurez [Benito Jurez's birthday]\"\n",
            "        if 2006 >= year >= 1917:\n",
            "            self[date(year, MAR, 21)] = name\n",
            "        elif year >= 2007:\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO(+3))] = name\n",
            "\n",
            "        # Labor Day\n",
            "        if year >= 1923:\n",
            "            self[date(year, MAY, 1)] = \"Da del Trabajo [Labour Day]\"\n",
            "            if self.observed and date(year, MAY, 1).weekday() == SAT:\n",
            "                self[date(year, MAY, 1) + rd(days=-1)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, MAY, 1).weekday() == SUN:\n",
            "                self[date(year, MAY, 1) + rd(days=+1)] = name + \" (Observed)\"\n",
            "\n",
            "        # Independence Day\n",
            "        name = \"Da de la Independencia [Independence Day]\"\n",
            "        self[date(year, SEP, 16)] = name\n",
            "        if self.observed and date(year, SEP, 16).weekday() == SAT:\n",
            "            self[date(year, SEP, 16) + rd(days=-1)] = name + \\\n",
            "                \" (Observed)\"\n",
            "        elif self.observed and date(year, SEP, 16).weekday() == SUN:\n",
            "            self[date(year, SEP, 16) + rd(days=+1)] = name + \\\n",
            "                \" (Observed)\"\n",
            "\n",
            "        # Revolution Day\n",
            "        name = \"Da de la Revolucin [Revolution Day]\"\n",
            "        if 2006 >= year >= 1917:\n",
            "            self[date(year, NOV, 20)] = name\n",
            "        elif year >= 2007:\n",
            "            self[date(year, NOV, 1) + rd(weekday=MO(+3))] = name\n",
            "\n",
            "        # Change of Federal Government\n",
            "        # Every six years--next observance 2018\n",
            "        name = \"Transmisin del Poder Ejecutivo Federal\"\n",
            "        name += \" [Change of Federal Government]\"\n",
            "        if (2018 - year) % 6 == 0:\n",
            "            self[date(year, DEC, 1)] = name\n",
            "            if self.observed \\\n",
            "                    and date(year, DEC, 1).weekday() == SAT:\n",
            "                self[date(year, DEC, 1) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, DEC, 1).weekday() == SUN:\n",
            "                self[date(year, DEC, 1) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Navidad [Christmas]\"\n",
            "        if self.observed and date(year, DEC, 25).weekday() == SAT:\n",
            "            self[date(year, DEC, 25) + rd(days=-1)] = name + \" (Observed)\"\n",
            "        elif self.observed and date(year, DEC, 25).weekday() == SUN:\n",
            "            self[date(year, DEC, 25) + rd(days=+1)] = name + \" (Observed)\"\n",
            "\n",
            "\n",
            "class MX(Mexico):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Ukraine(HolidayBase):\n",
            "    \"\"\"\n",
            "    http://zakon1.rada.gov.ua/laws/show/322-08/paran454#n454\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"UA\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # The current set of holidays came into force in 1991\n",
            "        # But most holiday days was inplemented in 1981\n",
            "        if year < 1918:\n",
            "            return\n",
            "\n",
            "        # New Year's Day\n",
            "        if year >= 1898:\n",
            "            self[date(year, JAN, 1)] = \" \"\n",
            "\n",
            "        # Christmas Day (Orthodox)\n",
            "        if year >= 1991:\n",
            "            self[date(year, JAN, 7)] = \" \" \\\n",
            "                \" ()\"\n",
            "\n",
            "        # Women's Day\n",
            "        if year > 1965:\n",
            "            self[date(year, MAR, 8)] = \"  \"\n",
            "\n",
            "        # Easter\n",
            "        if year >= 1991:\n",
            "            self[easter(year, method=EASTER_ORTHODOX)] = \"\" \\\n",
            "                                                         \" ()\"\n",
            "\n",
            "        # Holy trinity\n",
            "        if year >= 1991:\n",
            "            self[easter(year, method=EASTER_ORTHODOX) + rd(days=49)] = \"\"\n",
            "\n",
            "        # Labour Day\n",
            "        if year > 2017:\n",
            "            name = \" \"\n",
            "        elif 1917 < year <= 2017:\n",
            "            name = \"   \"\n",
            "        self[date(year, MAY, 1)] = name\n",
            "\n",
            "        # Labour Day in past\n",
            "        if 1928 < year < 2018:\n",
            "            self[date(year, MAY, 2)] = \"   \"\n",
            "\n",
            "        # Victory Day\n",
            "        name = \" \"\n",
            "        if year >= 1965:\n",
            "            self[date(year, MAY, 9)] = name\n",
            "        if 1945 <= year < 1947:\n",
            "            self[date(year, MAY, 9)] = name\n",
            "            self[date(year, SEP, 3)] = \"   \"\n",
            "\n",
            "        # Constitution Day\n",
            "        if year >= 1997:\n",
            "            self[date(year, JUN, 28)] = \"  \"\n",
            "\n",
            "        # Independence Day\n",
            "        name = \"  \"\n",
            "        if year > 1991:\n",
            "            self[date(year, AUG, 24)] = name\n",
            "        elif year == 1991:\n",
            "            self[date(year, JUL, 16)] = name\n",
            "\n",
            "        # Day of the defender of Ukraine\n",
            "        if year >= 2015:\n",
            "            self[date(year, OCT, 14)] = \"  \"\n",
            "\n",
            "        # USSR Constitution day\n",
            "        name = \"  \"\n",
            "        if 1981 <= year < 1991:\n",
            "            self[date(year, OCT, 7)] = name\n",
            "        elif 1937 <= year < 1981:\n",
            "            self[date(year, DEC, 5)] = name\n",
            "\n",
            "        # October Revolution\n",
            "        if 1917 < year < 2000:\n",
            "            if year <= 1991:\n",
            "                name = \"  \" \\\n",
            "                       \"  \"\n",
            "            else:\n",
            "                name = \"  \"\n",
            "            self[date(year, NOV, 7)] = name\n",
            "            self[date(year, NOV, 8)] = name\n",
            "\n",
            "        # Christmas Day (Catholic)\n",
            "        if year >= 2017:\n",
            "            self[date(year, DEC, 25)] = \" \" \\\n",
            "                \" ()\"\n",
            "        # USSR holidays\n",
            "        # Bloody_Sunday_(1905)\n",
            "        if 1917 <= year < 1951:\n",
            "            self[date(year, JAN, 22)] = \" ' 9  1905 \"\n",
            "\n",
            "        # Paris_Commune\n",
            "        if 1917 < year < 1929:\n",
            "            self[date(year, MAR, 18)] = \"  \"\n",
            "\n",
            "\n",
            "class UA(Ukraine):\n",
            "    pass\n",
            "\n",
            "\n",
            "class UnitedStates(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_the_United_States\n",
            "\n",
            "    STATES = ['AL', 'AK', 'AS', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL',\n",
            "              'GA', 'GU', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME',\n",
            "              'MD', 'MH', 'MA', 'MI', 'FM', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV',\n",
            "              'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'MP', 'OH', 'OK', 'OR', 'PW',\n",
            "              'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'VI',\n",
            "              'WA', 'WV', 'WI', 'WY']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'US'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        if year > 1870:\n",
            "            name = \"New Year's Day\"\n",
            "            self[date(year, JAN, 1)] = name\n",
            "            if self.observed and date(year, JAN, 1).weekday() == SUN:\n",
            "                self[date(year, JAN, 1) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, JAN, 1).weekday() == SAT:\n",
            "                # Add Dec 31st from the previous year without triggering\n",
            "                # the entire year to be added\n",
            "                expand = self.expand\n",
            "                self.expand = False\n",
            "                self[date(year, JAN, 1) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "                self.expand = expand\n",
            "            # The next year's observed New Year's Day can be in this year\n",
            "            # when it falls on a Friday (Jan 1st is a Saturday)\n",
            "            if self.observed and date(year, DEC, 31).weekday() == FRI:\n",
            "                self[date(year, DEC, 31)] = name + \" (Observed)\"\n",
            "\n",
            "        # Epiphany\n",
            "        if self.state == 'PR':\n",
            "            self[date(year, JAN, 6)] = \"Epiphany\"\n",
            "\n",
            "        # Three King's Day\n",
            "        if self.state == 'VI':\n",
            "            self[date(year, JAN, 6)] = \"Three King's Day\"\n",
            "\n",
            "        # Lee Jackson Day\n",
            "        name = \"Lee Jackson Day\"\n",
            "        if self.state == 'VA' and year >= 2000:\n",
            "            dt = date(year, JAN, 1) + rd(weekday=MO(+3)) + rd(\n",
            "                weekday=FR(-1))\n",
            "            self[dt] = name\n",
            "        elif self.state == 'VA' and year >= 1983:\n",
            "            self[date(year, JAN, 1) + rd(weekday=MO(+3))] = name\n",
            "        elif self.state == 'VA' and year >= 1889:\n",
            "            self[date(year, JAN, 19)] = name\n",
            "\n",
            "        # Inauguration Day\n",
            "        if self.state in ('DC', 'LA', 'MD', 'VA') and year >= 1789:\n",
            "            name = \"Inauguration Day\"\n",
            "            if (year - 1789) % 4 == 0 and year >= 1937:\n",
            "                self[date(year, JAN, 20)] = name\n",
            "                if date(year, JAN, 20).weekday() == SUN:\n",
            "                    self[date(year, JAN, 21)] = name + \" (Observed)\"\n",
            "            elif (year - 1789) % 4 == 0:\n",
            "                self[date(year, MAR, 4)] = name\n",
            "                if date(year, MAR, 4).weekday() == SUN:\n",
            "                    self[date(year, MAR, 5)] = name + \" (Observed)\"\n",
            "\n",
            "        # Martin Luther King, Jr. Day\n",
            "        if year >= 1986:\n",
            "            name = \"Martin Luther King, Jr. Day\"\n",
            "            if self.state == 'AL':\n",
            "                name = \"Robert E. Lee/Martin Luther King Birthday\"\n",
            "            elif self.state in ('AS', 'MS'):\n",
            "                name = (\"Dr. Martin Luther King Jr. \"\n",
            "                        \"and Robert E. Lee's Birthdays\")\n",
            "            elif self.state in ('AZ', 'NH'):\n",
            "                name = \"Dr. Martin Luther King Jr./Civil Rights Day\"\n",
            "            elif self.state == 'GA' and year < 2012:\n",
            "                name = \"Robert E. Lee's Birthday\"\n",
            "            elif self.state == 'ID' and year >= 2006:\n",
            "                name = \"Martin Luther King, Jr. - Idaho Human Rights Day\"\n",
            "            self[date(year, JAN, 1) + rd(weekday=MO(+3))] = name\n",
            "\n",
            "        # Lincoln's Birthday\n",
            "        name = \"Lincoln's Birthday\"\n",
            "        if (self.state in ('CT', 'IL', 'IA', 'NJ', 'NY') and year >= 1971) \\\n",
            "                or (self.state == 'CA' and 1971 <= year <= 2009):\n",
            "            self[date(year, FEB, 12)] = name\n",
            "            if self.observed \\\n",
            "                    and date(year, FEB, 12).weekday() == SAT:\n",
            "                self[date(year, FEB, 11)] = name + \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, FEB, 12).weekday() == SUN:\n",
            "                self[date(year, FEB, 13)] = name + \" (Observed)\"\n",
            "\n",
            "        # Susan B. Anthony Day\n",
            "        if (self.state == 'CA' and year >= 2014) \\\n",
            "                or (self.state == 'FL' and year >= 2011) \\\n",
            "                or (self.state == 'NY' and year >= 2004) \\\n",
            "                or (self.state == 'WI' and year >= 1976):\n",
            "            self[date(year, FEB, 15)] = \"Susan B. Anthony Day\"\n",
            "\n",
            "        # Washington's Birthday\n",
            "        name = \"Washington's Birthday\"\n",
            "        if self.state == 'AL':\n",
            "            name = \"George Washington/Thomas Jefferson Birthday\"\n",
            "        elif self.state == 'AS':\n",
            "            name = \"George Washington's Birthday and Daisy Gatson Bates Day\"\n",
            "        elif self.state in ('PR', 'VI'):\n",
            "            name = \"Presidents' Day\"\n",
            "        if self.state not in ('DE', 'FL', 'GA', 'NM', 'PR'):\n",
            "            if year > 1970:\n",
            "                self[date(year, FEB, 1) + rd(weekday=MO(+3))] = name\n",
            "            elif year >= 1879:\n",
            "                self[date(year, FEB, 22)] = name\n",
            "        elif self.state == 'GA':\n",
            "            if date(year, DEC, 24).weekday() != WED:\n",
            "                self[date(year, DEC, 24)] = name\n",
            "            else:\n",
            "                self[date(year, DEC, 26)] = name\n",
            "        elif self.state in ('PR', 'VI'):\n",
            "            self[date(year, FEB, 1) + rd(weekday=MO(+3))] = name\n",
            "\n",
            "        # Mardi Gras\n",
            "        if self.state == 'LA' and year >= 1857:\n",
            "            self[easter(year) + rd(days=-47)] = \"Mardi Gras\"\n",
            "\n",
            "        # Guam Discovery Day\n",
            "        if self.state == 'GU' and year >= 1970:\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO)] = \"Guam Discovery Day\"\n",
            "\n",
            "        # Casimir Pulaski Day\n",
            "        if self.state == 'IL' and year >= 1978:\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO)] = \"Casimir Pulaski Day\"\n",
            "\n",
            "        # Texas Independence Day\n",
            "        if self.state == 'TX' and year >= 1874:\n",
            "            self[date(year, MAR, 2)] = \"Texas Independence Day\"\n",
            "\n",
            "        # Town Meeting Day\n",
            "        if self.state == 'VT' and year >= 1800:\n",
            "            self[date(year, MAR, 1) + rd(weekday=TU)] = \"Town Meeting Day\"\n",
            "\n",
            "        # Evacuation Day\n",
            "        if self.state == 'MA' and year >= 1901:\n",
            "            name = \"Evacuation Day\"\n",
            "            self[date(year, MAR, 17)] = name\n",
            "            if date(year, MAR, 17).weekday() in WEEKEND:\n",
            "                self[date(year, MAR, 17) + rd(weekday=MO)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Emancipation Day\n",
            "        if self.state == 'PR':\n",
            "            self[date(year, MAR, 22)] = \"Emancipation Day\"\n",
            "            if self.observed and date(year, MAR, 22).weekday() == SUN:\n",
            "                self[date(year, MAR, 23)] = \"Emancipation Day (Observed)\"\n",
            "\n",
            "        # Prince Jonah Kuhio Kalanianaole Day\n",
            "        if self.state == 'HI' and year >= 1949:\n",
            "            name = \"Prince Jonah Kuhio Kalanianaole Day\"\n",
            "            self[date(year, MAR, 26)] = name\n",
            "            if self.observed and date(year, MAR, 26).weekday() == SAT:\n",
            "                self[date(year, MAR, 25)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, MAR, 26).weekday() == SUN:\n",
            "                self[date(year, MAR, 27)] = name + \" (Observed)\"\n",
            "\n",
            "        # Steward's Day\n",
            "        name = \"Steward's Day\"\n",
            "        if self.state == 'AK' and year >= 1955:\n",
            "            self[date(year, APR, 1) + rd(days=-1, weekday=MO(-1))] = name\n",
            "        elif self.state == 'AK' and year >= 1918:\n",
            "            self[date(year, MAR, 30)] = name\n",
            "\n",
            "        # Csar Chvez Day\n",
            "        name = \"Csar Chvez Day\"\n",
            "        if self.state == 'CA' and year >= 1995:\n",
            "            self[date(year, MAR, 31)] = name\n",
            "            if self.observed and date(year, MAR, 31).weekday() == SUN:\n",
            "                self[date(year, APR, 1)] = name + \" (Observed)\"\n",
            "        elif self.state == 'TX' and year >= 2000:\n",
            "            self[date(year, MAR, 31)] = name\n",
            "\n",
            "        # Transfer Day\n",
            "        if self.state == 'VI':\n",
            "            self[date(year, MAR, 31)] = \"Transfer Day\"\n",
            "\n",
            "        # Emancipation Day\n",
            "        if self.state == 'DC' and year >= 2005:\n",
            "            name = \"Emancipation Day\"\n",
            "            self[date(year, APR, 16)] = name\n",
            "            if self.observed and date(year, APR, 16).weekday() == SAT:\n",
            "                self[date(year, APR, 15)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, APR, 16).weekday() == SUN:\n",
            "                self[date(year, APR, 17)] = name + \" (Observed)\"\n",
            "\n",
            "        # Patriots' Day\n",
            "        if self.state in ('ME', 'MA') and year >= 1969:\n",
            "            self[date(year, APR, 1) + rd(weekday=MO(+3))] = \"Patriots' Day\"\n",
            "        elif self.state in ('ME', 'MA') and year >= 1894:\n",
            "            self[date(year, APR, 19)] = \"Patriots' Day\"\n",
            "\n",
            "        # Holy Thursday\n",
            "        if self.state == 'VI':\n",
            "            self[easter(year) + rd(weekday=TH(-1))] = \"Holy Thursday\"\n",
            "\n",
            "        # Good Friday\n",
            "        if self.state in ('CT', 'DE', 'GU', 'IN', 'KY', 'LA',\n",
            "                          'NJ', 'NC', 'PR', 'TN', 'TX', 'VI'):\n",
            "            self[easter(year) + rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "\n",
            "        # Easter Monday\n",
            "        if self.state == 'VI':\n",
            "            self[easter(year) + rd(weekday=MO)] = \"Easter Monday\"\n",
            "\n",
            "        # Confederate Memorial Day\n",
            "        name = \"Confederate Memorial Day\"\n",
            "        if self.state in ('AL', 'GA', 'MS', 'SC') and year >= 1866:\n",
            "            if self.state == 'GA' and year >= 2016:\n",
            "                name = \"State Holiday\"\n",
            "            self[date(year, APR, 1) + rd(weekday=MO(+4))] = name\n",
            "        elif self.state == 'TX' and year >= 1931:\n",
            "            self[date(year, JAN, 19)] = name\n",
            "\n",
            "        # San Jacinto Day\n",
            "        if self.state == 'TX' and year >= 1875:\n",
            "            self[date(year, APR, 21)] = \"San Jacinto Day\"\n",
            "\n",
            "        # Arbor Day\n",
            "        if self.state == 'NE' and year >= 1989:\n",
            "            self[date(year, APR, 30) + rd(weekday=FR(-1))] = \"Arbor Day\"\n",
            "        elif self.state == 'NE' and year >= 1875:\n",
            "            self[date(year, APR, 22)] = \"Arbor Day\"\n",
            "\n",
            "        # Primary Election Day\n",
            "        if self.state == 'IN' and \\\n",
            "                ((year >= 2006 and year % 2 == 0) or year >= 2015):\n",
            "            dt = date(year, MAY, 1) + rd(weekday=MO)\n",
            "            self[dt + rd(days=+1)] = \"Primary Election Day\"\n",
            "\n",
            "        # Truman Day\n",
            "        if self.state == 'MO' and year >= 1949:\n",
            "            name = \"Truman Day\"\n",
            "            self[date(year, MAY, 8)] = name\n",
            "            if self.observed and date(year, MAY, 8).weekday() == SAT:\n",
            "                self[date(year, MAY, 7)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, MAY, 8).weekday() == SUN:\n",
            "                self[date(year, MAY, 10)] = name + \" (Observed)\"\n",
            "\n",
            "        # Memorial Day\n",
            "        if year > 1970:\n",
            "            self[date(year, MAY, 31) + rd(weekday=MO(-1))] = \"Memorial Day\"\n",
            "        elif year >= 1888:\n",
            "            self[date(year, MAY, 30)] = \"Memorial Day\"\n",
            "\n",
            "        # Jefferson Davis Birthday\n",
            "        name = \"Jefferson Davis Birthday\"\n",
            "        if self.state == 'AL' and year >= 1890:\n",
            "            self[date(year, JUN, 1) + rd(weekday=MO)] = name\n",
            "\n",
            "        # Kamehameha Day\n",
            "        if self.state == 'HI' and year >= 1872:\n",
            "            self[date(year, JUN, 11)] = \"Kamehameha Day\"\n",
            "            if self.observed and year >= 2011:\n",
            "                if date(year, JUN, 11).weekday() == SAT:\n",
            "                    self[date(year, JUN, 10)] = \"Kamehameha Day (Observed)\"\n",
            "                elif date(year, JUN, 11).weekday() == SUN:\n",
            "                    self[date(year, JUN, 12)] = \"Kamehameha Day (Observed)\"\n",
            "\n",
            "        # Emancipation Day In Texas\n",
            "        if self.state == 'TX' and year >= 1980:\n",
            "            self[date(year, JUN, 19)] = \"Emancipation Day In Texas\"\n",
            "\n",
            "        # West Virginia Day\n",
            "        name = \"West Virginia Day\"\n",
            "        if self.state == 'WV' and year >= 1927:\n",
            "            self[date(year, JUN, 20)] = name\n",
            "            if self.observed and date(year, JUN, 20).weekday() == SAT:\n",
            "                self[date(year, JUN, 19)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, JUN, 20).weekday() == SUN:\n",
            "                self[date(year, JUN, 21)] = name + \" (Observed)\"\n",
            "\n",
            "        # Emancipation Day in US Virgin Islands\n",
            "        if self.state == 'VI':\n",
            "            self[date(year, JUL, 3)] = \"Emancipation Day\"\n",
            "\n",
            "        # Independence Day\n",
            "        if year > 1870:\n",
            "            name = \"Independence Day\"\n",
            "            self[date(year, JUL, 4)] = name\n",
            "            if self.observed and date(year, JUL, 4).weekday() == SAT:\n",
            "                self[date(year, JUL, 4) + rd(days=-1)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, JUL, 4).weekday() == SUN:\n",
            "                self[date(year, JUL, 4) + rd(days=+1)] = name + \" (Observed)\"\n",
            "\n",
            "        # Liberation Day (Guam)\n",
            "        if self.state == 'GU' and year >= 1945:\n",
            "            self[date(year, JUL, 21)] = \"Liberation Day (Guam)\"\n",
            "\n",
            "        # Pioneer Day\n",
            "        if self.state == 'UT' and year >= 1849:\n",
            "            name = \"Pioneer Day\"\n",
            "            self[date(year, JUL, 24)] = name\n",
            "            if self.observed and date(year, JUL, 24).weekday() == SAT:\n",
            "                self[date(year, JUL, 24) + rd(days=-1)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, JUL, 24).weekday() == SUN:\n",
            "                self[date(year, JUL, 24) + rd(days=+1)] = name + \" (Observed)\"\n",
            "\n",
            "        # Constitution Day\n",
            "        if self.state == 'PR':\n",
            "            self[date(year, JUL, 25)] = \"Constitution Day\"\n",
            "            if self.observed and date(year, JUL, 25).weekday() == SUN:\n",
            "                self[date(year, JUL, 26)] = \"Constitution Day (Observed)\"\n",
            "\n",
            "        # Victory Day\n",
            "        if self.state == 'RI' and year >= 1948:\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO(+2))] = \"Victory Day\"\n",
            "\n",
            "        # Statehood Day (Hawaii)\n",
            "        if self.state == 'HI' and year >= 1959:\n",
            "            self[date(year, AUG, 1) + rd(weekday=FR(+3))] = \"Statehood Day\"\n",
            "\n",
            "        # Bennington Battle Day\n",
            "        if self.state == 'VT' and year >= 1778:\n",
            "            name = \"Bennington Battle Day\"\n",
            "            self[date(year, AUG, 16)] = name\n",
            "            if self.observed and date(year, AUG, 16).weekday() == SAT:\n",
            "                self[date(year, AUG, 15)] = name + \" (Observed)\"\n",
            "            elif self.observed and date(year, AUG, 16).weekday() == SUN:\n",
            "                self[date(year, AUG, 17)] = name + \" (Observed)\"\n",
            "\n",
            "        # Lyndon Baines Johnson Day\n",
            "        if self.state == 'TX' and year >= 1973:\n",
            "            self[date(year, AUG, 27)] = \"Lyndon Baines Johnson Day\"\n",
            "\n",
            "        # Labor Day\n",
            "        if year >= 1894:\n",
            "            self[date(year, SEP, 1) + rd(weekday=MO)] = \"Labor Day\"\n",
            "\n",
            "        # Columbus Day\n",
            "        if self.state not in ('AK', 'AR', 'DE', 'FL', 'HI', 'NV'):\n",
            "            if self.state == 'SD':\n",
            "                name = \"Native American Day\"\n",
            "            elif self.state == 'VI':\n",
            "                name = \"Columbus Day and Puerto Rico Friendship Day\"\n",
            "            else:\n",
            "                name = \"Columbus Day\"\n",
            "            if year >= 1970:\n",
            "                self[date(year, OCT, 1) + rd(weekday=MO(+2))] = name\n",
            "            elif year >= 1937:\n",
            "                self[date(year, OCT, 12)] = name\n",
            "\n",
            "        # Alaska Day\n",
            "        if self.state == 'AK' and year >= 1867:\n",
            "            self[date(year, OCT, 18)] = \"Alaska Day\"\n",
            "            if self.observed \\\n",
            "                    and date(year, OCT, 18).weekday() == SAT:\n",
            "                self[date(year, OCT, 18) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, OCT, 18).weekday() == SUN:\n",
            "                self[date(year, OCT, 18) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Nevada Day\n",
            "        if self.state == 'NV' and year >= 1933:\n",
            "            dt = date(year, OCT, 31)\n",
            "            if year >= 2000:\n",
            "                dt += rd(weekday=FR(-1))\n",
            "            self[dt] = \"Nevada Day\"\n",
            "            if self.observed and dt.weekday() == SAT:\n",
            "                self[dt + rd(days=-1)] = \"Nevada Day (Observed)\"\n",
            "            elif self.observed and dt.weekday() == SUN:\n",
            "                self[dt + rd(days=+1)] = \"Nevada Day (Observed)\"\n",
            "\n",
            "        # Liberty Day\n",
            "        if self.state == 'VI':\n",
            "            self[date(year, NOV, 1)] = \"Liberty Day\"\n",
            "\n",
            "        # Election Day\n",
            "        if (self.state in ('DE', 'HI', 'IL', 'IN', 'LA',\n",
            "                           'MT', 'NH', 'NJ', 'NY', 'WV') and\n",
            "            year >= 2008 and year % 2 == 0) \\\n",
            "                or (self.state in ('IN', 'NY') and year >= 2015):\n",
            "            dt = date(year, NOV, 1) + rd(weekday=MO)\n",
            "            self[dt + rd(days=+1)] = \"Election Day\"\n",
            "\n",
            "        # All Souls' Day\n",
            "        if self.state == 'GU':\n",
            "            self[date(year, NOV, 2)] = \"All Souls' Day\"\n",
            "\n",
            "        # Veterans Day\n",
            "        if year > 1953:\n",
            "            name = \"Veterans Day\"\n",
            "        else:\n",
            "            name = \"Armistice Day\"\n",
            "        if 1978 > year > 1970:\n",
            "            self[date(year, OCT, 1) + rd(weekday=MO(+4))] = name\n",
            "        elif year >= 1938:\n",
            "            self[date(year, NOV, 11)] = name\n",
            "            if self.observed \\\n",
            "                    and date(year, NOV, 11).weekday() == SAT:\n",
            "                self[date(year, NOV, 11) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, NOV, 11).weekday() == SUN:\n",
            "                self[date(year, NOV, 11) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Discovery Day\n",
            "        if self.state == 'PR':\n",
            "            self[date(year, NOV, 19)] = \"Discovery Day\"\n",
            "            if self.observed and date(year, NOV, 19).weekday() == SUN:\n",
            "                self[date(year, NOV, 20)] = \"Discovery Day (Observed)\"\n",
            "\n",
            "        # Thanksgiving\n",
            "        if year > 1870:\n",
            "            self[date(year, NOV, 1) + rd(weekday=TH(+4))] = \"Thanksgiving\"\n",
            "\n",
            "        # Day After Thanksgiving\n",
            "        # Friday After Thanksgiving\n",
            "        # Lincoln's Birthday\n",
            "        # American Indian Heritage Day\n",
            "        # Family Day\n",
            "        # New Mexico Presidents' Day\n",
            "        if (self.state in ('DE', 'FL', 'NH', 'NC', 'OK', 'TX', 'WV') and\n",
            "            year >= 1975) \\\n",
            "                or (self.state == 'IN' and year >= 2010) \\\n",
            "                or (self.state == 'MD' and year >= 2008) \\\n",
            "                or self.state in ('NV', 'NM'):\n",
            "            if self.state in ('DE', 'NH', 'NC', 'OK', 'WV'):\n",
            "                name = \"Day After Thanksgiving\"\n",
            "            elif self.state in ('FL', 'TX'):\n",
            "                name = \"Friday After Thanksgiving\"\n",
            "            elif self.state == 'IN':\n",
            "                name = \"Lincoln's Birthday\"\n",
            "            elif self.state == 'MD' and year >= 2008:\n",
            "                name = \"American Indian Heritage Day\"\n",
            "            elif self.state == 'NV':\n",
            "                name = \"Family Day\"\n",
            "            elif self.state == 'NM':\n",
            "                name = \"Presidents' Day\"\n",
            "            dt = date(year, NOV, 1) + rd(weekday=TH(+4))\n",
            "            self[dt + rd(days=+1)] = name\n",
            "\n",
            "        # Robert E. Lee's Birthday\n",
            "        if self.state == 'GA' and year >= 1986:\n",
            "            if year >= 2016:\n",
            "                name = \"State Holiday\"\n",
            "            else:\n",
            "                name = \"Robert E. Lee's Birthday\"\n",
            "            self[date(year, NOV, 29) + rd(weekday=FR(-1))] = name\n",
            "\n",
            "        # Lady of Camarin Day\n",
            "        if self.state == 'GU':\n",
            "            self[date(year, DEC, 8)] = \"Lady of Camarin Day\"\n",
            "\n",
            "        # Christmas Eve\n",
            "        if self.state == 'AS' or \\\n",
            "                (self.state in ('KS', 'MI', 'NC') and year >= 2013) or \\\n",
            "                (self.state == 'TX' and year >= 1981) or \\\n",
            "                (self.state == 'WI' and year >= 2012):\n",
            "            name = \"Christmas Eve\"\n",
            "            self[date(year, DEC, 24)] = name\n",
            "            name = name + \" (Observed)\"\n",
            "            # If on Friday, observed on Thursday\n",
            "            if self.observed and date(year, DEC, 24).weekday() == FRI:\n",
            "                self[date(year, DEC, 24) + rd(days=-1)] = name\n",
            "            # If on Saturday or Sunday, observed on Friday\n",
            "            elif self.observed \\\n",
            "                    and date(year, DEC, 24).weekday() in WEEKEND:\n",
            "                self[date(year, DEC, 24) + rd(weekday=FR(-1))] = name\n",
            "\n",
            "        # Christmas Day\n",
            "        if year > 1870:\n",
            "            name = \"Christmas Day\"\n",
            "            self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "            if self.observed \\\n",
            "                    and date(year, DEC, 25).weekday() == SAT:\n",
            "                self[date(year, DEC, 25) + rd(days=-1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, DEC, 25).weekday() == SUN:\n",
            "                self[date(year, DEC, 25) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Day After Christmas\n",
            "        if self.state == 'NC' and year >= 2013:\n",
            "            name = \"Day After Christmas\"\n",
            "            self[date(year, DEC, 26)] = name\n",
            "            name = name + \" (Observed)\"\n",
            "            # If on Saturday or Sunday, observed on Monday\n",
            "            if self.observed and date(year, DEC, 26).weekday() in WEEKEND:\n",
            "                self[date(year, DEC, 26) + rd(weekday=MO)] = name\n",
            "            # If on Monday, observed on Tuesday\n",
            "            elif self.observed \\\n",
            "                    and date(year, DEC, 26).weekday() == MON:\n",
            "                self[date(year, DEC, 26) + rd(days=+1)] = name\n",
            "        elif self.state == 'TX' and year >= 1981:\n",
            "            self[date(year, DEC, 26)] = \"Day After Christmas\"\n",
            "        elif self.state == 'VI':\n",
            "            self[date(year, DEC, 26)] = \"Christmas Second Day\"\n",
            "\n",
            "        # New Year's Eve\n",
            "        if (self.state in ('KY', 'MI') and year >= 2013) or \\\n",
            "                (self.state == 'WI' and year >= 2012):\n",
            "            name = \"New Year's Eve\"\n",
            "            self[date(year, DEC, 31)] = name\n",
            "            if self.observed \\\n",
            "                    and date(year, DEC, 31).weekday() == SAT:\n",
            "                self[date(year, DEC, 30)] = name + \" (Observed)\"\n",
            "\n",
            "\n",
            "class US(UnitedStates):\n",
            "    pass\n",
            "\n",
            "\n",
            "class NewZealand(HolidayBase):\n",
            "    PROVINCES = ['NTL', 'AUK', 'TKI', 'HKB', 'WGN', 'MBH', 'NSN', 'CAN',\n",
            "                 'STC', 'WTL', 'OTA', 'STL', 'CIT']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'NZ'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Bank Holidays Act 1873\n",
            "        # The Employment of Females Act 1873\n",
            "        # Factories Act 1894\n",
            "        # Industrial Conciliation and Arbitration Act 1894\n",
            "        # Labour Day Act 1899\n",
            "        # Anzac Day Act 1920, 1949, 1956\n",
            "        # New Zealand Day Act 1973\n",
            "        # Waitangi Day Act 1960, 1976\n",
            "        # Sovereign's Birthday Observance Act 1937, 1952\n",
            "        # Holidays Act 1981, 2003\n",
            "        if year < 1894:\n",
            "            return\n",
            "\n",
            "        # New Year's Day\n",
            "        name = \"New Year's Day\"\n",
            "        jan1 = date(year, JAN, 1)\n",
            "        self[jan1] = name\n",
            "        if self.observed and jan1.weekday() in WEEKEND:\n",
            "            self[date(year, JAN, 3)] = name + \" (Observed)\"\n",
            "\n",
            "        name = \"Day after New Year's Day\"\n",
            "        jan2 = date(year, JAN, 2)\n",
            "        self[jan2] = name\n",
            "        if self.observed and jan2.weekday() in WEEKEND:\n",
            "            self[date(year, JAN, 4)] = name + \" (Observed)\"\n",
            "\n",
            "        # Waitangi Day\n",
            "        if year > 1973:\n",
            "            name = \"New Zealand Day\"\n",
            "            if year > 1976:\n",
            "                name = \"Waitangi Day\"\n",
            "            feb6 = date(year, FEB, 6)\n",
            "            self[feb6] = name\n",
            "            if self.observed and year >= 2014 and feb6.weekday() in WEEKEND:\n",
            "                self[feb6 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "\n",
            "        # Easter\n",
            "        self[easter(year) + rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Easter Monday\"\n",
            "\n",
            "        # Anzac Day\n",
            "        if year > 1920:\n",
            "            name = \"Anzac Day\"\n",
            "            apr25 = date(year, APR, 25)\n",
            "            self[apr25] = name\n",
            "            if self.observed and year >= 2014 and apr25.weekday() in WEEKEND:\n",
            "                self[apr25 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "\n",
            "        # Sovereign's Birthday\n",
            "        if year >= 1952:\n",
            "            name = \"Queen's Birthday\"\n",
            "        elif year > 1901:\n",
            "            name = \"King's Birthday\"\n",
            "        if year == 1952:\n",
            "            self[date(year, JUN, 2)] = name  # Elizabeth II\n",
            "        elif year > 1937:\n",
            "            self[date(year, JUN, 1) + rd(weekday=MO(+1))] = name  # EII & GVI\n",
            "        elif year == 1937:\n",
            "            self[date(year, JUN, 9)] = name  # George VI\n",
            "        elif year == 1936:\n",
            "            self[date(year, JUN, 23)] = name  # Edward VIII\n",
            "        elif year > 1911:\n",
            "            self[date(year, JUN, 3)] = name  # George V\n",
            "        elif year > 1901:\n",
            "            # http://paperspast.natlib.govt.nz/cgi-bin/paperspast?a=d&d=NZH19091110.2.67\n",
            "            self[date(year, NOV, 9)] = name  # Edward VII\n",
            "\n",
            "        # Labour Day\n",
            "        name = \"Labour Day\"\n",
            "        if year >= 1910:\n",
            "            self[date(year, OCT, 1) + rd(weekday=MO(+4))] = name\n",
            "        elif year > 1899:\n",
            "            self[date(year, OCT, 1) + rd(weekday=WE(+2))] = name\n",
            "\n",
            "        # Christmas Day\n",
            "        name = \"Christmas Day\"\n",
            "        dec25 = date(year, DEC, 25)\n",
            "        self[dec25] = name\n",
            "        if self.observed and dec25.weekday() in WEEKEND:\n",
            "            self[date(year, DEC, 27)] = name + \" (Observed)\"\n",
            "\n",
            "        # Boxing Day\n",
            "        name = \"Boxing Day\"\n",
            "        dec26 = date(year, DEC, 26)\n",
            "        self[dec26] = name\n",
            "        if self.observed and dec26.weekday() in WEEKEND:\n",
            "            self[date(year, DEC, 28)] = name + \" (Observed)\"\n",
            "\n",
            "        # Province Anniversary Day\n",
            "        if self.prov in ('NTL', 'Northland', 'AUK', 'Auckland'):\n",
            "            if 1963 < year <= 1973 and self.prov in ('NTL', 'Northland'):\n",
            "                name = \"Waitangi Day\"\n",
            "                dt = date(year, FEB, 6)\n",
            "            else:\n",
            "                name = \"Auckland Anniversary Day\"\n",
            "                dt = date(year, JAN, 29)\n",
            "            if dt.weekday() in (TUE, WED, THU):\n",
            "                self[dt + rd(weekday=MO(-1))] = name\n",
            "            else:\n",
            "                self[dt + rd(weekday=MO)] = name\n",
            "\n",
            "        elif self.prov in ('TKI', 'Taranaki', 'New Plymouth'):\n",
            "            name = \"Taranaki Anniversary Day\"\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO(+2))] = name\n",
            "\n",
            "        elif self.prov in ('HKB', \"Hawke's Bay\"):\n",
            "            name = \"Hawke's Bay Anniversary Day\"\n",
            "            labour_day = date(year, OCT, 1) + rd(weekday=MO(+4))\n",
            "            self[labour_day + rd(weekday=FR(-1))] = name\n",
            "\n",
            "        elif self.prov in ('WGN', 'Wellington'):\n",
            "            name = \"Wellington Anniversary Day\"\n",
            "            jan22 = date(year, JAN, 22)\n",
            "            if jan22.weekday() in (TUE, WED, THU):\n",
            "                self[jan22 + rd(weekday=MO(-1))] = name\n",
            "            else:\n",
            "                self[jan22 + rd(weekday=MO)] = name\n",
            "\n",
            "        elif self.prov in ('MBH', 'Marlborough'):\n",
            "            name = \"Marlborough Anniversary Day\"\n",
            "            labour_day = date(year, OCT, 1) + rd(weekday=MO(+4))\n",
            "            self[labour_day + rd(weeks=1)] = name\n",
            "\n",
            "        elif self.prov in ('NSN', 'Nelson'):\n",
            "            name = \"Nelson Anniversary Day\"\n",
            "            feb1 = date(year, FEB, 1)\n",
            "            if feb1.weekday() in (TUE, WED, THU):\n",
            "                self[feb1 + rd(weekday=MO(-1))] = name\n",
            "            else:\n",
            "                self[feb1 + rd(weekday=MO)] = name\n",
            "\n",
            "        elif self.prov in ('CAN', 'Canterbury'):\n",
            "            name = \"Canterbury Anniversary Day\"\n",
            "            showday = date(year, NOV, 1) + rd(weekday=TU) + \\\n",
            "                rd(weekday=FR(+2))\n",
            "            self[showday] = name\n",
            "\n",
            "        elif self.prov in ('STC', 'South Canterbury'):\n",
            "            name = \"South Canterbury Anniversary Day\"\n",
            "            dominion_day = date(year, SEP, 1) + rd(weekday=MO(4))\n",
            "            self[dominion_day] = name\n",
            "\n",
            "        elif self.prov in ('WTL', 'Westland'):\n",
            "            name = \"Westland Anniversary Day\"\n",
            "            dec1 = date(year, DEC, 1)\n",
            "            # Observance varies?!?!\n",
            "            if year == 2005:  # special case?!?!\n",
            "                self[date(year, DEC, 5)] = name\n",
            "            elif dec1.weekday() in (TUE, WED, THU):\n",
            "                self[dec1 + rd(weekday=MO(-1))] = name\n",
            "            else:\n",
            "                self[dec1 + rd(weekday=MO)] = name\n",
            "\n",
            "        elif self.prov in ('OTA', 'Otago'):\n",
            "            name = \"Otago Anniversary Day\"\n",
            "            mar23 = date(year, MAR, 23)\n",
            "            # there is no easily determined single day of local observance?!?!\n",
            "            if mar23.weekday() in (TUE, WED, THU):\n",
            "                dt = mar23 + rd(weekday=MO(-1))\n",
            "            else:\n",
            "                dt = mar23 + rd(weekday=MO)\n",
            "            if dt == easter(year) + rd(weekday=MO):  # Avoid Easter Monday\n",
            "                dt += rd(days=1)\n",
            "            self[dt] = name\n",
            "\n",
            "        elif self.prov in ('STL', 'Southland'):\n",
            "            name = \"Southland Anniversary Day\"\n",
            "            jan17 = date(year, JAN, 17)\n",
            "            if year > 2011:\n",
            "                self[easter(year) + rd(weekday=TU)] = name\n",
            "            else:\n",
            "                if jan17.weekday() in (TUE, WED, THU):\n",
            "                    self[jan17 + rd(weekday=MO(-1))] = name\n",
            "                else:\n",
            "                    self[jan17 + rd(weekday=MO)] = name\n",
            "\n",
            "        elif self.prov in ('CIT', 'Chatham Islands'):\n",
            "            name = \"Chatham Islands Anniversary Day\"\n",
            "            nov30 = date(year, NOV, 30)\n",
            "            if nov30.weekday() in (TUE, WED, THU):\n",
            "                self[nov30 + rd(weekday=MO(-1))] = name\n",
            "            else:\n",
            "                self[nov30 + rd(weekday=MO)] = name\n",
            "\n",
            "\n",
            "class NZ(NewZealand):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Australia(HolidayBase):\n",
            "    PROVINCES = ['ACT', 'NSW', 'NT', 'QLD', 'SA', 'TAS', 'VIC', 'WA']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'AU'\n",
            "        self.prov = kwargs.pop('prov', None)\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # ACT:  Holidays Act 1958\n",
            "        # NSW:  Public Holidays Act 2010\n",
            "        # NT:   Public Holidays Act 2013\n",
            "        # QLD:  Holidays Act 1983\n",
            "        # SA:   Holidays Act 1910\n",
            "        # TAS:  Statutory Holidays Act 2000\n",
            "        # VIC:  Public Holidays Act 1993\n",
            "        # WA:   Public and Bank Holidays Act 1972\n",
            "\n",
            "        # TODO do more research on history of Aus holidays\n",
            "\n",
            "        # New Year's Day\n",
            "        name = \"New Year's Day\"\n",
            "        jan1 = date(year, JAN, 1)\n",
            "        self[jan1] = name\n",
            "        if self.observed and jan1.weekday() in WEEKEND:\n",
            "            self[jan1 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "\n",
            "        # Australia Day\n",
            "        jan26 = date(year, JAN, 26)\n",
            "        if year >= 1935:\n",
            "            if self.prov == 'NSW' and year < 1946:\n",
            "                name = \"Anniversary Day\"\n",
            "            else:\n",
            "                name = \"Australia Day\"\n",
            "            self[jan26] = name\n",
            "            if self.observed and year >= 1946 and jan26.weekday() in WEEKEND:\n",
            "                self[jan26 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "        elif year >= 1888 and self.prov != 'SA':\n",
            "            name = \"Anniversary Day\"\n",
            "            self[jan26] = name\n",
            "\n",
            "        # Adelaide Cup\n",
            "        if self.prov == 'SA':\n",
            "            name = \"Adelaide Cup\"\n",
            "            if year >= 2006:\n",
            "                # subject to proclamation ?!?!\n",
            "                self[date(year, MAR, 1) + rd(weekday=MO(+2))] = name\n",
            "            else:\n",
            "                self[date(year, MAR, 1) + rd(weekday=MO(+3))] = name\n",
            "\n",
            "        # Canberra Day\n",
            "        # Info from https://www.timeanddate.com/holidays/australia/canberra-day\n",
            "        # and https://en.wikipedia.org/wiki/Canberra_Day\n",
            "        if self.prov == 'ACT' and year >= 1913:\n",
            "            name = \"Canberra Day\"\n",
            "            if year >= 1913 and year <= 1957:\n",
            "                self[date(year, MAR, 12)] = name\n",
            "            elif year >= 1958 and year <= 2007:\n",
            "                self[date(year, MAR, 1) + rd(weekday=MO(+3))] = name\n",
            "            elif year >= 2008 and year != 2012:\n",
            "                self[date(year, MAR, 1) + rd(weekday=MO(+2))] = name\n",
            "            elif year == 2012:\n",
            "                self[date(year, MAR, 12)] = name\n",
            "\n",
            "        # Easter\n",
            "        self[easter(year) + rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "        if self.prov in ('ACT', 'NSW', 'NT', 'QLD', 'SA', 'VIC'):\n",
            "            self[easter(year) + rd(weekday=SA(-1))] = \"Easter Saturday\"\n",
            "        if self.prov in ('ACT', 'NSW', 'QLD', 'VIC'):\n",
            "            self[easter(year)] = \"Easter Sunday\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Easter Monday\"\n",
            "\n",
            "        # Anzac Day\n",
            "        if year > 1920:\n",
            "            name = \"Anzac Day\"\n",
            "            apr25 = date(year, APR, 25)\n",
            "            self[apr25] = name\n",
            "            if self.observed:\n",
            "                if apr25.weekday() == SAT and self.prov in ('WA', 'NT'):\n",
            "                    self[apr25 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "                elif (apr25.weekday() == SUN and\n",
            "                      self.prov in ('ACT', 'QLD', 'SA', 'WA', 'NT')):\n",
            "                    self[apr25 + rd(weekday=MO)] = name + \" (Observed)\"\n",
            "\n",
            "        # Western Australia Day\n",
            "        if self.prov == 'WA' and year > 1832:\n",
            "            if year >= 2015:\n",
            "                name = \"Western Australia Day\"\n",
            "            else:\n",
            "                name = \"Foundation Day\"\n",
            "            self[date(year, JUN, 1) + rd(weekday=MO(+1))] = name\n",
            "\n",
            "        # Sovereign's Birthday\n",
            "        if year >= 1952:\n",
            "            name = \"Queen's Birthday\"\n",
            "        elif year > 1901:\n",
            "            name = \"King's Birthday\"\n",
            "        if year >= 1936:\n",
            "            name = \"Queen's Birthday\"\n",
            "            if self.prov == 'QLD':\n",
            "                if year == 2012:\n",
            "                    self[date(year, JUN, 11)] = \"Queen's Diamond Jubilee\"\n",
            "                if year < 2016 and year != 2012:\n",
            "                    dt = date(year, JUN, 1) + rd(weekday=MO(+2))\n",
            "                    self[dt] = name\n",
            "                else:\n",
            "                    dt = date(year, OCT, 1) + rd(weekday=MO)\n",
            "                    self[dt] = name\n",
            "            elif self.prov == 'WA':\n",
            "                # by proclamation ?!?!\n",
            "                self[date(year, OCT, 1) + rd(weekday=MO(-1))] = name\n",
            "            elif self.prov in ('NSW', 'VIC', 'ACT', 'SA', 'NT', 'TAS'):\n",
            "                dt = date(year, JUN, 1) + rd(weekday=MO(+2))\n",
            "                self[dt] = name\n",
            "        elif year > 1911:\n",
            "            self[date(year, JUN, 3)] = name  # George V\n",
            "        elif year > 1901:\n",
            "            self[date(year, NOV, 9)] = name  # Edward VII\n",
            "\n",
            "        # Picnic Day\n",
            "        if self.prov == 'NT':\n",
            "            name = \"Picnic Day\"\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = name\n",
            "\n",
            "        # Bank Holiday\n",
            "        if self.prov == 'NSW':\n",
            "            if year >= 1912:\n",
            "                name = \"Bank Holiday\"\n",
            "                self[date(year, 8, 1) + rd(weekday=MO)] = name\n",
            "\n",
            "        # Labour Day\n",
            "        name = \"Labour Day\"\n",
            "        if self.prov in ('NSW', 'ACT', 'SA'):\n",
            "            self[date(year, OCT, 1) + rd(weekday=MO)] = name\n",
            "        elif self.prov == 'WA':\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO)] = name\n",
            "        elif self.prov == 'VIC':\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO(+2))] = name\n",
            "        elif self.prov == 'QLD':\n",
            "            if 2013 <= year <= 2015:\n",
            "                self[date(year, OCT, 1) + rd(weekday=MO)] = name\n",
            "            else:\n",
            "                self[date(year, MAY, 1) + rd(weekday=MO)] = name\n",
            "        elif self.prov == 'NT':\n",
            "            name = \"May Day\"\n",
            "            self[date(year, MAY, 1) + rd(weekday=MO)] = name\n",
            "        elif self.prov == 'TAS':\n",
            "            name = \"Eight Hours Day\"\n",
            "            self[date(year, MAR, 1) + rd(weekday=MO(+2))] = name\n",
            "\n",
            "        # Family & Community Day\n",
            "        if self.prov == 'ACT':\n",
            "            name = \"Family & Community Day\"\n",
            "            if 2007 <= year <= 2009:\n",
            "                self[date(year, NOV, 1) + rd(weekday=TU)] = name\n",
            "            elif year == 2010:\n",
            "                # first Monday of the September/October school holidays\n",
            "                # moved to the second Monday if this falls on Labour day\n",
            "                # TODO need a formula for the ACT school holidays then\n",
            "                # http://www.cmd.act.gov.au/communication/holidays\n",
            "                self[date(year, SEP, 26)] = name\n",
            "            elif year == 2011:\n",
            "                self[date(year, OCT, 10)] = name\n",
            "            elif year == 2012:\n",
            "                self[date(year, OCT, 8)] = name\n",
            "            elif year == 2013:\n",
            "                self[date(year, SEP, 30)] = name\n",
            "            elif year == 2014:\n",
            "                self[date(year, SEP, 29)] = name\n",
            "            elif year == 2015:\n",
            "                self[date(year, SEP, 28)] = name\n",
            "            elif year == 2016:\n",
            "                self[date(year, SEP, 26)] = name\n",
            "            elif year == 2017:\n",
            "                self[date(year, SEP, 25)] = name\n",
            "\n",
            "        # Reconciliation Day\n",
            "        if self.prov == 'ACT':\n",
            "            name = \"Reconciliation Day\"\n",
            "            if year >= 2018:\n",
            "                self[date(year, 5, 27) + rd(weekday=MO)] = name\n",
            "\n",
            "        if self.prov == 'VIC':\n",
            "            # Grand Final Day\n",
            "            if year >= 2015:\n",
            "                self[date(year, SEP, 24) + rd(weekday=FR)] = \"Grand Final Day\"\n",
            "\n",
            "            # Melbourne Cup\n",
            "            self[date(year, NOV, 1) + rd(weekday=TU)] = \"Melbourne Cup\"\n",
            "\n",
            "        # The Royal Queensland Show (Ekka)\n",
            "        # The Show starts on the first Friday of August - providing this is\n",
            "        # not prior to the 5th - in which case it will begin on the second\n",
            "        # Friday. The Wednesday during the show is a public holiday.\n",
            "        if self.prov == 'QLD':\n",
            "            name = \"The Royal Queensland Show\"\n",
            "            self[date(year, AUG, 5) + rd(weekday=FR) + rd(weekday=WE)] = \\\n",
            "                name\n",
            "\n",
            "        # Christmas Day\n",
            "        name = \"Christmas Day\"\n",
            "        dec25 = date(year, DEC, 25)\n",
            "        self[dec25] = name\n",
            "        if self.observed and dec25.weekday() in WEEKEND:\n",
            "            self[date(year, DEC, 27)] = name + \" (Observed)\"\n",
            "\n",
            "        # Boxing Day\n",
            "        if self.prov == 'SA':\n",
            "            name = \"Proclamation Day\"\n",
            "        else:\n",
            "            name = \"Boxing Day\"\n",
            "        dec26 = date(year, DEC, 26)\n",
            "        self[dec26] = name\n",
            "        if self.observed and dec26.weekday() in WEEKEND:\n",
            "            self[date(year, DEC, 28)] = name + \" (Observed)\"\n",
            "\n",
            "\n",
            "class AU(Australia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Germany(HolidayBase):\n",
            "    \"\"\"Official holidays for Germany in its current form.\n",
            "\n",
            "    This class doesn't return any holidays before 1990-10-03.\n",
            "\n",
            "    Before that date the current Germany was separated into the \"German\n",
            "    Democratic Republic\" and the \"Federal Republic of Germany\" which both had\n",
            "    somewhat different holidays. Since this class is called \"Germany\" it\n",
            "    doesn't really make sense to include the days from the two former\n",
            "    countries.\n",
            "\n",
            "    Note that Germany doesn't have rules for holidays that happen on a\n",
            "    Sunday. Those holidays are still holiday days but there is no additional\n",
            "    day to make up for the \"lost\" day.\n",
            "\n",
            "    Also note that German holidays are partly declared by each province there\n",
            "    are some weired edge cases:\n",
            "\n",
            "        - \"Mari Himmelfahrt\" is only a holiday in Bavaria (BY) if your\n",
            "          municipality is mostly catholic which in term depends on census data.\n",
            "          Since we don't have this data but most municipalities in Bavaria\n",
            "          *are* mostly catholic, we count that as holiday for whole Bavaria.\n",
            "        - There is an \"Augsburger Friedensfest\" which only exists in the town\n",
            "          Augsburg. This is excluded for Bavaria.\n",
            "        - \"Grndonnerstag\" (Thursday before easter) is not a holiday but pupil\n",
            "           don't have to go to school (but only in Baden Wrttemberg) which is\n",
            "           solved by adjusting school holidays to include this day. It is\n",
            "           excluded from our list.\n",
            "        - \"Fronleichnam\" is a holiday in certain, explicitly defined\n",
            "          municipalities in Saxony (SN) and Thuringia (TH). We exclude it from\n",
            "          both provinces.\n",
            "    \"\"\"\n",
            "\n",
            "    PROVINCES = ['BW', 'BY', 'BE', 'BB', 'HB', 'HH', 'HE', 'MV', 'NI', 'NW',\n",
            "                 'RP', 'SL', 'SN', 'ST', 'SH', 'TH']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'DE'\n",
            "        self.prov = kwargs.pop('prov', 'SH')\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        if year <= 1989:\n",
            "            return\n",
            "\n",
            "        if year == 1990:\n",
            "            self[date(year, JUN, 17)] = 'Tag der deutschen Einheit'\n",
            "\n",
            "        if year > 1990:\n",
            "\n",
            "            self[date(year, JAN, 1)] = 'Neujahr'\n",
            "\n",
            "            if self.prov in ('BW', 'BY', 'ST'):\n",
            "                self[date(year, JAN, 6)] = 'Heilige Drei Knige'\n",
            "\n",
            "            self[easter(year) - rd(days=2)] = 'Karfreitag'\n",
            "\n",
            "            if self.prov == \"BB\":\n",
            "                # will always be a Sunday and we have no \"observed\" rule so\n",
            "                # this is pretty pointless but it's nonetheless an official\n",
            "                # holiday by law\n",
            "                self[easter(year)] = \"Ostersonntag\"\n",
            "\n",
            "            self[easter(year) + rd(days=1)] = 'Ostermontag'\n",
            "\n",
            "            self[date(year, MAY, 1)] = 'Erster Mai'\n",
            "\n",
            "            if self.prov == \"BE\" and year == 2020:\n",
            "                self[date(year, MAY, 8)] = \\\n",
            "                    \"75. Jahrestag der Befreiung vom Nationalsozialismus \" \\\n",
            "                    \"und der Beendigung des Zweiten Weltkriegs in Europa\"\n",
            "\n",
            "            self[easter(year) + rd(days=39)] = 'Christi Himmelfahrt'\n",
            "\n",
            "            if self.prov == \"BB\":\n",
            "                # will always be a Sunday and we have no \"observed\" rule so\n",
            "                # this is pretty pointless but it's nonetheless an official\n",
            "                # holiday by law\n",
            "                self[easter(year) + rd(days=49)] = \"Pfingstsonntag\"\n",
            "\n",
            "            self[easter(year) + rd(days=50)] = 'Pfingstmontag'\n",
            "\n",
            "            if self.prov in ('BW', 'BY', 'HE', 'NW', 'RP', 'SL'):\n",
            "                self[easter(year) + rd(days=60)] = 'Fronleichnam'\n",
            "\n",
            "            if self.prov in ('BY', 'SL'):\n",
            "                self[date(year, AUG, 15)] = 'Mari Himmelfahrt'\n",
            "\n",
            "            self[date(year, OCT, 3)] = 'Tag der Deutschen Einheit'\n",
            "\n",
            "        if self.prov in ('BB', 'MV', 'SN', 'ST', 'TH'):\n",
            "            self[date(year, OCT, 31)] = 'Reformationstag'\n",
            "\n",
            "        if self.prov in ('HB', 'SH', 'NI', 'HH') and year >= 2018:\n",
            "            self[date(year, OCT, 31)] = 'Reformationstag'\n",
            "\n",
            "        # in 2017 all states got the Reformationstag (500th anniversary of\n",
            "        # Luther's thesis)\n",
            "        if year == 2017:\n",
            "            self[date(year, OCT, 31)] = 'Reformationstag'\n",
            "\n",
            "        if self.prov in ('BW', 'BY', 'NW', 'RP', 'SL'):\n",
            "            self[date(year, NOV, 1)] = 'Allerheiligen'\n",
            "\n",
            "        if (year >= 1990 and year <= 1994) or self.prov == 'SN':\n",
            "            # can be calculated as \"last wednesday before year-11-23\" which is\n",
            "            # why we need to go back two wednesdays if year-11-23 happens to be\n",
            "            # a wednesday\n",
            "            base_data = date(year, NOV, 23)\n",
            "            weekday_delta = WE(-2) if base_data.weekday() == 2 else WE(-1)\n",
            "            self[base_data + rd(weekday=weekday_delta)] = 'Bu- und Bettag'\n",
            "\n",
            "        if (year >= 2019):\n",
            "            if self.prov == 'TH':\n",
            "                self[date(year, SEP, 20)] = 'Weltkindertag'\n",
            "\n",
            "            if self.prov == 'BE':\n",
            "                self[date(year, MAR, 8)] = 'Internationaler Frauentag'\n",
            "\n",
            "        self[date(year, DEC, 25)] = 'Erster Weihnachtstag'\n",
            "        self[date(year, DEC, 26)] = 'Zweiter Weihnachtstag'\n",
            "\n",
            "\n",
            "class DE(Germany):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Austria(HolidayBase):\n",
            "    PROVINCES = ['B', 'K', 'N', 'O', 'S', 'ST', 'T', 'V', 'W']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'AT'\n",
            "        self.prov = kwargs.pop('prov', kwargs.pop('state', 'W'))\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # public holidays\n",
            "        self[date(year, JAN, 1)] = \"Neujahr\"\n",
            "        self[date(year, JAN, 6)] = \"Heilige Drei Knige\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Ostermontag\"\n",
            "        self[date(year, MAY, 1)] = \"Staatsfeiertag\"\n",
            "        self[easter(year) + rd(days=39)] = \"Christi Himmelfahrt\"\n",
            "        self[easter(year) + rd(days=50)] = \"Pfingstmontag\"\n",
            "        self[easter(year) + rd(days=60)] = \"Fronleichnam\"\n",
            "        self[date(year, AUG, 15)] = \"Maria Himmelfahrt\"\n",
            "        if 1919 <= year <= 1934:\n",
            "            self[date(year, NOV, 12)] = \"Nationalfeiertag\"\n",
            "        if year >= 1967:\n",
            "            self[date(year, OCT, 26)] = \"Nationalfeiertag\"\n",
            "        self[date(year, NOV, 1)] = \"Allerheiligen\"\n",
            "        self[date(year, DEC, 8)] = \"Maria Empfngnis\"\n",
            "        self[date(year, DEC, 25)] = \"Christtag\"\n",
            "        self[date(year, DEC, 26)] = \"Stefanitag\"\n",
            "\n",
            "\n",
            "class AT(Austria):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Denmark(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Denmark\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'DK'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Public holidays\n",
            "        self[date(year, JAN, 1)] = \"Nytrsdag\"\n",
            "        self[easter(year) + rd(weekday=SU(-2))] = \"Palmesndag\"\n",
            "        self[easter(year) + rd(weekday=TH(-1))] = \"Skrtorsdag\"\n",
            "        self[easter(year) + rd(weekday=FR(-1))] = \"Langfredag\"\n",
            "        self[easter(year)] = \"Pskedag\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Anden pskedag\"\n",
            "        self[easter(year) + rd(weekday=FR(+4))] = \"Store bededag\"\n",
            "        self[easter(year) + rd(days=39)] = \"Kristi himmelfartsdag\"\n",
            "        self[easter(year) + rd(days=49)] = \"Pinsedag\"\n",
            "        self[easter(year) + rd(days=50)] = \"Anden pinsedag\"\n",
            "        self[date(year, DEC, 25)] = \"Juledag\"\n",
            "        self[date(year, DEC, 26)] = \"Anden juledag\"\n",
            "\n",
            "\n",
            "class DK(Denmark):\n",
            "    pass\n",
            "\n",
            "\n",
            "class UnitedKingdom(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_the_United_Kingdom\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'UK'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "\n",
            "        # New Year's Day\n",
            "        if year >= 1974:\n",
            "            name = \"New Year's Day\"\n",
            "            self[date(year, JAN, 1)] = name\n",
            "            if self.observed and date(year, JAN, 1).weekday() == SUN:\n",
            "                self[date(year, JAN, 1) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed \\\n",
            "                    and date(year, JAN, 1).weekday() == SAT:\n",
            "                self[date(year, JAN, 1) + rd(days=+2)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # New Year Holiday\n",
            "        if self.country in ('UK', 'Scotland'):\n",
            "            name = \"New Year Holiday\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [Scotland]\"\n",
            "            self[date(year, JAN, 2)] = name\n",
            "            if self.observed and date(year, JAN, 2).weekday() in WEEKEND:\n",
            "                self[date(year, JAN, 2) + rd(days=+2)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "            elif self.observed and date(year, JAN, 2).weekday() == MON:\n",
            "                self[date(year, JAN, 2) + rd(days=+1)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # St. Patrick's Day\n",
            "        if self.country in ('UK', 'Northern Ireland', 'Ireland'):\n",
            "            name = \"St. Patrick's Day\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [Northern Ireland]\"\n",
            "            self[date(year, MAR, 17)] = name\n",
            "            if self.observed and date(year, MAR, 17).weekday() in WEEKEND:\n",
            "                self[date(year, MAR, 17) + rd(weekday=MO)] = name + \\\n",
            "                    \" (Observed)\"\n",
            "\n",
            "        # Good Friday\n",
            "        if self.country != 'Ireland':\n",
            "            self[easter(year) + rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "\n",
            "        # Easter Monday\n",
            "        if self.country != 'Scotland':\n",
            "            name = \"Easter Monday\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [England, Wales, Northern Ireland]\"\n",
            "            self[easter(year) + rd(weekday=MO)] = name\n",
            "\n",
            "        # May Day bank holiday (first Monday in May)\n",
            "        if year >= 1978:\n",
            "            name = \"May Day\"\n",
            "            if year == 1995:\n",
            "                dt = date(year, MAY, 8)\n",
            "            else:\n",
            "                dt = date(year, MAY, 1)\n",
            "            if dt.weekday() == MON:\n",
            "                self[dt] = name\n",
            "            elif dt.weekday() == TUE:\n",
            "                self[dt + rd(days=+6)] = name\n",
            "            elif dt.weekday() == WED:\n",
            "                self[dt + rd(days=+5)] = name\n",
            "            elif dt.weekday() == THU:\n",
            "                self[dt + rd(days=+4)] = name\n",
            "            elif dt.weekday() == FRI:\n",
            "                self[dt + rd(days=+3)] = name\n",
            "            elif dt.weekday() == SAT:\n",
            "                self[dt + rd(days=+2)] = name\n",
            "            elif dt.weekday() == SUN:\n",
            "                self[dt + rd(days=+1)] = name\n",
            "\n",
            "        # Spring bank holiday (last Monday in May)\n",
            "        if self.country != 'Ireland':\n",
            "            name = \"Spring Bank Holiday\"\n",
            "            if year == 2012:\n",
            "                self[date(year, JUN, 4)] = name\n",
            "            elif year >= 1971:\n",
            "                self[date(year, MAY, 31) + rd(weekday=MO(-1))] = name\n",
            "\n",
            "        # June bank holiday (first Monday in June)\n",
            "        if self.country == 'Ireland':\n",
            "            self[date(year, JUN, 1) + rd(weekday=MO)] = \"June Bank Holiday\"\n",
            "\n",
            "        # TT bank holiday (first Friday in June)\n",
            "        if self.country == 'Isle of Man':\n",
            "            self[date(year, JUN, 1) + rd(weekday=FR)] = \"TT Bank Holiday\"\n",
            "\n",
            "        # Tynwald Day\n",
            "        if self.country == 'Isle of Man':\n",
            "            self[date(year, JUL, 5)] = \"Tynwald Day\"\n",
            "\n",
            "        # Battle of the Boyne\n",
            "        if self.country in ('UK', 'Northern Ireland'):\n",
            "            name = \"Battle of the Boyne\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [Northern Ireland]\"\n",
            "            self[date(year, JUL, 12)] = name\n",
            "\n",
            "        # Summer bank holiday (first Monday in August)\n",
            "        if self.country in ('UK', 'Scotland', 'Ireland'):\n",
            "            name = \"Summer Bank Holiday\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [Scotland]\"\n",
            "            self[date(year, AUG, 1) + rd(weekday=MO)] = name\n",
            "\n",
            "        # Late Summer bank holiday (last Monday in August)\n",
            "        if self.country not in ('Scotland', 'Ireland') and year >= 1971:\n",
            "            name = \"Late Summer Bank Holiday\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [England, Wales, Northern Ireland]\"\n",
            "            self[date(year, AUG, 31) + rd(weekday=MO(-1))] = name\n",
            "\n",
            "        # October Bank Holiday (last Monday in October)\n",
            "        if self.country == 'Ireland':\n",
            "            name = \"October Bank Holiday\"\n",
            "            self[date(year, OCT, 31) + rd(weekday=MO(-1))] = name\n",
            "\n",
            "        # St. Andrew's Day\n",
            "        if self.country in ('UK', 'Scotland'):\n",
            "            name = \"St. Andrew's Day\"\n",
            "            if self.country == 'UK':\n",
            "                name += \" [Scotland]\"\n",
            "            self[date(year, NOV, 30)] = name\n",
            "\n",
            "        # Christmas Day\n",
            "        name = \"Christmas Day\"\n",
            "        self[date(year, DEC, 25)] = name\n",
            "        if self.observed and date(year, DEC, 25).weekday() == SAT:\n",
            "            self[date(year, DEC, 27)] = name + \" (Observed)\"\n",
            "        elif self.observed and date(year, DEC, 25).weekday() == SUN:\n",
            "            self[date(year, DEC, 27)] = name + \" (Observed)\"\n",
            "\n",
            "        # Boxing Day\n",
            "        name = \"Boxing Day\"\n",
            "        self[date(year, DEC, 26)] = name\n",
            "        if self.observed and date(year, DEC, 26).weekday() == SAT:\n",
            "            self[date(year, DEC, 28)] = name + \" (Observed)\"\n",
            "        elif self.observed and date(year, DEC, 26).weekday() == SUN:\n",
            "            self[date(year, DEC, 28)] = name + \" (Observed)\"\n",
            "\n",
            "        # Special holidays\n",
            "        if self.country != 'Ireland':\n",
            "            if year == 1977:\n",
            "                self[date(year, JUN, 7)] = \"Silver Jubilee of Elizabeth II\"\n",
            "            elif year == 1981:\n",
            "                self[date(year, JUL, 29)] = \"Wedding of Charles and Diana\"\n",
            "            elif year == 1999:\n",
            "                self[date(year, DEC, 31)] = \"Millennium Celebrations\"\n",
            "            elif year == 2002:\n",
            "                self[date(year, JUN, 3)] = \"Golden Jubilee of Elizabeth II\"\n",
            "            elif year == 2011:\n",
            "                self[date(year, APR, 29)] = \"Wedding of William and\" \\\n",
            "                    \" Catherine\"\n",
            "            elif year == 2012:\n",
            "                self[date(year, JUN, 5)] = \"Diamond Jubilee of Elizabeth II\"\n",
            "\n",
            "\n",
            "class UK(UnitedKingdom):\n",
            "    pass\n",
            "\n",
            "\n",
            "class GB(UnitedKingdom):\n",
            "    pass\n",
            "\n",
            "\n",
            "class England(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'England'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class Wales(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'Wales'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class Scotland(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'Scotland'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class IsleOfMan(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'Isle of Man'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class NorthernIreland(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'Northern Ireland'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class Ireland(UnitedKingdom):\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'Ireland'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "\n",
            "class IE(Ireland):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Spain(HolidayBase):\n",
            "    PROVINCES = ['AND', 'ARG', 'AST', 'CAN', 'CAM', 'CAL', 'CAT', 'CVA',\n",
            "                 'EXT', 'GAL', 'IBA', 'ICA', 'MAD', 'MUR', 'NAV', 'PVA', 'RIO']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'ES'\n",
            "        self.prov = kwargs.pop('prov', kwargs.pop('state', ''))\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"Ao nuevo\"\n",
            "        self[date(year, JAN, 6)] = \"Epifana del Seor\"\n",
            "        if self.prov and self.prov in ['CVA', 'MUR', 'MAD', 'NAV', 'PVA']:\n",
            "            self[date(year, MAR, 19)] = \"San Jos\"\n",
            "        if self.prov and self.prov != 'CAT':\n",
            "            self[easter(year) + rd(weeks=-1, weekday=TH)] = \"Jueves Santo\"\n",
            "        self[easter(year) + rd(weeks=-1, weekday=FR)] = \"Viernes Santo\"\n",
            "        if self.prov and self.prov in ['CAT', 'PVA', 'NAV', 'CVA', 'IBA']:\n",
            "            self[easter(year) + rd(weekday=MO)] = \"Lunes de Pascua\"\n",
            "        self[date(year, MAY, 1)] = \"Da del Trabajador\"\n",
            "        if self.prov and self.prov in ['CAT', 'GAL']:\n",
            "            self[date(year, JUN, 24)] = \"San Juan\"\n",
            "        self[date(year, AUG, 15)] = \"Asuncin de la Virgen\"\n",
            "        self[date(year, OCT, 12)] = \"Da de la Hispanidad\"\n",
            "        self[date(year, NOV, 1)] = \"Todos los Santos\"\n",
            "        self[date(year, DEC, 6)] = \"Da de la constitucin Espaola\"\n",
            "        self[date(year, DEC, 8)] = \"La Inmaculada Concepcin\"\n",
            "        self[date(year, DEC, 25)] = \"Navidad\"\n",
            "        if self.prov and self.prov in ['CAT', 'IBA']:\n",
            "            self[date(year, DEC, 26)] = \"San Esteban\"\n",
            "        # Provinces festive day\n",
            "        if self.prov:\n",
            "            if self.prov == 'AND':\n",
            "                self[date(year, FEB, 28)] = \"Da de Andalucia\"\n",
            "            elif self.prov == 'ARG':\n",
            "                self[date(year, APR, 23)] = \"Da de San Jorge\"\n",
            "            elif self.prov == 'AST':\n",
            "                self[date(year, MAR, 8)] = \"Da de Asturias\"\n",
            "            elif self.prov == 'CAN':\n",
            "                self[date(year, FEB, 28)] = \"Da de la Montaa\"\n",
            "            elif self.prov == 'CAM':\n",
            "                self[date(year, FEB, 28)] = \"Da de Castilla - La Mancha\"\n",
            "            elif self.prov == 'CAL':\n",
            "                self[date(year, APR, 23)] = \"Da de Castilla y Leon\"\n",
            "            elif self.prov == 'CAT':\n",
            "                self[date(year, SEP, 11)] = \"Da Nacional de Catalunya\"\n",
            "            elif self.prov == 'CVA':\n",
            "                self[date(year, OCT, 9)] = \"Da de la Comunidad Valenciana\"\n",
            "            elif self.prov == 'EXT':\n",
            "                self[date(year, SEP, 8)] = \"Da de Extremadura\"\n",
            "            elif self.prov == 'GAL':\n",
            "                self[date(year, JUL, 25)] = \"Da Nacional de Galicia\"\n",
            "            elif self.prov == 'IBA':\n",
            "                self[date(year, MAR, 1)] = \"Da de las Islas Baleares\"\n",
            "            elif self.prov == 'ICA':\n",
            "                self[date(year, MAY, 30)] = \"Da de Canarias\"\n",
            "            elif self.prov == 'MAD':\n",
            "                self[date(year, MAY, 2)] = \"Da de Comunidad De Madrid\"\n",
            "            elif self.prov == 'MUR':\n",
            "                self[date(year, JUN, 9)] = \"Da de la Regin de Murcia\"\n",
            "            elif self.prov == 'NAV':\n",
            "                self[date(year, SEP, 27)] = \"Da de Navarra\"\n",
            "            elif self.prov == 'PVA':\n",
            "                self[date(year, OCT, 25)] = \"Da del Pis Vasco\"\n",
            "            elif self.prov == 'RIO':\n",
            "                self[date(year, JUN, 9)] = \"Da de La Rioja\"\n",
            "\n",
            "\n",
            "class ES(Spain):\n",
            "    pass\n",
            "\n",
            "\n",
            "class EuropeanCentralBank(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/TARGET2\n",
            "    # http://www.ecb.europa.eu/press/pr/date/2000/html/pr001214_4.en.html\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'EU'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"New Year's Day\"\n",
            "        e = easter(year)\n",
            "        self[e - rd(days=2)] = \"Good Friday\"\n",
            "        self[e + rd(days=1)] = \"Easter Monday\"\n",
            "        self[date(year, MAY, 1)] = \"1 May (Labour Day)\"\n",
            "        self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "        self[date(year, DEC, 26)] = \"26 December\"\n",
            "\n",
            "\n",
            "class ECB(EuropeanCentralBank):\n",
            "    pass\n",
            "\n",
            "\n",
            "class TAR(EuropeanCentralBank):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Czechia(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_the_Czech_Republic\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'CZ'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"Den obnovy samostatnho eskho\" \\\n",
            "                                   \" sttu\" \\\n",
            "            if year >= 2000 else \\\n",
            "            \"Nov rok\"\n",
            "\n",
            "        e = easter(year)\n",
            "        if year <= 1951 or year >= 2016:\n",
            "            self[e - rd(days=2)] = \"Velk ptek\"\n",
            "        self[e + rd(days=1)] = \"Velikonon pondl\"\n",
            "\n",
            "        if year >= 1951:\n",
            "            self[date(year, MAY, 1)] = \"Svtek prce\"\n",
            "        if year >= 1992:\n",
            "            self[date(year, MAY, 8)] = \"Den vtzstv\"\n",
            "        elif year >= 1947:\n",
            "            self[date(year, MAY, 9)] = \"Den vtzstv nad hitlerovskm\" \\\n",
            "                                       \" faismem\"\n",
            "        if year >= 1951:\n",
            "            self[date(year, JUL, 5)] = \"Den slovanskch vrozvst \" \\\n",
            "                \"Cyrila a Metodje\"\n",
            "            self[date(year, JUL, 6)] = \"Den uplen mistra Jana Husa\"\n",
            "        if year >= 2000:\n",
            "            self[date(year, SEP, 28)] = \"Den esk sttnosti\"\n",
            "        if year >= 1951:\n",
            "            self[date(year, OCT, 28)] = \"Den vzniku samostatnho \" \\\n",
            "                \"eskoslovenskho sttu\"\n",
            "        if year >= 1990:\n",
            "            self[date(year, NOV, 17)] = \"Den boje za svobodu a demokracii\"\n",
            "\n",
            "        if year >= 1990:\n",
            "            self[date(year, DEC, 24)] = \"tdr den\"\n",
            "        if year >= 1951:\n",
            "            self[date(year, DEC, 25)] = \"1. svtek vnon\"\n",
            "            self[date(year, DEC, 26)] = \"2. svtek vnon\"\n",
            "\n",
            "\n",
            "class CZ(Czechia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Czech(Czechia):\n",
            "    def __init__(self, **kwargs):\n",
            "        warnings.warn(\"Czech is deprecated, use Czechia instead.\",\n",
            "                      DeprecationWarning)\n",
            "        super(Czech, self).__init__()\n",
            "\n",
            "\n",
            "class Slovakia(HolidayBase):\n",
            "    # https://sk.wikipedia.org/wiki/Sviatok\n",
            "    # https://www.slov-lex.sk/pravne-predpisy/SK/ZZ/1993/241/20181011.html\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'SK'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"De vzniku Slovenskej republiky\"\n",
            "        self[date(year, JAN, 6)] = \"Zjavenie Pna (Traja krli a\" \\\n",
            "                                   \" vianonsviatok pravoslvnych\" \\\n",
            "                                   \" kresanov)\"\n",
            "\n",
            "        e = easter(year)\n",
            "        self[e - rd(days=2)] = \"Vek piatok\"\n",
            "        self[e + rd(days=1)] = \"Vekonon pondelok\"\n",
            "\n",
            "        self[date(year, MAY, 1)] = \"Sviatok prce\"\n",
            "\n",
            "        if year >= 1997:\n",
            "            self[date(year, MAY, 8)] = \"De vazstva nad faizmom\"\n",
            "\n",
            "        self[date(year, JUL, 5)] = \"Sviatok svtho Cyrila a svtho Metoda\"\n",
            "\n",
            "        self[date(year, AUG, 29)] = \"Vroie Slovenskho nrodnho\" \\\n",
            "                                    \" povstania\"\n",
            "\n",
            "        self[date(year, SEP, 1)] = \"De stavy Slovenskej republiky\"\n",
            "\n",
            "        self[date(year, SEP, 15)] = \"Sedembolestn Panna Mria\"\n",
            "        if year == 2018:\n",
            "            self[date(year, OCT, 30)] = \"100. vroie prijatia\" \\\n",
            "                \" Deklarcie slovenskho nroda\"\n",
            "        self[date(year, NOV, 1)] = \"Sviatok Vetkch svtch\"\n",
            "\n",
            "        if year >= 2001:\n",
            "            self[date(year, NOV, 17)] = \"De boja za slobodu a demokraciu\"\n",
            "\n",
            "        self[date(year, DEC, 24)] = \"tedr de\"\n",
            "\n",
            "        self[date(year, DEC, 25)] = \"Prv sviatok vianon\"\n",
            "\n",
            "        self[date(year, DEC, 26)] = \"Druh sviatok vianon\"\n",
            "\n",
            "\n",
            "class SK(Slovakia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Slovak(Slovakia):\n",
            "    def __init__(self, **kwargs):\n",
            "        warnings.warn(\"Slovak is deprecated, use Slovakia instead.\",\n",
            "                      DeprecationWarning)\n",
            "        super(Slovak, self).__init__()\n",
            "\n",
            "\n",
            "class Poland(HolidayBase):\n",
            "    # https://pl.wikipedia.org/wiki/Dni_wolne_od_pracy_w_Polsce\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'PL'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = 'Nowy Rok'\n",
            "        if year >= 2011:\n",
            "            self[date(year, JAN, 6)] = 'wito Trzech Krli'\n",
            "\n",
            "        e = easter(year)\n",
            "        self[e] = 'Niedziela Wielkanocna'\n",
            "        self[e + rd(days=1)] = 'Poniedziaek Wielkanocny'\n",
            "\n",
            "        if year >= 1950:\n",
            "            self[date(year, MAY, 1)] = 'wito Pastwowe'\n",
            "        if year >= 1919:\n",
            "            self[date(year, MAY, 3)] = 'wito Narodowe Trzeciego Maja'\n",
            "\n",
            "        self[e + rd(days=49)] = 'Zielone witki'\n",
            "        self[e + rd(days=60)] = 'Dzie Boego Ciaa'\n",
            "\n",
            "        self[date(year, AUG, 15)] = 'Wniebowzicie Najwitszej Marii Panny'\n",
            "\n",
            "        self[date(year, NOV, 1)] = 'Uroczysto Wszystkich witych'\n",
            "        if (1937 <= year <= 1945) or year >= 1989:\n",
            "            self[date(year, NOV, 11)] = 'Narodowe wito Niepodlegoci'\n",
            "\n",
            "        self[date(year, DEC, 25)] = 'Boe Narodzenie (pierwszy dzie)'\n",
            "        self[date(year, DEC, 26)] = 'Boe Narodzenie (drugi dzie)'\n",
            "\n",
            "\n",
            "class Polish(Poland):\n",
            "    def __init__(self, **kwargs):\n",
            "        warnings.warn(\"Polish is deprecated, use Poland instead.\",\n",
            "                      DeprecationWarning)\n",
            "        super(Polish, self).__init__()\n",
            "\n",
            "\n",
            "class PL(Poland):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Portugal(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Portugal\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'PT'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"Ano Novo\"\n",
            "\n",
            "        e = easter(year)\n",
            "\n",
            "        # carnival is no longer a holiday, but some companies let workers off.\n",
            "        # @todo recollect the years in which it was a public holiday\n",
            "        # self[e - rd(days=47)] = \"Carnaval\"\n",
            "        self[e - rd(days=2)] = \"Sexta-feira Santa\"\n",
            "        self[e] = \"Pscoa\"\n",
            "\n",
            "        # Revoked holidays in 20132015\n",
            "        if year < 2013 or year > 2015:\n",
            "            self[e + rd(days=60)] = \"Corpo de Deus\"\n",
            "            self[date(year, OCT, 5)] = \"Implantao da Repblica\"\n",
            "            self[date(year, NOV, 1)] = \"Dia de Todos os Santos\"\n",
            "            self[date(year, DEC, 1)] = \"Restaurao da Independncia\"\n",
            "\n",
            "        self[date(year, 4, 25)] = \"Dia da Liberdade\"\n",
            "        self[date(year, 5, 1)] = \"Dia do Trabalhador\"\n",
            "        self[date(year, 6, 10)] = \"Dia de Portugal\"\n",
            "        self[date(year, 8, 15)] = \"Assuno de Nossa Senhora\"\n",
            "        self[date(year, DEC, 8)] = \"Imaculada Conceio\"\n",
            "        self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "\n",
            "\n",
            "class PT(Portugal):\n",
            "    pass\n",
            "\n",
            "\n",
            "class PortugalExt(Portugal):\n",
            "    \"\"\"\n",
            "    Adds extended days that most people have as a bonus from their companies:\n",
            "    - Carnival\n",
            "    - the day before and after xmas\n",
            "    - the day before the new year\n",
            "    - Lisbon's city holiday\n",
            "    \"\"\"\n",
            "\n",
            "    def _populate(self, year):\n",
            "        super(PortugalExt, self)._populate(year)\n",
            "\n",
            "        e = easter(year)\n",
            "        self[e - rd(days=47)] = \"Carnaval\"\n",
            "        self[date(year, DEC, 24)] = \"Vespera de Natal\"\n",
            "        self[date(year, DEC, 26)] = \"26 de Dezembro\"\n",
            "        self[date(year, DEC, 31)] = \"Vespera de Ano novo\"\n",
            "        self[date(year, 6, 13)] = \"Dia de Santo Antnio\"\n",
            "\n",
            "        # TODO add bridging days\n",
            "        # - get Holidays that occur on Tuesday  and add Monday (-1 day)\n",
            "        # - get Holidays that occur on Thursday and add Friday (+1 day)\n",
            "\n",
            "\n",
            "class PTE(PortugalExt):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Netherlands(HolidayBase):\n",
            "    SUN = 6\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        # http://www.iamsterdam.com/en/plan-your-trip/practical-info/public-holidays\n",
            "        self.country = \"NL\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New years\n",
            "        self[date(year, JAN, 1)] = \"Nieuwjaarsdag\"\n",
            "\n",
            "        easter_date = easter(year)\n",
            "\n",
            "        # Easter\n",
            "        self[easter_date] = \"Eerste paasdag\"\n",
            "\n",
            "        # Second easter day\n",
            "        self[easter_date + rd(days=1)] = \"Tweede paasdag\"\n",
            "\n",
            "        # Ascension day\n",
            "        self[easter_date + rd(days=39)] = \"Hemelvaart\"\n",
            "\n",
            "        # Pentecost\n",
            "        self[easter_date + rd(days=49)] = \"Eerste Pinksterdag\"\n",
            "\n",
            "        # Pentecost monday\n",
            "        self[easter_date + rd(days=50)] = \"Tweede Pinksterdag\"\n",
            "\n",
            "        # First christmas\n",
            "        self[date(year, DEC, 25)] = \"Eerste Kerstdag\"\n",
            "\n",
            "        # Second christmas\n",
            "        self[date(year, DEC, 26)] = \"Tweede Kerstdag\"\n",
            "\n",
            "        # Liberation day\n",
            "        if year >= 1945 and year % 5 == 0:\n",
            "            self[date(year, MAY, 5)] = \"Bevrijdingsdag\"\n",
            "\n",
            "        # Kingsday\n",
            "        if year >= 2014:\n",
            "            kings_day = date(year, APR, 27)\n",
            "            if kings_day.weekday() == self.SUN:\n",
            "                kings_day = kings_day - rd(days=1)\n",
            "\n",
            "            self[kings_day] = \"Koningsdag\"\n",
            "\n",
            "        # Queen's day\n",
            "        if 1891 <= year <= 2013:\n",
            "            queens_day = date(year, APR, 30)\n",
            "            if year <= 1948:\n",
            "                queens_day = date(year, AUG, 31)\n",
            "\n",
            "            if queens_day.weekday() == self.SUN:\n",
            "                if year < 1980:\n",
            "                    queens_day = queens_day + rd(days=1)\n",
            "                else:\n",
            "                    queens_day = queens_day - rd(days=1)\n",
            "\n",
            "            self[queens_day] = \"Koninginnedag\"\n",
            "\n",
            "\n",
            "class NL(Netherlands):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Norway(HolidayBase):\n",
            "    \"\"\"\n",
            "    Norwegian holidays.\n",
            "    Note that holidays falling on a sunday is \"lost\",\n",
            "    it will not be moved to another day to make up for the collision.\n",
            "\n",
            "    In Norway, ALL sundays are considered a holiday (https://snl.no/helligdag).\n",
            "    Initialize this class with include_sundays=False\n",
            "    to not include sundays as a holiday.\n",
            "\n",
            "    Primary sources:\n",
            "    https://lovdata.no/dokument/NL/lov/1947-04-26-1\n",
            "    https://no.wikipedia.org/wiki/Helligdager_i_Norge\n",
            "    https://www.timeanddate.no/merkedag/norge/\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, include_sundays=True, **kwargs):\n",
            "        \"\"\"\n",
            "\n",
            "        :param include_sundays: Whether to consider sundays as a holiday\n",
            "        (which they are in Norway)\n",
            "        :param kwargs:\n",
            "        \"\"\"\n",
            "        self.country = \"NO\"\n",
            "        self.include_sundays = include_sundays\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Add all the sundays of the year before adding the \"real\" holidays\n",
            "        if self.include_sundays:\n",
            "            first_day_of_year = date(year, JAN, 1)\n",
            "            first_sunday_of_year = \\\n",
            "                first_day_of_year + \\\n",
            "                rd(days=SUN - first_day_of_year.weekday())\n",
            "            cur_date = first_sunday_of_year\n",
            "\n",
            "            while cur_date < date(year + 1, 1, 1):\n",
            "                assert cur_date.weekday() == SUN\n",
            "\n",
            "                self[cur_date] = \"Sndag\"\n",
            "                cur_date += rd(days=7)\n",
            "\n",
            "        # ========= Static holidays =========\n",
            "        self[date(year, JAN, 1)] = \"Frste nyttrsdag\"\n",
            "\n",
            "        # Source: https://lovdata.no/dokument/NL/lov/1947-04-26-1\n",
            "        if year >= 1947:\n",
            "            self[date(year, MAY, 1)] = \"Arbeidernes dag\"\n",
            "            self[date(year, MAY, 17)] = \"Grunnlovsdag\"\n",
            "\n",
            "        # According to https://no.wikipedia.org/wiki/F%C3%B8rste_juledag,\n",
            "        # these dates are only valid from year > 1700\n",
            "        # Wikipedia has no source for the statement, so leaving this be for now\n",
            "        self[date(year, DEC, 25)] = \"Frste juledag\"\n",
            "        self[date(year, DEC, 26)] = \"Andre juledag\"\n",
            "\n",
            "        # ========= Moving holidays =========\n",
            "        # NOTE: These are probably subject to the same > 1700\n",
            "        # restriction as the above dates. The only source I could find for how\n",
            "        # long Easter has been celebrated in Norway was\n",
            "        # https://www.hf.uio.no/ikos/tjenester/kunnskap/samlinger/norsk-folkeminnesamling/livs-og-arshoytider/paske.html\n",
            "        # which says\n",
            "        # \"(...) has been celebrated for over 1000 years (...)\" (in Norway)\n",
            "        e = easter(year)\n",
            "        maundy_thursday = e - rd(days=3)\n",
            "        good_friday = e - rd(days=2)\n",
            "        resurrection_sunday = e\n",
            "        easter_monday = e + rd(days=1)\n",
            "        ascension_thursday = e + rd(days=39)\n",
            "        pentecost = e + rd(days=49)\n",
            "        pentecost_day_two = e + rd(days=50)\n",
            "\n",
            "        assert maundy_thursday.weekday() == THU\n",
            "        assert good_friday.weekday() == FRI\n",
            "        assert resurrection_sunday.weekday() == SUN\n",
            "        assert easter_monday.weekday() == MON\n",
            "        assert ascension_thursday.weekday() == THU\n",
            "        assert pentecost.weekday() == SUN\n",
            "        assert pentecost_day_two.weekday() == MON\n",
            "\n",
            "        self[maundy_thursday] = \"Skjrtorsdag\"\n",
            "        self[good_friday] = \"Langfredag\"\n",
            "        self[resurrection_sunday] = \"Frste pskedag\"\n",
            "        self[easter_monday] = \"Andre pskedag\"\n",
            "        self[ascension_thursday] = \"Kristi himmelfartsdag\"\n",
            "        self[pentecost] = \"Frste pinsedag\"\n",
            "        self[pentecost_day_two] = \"Andre pinsedag\"\n",
            "\n",
            "\n",
            "class NO(Norway):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Italy(HolidayBase):\n",
            "    PROVINCES = ['AN', 'AO', 'BA', 'BL', 'BO',\n",
            "                 'BZ', 'BS', 'CB', 'CT', 'Cesena',\n",
            "                 'CH', 'CS', 'KR', 'EN', 'FE', 'FI',\n",
            "                 'FC', 'Forli', 'FR', 'GE', 'GO', 'IS',\n",
            "                 'SP', 'LT', 'MN', 'MS', 'MI',\n",
            "                 'MO', 'MB', 'NA', 'PD', 'PA',\n",
            "                 'PR', 'PG', 'PE', 'PC', 'PI',\n",
            "                 'PD', 'PT', 'RA', 'RE',\n",
            "                 'RI', 'RN', 'RM', 'RO', 'SA',\n",
            "                 'SR', 'TE', 'TO', 'TS', 'Pesaro', 'PU',\n",
            "                 'Urbino', 'VE', 'VC', 'VI']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'IT'\n",
            "        self.prov = kwargs.pop('prov', kwargs.pop('state', ''))\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        self[date(year, JAN, 1)] = \"Capodanno\"\n",
            "        self[date(year, JAN, 6)] = \"Epifania del Signore\"\n",
            "        self[easter(year)] = \"Pasqua di Resurrezione\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Luned dell'Angelo\"\n",
            "        if year >= 1946:\n",
            "            self[date(year, APR, 25)] = \"Festa della Liberazione\"\n",
            "        self[date(year, MAY, 1)] = \"Festa dei Lavoratori\"\n",
            "        if year >= 1948:\n",
            "            self[date(year, JUN, 2)] = \"Festa della Repubblica\"\n",
            "        self[date(year, AUG, 15)] = \"Assunzione della Vergine\"\n",
            "        self[date(year, NOV, 1)] = \"Tutti i Santi\"\n",
            "        self[date(year, DEC, 8)] = \"Immacolata Concezione\"\n",
            "        self[date(year, DEC, 25)] = \"Natale\"\n",
            "        self[date(year, DEC, 26)] = \"Santo Stefano\"\n",
            "\n",
            "        # Provinces holidays\n",
            "        if self.prov:\n",
            "            if self.prov == 'AN':\n",
            "                self[date(year, MAY, 4)] = \"San Ciriaco\"\n",
            "            elif self.prov == 'AO':\n",
            "                self[date(year, SEP, 7)] = \"San Grato\"\n",
            "            elif self.prov in ('BA'):\n",
            "                self[date(year, DEC, 6)] = \"San Nicola\"\n",
            "            elif self.prov == 'BL':\n",
            "                self[date(year, NOV, 11)] = \"San Martino\"\n",
            "            elif self.prov in ('BO'):\n",
            "                self[date(year, OCT, 4)] = \"San Petronio\"\n",
            "            elif self.prov == 'BZ':\n",
            "                self[date(year, AUG, 15)] = \"Maria Santissima Assunta\"\n",
            "            elif self.prov == 'BS':\n",
            "                self[date(year, FEB, 15)] = \"Santi Faustino e Giovita\"\n",
            "            elif self.prov == 'CB':\n",
            "                self[date(year, APR, 23)] = \"San Giorgio\"\n",
            "            elif self.prov == 'CT':\n",
            "                self[date(year, FEB, 5)] = \"Sant'Agata\"\n",
            "            elif self.prov in ('FC', 'Cesena'):\n",
            "                self[date(year, JUN, 24)] = \"San Giovanni Battista\"\n",
            "            if self.prov in ('FC', 'Forl'):\n",
            "                self[date(year, FEB, 4)] = \"Madonna del Fuoco\"\n",
            "            elif self.prov == 'CH':\n",
            "                self[date(year, MAY, 11)] = \"San Giustino di Chieti\"\n",
            "            elif self.prov == 'CS':\n",
            "                self[date(year, FEB, 12)] = \"Madonna del Pilerio\"\n",
            "            elif self.prov == 'KR':\n",
            "                self[date(year, OCT, 9)] = \"San Dionigi\"\n",
            "            elif self.prov == 'EN':\n",
            "                self[date(year, JUL, 2)] = \"Madonna della Visitazione\"\n",
            "            elif self.prov == 'FE':\n",
            "                self[date(year, APR, 22)] = \"San Giorgio\"\n",
            "            elif self.prov == 'FI':\n",
            "                self[date(year, JUN, 24)] = \"San Giovanni Battista\"\n",
            "            elif self.prov == 'FR':\n",
            "                self[date(year, JUN, 20)] = \"San Silverio\"\n",
            "            elif self.prov == 'GE':\n",
            "                self[date(year, JUN, 24)] = \"San Giovanni Battista\"\n",
            "            elif self.prov == 'GO':\n",
            "                self[date(year, MAR, 16)] = \"Santi Ilario e Taziano\"\n",
            "            elif self.prov == 'IS':\n",
            "                self[date(year, MAY, 19)] = \"San Pietro Celestino\"\n",
            "            elif self.prov == 'SP':\n",
            "                self[date(year, MAR, 19)] = \"San Giuseppe\"\n",
            "            elif self.prov == 'LT':\n",
            "                self[date(year, APR, 25)] = \"San Marco evangelista\"\n",
            "            elif self.prov == 'MI':\n",
            "                self[date(year, DEC, 7)] = \"Sant'Ambrogio\"\n",
            "            elif self.prov == 'MN':\n",
            "                self[date(year, MAR, 18)] = \"Sant'Anselmo da Baggio\"\n",
            "            elif self.prov == 'MS':\n",
            "                self[date(year, OCT, 4)] = \"San Francesco d'Assisi\"\n",
            "            elif self.prov == 'MO':\n",
            "                self[date(year, JAN, 31)] = \"San Geminiano\"\n",
            "            elif self.prov == 'MB':\n",
            "                self[date(year, JUN, 24)] = \"San Giovanni Battista\"\n",
            "            elif self.prov == 'NA':\n",
            "                self[date(year, SEP, 19)] = \"San Gennaro\"\n",
            "            elif self.prov == 'PD':\n",
            "                self[date(year, JUN, 13)] = \"Sant'Antonio di Padova\"\n",
            "            elif self.prov == 'PA':\n",
            "                self[date(year, JUL, 15)] = \"San Giovanni\"\n",
            "            elif self.prov == 'PR':\n",
            "                self[date(year, JAN, 13)] = \"Sant'Ilario di Poitiers\"\n",
            "            elif self.prov == 'PG':\n",
            "                self[date(year, JAN, 29)] = \"Sant'Ercolano e San Lorenzo\"\n",
            "            elif self.prov == 'PC':\n",
            "                self[date(year, JUL, 4)] = \"Sant'Antonino di Piacenza\"\n",
            "            elif self.prov == 'RM':\n",
            "                self[date(year, JUN, 29)] = \"Santi Pietro e Paolo\"\n",
            "            elif self.prov == 'TS':\n",
            "                self[date(year, NOV, 3)] = \"San Giusto\"\n",
            "            elif self.prov == 'VI':\n",
            "                self[date(year, APR, 25)] = \"San Marco\"\n",
            "\n",
            "        # TODO: add missing provinces' holidays:\n",
            "        # 'Pisa', 'Pordenone', 'Potenza', 'Ravenna',\n",
            "        # 'Reggio Emilia', 'Rieti', 'Rimini', 'Rovigo',\n",
            "        # 'Salerno', 'Siracusa', 'Teramo', 'Torino', 'Urbino',\n",
            "        # 'Venezia'\n",
            "\n",
            "\n",
            "class IT(Italy):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Sweden(HolidayBase):\n",
            "    \"\"\"\n",
            "    Swedish holidays.\n",
            "    Note that holidays falling on a sunday are \"lost\",\n",
            "    it will not be moved to another day to make up for the collision.\n",
            "    In Sweden, ALL sundays are considered a holiday\n",
            "    (https://sv.wikipedia.org/wiki/Helgdagar_i_Sverige).\n",
            "    Initialize this class with include_sundays=False\n",
            "    to not include sundays as a holiday.\n",
            "    Primary sources:\n",
            "    https://sv.wikipedia.org/wiki/Helgdagar_i_Sverige and\n",
            "    http://www.riksdagen.se/sv/dokument-lagar/dokument/svensk-forfattningssamling/lag-1989253-om-allmanna-helgdagar_sfs-1989-253\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, include_sundays=True, **kwargs):\n",
            "        \"\"\"\n",
            "        :param include_sundays: Whether to consider sundays as a holiday\n",
            "        (which they are in Sweden)\n",
            "        :param kwargs:\n",
            "        \"\"\"\n",
            "        self.country = \"SE\"\n",
            "        self.include_sundays = include_sundays\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Add all the sundays of the year before adding the \"real\" holidays\n",
            "        if self.include_sundays:\n",
            "            first_day_of_year = date(year, JAN, 1)\n",
            "            first_sunday_of_year = \\\n",
            "                first_day_of_year + \\\n",
            "                rd(days=SUN - first_day_of_year.weekday())\n",
            "            cur_date = first_sunday_of_year\n",
            "\n",
            "            while cur_date < date(year + 1, 1, 1):\n",
            "                assert cur_date.weekday() == SUN\n",
            "\n",
            "                self[cur_date] = \"Sndag\"\n",
            "                cur_date += rd(days=7)\n",
            "\n",
            "        # ========= Static holidays =========\n",
            "        self[date(year, JAN, 1)] = \"Nyrsdagen\"\n",
            "\n",
            "        self[date(year, JAN, 6)] = \"Trettondedag jul\"\n",
            "\n",
            "        # Source: https://sv.wikipedia.org/wiki/F%C3%B6rsta_maj\n",
            "        if year >= 1939:\n",
            "            self[date(year, MAY, 1)] = \"Frsta maj\"\n",
            "\n",
            "        # Source: https://sv.wikipedia.org/wiki/Sveriges_nationaldag\n",
            "        if year >= 2005:\n",
            "            self[date(year, JUN, 6)] = \"Sveriges nationaldag\"\n",
            "\n",
            "        self[date(year, DEC, 24)] = \"Julafton\"\n",
            "        self[date(year, DEC, 25)] = \"Juldagen\"\n",
            "        self[date(year, DEC, 26)] = \"Annandag jul\"\n",
            "        self[date(year, DEC, 31)] = \"Nyrsafton\"\n",
            "\n",
            "        # ========= Moving holidays =========\n",
            "        e = easter(year)\n",
            "        maundy_thursday = e - rd(days=3)\n",
            "        good_friday = e - rd(days=2)\n",
            "        easter_saturday = e - rd(days=1)\n",
            "        resurrection_sunday = e\n",
            "        easter_monday = e + rd(days=1)\n",
            "        ascension_thursday = e + rd(days=39)\n",
            "        pentecost = e + rd(days=49)\n",
            "        pentecost_day_two = e + rd(days=50)\n",
            "\n",
            "        assert maundy_thursday.weekday() == THU\n",
            "        assert good_friday.weekday() == FRI\n",
            "        assert easter_saturday.weekday() == SAT\n",
            "        assert resurrection_sunday.weekday() == SUN\n",
            "        assert easter_monday.weekday() == MON\n",
            "        assert ascension_thursday.weekday() == THU\n",
            "        assert pentecost.weekday() == SUN\n",
            "        assert pentecost_day_two.weekday() == MON\n",
            "\n",
            "        self[good_friday] = \"Lngfredagen\"\n",
            "        self[resurrection_sunday] = \"Pskdagen\"\n",
            "        self[easter_monday] = \"Annandag psk\"\n",
            "        self[ascension_thursday] = \"Kristi himmelsfrdsdag\"\n",
            "        self[pentecost] = \"Pingstdagen\"\n",
            "        if year <= 2004:\n",
            "            self[pentecost_day_two] = \"Annandag pingst\"\n",
            "\n",
            "        # Midsummer evening. Friday between June 19th and June 25th\n",
            "        self[date(year, JUN, 19) + rd(weekday=FR)] = \"Midsommarafton\"\n",
            "\n",
            "        # Midsummer day. Saturday between June 20th and June 26th\n",
            "        if year >= 1953:\n",
            "            self[date(year, JUN, 20) + rd(weekday=SA)] = \"Midsommardagen\"\n",
            "        else:\n",
            "            self[date(year, JUN, 24)] = \"Midsommardagen\"\n",
            "            # All saints day. Friday between October 31th and November 6th\n",
            "        self[date(year, OCT, 31) + rd(weekday=SA)] = \"Alla helgons dag\"\n",
            "\n",
            "        if year <= 1953:\n",
            "            self[date(year, MAR, 25)] = \"Jungfru Marie bebdelsedag\"\n",
            "\n",
            "\n",
            "class SE(Sweden):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Japan(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Japan\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'JP'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        if year < 1949 or year > 2099:\n",
            "            raise NotImplementedError\n",
            "\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \"\"\n",
            "\n",
            "        # Coming of Age Day\n",
            "        if year <= 1999:\n",
            "            self[date(year, JAN, 15)] = \"\"\n",
            "        else:\n",
            "            self[date(year, JAN, 1) + rd(weekday=MO(+2))] = \"\"\n",
            "\n",
            "        # Foundation Day\n",
            "        self[date(year, FEB, 11)] = \"\"\n",
            "\n",
            "        # Reiwa Emperor's Birthday\n",
            "        if year >= 2020:\n",
            "            self[date(year, FEB, 23)] = ''\n",
            "\n",
            "        # Vernal Equinox Day\n",
            "        self[self._vernal_equinox_day(year)] = \"\"\n",
            "\n",
            "        # Showa Emperor's Birthday, Greenery Day or Showa Day\n",
            "        if year <= 1988:\n",
            "            self[date(year, APR, 29)] = \"\"\n",
            "        elif year <= 2006:\n",
            "            self[date(year, APR, 29)] = \"\"\n",
            "        else:\n",
            "            self[date(year, APR, 29)] = \"\"\n",
            "\n",
            "        # Constitution Memorial Day\n",
            "        self[date(year, MAY, 3)] = \"\"\n",
            "\n",
            "        # Greenery Day\n",
            "        if year >= 2007:\n",
            "            self[date(year, MAY, 4)] = \"\"\n",
            "\n",
            "        # Children's Day\n",
            "        self[date(year, MAY, 5)] = \"\"\n",
            "\n",
            "        # Marine Day\n",
            "        if 1996 <= year <= 2002:\n",
            "            self[date(year, JUL, 20)] = \"\"\n",
            "        elif year >= 2003:\n",
            "            self[date(year, JUL, 1) + rd(weekday=MO(+3))] = \"\"\n",
            "\n",
            "        # Mountain Day\n",
            "        if year >= 2016:\n",
            "            self[date(year, AUG, 11)] = \"\"\n",
            "\n",
            "        # Respect for the Aged Day\n",
            "        if 1966 <= year <= 2002:\n",
            "            self[date(year, SEP, 15)] = \"\"\n",
            "        elif year >= 2003:\n",
            "            self[date(year, SEP, 1) + rd(weekday=MO(+3))] = \"\"\n",
            "\n",
            "        # Autumnal Equinox Day\n",
            "        self[self._autumnal_equinox_day(year)] = \"\"\n",
            "\n",
            "        # Health and Sports Day\n",
            "        if 1966 <= year <= 1999:\n",
            "            self[date(year, OCT, 10)] = \"\"\n",
            "        elif year >= 2000:\n",
            "            self[date(year, OCT, 1) + rd(weekday=MO(+2))] = \"\"\n",
            "\n",
            "        # Culture Day\n",
            "        self[date(year, NOV, 3)] = \"\"\n",
            "\n",
            "        # Labour Thanksgiving Day\n",
            "        self[date(year, NOV, 23)] = \"\"\n",
            "\n",
            "        # Heisei Emperor's Birthday\n",
            "        if 1989 <= year <= 2018:\n",
            "            self[date(year, DEC, 23)] = \"\"\n",
            "\n",
            "        # Regarding the Emperor of Reiwa\n",
            "        if year == 2019:\n",
            "            # Enthronement Day\n",
            "            self[date(year, MAY, 1)] = ''\n",
            "            # Enthronement ceremony\n",
            "            self[date(year, OCT, 22)] = ''\n",
            "\n",
            "        # A weekday between national holidays becomes a holiday too ()\n",
            "        self._add_national_holidays(year)\n",
            "\n",
            "        # Substitute holidays\n",
            "        self._add_substitute_holidays(year)\n",
            "\n",
            "    def _vernal_equinox_day(self, year):\n",
            "        day = 20\n",
            "        if year % 4 == 0:\n",
            "            if year <= 1956:\n",
            "                day = 21\n",
            "            elif year >= 2092:\n",
            "                day = 19\n",
            "        elif year % 4 == 1:\n",
            "            if year <= 1989:\n",
            "                day = 21\n",
            "        elif year % 4 == 2:\n",
            "            if year <= 2022:\n",
            "                day = 21\n",
            "        elif year % 4 == 3:\n",
            "            if year <= 2055:\n",
            "                day = 21\n",
            "        return date(year, MAR, day)\n",
            "\n",
            "    def _autumnal_equinox_day(self, year):\n",
            "        day = 22\n",
            "        if year % 4 == 0:\n",
            "            if year <= 2008:\n",
            "                day = 23\n",
            "        elif year % 4 == 1:\n",
            "            if year <= 2041:\n",
            "                day = 23\n",
            "        elif year % 4 == 2:\n",
            "            if year <= 2074:\n",
            "                day = 23\n",
            "        elif year % 4 == 3:\n",
            "            if year <= 1979:\n",
            "                day = 24\n",
            "            else:\n",
            "                day = 23\n",
            "        return date(year, SEP, day)\n",
            "\n",
            "    def _add_national_holidays(self, year):\n",
            "        if year in (1993, 1999, 2004, 1988, 1994, 2005, 1989, 1995, 2000, 2006,\n",
            "                    1990, 2001, 1991, 1996, 2002):\n",
            "            self[date(year, MAY, 4)] = \"\"\n",
            "\n",
            "        if year in (2032, 2049, 2060, 2077, 2088, 2094):\n",
            "            self[date(year, SEP, 21)] = \"\"\n",
            "\n",
            "        if year in (2009, 2015, 2026, 2037, 2043, 2054, 2065, 2071, 2099):\n",
            "            self[date(year, SEP, 22)] = \"\"\n",
            "\n",
            "        if year == 2019:\n",
            "            self[date(year, APR, 30)] = ''\n",
            "            self[date(year, MAY, 2)] = ''\n",
            "\n",
            "    def _add_substitute_holidays(self, year):\n",
            "        table = (\n",
            "            (1, 2, (1978, 1984, 1989, 1995, 2006, 2012, 2017, 2023, 2034, 2040,\n",
            "                    2045)),\n",
            "            (1, 16, (1978, 1984, 1989, 1995)),\n",
            "            (2, 12, (1979, 1990, 1996, 2001, 2007, 2018, 2024, 2029, 2035,\n",
            "                     2046)),\n",
            "            (2, 24, (2020,)),\n",
            "            (3, 21, (1988, 2005, 2016, 2033, 2044, 2050)),\n",
            "            (3, 22, (1982, 1999, 2010, 2027)),\n",
            "            (4, 30, (1973, 1979, 1984, 1990, 2001, 2007, 2012, 2018, 2029,\n",
            "                     2035, 2040, 2046)),\n",
            "            (5, 4, (1981, 1987, 1992, 1998)),\n",
            "            (5, 6, (1985, 1991, 1996, 2002, 2013, 2019, 2024, 2030, 2041, 2047,\n",
            "                    2008, 2014, 2025, 2031, 2036, 2042, 2009, 2015, 2020, 2026,\n",
            "                    2037, 2043, 2048)),\n",
            "            (7, 21, (1997,)),\n",
            "            (8, 12, (2019, 2024, 2030, 2041, 2047)),\n",
            "            (9, 16, (1974, 1985, 1991, 1996, 2002)),\n",
            "            (9, 23, (2024,)),\n",
            "            (9, 24, (1973, 1984, 1990, 2001, 2007, 2018, 2029, 2035, 2046)),\n",
            "            (10, 11, (1976, 1982, 1993, 1999)),\n",
            "            (11, 4, (1974, 1985, 1991, 1996, 2002, 2013, 2019, 2024, 2030,\n",
            "                     2041, 2047)),\n",
            "            (11, 24, (1975, 1980, 1986, 1997, 2003, 2008, 2014, 2025, 2031,\n",
            "                      2036, 2042)),\n",
            "            (12, 24, (1990, 2001, 2007, 2012, 2018)),\n",
            "        )\n",
            "        for holiday in table:\n",
            "            month = holiday[0]\n",
            "            day = holiday[1]\n",
            "            years = holiday[2]\n",
            "            if year in years:\n",
            "                self[date(year, month, day)] = \"\"\n",
            "\n",
            "\n",
            "class JP(Japan):\n",
            "    pass\n",
            "\n",
            "\n",
            "class France(HolidayBase):\n",
            "    \"\"\"Official French holidays.\n",
            "\n",
            "    Some provinces have specific holidays, only those are included in the\n",
            "    PROVINCES, because these provinces have different administrative status,\n",
            "    which makes it difficult to enumerate.\n",
            "\n",
            "    For religious holidays usually happening on Sundays (Easter, Pentecost),\n",
            "    only the following Monday is considered a holiday.\n",
            "\n",
            "    Primary sources:\n",
            "        https://fr.wikipedia.org/wiki/Ftes_et_jours_fris_en_France\n",
            "        https://www.service-public.fr/particuliers/vosdroits/F2405\n",
            "    \"\"\"\n",
            "\n",
            "    PROVINCES = ['Mtropole', 'Alsace-Moselle', 'Guadeloupe', 'Guyane',\n",
            "                 'Martinique', 'Mayotte', 'Nouvelle-Caldonie', 'La Runion',\n",
            "                 'Polynsie Franaise', 'Saint-Barthlmy', 'Saint-Martin',\n",
            "                 'Wallis-et-Futuna']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'FR'\n",
            "        self.prov = kwargs.pop('prov', 'Mtropole')\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Civil holidays\n",
            "        if year > 1810:\n",
            "            self[date(year, JAN, 1)] = \"Jour de l'an\"\n",
            "\n",
            "        if year > 1919:\n",
            "            name = 'Fte du Travail'\n",
            "            if year <= 1948:\n",
            "                name += ' et de la Concorde sociale'\n",
            "            self[date(year, MAY, 1)] = name\n",
            "\n",
            "        if (1953 <= year <= 1959) or year > 1981:\n",
            "            self[date(year, MAY, 8)] = 'Armistice 1945'\n",
            "\n",
            "        if year >= 1880:\n",
            "            self[date(year, JUL, 14)] = 'Fte nationale'\n",
            "\n",
            "        if year >= 1918:\n",
            "            self[date(year, NOV, 11)] = 'Armistice 1918'\n",
            "\n",
            "        # Religious holidays\n",
            "        if self.prov in ['Alsace-Moselle', 'Guadeloupe', 'Guyane',\n",
            "                         'Martinique', 'Polynsie Franaise']:\n",
            "            self[easter(year) - rd(days=2)] = 'Vendredi saint'\n",
            "\n",
            "        if self.prov == 'Alsace-Moselle':\n",
            "            self[date(year, DEC, 26)] = 'Deuxime jour de Nol'\n",
            "\n",
            "        if year >= 1886:\n",
            "            self[easter(year) + rd(days=1)] = 'Lundi de Pques'\n",
            "            self[easter(year) + rd(days=50)] = 'Lundi de Pentecte'\n",
            "\n",
            "        if year >= 1802:\n",
            "            self[easter(year) + rd(days=39)] = 'Ascension'\n",
            "            self[date(year, AUG, 15)] = 'Assomption'\n",
            "            self[date(year, NOV, 1)] = 'Toussaint'\n",
            "\n",
            "            name = 'Nol'\n",
            "            if self.prov == 'Alsace-Moselle':\n",
            "                name = 'Premier jour de ' + name\n",
            "            self[date(year, DEC, 25)] = name\n",
            "\n",
            "        # Non-metropolitan holidays (starting dates missing)\n",
            "        if self.prov == 'Mayotte':\n",
            "            self[date(year, APR, 27)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "        if self.prov == 'Wallis-et-Futuna':\n",
            "            self[date(year, APR, 28)] = 'Saint Pierre Chanel'\n",
            "\n",
            "        if self.prov == 'Martinique':\n",
            "            self[date(year, MAY, 22)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "        if self.prov in ['Guadeloupe', 'Saint-Martin']:\n",
            "            self[date(year, MAY, 27)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "        if self.prov == 'Guyane':\n",
            "            self[date(year, JUN, 10)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "        if self.prov == 'Polynsie Franaise':\n",
            "            self[date(year, JUN, 29)] = \"Fte de l'autonomie\"\n",
            "\n",
            "        if self.prov in ['Guadeloupe', 'Martinique']:\n",
            "            self[date(year, JUL, 21)] = 'Fte Victor Schoelcher'\n",
            "\n",
            "        if self.prov == 'Wallis-et-Futuna':\n",
            "            self[date(year, JUL, 29)] = 'Fte du Territoire'\n",
            "\n",
            "        if self.prov == 'Nouvelle-Caldonie':\n",
            "            self[date(year, SEP, 24)] = 'Fte de la Citoyennet'\n",
            "\n",
            "        if self.prov == 'Saint-Barthlmy':\n",
            "            self[date(year, OCT, 9)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "        if self.prov == 'La Runion' and year >= 1981:\n",
            "            self[date(year, DEC, 20)] = \"Abolition de l'esclavage\"\n",
            "\n",
            "\n",
            "# FR already exists (Friday), we don't want to mess it up\n",
            "class FRA(France):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Belgium(HolidayBase):\n",
            "    \"\"\"\n",
            "    https://www.belgium.be/nl/over_belgie/land/belgie_in_een_notendop/feestdagen\n",
            "    https://nl.wikipedia.org/wiki/Feestdagen_in_Belgi%C3%AB\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"BE\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New years\n",
            "        self[date(year, JAN, 1)] = \"Nieuwjaarsdag\"\n",
            "\n",
            "        easter_date = easter(year)\n",
            "\n",
            "        # Easter\n",
            "        self[easter_date] = \"Pasen\"\n",
            "\n",
            "        # Second easter day\n",
            "        self[easter_date + rd(days=1)] = \"Paasmaandag\"\n",
            "\n",
            "        # Ascension day\n",
            "        self[easter_date + rd(days=39)] = \"O.L.H. Hemelvaart\"\n",
            "\n",
            "        # Pentecost\n",
            "        self[easter_date + rd(days=49)] = \"Pinksteren\"\n",
            "\n",
            "        # Pentecost monday\n",
            "        self[easter_date + rd(days=50)] = \"Pinkstermaandag\"\n",
            "\n",
            "        # International Workers' Day\n",
            "        self[date(year, MAY, 1)] = \"Dag van de Arbeid\"\n",
            "\n",
            "        # Belgian National Day\n",
            "        self[date(year, JUL, 21)] = \"Nationale feestdag\"\n",
            "\n",
            "        # Assumption of Mary\n",
            "        self[date(year, AUG, 15)] = \"O.L.V. Hemelvaart\"\n",
            "\n",
            "        # All Saints' Day\n",
            "        self[date(year, NOV, 1)] = \"Allerheiligen\"\n",
            "\n",
            "        # Armistice Day\n",
            "        self[date(year, NOV, 11)] = \"Wapenstilstand\"\n",
            "\n",
            "        # First christmas\n",
            "        self[date(year, DEC, 25)] = \"Kerstmis\"\n",
            "\n",
            "\n",
            "class BE(Belgium):\n",
            "    pass\n",
            "\n",
            "\n",
            "class SouthAfrica(HolidayBase):\n",
            "    def __init__(self, **kwargs):\n",
            "        # http://www.gov.za/about-sa/public-holidays\n",
            "        # https://en.wikipedia.org/wiki/Public_holidays_in_South_Africa\n",
            "        self.country = \"ZA\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Observed since 1910, with a few name changes\n",
            "        if year > 1909:\n",
            "            self[date(year, 1, 1)] = \"New Year's Day\"\n",
            "\n",
            "            e = easter(year)\n",
            "            good_friday = e - rd(days=2)\n",
            "            easter_monday = e + rd(days=1)\n",
            "            self[good_friday] = \"Good Friday\"\n",
            "            if year > 1979:\n",
            "                self[easter_monday] = \"Family Day\"\n",
            "            else:\n",
            "                self[easter_monday] = \"Easter Monday\"\n",
            "\n",
            "            if 1909 < year < 1952:\n",
            "                dec_16_name = \"Dingaan's Day\"\n",
            "            elif 1951 < year < 1980:\n",
            "                dec_16_name = \"Day of the Covenant\"\n",
            "            elif 1979 < year < 1995:\n",
            "                dec_16_name = \"Day of the Vow\"\n",
            "            else:\n",
            "                dec_16_name = \"Day of Reconciliation\"\n",
            "            self[date(year, DEC, 16)] = dec_16_name\n",
            "\n",
            "            self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "\n",
            "            if year > 1979:\n",
            "                dec_26_name = \"Day of Goodwill\"\n",
            "            else:\n",
            "                dec_26_name = \"Boxing Day\"\n",
            "            self[date(year, 12, 26)] = dec_26_name\n",
            "\n",
            "        # Observed since 1995/1/1\n",
            "        if year > 1994:\n",
            "            self[date(year, MAR, 21)] = \"Human Rights Day\"\n",
            "            self[date(year, APR, 27)] = \"Freedom Day\"\n",
            "            self[date(year, MAY, 1)] = \"Workers' Day\"\n",
            "            self[date(year, JUN, 16)] = \"Youth Day\"\n",
            "            self[date(year, AUG, 9)] = \"National Women's Day\"\n",
            "            self[date(year, SEP, 24)] = \"Heritage Day\"\n",
            "\n",
            "        # Once-off public holidays\n",
            "        national_election = \"National and provincial government elections\"\n",
            "        y2k = \"Y2K changeover\"\n",
            "        local_election = \"Local government elections\"\n",
            "        presidential = \"By presidential decree\"\n",
            "\n",
            "        self[date(1999, JUN, 2)] = national_election\n",
            "        self[date(1999, DEC, 31)] = y2k\n",
            "        self[date(2000, JAN, 2)] = y2k\n",
            "        self[date(2004, APR, 14)] = national_election\n",
            "        self[date(2006, MAR, 1)] = local_election\n",
            "        self[date(2008, MAY, 2)] = presidential\n",
            "        self[date(2009, APR, 22)] = national_election\n",
            "        self[date(2011, MAY, 18)] = local_election\n",
            "        self[date(2011, DEC, 27)] = presidential\n",
            "        self[date(2014, MAY, 7)] = national_election\n",
            "        self[date(2016, AUG, 3)] = local_election\n",
            "        self[date(2019, MAY, 8)] = national_election\n",
            "\n",
            "        # As of 1995/1/1, whenever a public holiday falls on a Sunday,\n",
            "        # it rolls over to the following Monday\n",
            "        for k, v in list(self.items()):\n",
            "            if self.observed and year > 1994 and k.weekday() == SUN:\n",
            "                self[k + rd(days=1)] = v + \" (Observed)\"\n",
            "\n",
            "        # Historic public holidays no longer observed\n",
            "        if 1951 < year < 1974:\n",
            "            self[date(year, APR, 6)] = \"Van Riebeeck's Day\"\n",
            "        elif 1979 < year < 1995:\n",
            "            self[date(year, APR, 6)] = \"Founder's Day\"\n",
            "\n",
            "        if 1986 < year < 1990:\n",
            "            historic_workers_day = datetime(year, MAY, 1)\n",
            "            # observed on first Friday in May\n",
            "            while historic_workers_day.weekday() != FRI:\n",
            "                historic_workers_day += rd(days=1)\n",
            "\n",
            "            self[historic_workers_day] = \"Workers' Day\"\n",
            "\n",
            "        if 1909 < year < 1994:\n",
            "            ascension_day = e + rd(days=40)\n",
            "            self[ascension_day] = \"Ascension Day\"\n",
            "\n",
            "        if 1909 < year < 1952:\n",
            "            self[date(year, MAY, 24)] = \"Empire Day\"\n",
            "\n",
            "        if 1909 < year < 1961:\n",
            "            self[date(year, MAY, 31)] = \"Union Day\"\n",
            "        elif 1960 < year < 1994:\n",
            "            self[date(year, MAY, 31)] = \"Republic Day\"\n",
            "\n",
            "        if 1951 < year < 1961:\n",
            "            queens_birthday = datetime(year, JUN, 7)\n",
            "            # observed on second Monday in June\n",
            "            while queens_birthday.weekday() != 0:\n",
            "                queens_birthday += rd(days=1)\n",
            "\n",
            "            self[queens_birthday] = \"Queen's Birthday\"\n",
            "\n",
            "        if 1960 < year < 1974:\n",
            "            self[date(year, JUL, 10)] = \"Family Day\"\n",
            "\n",
            "        if 1909 < year < 1952:\n",
            "            kings_birthday = datetime(year, AUG, 1)\n",
            "            # observed on first Monday in August\n",
            "            while kings_birthday.weekday() != 0:\n",
            "                kings_birthday += rd(days=1)\n",
            "\n",
            "            self[kings_birthday] = \"King's Birthday\"\n",
            "\n",
            "        if 1951 < year < 1980:\n",
            "            settlers_day = datetime(year, SEP, 1)\n",
            "            while settlers_day.weekday() != 0:\n",
            "                settlers_day += rd(days=1)\n",
            "\n",
            "            self[settlers_day] = \"Settlers' Day\"\n",
            "\n",
            "        if 1951 < year < 1994:\n",
            "            self[date(year, OCT, 10)] = \"Kruger Day\"\n",
            "\n",
            "\n",
            "class ZA(SouthAfrica):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Slovenia(HolidayBase):\n",
            "    \"\"\"\n",
            "    Contains all work-free public holidays in Slovenia.\n",
            "    No holidays are returned before year 1991 when Slovenia became independent\n",
            "    country. Before that Slovenia was part of Socialist federal republic of\n",
            "    Yugoslavia.\n",
            "\n",
            "    List of holidays (including those that are not work-free:\n",
            "    https://en.wikipedia.org/wiki/Public_holidays_in_Slovenia\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'SI'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        if year <= 1990:\n",
            "            return\n",
            "\n",
            "        if year > 1991:\n",
            "            self[date(year, JAN, 1)] = \"novo leto\"\n",
            "\n",
            "            # Between 2012 and 2017 2nd January was not public holiday,\n",
            "            # or at least not work-free day\n",
            "            if year < 2013 or year > 2016:\n",
            "                self[date(year, JAN, 2)] = \"novo leto\"\n",
            "\n",
            "            # Preeren's day, slovenian cultural holiday\n",
            "            self[date(year, FEB, 8)] = \"Preernov dan\"\n",
            "\n",
            "            # Easter monday is the only easter related work-free day\n",
            "            easter_day = easter(year)\n",
            "            self[easter_day + rd(days=1)] = \"Velikononi ponedeljek\"\n",
            "\n",
            "            # Day of uprising against occupation\n",
            "            self[date(year, APR, 27)] = \"dan upora proti okupatorju\"\n",
            "\n",
            "            # Labour day, two days of it!\n",
            "            self[date(year, MAY, 1)] = \"praznik dela\"\n",
            "            self[date(year, MAY, 2)] = \"praznik dela\"\n",
            "\n",
            "            # Statehood day\n",
            "            self[date(year, JUN, 25)] = \"dan dravnosti\"\n",
            "\n",
            "            # Assumption day\n",
            "            self[date(year, AUG, 15)] = \"Marijino vnebovzetje\"\n",
            "\n",
            "            # Reformation day\n",
            "            self[date(year, OCT, 31)] = \"dan reformacije\"\n",
            "\n",
            "            # Remembrance day\n",
            "            self[date(year, NOV, 1)] = \"dan spomina na mrtve\"\n",
            "\n",
            "            # Christmas\n",
            "            self[date(year, DEC, 25)] = \"Boi\"\n",
            "\n",
            "            # Day of independence and unity\n",
            "            self[date(year, DEC, 26)] = \"dan samostojnosti in enotnosti\"\n",
            "\n",
            "\n",
            "class SI(Slovenia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Finland(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Finland\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"FI\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        e = easter(year)\n",
            "\n",
            "        self[date(year, JAN, 1)] = \"Uudenvuodenpiv\"\n",
            "        self[date(year, JAN, 6)] = \"Loppiainen\"\n",
            "        self[e - rd(days=2)] = \"Pitkperjantai\"\n",
            "        self[e] = \"Psiispiv\"\n",
            "        self[e + rd(days=1)] = \"2. psiispiv\"\n",
            "        self[date(year, MAY, 1)] = \"Vappu\"\n",
            "        self[e + rd(days=39)] = \"Helatorstai\"\n",
            "        self[e + rd(days=49)] = \"Helluntaipiv\"\n",
            "        self[date(year, JUN, 20) + rd(weekday=SA)] = \"Juhannuspiv\"\n",
            "        self[date(year, OCT, 31) + rd(weekday=SA)] = \"Pyhinpiv\"\n",
            "        self[date(year, DEC, 6)] = \"Itsenisyyspiv\"\n",
            "        self[date(year, DEC, 25)] = \"Joulupiv\"\n",
            "        self[date(year, DEC, 26)] = \"Tapaninpiv\"\n",
            "\n",
            "        # Juhannusaatto (Midsummer Eve) and Jouluaatto (Christmas Eve) are not\n",
            "        # official holidays, but are de facto.\n",
            "        self[date(year, JUN, 19) + rd(weekday=FR)] = \"Juhannusaatto\"\n",
            "        self[date(year, DEC, 24)] = \"Jouluaatto\"\n",
            "\n",
            "\n",
            "class FI(Finland):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Switzerland(HolidayBase):\n",
            "    PROVINCES = ['AG', 'AR', 'AI', 'BL', 'BS', 'BE', 'FR', 'GE', 'GL',\n",
            "                 'GR', 'JU', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SZ',\n",
            "                 'SO', 'TG', 'TI', 'UR', 'VD', 'VS', 'ZG', 'ZH']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'CH'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # public holidays\n",
            "        self[date(year, JAN, 1)] = 'Neujahrestag'\n",
            "\n",
            "        if self.prov in ('AG', 'BE', 'FR', 'GE', 'GL', 'GR', 'JU', 'LU',\n",
            "                         'NE', 'OW', 'SH', 'SO', 'TG', 'VD', 'ZG', 'ZH'):\n",
            "            self[date(year, JAN, 2)] = 'Berchtoldstag'\n",
            "\n",
            "        if self.prov in ('SZ', 'TI', 'UR'):\n",
            "            self[date(year, JAN, 6)] = 'Heilige Drei Knige'\n",
            "\n",
            "        if self.prov == 'NE':\n",
            "            self[date(year, MAR, 1)] = 'Jahrestag der Ausrufung der Republik'\n",
            "\n",
            "        if self.prov in ('NW', 'SZ', 'TI', 'UR', 'VS'):\n",
            "            self[date(year, MAR, 19)] = 'Josefstag'\n",
            "\n",
            "        # Nfelser Fahrt (first Thursday in April but not in Holy Week)\n",
            "        if self.prov == 'GL' and year >= 1835:\n",
            "            if ((date(year, APR, 1) + rd(weekday=FR)) !=\n",
            "                    (easter(year) - rd(days=2))):\n",
            "                self[date(year, APR, 1) + rd(weekday=TH)] = 'Nfelser Fahrt'\n",
            "            else:\n",
            "                self[date(year, APR, 8) + rd(weekday=TH)] = 'Nfelser Fahrt'\n",
            "\n",
            "        # it's a Holiday on a Sunday\n",
            "        self[easter(year)] = 'Ostern'\n",
            "\n",
            "        # VS don't have easter\n",
            "        if self.prov != 'VS':\n",
            "            self[easter(year) - rd(days=2)] = 'Karfreitag'\n",
            "            self[easter(year) + rd(weekday=MO)] = 'Ostermontag'\n",
            "\n",
            "        if self.prov in ('BL', 'BS', 'JU', 'NE', 'SH', 'SO', 'TG', 'TI',\n",
            "                         'ZH'):\n",
            "            self[date(year, MAY, 1)] = 'Tag der Arbeit'\n",
            "\n",
            "        self[easter(year) + rd(days=39)] = 'Auffahrt'\n",
            "\n",
            "        # it's a Holiday on a Sunday\n",
            "        self[easter(year) + rd(days=49)] = 'Pfingsten'\n",
            "\n",
            "        self[easter(year) + rd(days=50)] = 'Pfingstmontag'\n",
            "\n",
            "        if self.prov in ('AI', 'JU', 'LU', 'NW', 'OW', 'SZ', 'TI', 'UR',\n",
            "                         'VS', 'ZG'):\n",
            "            self[easter(year) + rd(days=60)] = 'Fronleichnam'\n",
            "\n",
            "        if self.prov == 'JU':\n",
            "            self[date(year, JUN, 23)] = 'Fest der Unabhngigkeit'\n",
            "\n",
            "        if self.prov == 'TI':\n",
            "            self[date(year, JUN, 29)] = 'Peter und Paul'\n",
            "\n",
            "        if year >= 1291:\n",
            "            self[date(year, AUG, 1)] = 'Nationalfeiertag'\n",
            "\n",
            "        if self.prov in ('AI', 'JU', 'LU', 'NW', 'OW', 'SZ', 'TI', 'UR',\n",
            "                         'VS', 'ZG'):\n",
            "            self[date(year, AUG, 15)] = 'Maria Himmelfahrt'\n",
            "\n",
            "        if self.prov == 'VD':\n",
            "            # Monday after the third Sunday of September\n",
            "            dt = date(year, SEP, 1) + rd(weekday=SU(+3)) + rd(weekday=MO)\n",
            "            self[dt] = 'Lundi du Jene'\n",
            "\n",
            "        if self.prov == 'OW':\n",
            "            self[date(year, SEP, 25)] = 'Bruder Klaus'\n",
            "\n",
            "        if self.prov in ('AI', 'GL', 'JU', 'LU', 'NW', 'OW', 'SG', 'SZ',\n",
            "                         'TI', 'UR', 'VS', 'ZG'):\n",
            "            self[date(year, NOV, 1)] = 'Allerheiligen'\n",
            "\n",
            "        if self.prov in ('AI', 'LU', 'NW', 'OW', 'SZ', 'TI', 'UR', 'VS',\n",
            "                         'ZG'):\n",
            "            self[date(year, DEC, 8)] = 'Maria Empfngnis'\n",
            "\n",
            "        if self.prov == 'GE':\n",
            "            self[date(year, DEC, 12)] = 'Escalade de Genve'\n",
            "\n",
            "        self[date(year, DEC, 25)] = 'Weihnachten'\n",
            "\n",
            "        if self.prov in ('AG', 'AR', 'AI', 'BL', 'BS', 'BE', 'FR', 'GL',\n",
            "                         'GR', 'LU', 'NE', 'NW', 'OW', 'SG', 'SH', 'SZ',\n",
            "                         'SO', 'TG', 'TI', 'UR', 'ZG', 'ZH'):\n",
            "            self[date(year, DEC, 26)] = 'Stephanstag'\n",
            "\n",
            "        if self.prov == 'GE':\n",
            "            self[date(year, DEC, 31)] = 'Wiederherstellung der Republik'\n",
            "\n",
            "\n",
            "class CH(Switzerland):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Honduras(HolidayBase):\n",
            "    # https://www.timeanddate.com/holidays/honduras/\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"HND\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        if self.observed and date(year, JAN, 1):\n",
            "            self[date(year, JAN, 1)] = \"Ao Nuevo [New Year's Day]\"\n",
            "\n",
            "        # The Three Wise Men Day\n",
            "        if self.observed and date(year, JAN, 6):\n",
            "            name = \"Da de los Reyes Magos [The Three Wise Men Day] (Observed)\"\n",
            "            self[date(year, JAN, 6)] = name\n",
            "\n",
            "        # The Three Wise Men Day\n",
            "        if self.observed and date(year, FEB, 3):\n",
            "            name = \"Da de la virgen de Suyapa [Our Lady of Suyapa] (Observed)\"\n",
            "            self[date(year, FEB, 3)] = name\n",
            "\n",
            "        # The Father's Day\n",
            "        if self.observed and date(year, MAR, 19):\n",
            "            name = \"Da del Padre [Father's Day] (Observed)\"\n",
            "            self[date(year, MAR, 19)] = name\n",
            "\n",
            "        # Maundy Thursday\n",
            "        self[easter(year) + rd(weekday=TH(-1))\n",
            "             ] = \"Jueves Santo [Maundy Thursday]\"\n",
            "\n",
            "        # Good Friday\n",
            "        self[easter(year) + rd(weekday=FR(-1))\n",
            "             ] = \"Viernes Santo [Good Friday]\"\n",
            "\n",
            "        # Holy Saturday\n",
            "        self[easter(year) + rd(weekday=SA(-1))\n",
            "             ] = \"Sbado de Gloria [Holy Saturday]\"\n",
            "\n",
            "        # Easter Sunday\n",
            "        self[easter(year) + rd(weekday=SU(-1))\n",
            "             ] = \"Domingo de Resurreccin [Easter Sunday]\"\n",
            "\n",
            "        # America Day\n",
            "        if self.observed and date(year, APR, 14):\n",
            "            self[date(year, APR, 14)] = \"Da de las Amricas [America Day]\"\n",
            "\n",
            "        # Labor Day\n",
            "        if self.observed and date(year, MAY, 1):\n",
            "            self[date(year, MAY, 1)] = \"Da del Trabajo [Labour Day]\"\n",
            "\n",
            "        # Mother's Day\n",
            "        may_first = date(int(year), 5, 1)\n",
            "        weekday_seq = may_first.weekday()\n",
            "        mom_day = (14 - weekday_seq)\n",
            "        if self.observed and date(year, MAY, mom_day):\n",
            "            str_day = \"Da de la madre [Mother's Day] (Observed)\"\n",
            "            self[date(year, MAY, mom_day)] = str_day\n",
            "\n",
            "        # Children's Day\n",
            "        if self.observed and date(year, SEP, 10):\n",
            "            name = \"Da del nio [Children day] (Observed)\"\n",
            "            self[date(year, SEP, 10)] = name\n",
            "\n",
            "        # Independence Day\n",
            "        if self.observed and date(year, SEP, 15):\n",
            "            name = \"Da de la Independencia [Independence Day]\"\n",
            "            self[date(year, SEP, 15)] = name\n",
            "\n",
            "        # Teacher's Day\n",
            "        if self.observed and date(year, SEP, 17):\n",
            "            name = \"Da del Maestro [Teacher's day] (Observed)\"\n",
            "            self[date(year, SEP, 17)] = name\n",
            "\n",
            "        # October Holidays are joined on 3 days starting at October 3 to 6.\n",
            "        # Some companies work medium day and take the rest on saturday.\n",
            "        # This holiday is variant and some companies work normally.\n",
            "        # If start day is weekend is ignored.\n",
            "        # The main objective of this is to increase the tourism.\n",
            "\n",
            "        # https://www.hondurastips.hn/2017/09/20/de-donde-nace-el-feriado-morazanico/\n",
            "\n",
            "        if year <= 2014:\n",
            "            # Morazan's Day\n",
            "            if self.observed and date(year, OCT, 3):\n",
            "                self[date(year, OCT, 3)] = \"Da de Morazn [Morazan's Day]\"\n",
            "\n",
            "            # Columbus Day\n",
            "            if self.observed and date(year, OCT, 12):\n",
            "                self[date(year, OCT, 12)] = \"Da de la Raza [Columbus Day]\"\n",
            "\n",
            "            # Amy Day\n",
            "            if self.observed and date(year, OCT, 21):\n",
            "                str_day = \"Da de las Fuerzas Armadas [Army Day]\"\n",
            "                self[date(year, OCT, 21)] = str_day\n",
            "        else:\n",
            "            # Morazan Weekend\n",
            "            if self.observed and date(year, OCT, 3):\n",
            "                name = \"Semana Moraznica [Morazan Weekend]\"\n",
            "                self[date(year, OCT, 3)] = name\n",
            "\n",
            "            # Morazan Weekend\n",
            "            if self.observed and date(year, OCT, 4):\n",
            "                name = \"Semana Moraznica [Morazan Weekend]\"\n",
            "                self[date(year, OCT, 4)] = name\n",
            "\n",
            "            # Morazan Weekend\n",
            "            if self.observed and date(year, OCT, 5):\n",
            "                name = \"Semana Moraznica [Morazan Weekend]\"\n",
            "                self[date(year, OCT, 5)] = name\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Navidad [Christmas]\"\n",
            "\n",
            "\n",
            "class HND(Honduras):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Hungary(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Hungary\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"HU\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New years\n",
            "        self[date(year, JAN, 1)] = \"jv\"\n",
            "\n",
            "        # National Day\n",
            "        self[date(year, MAR, 15)] = \"Nemzeti nnep\"\n",
            "\n",
            "        easter_date = easter(year)\n",
            "\n",
            "        # Good Friday\n",
            "        self[easter_date + rd(weekday=FR(-1))] = \"Nagypntek\"\n",
            "\n",
            "        # Easter\n",
            "        self[easter_date] = \"Hsvt\"\n",
            "\n",
            "        # Second easter day\n",
            "        self[easter_date + rd(days=1)] = \"Hsvt Htf\"\n",
            "\n",
            "        # Pentecost\n",
            "        self[easter_date + rd(days=49)] = \"Pnksd\"\n",
            "\n",
            "        # Pentecost monday\n",
            "        self[easter_date + rd(days=50)] = \"Pnksdhtf\"\n",
            "\n",
            "        # International Workers' Day\n",
            "        self[date(year, MAY, 1)] = \"A Munka nnepe\"\n",
            "\n",
            "        # State Foundation Day\n",
            "        self[date(year, AUG, 20)] = \"Az llamalapts nnepe\"\n",
            "\n",
            "        # National Day\n",
            "        self[date(year, OCT, 23)] = \"Nemzeti nnep\"\n",
            "\n",
            "        # All Saints' Day\n",
            "        self[date(year, NOV, 1)] = \"Mindenszentek\"\n",
            "\n",
            "        # First christmas\n",
            "        self[date(year, DEC, 25)] = \"Karcsony\"\n",
            "\n",
            "        # Second christmas\n",
            "        self[date(year, DEC, 26)] = \"Karcsony msnapja\"\n",
            "\n",
            "\n",
            "class HU(Hungary):\n",
            "    pass\n",
            "\n",
            "\n",
            "class India(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_India\n",
            "    # https://www.calendarlabs.com/holidays/india/\n",
            "    # https://slusi.dacnet.nic.in/watershedatlas/list_of_state_abbreviation.htm\n",
            "\n",
            "    PROVINCES = ['AS', 'CG', 'SK', 'KA', 'GJ', 'BR', 'RJ', 'OD',\n",
            "                 'TN', 'AP', 'WB', 'KL', 'HR', 'MH', 'MP', 'UP', 'UK']\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"IND\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New year\n",
            "        self[date(year, JAN, 1)] = \"New Year\"\n",
            "\n",
            "        # Pongal/ Makar Sankranti\n",
            "        self[date(year, JAN, 14)] = \"Makar Sankranti / Pongal\"\n",
            "\n",
            "        if year >= 1950:\n",
            "            # Republic Day\n",
            "            self[date(year, JAN, 26)] = \"Republic Day\"\n",
            "\n",
            "        if year >= 1947:\n",
            "            # Independence Day\n",
            "            self[date(year, AUG, 15)] = \"Independence Day\"\n",
            "\n",
            "        # Gandhi Jayanti\n",
            "        self[date(year, OCT, 2)] = \"Gandhi Jayanti\"\n",
            "\n",
            "        # Labour Day\n",
            "        self[date(year, MAY, 1)] = \"Labour Day\"\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Christmas\"\n",
            "\n",
            "        # GJ: Gujarat\n",
            "        if self.prov == \"GJ\":\n",
            "            self[date(year, JAN, 14)] = \"Uttarayan\"\n",
            "            self[date(year, MAY, 1)] = \"Gujarat Day\"\n",
            "            self[date(year, OCT, 31)] = \"Sardar Patel Jayanti\"\n",
            "\n",
            "        if self.prov == 'BR':\n",
            "            self[date(year, MAR, 22)] = \"Bihar Day\"\n",
            "\n",
            "        if self.prov == 'RJ':\n",
            "            self[date(year, MAR, 30)] = \"Rajasthan Day\"\n",
            "            self[date(year, JUN, 15)] = \"Maharana Pratap Jayanti\"\n",
            "\n",
            "        if self.prov == 'OD':\n",
            "            self[date(year, APR, 1)] = \"Odisha Day (Utkala Dibasa)\"\n",
            "            self[date(year, APR, 15)] = \"Maha Vishuva Sankranti / Pana\" \\\n",
            "                                        \" Sankranti\"\n",
            "\n",
            "        if self.prov in ('OD', 'AP', 'BR', 'WB', 'KL',\n",
            "                         'HR', 'MH', 'UP', 'UK', 'TN'):\n",
            "            self[date(year, APR, 14)] = \"Dr. B. R. Ambedkar's Jayanti\"\n",
            "\n",
            "        if self.prov == 'TN':\n",
            "            self[date(year, APR, 14)] = \"Puthandu (Tamil New Year)\"\n",
            "            self[date(year, APR, 15)] = \"Puthandu (Tamil New Year)\"\n",
            "\n",
            "        if self.prov == 'WB':\n",
            "            self[date(year, APR, 14)] = \"Pohela Boishakh\"\n",
            "            self[date(year, APR, 15)] = \"Pohela Boishakh\"\n",
            "            self[date(year, MAY, 9)] = \"Rabindra Jayanti\"\n",
            "\n",
            "        if self.prov == 'AS':\n",
            "            self[date(year, APR, 15)] = \"Bihu (Assamese New Year)\"\n",
            "\n",
            "        if self.prov == 'MH':\n",
            "            self[date(year, MAY, 1)] = \"Maharashtra Day\"\n",
            "\n",
            "        if self.prov == 'SK':\n",
            "            self[date(year, MAY, 16)] = \"Annexation Day\"\n",
            "\n",
            "        if self.prov == 'KA':\n",
            "            self[date(year, NOV, 1)] = \"Karnataka Rajyotsava\"\n",
            "\n",
            "        if self.prov == 'AP':\n",
            "            self[date(year, NOV, 1)] = \"Andhra Pradesh Foundation Day\"\n",
            "\n",
            "        if self.prov == 'HR':\n",
            "            self[date(year, NOV, 1)] = \"Haryana Foundation Day\"\n",
            "\n",
            "        if self.prov == 'MP':\n",
            "            self[date(year, NOV, 1)] = \"Madhya Pradesh Foundation Day\"\n",
            "\n",
            "        if self.prov == 'KL':\n",
            "            self[date(year, NOV, 1)] = \"Kerala Foundation Day\"\n",
            "\n",
            "        if self.prov == 'CG':\n",
            "            self[date(year, NOV, 1)] = \"Chhattisgarh Foundation Day\"\n",
            "\n",
            "\n",
            "class IND(India):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Croatia(HolidayBase):\n",
            "\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Croatia\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"HR\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New years\n",
            "        self[date(year, JAN, 1)] = \"Nova Godina\"\n",
            "        # Epiphany\n",
            "        self[date(year, JAN, 6)] = \"Sveta tri kralja\"\n",
            "        easter_date = easter(year)\n",
            "\n",
            "        # Easter\n",
            "        self[easter_date] = \"Uskrs\"\n",
            "        # Easter Monday\n",
            "        self[easter_date + rd(days=1)] = \"Uskrsni ponedjeljak\"\n",
            "\n",
            "        # Corpus Christi\n",
            "        self[easter_date + rd(days=60)] = \"Tijelovo\"\n",
            "\n",
            "        # International Workers' Day\n",
            "        self[date(year, MAY, 1)] = \"Meunarodni praznik rada\"\n",
            "\n",
            "        # Anti-fascist struggle day\n",
            "        self[date(year, JUN, 22)] = \"Dan antifaistike borbe\"\n",
            "\n",
            "        # Statehood day\n",
            "        self[date(year, JUN, 25)] = \"Dan dravnosti\"\n",
            "\n",
            "        # Victory and Homeland Thanksgiving Day\n",
            "        self[date(year, AUG, 5)] = \"Dan pobjede i domovinske zahvalnosti\"\n",
            "\n",
            "        # Assumption of Mary\n",
            "        self[date(year, AUG, 15)] = \"Velika Gospa\"\n",
            "\n",
            "        # Independence Day\n",
            "        self[date(year, OCT, 8)] = \"Dan neovisnosti\"\n",
            "\n",
            "        # All Saints' Day\n",
            "        self[date(year, NOV, 1)] = \"Svi sveti\"\n",
            "\n",
            "        # Christmas day\n",
            "        self[date(year, DEC, 25)] = \"Boi\"\n",
            "\n",
            "        # St. Stephen's day\n",
            "        self[date(year, DEC, 26)] = \"Sveti Stjepan\"\n",
            "\n",
            "\n",
            "class HR(Croatia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Luxembourg(HolidayBase):\n",
            "\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Luxembourg\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = 'LU'\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Public holidays\n",
            "        self[date(year, JAN, 1)] = \"Neijoerschdag\"\n",
            "        self[easter(year) + rd(weekday=MO)] = \"Ouschtermindeg\"\n",
            "        self[date(year, MAY, 1)] = \"Dag vun der Aarbecht\"\n",
            "        if year >= 2019:\n",
            "            # Europe Day: not in legislation yet, but introduced starting 2019\n",
            "            self[date(year, MAY, 9)] = \"Europadag\"\n",
            "        self[easter(year) + rd(days=39)] = \"Christi Himmelfaart\"\n",
            "        self[easter(year) + rd(days=50)] = \"Pngschtmindeg\"\n",
            "        self[date(year, JUN, 23)] = \"Nationalfeierdag\"\n",
            "        self[date(year, AUG, 15)] = \"Liffrawschdag\"\n",
            "        self[date(year, NOV, 1)] = \"Allerhellgen\"\n",
            "        self[date(year, DEC, 25)] = \"Chrschtdag\"\n",
            "        self[date(year, DEC, 26)] = \"Stiefesdag\"\n",
            "\n",
            "\n",
            "class LU(Luxembourg):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Russia(HolidayBase):\n",
            "    \"\"\"\n",
            "    https://en.wikipedia.org/wiki/Public_holidays_in_Russia\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"RU\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 2)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 3)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 4)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 5)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 6)] = \" \"\n",
            "        # Christmas Day (Orthodox)\n",
            "        self[date(year, JAN, 7)] = \" \"\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 8)] = \" \"\n",
            "        # Man Day\n",
            "        self[date(year, FEB, 23)] = \"  \"\n",
            "        # Women's Day\n",
            "        self[date(year, MAR, 8)] = \" \"\n",
            "        # Labour Day\n",
            "        self[date(year, MAY, 1)] = \"   \"\n",
            "        # Victory Day\n",
            "        self[date(year, MAY, 9)] = \" \"\n",
            "        # Russia's Day\n",
            "        self[date(year, JUN, 12)] = \" \"\n",
            "        # Unity Day\n",
            "        self[date(year, NOV, 4)] = \"  \"\n",
            "\n",
            "\n",
            "class RU(Russia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Lithuania(HolidayBase):\n",
            "\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Lithuania\n",
            "    # https://www.kalendorius.today/\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"LT\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        self[date(year, 1, 1)] = \"Naujieji metai\"\n",
            "\n",
            "        # Day of Restoration of the State of Lithuania (1918)\n",
            "        if year >= 1918:\n",
            "            self[date(year, 2, 16)] = \"Lietuvos valstybs \" \\\n",
            "                                      \"atkrimo diena\"\n",
            "\n",
            "        # Day of Restoration of Independence of Lithuania\n",
            "        # (from the Soviet Union, 1990)\n",
            "        if year >= 1990:\n",
            "            self[date(year, 3, 11)] = \"Lietuvos nepriklausomybs \" \\\n",
            "                                      \"atkrimo diena\"\n",
            "\n",
            "        # Easter\n",
            "        easter_date = easter(year)\n",
            "        self[easter_date] = \"Velykos\"\n",
            "\n",
            "        # Easter 2nd day\n",
            "        self[easter_date + rd(days=1)] = \"Velyk antroji diena\"\n",
            "\n",
            "        # International Workers' Day\n",
            "        self[date(year, 5, 1)] = \"Tarptautin darbo diena\"\n",
            "\n",
            "        # Mother's day. First Sunday in May\n",
            "        self[date(year, 5, 1) + rd(weekday=SU)] = \"Motinos diena\"\n",
            "\n",
            "        # Fathers's day. First Sunday in June\n",
            "        self[date(year, 6, 1) + rd(weekday=SU)] = \"Tvo diena\"\n",
            "\n",
            "        # St. John's Day [Christian name],\n",
            "        # Day of Dew [original pagan name]\n",
            "        if year >= 2003:\n",
            "            self[date(year, 6, 24)] = \"Jonins, Rasos\"\n",
            "\n",
            "        # Statehood Day\n",
            "        if year >= 1991:\n",
            "            self[date(year, 7, 6)] = \"Valstybs (Lietuvos \" \\\n",
            "                                     \"karaliaus Mindaugo \" \\\n",
            "                                     \"karnavimo) diena\"\n",
            "\n",
            "        # Assumption Day\n",
            "        self[date(year, 8, 15)] = \"olin (v. Mergels \" \\\n",
            "                                  \"Marijos mimo  dang diena)\"\n",
            "\n",
            "        # All Saints' Day\n",
            "        self[date(year, 11, 1)] = \"Vis ventj diena (Vlins)\"\n",
            "\n",
            "        # Christmas Eve\n",
            "        self[date(year, 12, 24)] = \"v. Kios\"\n",
            "\n",
            "        # Christmas 1st day\n",
            "        self[date(year, 12, 25)] = \"v. Kald pirma diena\"\n",
            "\n",
            "        # Christmas 2nd day\n",
            "        self[date(year, 12, 26)] = \"v. Kald antra diena\"\n",
            "\n",
            "\n",
            "class LT(Lithuania):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Estonia(HolidayBase):\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"EE\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        e = easter(year)\n",
            "\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \"uusaasta\"\n",
            "\n",
            "        # Independence Day, anniversary of the Republic of Estonia\n",
            "        self[date(year, FEB, 24)] = \"iseseisvuspev\"\n",
            "\n",
            "        # Good Friday\n",
            "        self[e - rd(days=2)] = \"suur reede\"\n",
            "\n",
            "        # Easter Sunday\n",
            "        self[e] = \"lestusmisphade 1. pha\"\n",
            "\n",
            "        # Spring Day\n",
            "        self[date(year, MAY, 1)] = \"kevadpha\"\n",
            "\n",
            "        # Pentecost\n",
            "        self[e + rd(days=49)] = \"neliphade 1. pha\"\n",
            "\n",
            "        # Victory Day\n",
            "        self[date(year, JUN, 23)] = \"vidupha\"\n",
            "\n",
            "        # Midsummer Day\n",
            "        self[date(year, JUN, 24)] = \"jaanipev\"\n",
            "\n",
            "        # Day of Restoration of Independence\n",
            "        self[date(year, AUG, 20)] = \"taasiseseisvumispev\"\n",
            "\n",
            "        # Christmas Eve\n",
            "        self[date(year, DEC, 24)] = \"jululaupev\"\n",
            "\n",
            "        # Christmas Day\n",
            "        self[date(year, DEC, 25)] = \"esimene julupha\"\n",
            "\n",
            "        # Boxing Day\n",
            "        self[date(year, DEC, 26)] = \"teine julupha\"\n",
            "\n",
            "\n",
            "class EE(Estonia):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Iceland(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Iceland\n",
            "    # https://www.officeholidays.com/countries/iceland/index.php\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"IS\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Public holidays\n",
            "        self[date(year, JAN, 1)] = \"Nrsdagur\"\n",
            "        self[easter(year) - rd(days=3)] = \"Skrdagur\"\n",
            "        self[easter(year)] = \"Pskadagur\"\n",
            "        self[easter(year) + rd(days=1)] = \"Annar  pskum\"\n",
            "        self[date(year, APR, 19) + rd(weekday=TH(+1))] = \\\n",
            "            \"Sumardagurinn fyrsti\"\n",
            "        self[date(year, MAY, 1)] = \"Verkalsdagurinn\"\n",
            "        self[easter(year) + rd(days=39)] = \"Uppstigningardagur\"\n",
            "        self[easter(year) + rd(days=49)] = \"Hvtasunnudagur\"\n",
            "        self[easter(year) + rd(days=50)] = \"Annar  hvtasunnu\"\n",
            "        self[date(year, JUN, 17)] = \"jhtardagurinn\"\n",
            "        # First Monday of August\n",
            "        self[date(year, AUG, 1) + rd(weekday=MO(+1))] = \\\n",
            "            \"Frdagur verslunarmanna\"\n",
            "        self[date(year, DEC, 24)] = \"Afangadagur\"\n",
            "        self[date(year, DEC, 25)] = \"Jladagur\"\n",
            "        self[date(year, DEC, 26)] = \"Annar  jlum\"\n",
            "        self[date(year, DEC, 31)] = \"Gamlrsdagur\"\n",
            "\n",
            "\n",
            "class IS(Iceland):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Kenya(HolidayBase):\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Kenya\n",
            "    # http://kenyaembassyberlin.de/Public-Holidays-in-Kenya.48.0.html\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"KE\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # Public holidays\n",
            "        self[date(year, JAN, 1)] = \"New Year's Day\"\n",
            "        self[date(year, MAY, 1)] = \"Labour Day\"\n",
            "        self[date(year, JUN, 1)] = \"Madaraka Day\"\n",
            "        self[date(year, OCT, 20)] = \"Mashujaa Day\"\n",
            "        self[date(year, DEC, 12)] = \"Jamhuri (Independence) Day\"\n",
            "        self[date(year, DEC, 25)] = \"Christmas Day\"\n",
            "        self[date(year, DEC, 26)] = \"Boxing Day\"\n",
            "        for k, v in list(self.items()):\n",
            "            if self.observed and k.weekday() == SUN:\n",
            "                self[k + rd(days=1)] = v + \" (Observed)\"\n",
            "\n",
            "        self[easter(year) - rd(weekday=FR(-1))] = \"Good Friday\"\n",
            "        self[easter(year) + rd(weekday=MO(+1))] = \"Easter Monday\"\n",
            "\n",
            "\n",
            "class KE(Kenya):\n",
            "    pass\n",
            "\n",
            "\n",
            "class HongKong(HolidayBase):\n",
            "\n",
            "    # https://www.gov.hk/en/about/abouthk/holiday/2020.htm\n",
            "    # https://en.wikipedia.org/wiki/Public_holidays_in_Hong_Kong\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"HK\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "\n",
            "        day_following = \"The day following \"\n",
            "\n",
            "        # The first day of January\n",
            "        name = \"The first day of January\"\n",
            "        first_date = date(year, JAN, 1)\n",
            "        if self.observed:\n",
            "            if first_date.weekday() == SUN:\n",
            "                self[first_date + rd(days=+1)] = day_following + \\\n",
            "                    self.first_lower(name)\n",
            "                first_date = first_date + rd(days=+1)\n",
            "            else:\n",
            "                self[first_date] = name\n",
            "        else:\n",
            "            self[first_date] = name\n",
            "\n",
            "        # Lunar New Year\n",
            "        name = \"Lunar New Year's Day\"\n",
            "        preceding_day_lunar = \"The day preceding Lunar New Year's Day\"\n",
            "        second_day_lunar = \"The second day of Lunar New Year\"\n",
            "        third_day_lunar = \"The third day of Lunar New Year\"\n",
            "        fourth_day_lunar = \"The fourth day of Lunar New Year\"\n",
            "        dt = self.get_solar_date(year, 1, 1)\n",
            "        new_year_date = date(dt.year, dt.month, dt.day)\n",
            "        if self.observed:\n",
            "            self[new_year_date] = name\n",
            "            if new_year_date.weekday() in [MON, TUE, WED, THU]:\n",
            "                self[new_year_date] = name\n",
            "                self[new_year_date + rd(days=+1)] = second_day_lunar\n",
            "                self[new_year_date + rd(days=+2)] = third_day_lunar\n",
            "            elif new_year_date.weekday() == FRI:\n",
            "                self[new_year_date] = name\n",
            "                self[new_year_date + rd(days=+1)] = second_day_lunar\n",
            "                self[new_year_date + rd(days=+3)] = fourth_day_lunar\n",
            "            elif new_year_date.weekday() == SAT:\n",
            "                self[new_year_date] = name\n",
            "                self[new_year_date + rd(days=+2)] = third_day_lunar\n",
            "                self[new_year_date + rd(days=+3)] = fourth_day_lunar\n",
            "            elif new_year_date.weekday() == SUN:\n",
            "                if year in [2006, 2007, 2010]:\n",
            "                    self[new_year_date + rd(days=-1)] = preceding_day_lunar\n",
            "                    self[new_year_date + rd(days=+1)] = second_day_lunar\n",
            "                    self[new_year_date + rd(days=+2)] = third_day_lunar\n",
            "                else:\n",
            "                    self[new_year_date + rd(days=+1)] = second_day_lunar\n",
            "                    self[new_year_date + rd(days=+2)] = third_day_lunar\n",
            "                    self[new_year_date + rd(days=+3)] = fourth_day_lunar\n",
            "        else:\n",
            "            self[new_year_date] = name\n",
            "            self[new_year_date + rd(days=+1)] = second_day_lunar\n",
            "            self[new_year_date + rd(days=+2)] = third_day_lunar\n",
            "\n",
            "        # Ching Ming Festival\n",
            "        name = \"Ching Ming Festival\"\n",
            "        if self.isLeapYear(year) or (self.isLeapYear(year-1) and year > 2008):\n",
            "            ching_ming_date = date(year, APR, 4)\n",
            "        else:\n",
            "            ching_ming_date = date(year, APR, 5)\n",
            "        if self.observed:\n",
            "            if ching_ming_date.weekday() == SUN:\n",
            "                self[ching_ming_date + rd(days=+1)] = day_following + name\n",
            "                ching_ming_date = ching_ming_date + rd(days=+1)\n",
            "            else:\n",
            "                self[ching_ming_date] = name\n",
            "        else:\n",
            "            self[ching_ming_date] = name\n",
            "\n",
            "        # Easter Holiday\n",
            "        good_friday = \"Good Friday\"\n",
            "        easter_monday = \"Easter Monday\"\n",
            "        if self.observed:\n",
            "            self[easter(year) + rd(weekday=FR(-1))] = good_friday\n",
            "            self[easter(year) + rd(weekday=SA(-1))] = day_following + \\\n",
            "                good_friday\n",
            "            if ching_ming_date == easter(year) + rd(weekday=MO):\n",
            "                self[easter(year) + rd(weekday=MO) + rd(days=+1)] = \\\n",
            "                    day_following + easter_monday\n",
            "            else:\n",
            "                self[easter(year) + rd(weekday=MO)] = easter_monday\n",
            "        else:\n",
            "            self[easter(year) + rd(weekday=FR(-1))] = good_friday\n",
            "            self[easter(year) + rd(weekday=SA(-1))] = day_following + \\\n",
            "                good_friday\n",
            "            self[easter(year) + rd(weekday=MO)] = easter_monday\n",
            "\n",
            "        # Birthday of the Buddha\n",
            "        name = \"Birthday of the Buddha\"\n",
            "        dt = self.get_solar_date(year, 4, 8)\n",
            "        buddha_date = date(dt.year, dt.month, dt.day)\n",
            "        if self.observed:\n",
            "            if buddha_date.weekday() == SUN:\n",
            "                self[buddha_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[buddha_date] = name\n",
            "        else:\n",
            "            self[buddha_date] = name\n",
            "\n",
            "        # Labour Day\n",
            "        name = \"Labour Day\"\n",
            "        labour_date = date(year, MAY, 1)\n",
            "        if self.observed:\n",
            "            if labour_date.weekday() == SUN:\n",
            "                self[labour_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[labour_date] = name\n",
            "        else:\n",
            "            self[labour_date] = name\n",
            "\n",
            "        # Tuen Ng Festival\n",
            "        name = \"Tuen Ng Festival\"\n",
            "        dt = self.get_solar_date(year, 5, 5)\n",
            "        tuen_ng_date = date(dt.year, dt.month, dt.day)\n",
            "        if self.observed:\n",
            "            if tuen_ng_date.weekday() == SUN:\n",
            "                self[tuen_ng_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[tuen_ng_date] = name\n",
            "        else:\n",
            "            self[tuen_ng_date] = name\n",
            "\n",
            "        # Hong Kong Special Administrative Region Establishment Day\n",
            "        name = \"Hong Kong Special Administrative Region Establishment Day\"\n",
            "        hksar_date = date(year, JUL, 1)\n",
            "        if self.observed:\n",
            "            if hksar_date.weekday() == SUN:\n",
            "                self[hksar_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[hksar_date] = name\n",
            "        else:\n",
            "            self[hksar_date] = name\n",
            "\n",
            "        # Special holiday on 2015 - The 70thanniversary day of the victory\n",
            "        # of the Chinese people's war of resistance against Japanese aggression\n",
            "        name = \"The 70th anniversary day of the victory of the Chinese \" + \\\n",
            "            \"people's war of resistance against Japanese aggression\"\n",
            "        if year == 2015:\n",
            "            self[date(year, SEP, 3)] = name\n",
            "\n",
            "        # Chinese Mid-Autumn Festival\n",
            "        name = \"Chinese Mid-Autumn Festival\"\n",
            "        dt = self.get_solar_date(year, 8, 15)\n",
            "        mid_autumn_date = date(dt.year, dt.month, dt.day)\n",
            "        if self.observed:\n",
            "            if mid_autumn_date.weekday() == SAT:\n",
            "                self[mid_autumn_date] = name\n",
            "            else:\n",
            "                self[mid_autumn_date + rd(days=+1)] = day_following + \\\n",
            "                    \"the \" + name\n",
            "            mid_autumn_date = mid_autumn_date + rd(days=+1)\n",
            "        else:\n",
            "            self[mid_autumn_date] = name\n",
            "\n",
            "        # National Day\n",
            "        name = \"National Day\"\n",
            "        national_date = date(year, OCT, 1)\n",
            "        if self.observed:\n",
            "            if (national_date.weekday() == SUN or\n",
            "                    national_date == mid_autumn_date):\n",
            "                self[national_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[national_date] = name\n",
            "        else:\n",
            "            self[national_date] = name\n",
            "\n",
            "        # Chung Yeung Festival\n",
            "        name = \"Chung Yeung Festival\"\n",
            "        dt = self.get_solar_date(year, 9, 9)\n",
            "        chung_yeung_date = date(dt.year, dt.month, dt.day)\n",
            "        if self.observed:\n",
            "            if chung_yeung_date.weekday() == SUN:\n",
            "                self[chung_yeung_date + rd(days=+1)] = day_following + name\n",
            "            else:\n",
            "                self[chung_yeung_date] = name\n",
            "        else:\n",
            "            self[chung_yeung_date] = name\n",
            "\n",
            "        # Christmas Day\n",
            "        name = \"Christmas Day\"\n",
            "        first_after_christmas = \"The first weekday after \" + name\n",
            "        second_after_christmas = \"The second weekday after \" + name\n",
            "        christmas_date = date(year, DEC, 25)\n",
            "        if self.observed:\n",
            "            if christmas_date.weekday() == SUN:\n",
            "                self[christmas_date] = name\n",
            "                self[christmas_date + rd(days=+1)] = first_after_christmas\n",
            "                self[christmas_date + rd(days=+2)] = second_after_christmas\n",
            "            elif christmas_date.weekday() == SAT:\n",
            "                self[christmas_date] = name\n",
            "                self[christmas_date + rd(days=+2)] = first_after_christmas\n",
            "            else:\n",
            "                self[christmas_date] = name\n",
            "                self[christmas_date + rd(days=+1)] = first_after_christmas\n",
            "        else:\n",
            "            self[christmas_date] = name\n",
            "            self[christmas_date + rd(days=+1)] = day_following + name\n",
            "\n",
            "    def isLeapYear(self, year):\n",
            "        if year % 4 != 0:\n",
            "            return False\n",
            "        elif year % 100 != 0:\n",
            "            return True\n",
            "        elif year % 400 != 0:\n",
            "            return False\n",
            "        else:\n",
            "            return True\n",
            "\n",
            "    def first_lower(self, s):\n",
            "        return s[0].lower() + s[1:]\n",
            "\n",
            "    # Store the number of days per year from 1901 to 2099, and the number of\n",
            "    # days from the 1st to the 13th to store the monthly (including the month\n",
            "    # of the month), 1 means that the month is 30 days. 0 means the month is\n",
            "    # 29 days. The 12th to 15th digits indicate the month of the next month.\n",
            "    # If it is 0x0F, it means that there is no leap month.\n",
            "    g_lunar_month_days = [\n",
            "        0xF0EA4, 0xF1D4A, 0x52C94, 0xF0C96, 0xF1536,\n",
            "        0x42AAC, 0xF0AD4, 0xF16B2, 0x22EA4, 0xF0EA4,  # 1901-1910\n",
            "        0x6364A, 0xF164A, 0xF1496, 0x52956, 0xF055A,\n",
            "        0xF0AD6, 0x216D2, 0xF1B52, 0x73B24, 0xF1D24,  # 1911-1920\n",
            "        0xF1A4A, 0x5349A, 0xF14AC, 0xF056C, 0x42B6A,\n",
            "        0xF0DA8, 0xF1D52, 0x23D24, 0xF1D24, 0x61A4C,  # 1921-1930\n",
            "        0xF0A56, 0xF14AE, 0x5256C, 0xF16B4, 0xF0DA8,\n",
            "        0x31D92, 0xF0E92, 0x72D26, 0xF1526, 0xF0A56,  # 1931-1940\n",
            "        0x614B6, 0xF155A, 0xF0AD4, 0x436AA, 0xF1748,\n",
            "        0xF1692, 0x23526, 0xF152A, 0x72A5A, 0xF0A6C,  # 1941-1950\n",
            "        0xF155A, 0x52B54, 0xF0B64, 0xF1B4A, 0x33A94,\n",
            "        0xF1A94, 0x8152A, 0xF152E, 0xF0AAC, 0x6156A,  # 1951-1960\n",
            "        0xF15AA, 0xF0DA4, 0x41D4A, 0xF1D4A, 0xF0C94,\n",
            "        0x3192E, 0xF1536, 0x72AB4, 0xF0AD4, 0xF16D2,  # 1961-1970\n",
            "        0x52EA4, 0xF16A4, 0xF164A, 0x42C96, 0xF1496,\n",
            "        0x82956, 0xF055A, 0xF0ADA, 0x616D2, 0xF1B52,  # 1971-1980\n",
            "        0xF1B24, 0x43A4A, 0xF1A4A, 0xA349A, 0xF14AC,\n",
            "        0xF056C, 0x60B6A, 0xF0DAA, 0xF1D92, 0x53D24,  # 1981-1990\n",
            "        0xF1D24, 0xF1A4C, 0x314AC, 0xF14AE, 0x829AC,\n",
            "        0xF06B4, 0xF0DAA, 0x52D92, 0xF0E92, 0xF0D26,  # 1991-2000\n",
            "        0x42A56, 0xF0A56, 0xF14B6, 0x22AB4, 0xF0AD4,\n",
            "        0x736AA, 0xF1748, 0xF1692, 0x53526, 0xF152A,  # 2001-2010\n",
            "        0xF0A5A, 0x4155A, 0xF156A, 0x92B54, 0xF0BA4,\n",
            "        0xF1B4A, 0x63A94, 0xF1A94, 0xF192A, 0x42A5C,  # 2011-2020\n",
            "        0xF0AAC, 0xF156A, 0x22B64, 0xF0DA4, 0x61D52,\n",
            "        0xF0E4A, 0xF0C96, 0x5192E, 0xF1956, 0xF0AB4,  # 2021-2030\n",
            "        0x315AC, 0xF16D2, 0xB2EA4, 0xF16A4, 0xF164A,\n",
            "        0x63496, 0xF1496, 0xF0956, 0x50AB6, 0xF0B5A,  # 2031-2040\n",
            "        0xF16D4, 0x236A4, 0xF1B24, 0x73A4A, 0xF1A4A,\n",
            "        0xF14AA, 0x5295A, 0xF096C, 0xF0B6A, 0x31B54,  # 2041-2050\n",
            "        0xF1D92, 0x83D24, 0xF1D24, 0xF1A4C, 0x614AC,\n",
            "        0xF14AE, 0xF09AC, 0x40DAA, 0xF0EAA, 0xF0E92,  # 2051-2060\n",
            "        0x31D26, 0xF0D26, 0x72A56, 0xF0A56, 0xF14B6,\n",
            "        0x52AB4, 0xF0AD4, 0xF16CA, 0x42E94, 0xF1694,  # 2061-2070\n",
            "        0x8352A, 0xF152A, 0xF0A5A, 0x6155A, 0xF156A,\n",
            "        0xF0B54, 0x4174A, 0xF1B4A, 0xF1A94, 0x3392A,  # 2071-2080\n",
            "        0xF192C, 0x7329C, 0xF0AAC, 0xF156A, 0x52B64,\n",
            "        0xF0DA4, 0xF1D4A, 0x41C94, 0xF0C96, 0x8192E,  # 2081-2090\n",
            "        0xF0956, 0xF0AB6, 0x615AC, 0xF16D4, 0xF0EA4,\n",
            "        0x42E4A, 0xF164A, 0xF1516, 0x22936,           # 2090-2099\n",
            "    ]\n",
            "    # Define range of years\n",
            "    START_YEAR, END_YEAR = 1901, 1900 + len(g_lunar_month_days)\n",
            "    # 1901 The 1st day of the 1st month of the Gregorian calendar is 1901/2/19\n",
            "    LUNAR_START_DATE, SOLAR_START_DATE = (1901, 1, 1), datetime(1901, 2, 19)\n",
            "    # The Gregorian date for December 30, 2099 is 2100/2/8\n",
            "    LUNAR_END_DATE, SOLAR_END_DATE = (2099, 12, 30), datetime(2100, 2, 18)\n",
            "\n",
            "    def get_leap_month(self, lunar_year):\n",
            "        return (self.g_lunar_month_days[lunar_year-self.START_YEAR] >> 16) \\\n",
            "            & 0x0F\n",
            "\n",
            "    def lunar_month_days(self, lunar_year, lunar_month):\n",
            "        return 29 + ((self.g_lunar_month_days[lunar_year-self.START_YEAR] >>\n",
            "                      lunar_month) & 0x01)\n",
            "\n",
            "    def lunar_year_days(self, year):\n",
            "        days = 0\n",
            "        months_day = self.g_lunar_month_days[year - self.START_YEAR]\n",
            "        for i in range(1, 13 if self.get_leap_month(year) == 0x0F else 14):\n",
            "            day = 29 + ((months_day >> i) & 0x01)\n",
            "            days += day\n",
            "        return days\n",
            "\n",
            "    # Calculate the Gregorian date according to the lunar calendar\n",
            "    def get_solar_date(self, year, month, day):\n",
            "        span_days = 0\n",
            "        for y in range(self.START_YEAR, year):\n",
            "            span_days += self.lunar_year_days(y)\n",
            "        leap_month = self.get_leap_month(year)\n",
            "        for m in range(1, month + (month > leap_month)):\n",
            "            span_days += self.lunar_month_days(year, m)\n",
            "        span_days += day - 1\n",
            "        return self.SOLAR_START_DATE + timedelta(span_days)\n",
            "\n",
            "\n",
            "class HK(HongKong):\n",
            "    pass\n",
            "\n",
            "\n",
            "class Peru(HolidayBase):\n",
            "    # https://www.gob.pe/feriados\n",
            "    # https://es.wikipedia.org/wiki/Anexo:Das_feriados_en_el_Per\n",
            "    def __init__(self, **kwargs):\n",
            "        self.country = \"PE\"\n",
            "        HolidayBase.__init__(self, **kwargs)\n",
            "\n",
            "    def _populate(self, year):\n",
            "        # New Year's Day\n",
            "        self[date(year, JAN, 1)] = \"Ao Nuevo [New Year's Day]\"\n",
            "\n",
            "        # Feast of Saints Peter and Paul\n",
            "        name = \"San Pedro y San Pablo [Feast of Saints Peter and Paul]\"\n",
            "        self[date(year, JUN, 29)] = name\n",
            "\n",
            "        # Independence Day\n",
            "        name = \"Da de la Independencia [Independence Day]\"\n",
            "        self[date(year, JUL, 28)] = name\n",
            "\n",
            "        name = \"Da de las Fuerzas Armadas y la Polica del Per\"\n",
            "        self[date(year, JUL, 29)] = name\n",
            "\n",
            "        # Santa Rosa de Lima\n",
            "        name = \"Da de Santa Rosa de Lima\"\n",
            "        self[date(year, AUG, 30)] = name\n",
            "\n",
            "        # Battle of Angamos\n",
            "        name = \"Combate Naval de Angamos [Battle of Angamos]\"\n",
            "        self[date(year, OCT, 8)] = name\n",
            "\n",
            "        # Holy Thursday\n",
            "        self[easter(year) + rd(weekday=TH(-1))\n",
            "             ] = \"Jueves Santo [Maundy Thursday]\"\n",
            "\n",
            "        # Good Friday\n",
            "        self[easter(year) + rd(weekday=FR(-1))\n",
            "             ] = \"Viernes Santo [Good Friday]\"\n",
            "\n",
            "        # Holy Saturday\n",
            "        self[easter(year) + rd(weekday=SA(-1))\n",
            "             ] = \"Sbado de Gloria [Holy Saturday]\"\n",
            "\n",
            "        # Easter Sunday\n",
            "        self[easter(year) + rd(weekday=SU(-1))\n",
            "             ] = \"Domingo de Resurreccin [Easter Sunday]\"\n",
            "\n",
            "        # Labor Day\n",
            "        self[date(year, MAY, 1)] = \"Da del Trabajo [Labour Day]\"\n",
            "\n",
            "        # All Saints Day\n",
            "        name = \"Da de Todos Los Santos [All Saints Day]\"\n",
            "        self[date(year, NOV, 1)] = name\n",
            "\n",
            "        # Inmaculada Concepcin\n",
            "        name = \"Inmaculada Concepcin [Immaculate Conception]\"\n",
            "        self[date(year, DEC, 8)] = name\n",
            "\n",
            "        # Christmas\n",
            "        self[date(year, DEC, 25)] = \"Navidad [Christmas]\"\n",
            "\n",
            "\n",
            "class PE(Peru):\n",
            "    pass\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#!/usr/bin/python3\n",
            "#\n",
            "# Copyright 2007 Google Inc. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "#\n",
            "\"\"\"Pure python code for finding unused ports on a host.\n",
            "\n",
            "This module provides a pick_unused_port() function.\n",
            "It can also be called via the command line for use in shell scripts.\n",
            "When called from the command line, it takes one optional argument, which,\n",
            "if given, is sent to portserver instead of portpicker's PID.\n",
            "To reserve a port for the lifetime of a bash script, use $BASHPID as this\n",
            "argument.\n",
            "\n",
            "There is a race condition between picking a port and your application code\n",
            "binding to it.  The use of a port server to prevent that is recommended on\n",
            "loaded test hosts running many tests at a time.\n",
            "\n",
            "If your code can accept a bound socket as input rather than being handed a\n",
            "port number consider using socket.bind(('localhost', 0)) to bind to an\n",
            "available port without a race condition rather than using this library.\n",
            "\n",
            "Typical usage:\n",
            "  test_port = portpicker.pick_unused_port()\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import print_function\n",
            "\n",
            "import logging\n",
            "import os\n",
            "import random\n",
            "import socket\n",
            "import sys\n",
            "\n",
            "# The legacy Bind, IsPortFree, etc. names are not exported.\n",
            "__all__ = ('bind', 'is_port_free', 'pick_unused_port', 'return_port',\n",
            "           'add_reserved_port', 'get_port_from_port_server')\n",
            "\n",
            "_PROTOS = [(socket.SOCK_STREAM, socket.IPPROTO_TCP),\n",
            "           (socket.SOCK_DGRAM, socket.IPPROTO_UDP)]\n",
            "\n",
            "\n",
            "# Ports that are currently available to be given out.\n",
            "_free_ports = set()\n",
            "\n",
            "# Ports that are reserved or from the portserver that may be returned.\n",
            "_owned_ports = set()\n",
            "\n",
            "# Ports that we chose randomly that may be returned.\n",
            "_random_ports = set()\n",
            "\n",
            "\n",
            "def add_reserved_port(port):\n",
            "    \"\"\"Add a port that was acquired by means other than the port server.\"\"\"\n",
            "    _free_ports.add(port)\n",
            "\n",
            "\n",
            "def return_port(port):\n",
            "    \"\"\"Return a port that is no longer being used so it can be reused.\"\"\"\n",
            "    if port in _random_ports:\n",
            "        _random_ports.remove(port)\n",
            "    elif port in _owned_ports:\n",
            "        _owned_ports.remove(port)\n",
            "        _free_ports.add(port)\n",
            "    elif port in _free_ports:\n",
            "        logging.info(\"Returning a port that was already returned: %s\", port)\n",
            "    else:\n",
            "        logging.info(\"Returning a port that wasn't given by portpicker: %s\",\n",
            "                     port)\n",
            "\n",
            "\n",
            "def bind(port, socket_type, socket_proto):\n",
            "    \"\"\"Try to bind to a socket of the specified type, protocol, and port.\n",
            "\n",
            "    This is primarily a helper function for PickUnusedPort, used to see\n",
            "    if a particular port number is available.\n",
            "\n",
            "    For the port to be considered available, the kernel must support at least\n",
            "    one of (IPv6, IPv4), and the port must be available on each supported\n",
            "    family.\n",
            "\n",
            "    Args:\n",
            "      port: The port number to bind to, or 0 to have the OS pick a free port.\n",
            "      socket_type: The type of the socket (ex: socket.SOCK_STREAM).\n",
            "      socket_proto: The protocol of the socket (ex: socket.IPPROTO_TCP).\n",
            "\n",
            "    Returns:\n",
            "      The port number on success or None on failure.\n",
            "    \"\"\"\n",
            "    got_socket = False\n",
            "    for family in (socket.AF_INET6, socket.AF_INET):\n",
            "        try:\n",
            "            sock = socket.socket(family, socket_type, socket_proto)\n",
            "            got_socket = True\n",
            "        except socket.error:\n",
            "            continue\n",
            "        try:\n",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
            "            sock.bind(('', port))\n",
            "            if socket_type == socket.SOCK_STREAM:\n",
            "                sock.listen(1)\n",
            "            port = sock.getsockname()[1]\n",
            "        except socket.error:\n",
            "            return None\n",
            "        finally:\n",
            "            sock.close()\n",
            "    return port if got_socket else None\n",
            "\n",
            "Bind = bind  # legacy API. pylint: disable=invalid-name\n",
            "\n",
            "\n",
            "def is_port_free(port):\n",
            "    \"\"\"Check if specified port is free.\n",
            "\n",
            "    Args:\n",
            "      port: integer, port to check\n",
            "    Returns:\n",
            "      boolean, whether it is free to use for both TCP and UDP\n",
            "    \"\"\"\n",
            "    return bind(port, *_PROTOS[0]) and bind(port, *_PROTOS[1])\n",
            "\n",
            "IsPortFree = is_port_free  # legacy API. pylint: disable=invalid-name\n",
            "\n",
            "\n",
            "def pick_unused_port(pid=None):\n",
            "    \"\"\"A pure python implementation of PickUnusedPort.\n",
            "\n",
            "    Args:\n",
            "      pid: PID to tell the portserver to associate the reservation with. If\n",
            "        None,\n",
            "        the current process's PID is used.\n",
            "\n",
            "    Returns:\n",
            "      A port number that is unused on both TCP and UDP.\n",
            "    \"\"\"\n",
            "    if _free_ports:\n",
            "        port = _free_ports.pop()\n",
            "        _owned_ports.add(port)\n",
            "        return port\n",
            "    # Provide access to the portserver on an opt-in basis.\n",
            "    if 'PORTSERVER_ADDRESS' in os.environ:\n",
            "        port = get_port_from_port_server(os.environ['PORTSERVER_ADDRESS'],\n",
            "                                         pid=pid)\n",
            "        if port:\n",
            "            return port\n",
            "    return _pick_unused_port_without_server()\n",
            "\n",
            "PickUnusedPort = pick_unused_port  # legacy API. pylint: disable=invalid-name\n",
            "\n",
            "\n",
            "def _pick_unused_port_without_server():  # Protected. pylint: disable=invalid-name\n",
            "    \"\"\"Pick an available network port without the help of a port server.\n",
            "\n",
            "    This code ensures that the port is available on both TCP and UDP.\n",
            "\n",
            "    This function is an implementation detail of PickUnusedPort(), and\n",
            "    should not be called by code outside of this module.\n",
            "\n",
            "    Returns:\n",
            "      A port number that is unused on both TCP and UDP.  None on error.\n",
            "    \"\"\"\n",
            "    # Try random ports first.\n",
            "    rng = random.Random()\n",
            "    for _ in range(10):\n",
            "        port = int(rng.randrange(15000, 25000))\n",
            "        if is_port_free(port):\n",
            "            _random_ports.add(port)\n",
            "            return port\n",
            "\n",
            "    # Try OS-assigned ports next.\n",
            "    # Ambrose discovered that on the 2.6 kernel, calling Bind() on UDP socket\n",
            "    # returns the same port over and over. So always try TCP first.\n",
            "    while True:\n",
            "        # Ask the OS for an unused port.\n",
            "        port = bind(0, _PROTOS[0][0], _PROTOS[0][1])\n",
            "        # Check if this port is unused on the other protocol.\n",
            "        if port and bind(port, _PROTOS[1][0], _PROTOS[1][1]):\n",
            "            _random_ports.add(port)\n",
            "            return port\n",
            "\n",
            "\n",
            "def get_port_from_port_server(portserver_address, pid=None):\n",
            "    \"\"\"Request a free a port from a system-wide portserver.\n",
            "\n",
            "    This follows a very simple portserver protocol:\n",
            "    The request consists of our pid (in ASCII) followed by a newline.\n",
            "    The response is a port number and a newline, 0 on failure.\n",
            "\n",
            "    This function is an implementation detail of pick_unused_port().\n",
            "    It should not normally be called by code outside of this module.\n",
            "\n",
            "    Args:\n",
            "      portserver_address: The address (path) of a unix domain socket\n",
            "        with which to connect to the portserver.  A leading '@'\n",
            "        character indicates an address in the \"abstract namespace.\"\n",
            "      pid: The PID to tell the portserver to associate the reservation with.\n",
            "        If None, the current process's PID is used.\n",
            "\n",
            "    Returns:\n",
            "      The port number on success or None on failure.\n",
            "    \"\"\"\n",
            "    if not portserver_address:\n",
            "        return None\n",
            "    # An AF_UNIX address may start with a zero byte, in which case it is in the\n",
            "    # \"abstract namespace\", and doesn't have any filesystem representation.\n",
            "    # See 'man 7 unix' for details.\n",
            "    # The convention is to write '@' in the address to represent this zero byte.\n",
            "    if portserver_address[0] == '@':\n",
            "        portserver_address = '\\0' + portserver_address[1:]\n",
            "\n",
            "    if pid is None:\n",
            "        pid = os.getpid()\n",
            "\n",
            "    try:\n",
            "        # Create socket.\n",
            "        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n",
            "        try:\n",
            "            # Connect to portserver.\n",
            "            sock.connect(portserver_address)\n",
            "\n",
            "            # Write request.\n",
            "            sock.sendall(('%d\\n' % pid).encode('ascii'))\n",
            "\n",
            "            # Read response.\n",
            "            # 1K should be ample buffer space.\n",
            "            buf = sock.recv(1024)\n",
            "        finally:\n",
            "            sock.close()\n",
            "    except socket.error as e:\n",
            "        print('Socket error when connecting to portserver:', e,\n",
            "              file=sys.stderr)\n",
            "        return None\n",
            "\n",
            "    try:\n",
            "        port = int(buf.split(b'\\n')[0])\n",
            "    except ValueError:\n",
            "        print('Portserver failed to find a port.', file=sys.stderr)\n",
            "        return None\n",
            "    _owned_ports.add(port)\n",
            "    return port\n",
            "\n",
            "\n",
            "GetPortFromPortServer = get_port_from_port_server  # legacy API. pylint: disable=invalid-name\n",
            "\n",
            "\n",
            "def main(argv):\n",
            "    \"\"\"If passed an arg, treat it as a PID, otherwise portpicker uses getpid.\"\"\"\n",
            "    port = pick_unused_port(pid=int(argv[1]) if len(argv) > 1 else None)\n",
            "    if not port:\n",
            "        sys.exit(1)\n",
            "    print(port)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main(sys.argv)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Entry point for launching an IPython kernel.\n",
            "\n",
            "This is separate from the ipykernel package so we can avoid doing imports until\n",
            "after removing the cwd from sys.path.\n",
            "\"\"\"\n",
            "\n",
            "import sys\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    # Remove the CWD from sys.path while we load stuff.\n",
            "    # This is added back by InteractiveShellApp.init_path()\n",
            "    if sys.path[0] == '':\n",
            "        del sys.path[0]\n",
            "\n",
            "    from ipykernel import kernelapp as app\n",
            "    app.launch_new_instance()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "from __future__ import division, print_function\n",
            "\n",
            "__all__ = [\"PGM\", \"Node\", \"Edge\", \"Plate\"]\n",
            "__version__ = \"0.0.4\"\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "from matplotlib.patches import Ellipse\n",
            "from matplotlib.patches import FancyArrow\n",
            "from matplotlib.patches import Rectangle as Rectangle\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "\n",
            "class PGM(object):\n",
            "    \"\"\"\n",
            "    The base object for building a graphical model representation.\n",
            "\n",
            "    :param shape:\n",
            "        The number of rows and columns in the grid.\n",
            "\n",
            "    :param origin:\n",
            "        The coordinates of the bottom left corner of the plot.\n",
            "\n",
            "    :param grid_unit: (optional)\n",
            "        The size of the grid spacing measured in centimeters.\n",
            "\n",
            "    :param node_unit: (optional)\n",
            "        The base unit for the node size. This is a number in centimeters that\n",
            "        sets the default diameter of the nodes.\n",
            "\n",
            "    :param observed_style: (optional)\n",
            "        How should the \"observed\" nodes be indicated? This must be one of:\n",
            "        ``\"shaded\"``, ``\"inner\"`` or ``\"outer\"`` where ``inner`` and\n",
            "        ``outer`` nodes are shown as double circles with the second circle\n",
            "        plotted inside or outside of the standard one, respectively.\n",
            "\n",
            "    :param node_ec: (optional)\n",
            "        The default edge color for the nodes.\n",
            "\n",
            "    :param directed: (optional)\n",
            "        Should the edges be directed by default?\n",
            "\n",
            "    :param aspect: (optional)\n",
            "        The default aspect ratio for the nodes.\n",
            "\n",
            "    :param label_params: (optional)\n",
            "        Default node label parameters.\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, shape, origin=[0, 0],\n",
            "                 grid_unit=2, node_unit=1,\n",
            "                 observed_style=\"shaded\",\n",
            "                 line_width=1, node_ec=\"k\",\n",
            "                 directed=True, aspect=1.0,\n",
            "                 label_params={}):\n",
            "        self._nodes = {}\n",
            "        self._edges = []\n",
            "        self._plates = []\n",
            "\n",
            "        self._ctx = _rendering_context(shape=shape, origin=origin,\n",
            "                                       grid_unit=grid_unit,\n",
            "                                       node_unit=node_unit,\n",
            "                                       observed_style=observed_style,\n",
            "                                       line_width=line_width,\n",
            "                                       node_ec=node_ec, directed=directed,\n",
            "                                       aspect=aspect,\n",
            "                                       label_params=label_params)\n",
            "\n",
            "    def add_node(self, node):\n",
            "        \"\"\"\n",
            "        Add a :class:`Node` to the model.\n",
            "\n",
            "        :param node:\n",
            "            The :class:`Node` instance to add.\n",
            "\n",
            "        \"\"\"\n",
            "        self._nodes[node.name] = node\n",
            "        return node\n",
            "\n",
            "    def add_edge(self, name1, name2, directed=None, **kwargs):\n",
            "        \"\"\"\n",
            "        Construct an :class:`Edge` between two named :class:`Node` objects.\n",
            "\n",
            "        :param name1:\n",
            "            The name identifying the first node.\n",
            "\n",
            "        :param name2:\n",
            "            The name identifying the second node. If the edge is directed,\n",
            "            the arrow will point to this node.\n",
            "\n",
            "        :param directed: (optional)\n",
            "            Should this be a directed edge?\n",
            "\n",
            "        \"\"\"\n",
            "        if directed is None:\n",
            "            directed = self._ctx.directed\n",
            "\n",
            "        e = Edge(self._nodes[name1], self._nodes[name2], directed=directed,\n",
            "                 plot_params=kwargs)\n",
            "        self._edges.append(e)\n",
            "\n",
            "        return e\n",
            "\n",
            "    def add_plate(self, plate):\n",
            "        \"\"\"\n",
            "        Add a :class:`Plate` object to the model.\n",
            "\n",
            "        \"\"\"\n",
            "        self._plates.append(plate)\n",
            "        return None\n",
            "\n",
            "    def render(self):\n",
            "        \"\"\"\n",
            "        Render the :class:`Plate`, :class:`Edge` and :class:`Node` objects in\n",
            "        the model. This will create a new figure with the correct dimensions\n",
            "        and plot the model in this area.\n",
            "\n",
            "        \"\"\"\n",
            "        self.figure = self._ctx.figure()\n",
            "        self.ax = self._ctx.ax()\n",
            "\n",
            "        for plate in self._plates:\n",
            "            plate.render(self._ctx)\n",
            "\n",
            "        for edge in self._edges:\n",
            "            edge.render(self._ctx)\n",
            "\n",
            "        for name in self._nodes:\n",
            "            self._nodes[name].render(self._ctx)\n",
            "\n",
            "        return self.ax\n",
            "\n",
            "\n",
            "class Node(object):\n",
            "    \"\"\"\n",
            "    The representation of a random variable in a :class:`PGM`.\n",
            "\n",
            "    :param name:\n",
            "        The plain-text identifier for the node.\n",
            "\n",
            "    :param content:\n",
            "        The display form of the variable.\n",
            "\n",
            "    :param x:\n",
            "        The x-coordinate of the node in *model units*.\n",
            "\n",
            "    :param y:\n",
            "        The y-coordinate of the node.\n",
            "\n",
            "    :param scale: (optional)\n",
            "        The diameter (or height) of the node measured in multiples of\n",
            "        ``node_unit`` as defined by the :class:`PGM` object.\n",
            "\n",
            "    :param aspect: (optional)\n",
            "        The aspect ratio width/height for elliptical nodes; default 1.\n",
            "\n",
            "    :param observed: (optional)\n",
            "        Should this be a conditioned variable?\n",
            "\n",
            "    :param fixed: (optional)\n",
            "        Should this be a fixed (not permitted to vary) variable?\n",
            "        If `True`, modifies or over-rides ``diameter``, ``offset``,\n",
            "        ``facecolor``, and a few other ``plot_params`` settings.\n",
            "        This setting conflicts with ``observed``.\n",
            "\n",
            "    :param offset: (optional)\n",
            "        The ``(dx, dy)`` offset of the label (in points) from the default\n",
            "        centered position.\n",
            "\n",
            "    :param plot_params: (optional)\n",
            "        A dictionary of parameters to pass to the\n",
            "        :class:`matplotlib.patches.Ellipse` constructor.\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, name, content, x, y, scale=1, aspect=None,\n",
            "                 observed=False, fixed=False,\n",
            "                 offset=[0, 0], plot_params={}, label_params=None):\n",
            "        # Node style.\n",
            "        assert not (observed and fixed), \\\n",
            "            \"A node cannot be both 'observed' and 'fixed'.\"\n",
            "        self.observed = observed\n",
            "        self.fixed = fixed\n",
            "\n",
            "        # Metadata.\n",
            "        self.name = name\n",
            "        self.content = content\n",
            "\n",
            "        # Coordinates and dimensions.\n",
            "        self.x, self.y = x, y\n",
            "        self.scale = scale\n",
            "        if self.fixed:\n",
            "            self.scale /= 6.0\n",
            "        self.aspect = aspect\n",
            "\n",
            "        # Display parameters.\n",
            "        self.plot_params = dict(plot_params)\n",
            "\n",
            "        # Text parameters.\n",
            "        self.offset = list(offset)\n",
            "        if label_params is not None:\n",
            "            self.label_params = dict(label_params)\n",
            "        else:\n",
            "            self.label_params = None\n",
            "\n",
            "    def render(self, ctx):\n",
            "        \"\"\"\n",
            "        Render the node.\n",
            "\n",
            "        :param ctx:\n",
            "            The :class:`_rendering_context` object.\n",
            "\n",
            "        \"\"\"\n",
            "        # Get the axes and default plotting parameters from the rendering\n",
            "        # context.\n",
            "        ax = ctx.ax()\n",
            "\n",
            "        # Resolve the plotting parameters.\n",
            "        p = dict(self.plot_params)\n",
            "        p[\"lw\"] = _pop_multiple(p, ctx.line_width, \"lw\", \"linewidth\")\n",
            "\n",
            "        p[\"ec\"] = p[\"edgecolor\"] = _pop_multiple(p, ctx.node_ec,\n",
            "                                                 \"ec\", \"edgecolor\")\n",
            "\n",
            "        p[\"fc\"] = _pop_multiple(p, \"none\", \"fc\", \"facecolor\")\n",
            "        fc = p[\"fc\"]\n",
            "\n",
            "        p[\"alpha\"] = p.get(\"alpha\", 1)\n",
            "\n",
            "        # And the label parameters.\n",
            "        if self.label_params is None:\n",
            "            l = dict(ctx.label_params)\n",
            "        else:\n",
            "            l = dict(self.label_params)\n",
            "\n",
            "        l[\"va\"] = _pop_multiple(l, \"center\", \"va\", \"verticalalignment\")\n",
            "        l[\"ha\"] = _pop_multiple(l, \"center\", \"ha\", \"horizontalalignment\")\n",
            "\n",
            "        # Deal with ``fixed`` nodes.\n",
            "        scale = self.scale\n",
            "        if self.fixed:\n",
            "            # MAGIC: These magic numbers should depend on the grid/node units.\n",
            "            self.offset[1] += 6\n",
            "\n",
            "            l[\"va\"] = \"baseline\"\n",
            "            l.pop(\"verticalalignment\", None)\n",
            "            l.pop(\"ma\", None)\n",
            "\n",
            "            if p[\"fc\"] == \"none\":\n",
            "                p[\"fc\"] = \"k\"\n",
            "\n",
            "        diameter = ctx.node_unit * scale\n",
            "        if self.aspect is not None:\n",
            "            aspect = self.aspect\n",
            "        else:\n",
            "            aspect = ctx.aspect\n",
            "\n",
            "        # Set up an observed node. Note the fc INSANITY.\n",
            "        if self.observed:\n",
            "            # Update the plotting parameters depending on the style of\n",
            "            # observed node.\n",
            "            h = float(diameter)\n",
            "            w = aspect * float(diameter)\n",
            "            if ctx.observed_style == \"shaded\":\n",
            "                p[\"fc\"] = \"0.7\"\n",
            "            elif ctx.observed_style == \"outer\":\n",
            "                h = diameter + 0.1 * diameter\n",
            "                w = aspect * diameter + 0.1 * diameter\n",
            "            elif ctx.observed_style == \"inner\":\n",
            "                h = diameter - 0.1 * diameter\n",
            "                w = aspect * diameter - 0.1 * diameter\n",
            "                p[\"fc\"] = fc\n",
            "\n",
            "            # Draw the background ellipse.\n",
            "            bg = Ellipse(xy=ctx.convert(self.x, self.y),\n",
            "                         width=w, height=h, **p)\n",
            "            ax.add_artist(bg)\n",
            "\n",
            "            # Reset the face color.\n",
            "            p[\"fc\"] = fc\n",
            "\n",
            "        # Draw the foreground ellipse.\n",
            "        if ctx.observed_style == \"inner\" and not self.fixed:\n",
            "            p[\"fc\"] = \"none\"\n",
            "        el = Ellipse(xy=ctx.convert(self.x, self.y),\n",
            "                     width=diameter * aspect, height=diameter, **p)\n",
            "        ax.add_artist(el)\n",
            "\n",
            "        # Reset the face color.\n",
            "        p[\"fc\"] = fc\n",
            "\n",
            "        # Annotate the node.\n",
            "        ax.annotate(self.content, ctx.convert(self.x, self.y),\n",
            "                    xycoords=\"data\",\n",
            "                    xytext=self.offset, textcoords=\"offset points\",\n",
            "                    **l)\n",
            "\n",
            "        return el\n",
            "\n",
            "\n",
            "class Edge(object):\n",
            "    \"\"\"\n",
            "    An edge between two :class:`Node` objects.\n",
            "\n",
            "    :param node1:\n",
            "        The first :class:`Node`.\n",
            "\n",
            "    :param node2:\n",
            "        The second :class:`Node`. The arrow will point towards this node.\n",
            "\n",
            "    :param directed: (optional)\n",
            "        Should the edge be directed from ``node1`` to ``node2``? In other\n",
            "        words: should it have an arrow?\n",
            "\n",
            "    :param plot_params: (optional)\n",
            "        A dictionary of parameters to pass to the plotting command when\n",
            "        rendering.\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, node1, node2, directed=True, plot_params={}):\n",
            "        self.node1 = node1\n",
            "        self.node2 = node2\n",
            "        self.directed = directed\n",
            "        self.plot_params = dict(plot_params)\n",
            "\n",
            "    def _get_coords(self, ctx):\n",
            "        \"\"\"\n",
            "        Get the coordinates of the line.\n",
            "\n",
            "        :param conv:\n",
            "            A callable coordinate conversion.\n",
            "\n",
            "        :returns:\n",
            "            * ``x0``, ``y0``: the coordinates of the start of the line.\n",
            "            * ``dx0``, ``dy0``: the displacement vector.\n",
            "\n",
            "        \"\"\"\n",
            "        # Scale the coordinates appropriately.\n",
            "        x1, y1 = ctx.convert(self.node1.x, self.node1.y)\n",
            "        x2, y2 = ctx.convert(self.node2.x, self.node2.y)\n",
            "\n",
            "        # Aspect ratios.\n",
            "        a1, a2 = self.node1.aspect, self.node2.aspect\n",
            "        if a1 is None:\n",
            "            a1 = ctx.aspect\n",
            "        if a2 is None:\n",
            "            a2 = ctx.aspect\n",
            "\n",
            "        # Compute the distances.\n",
            "        dx, dy = x2 - x1, y2 - y1\n",
            "        dist1 = np.sqrt(dy * dy + dx * dx / float(a1 ** 2))\n",
            "        dist2 = np.sqrt(dy * dy + dx * dx / float(a2 ** 2))\n",
            "\n",
            "        # Compute the fractional effect of the radii of the nodes.\n",
            "        alpha1 = 0.5 * ctx.node_unit * self.node1.scale / dist1\n",
            "        alpha2 = 0.5 * ctx.node_unit * self.node2.scale / dist2\n",
            "\n",
            "        # Get the coordinates of the starting position.\n",
            "        x0, y0 = x1 + alpha1 * dx, y1 + alpha1 * dy\n",
            "\n",
            "        # Get the width and height of the line.\n",
            "        dx0 = dx * (1. - alpha1 - alpha2)\n",
            "        dy0 = dy * (1. - alpha1 - alpha2)\n",
            "\n",
            "        return x0, y0, dx0, dy0\n",
            "\n",
            "    def render(self, ctx):\n",
            "        \"\"\"\n",
            "        Render the edge in the given axes.\n",
            "\n",
            "        :param ctx:\n",
            "            The :class:`_rendering_context` object.\n",
            "\n",
            "        \"\"\"\n",
            "        ax = ctx.ax()\n",
            "\n",
            "        p = self.plot_params\n",
            "        p[\"linewidth\"] = _pop_multiple(p, ctx.line_width,\n",
            "                                       \"lw\", \"linewidth\")\n",
            "\n",
            "        # Add edge annotation.\n",
            "        if \"label\" in self.plot_params:\n",
            "            x, y, dx, dy = self._get_coords(ctx)\n",
            "            ax.annotate(self.plot_params[\"label\"],\n",
            "                        [x + 0.5 * dx, y + 0.5 * dy], xycoords=\"data\",\n",
            "                        xytext=[0, 3], textcoords=\"offset points\",\n",
            "                        ha=\"center\", va=\"center\")\n",
            "\n",
            "        if self.directed:\n",
            "            p[\"ec\"] = _pop_multiple(p, \"k\", \"ec\", \"edgecolor\")\n",
            "            p[\"fc\"] = _pop_multiple(p, \"k\", \"fc\", \"facecolor\")\n",
            "            p[\"head_length\"] = p.get(\"head_length\", 0.25)\n",
            "            p[\"head_width\"] = p.get(\"head_width\", 0.1)\n",
            "\n",
            "            # Build an arrow.\n",
            "            ar = FancyArrow(*self._get_coords(ctx), width=0,\n",
            "                            length_includes_head=True,\n",
            "                            **p)\n",
            "\n",
            "            # Add the arrow to the axes.\n",
            "            ax.add_artist(ar)\n",
            "            return ar\n",
            "        else:\n",
            "            p[\"color\"] = p.get(\"color\", \"k\")\n",
            "\n",
            "            # Get the right coordinates.\n",
            "            x, y, dx, dy = self._get_coords(ctx)\n",
            "\n",
            "            # Plot the line.\n",
            "            line = ax.plot([x, x + dx], [y, y + dy], **p)\n",
            "            return line\n",
            "\n",
            "\n",
            "class Plate(object):\n",
            "    \"\"\"\n",
            "    A plate to encapsulate repeated independent processes in the model.\n",
            "\n",
            "    :param rect:\n",
            "        The rectangle describing the plate bounds in model coordinates.\n",
            "\n",
            "    :param label: (optional)\n",
            "        A string to annotate the plate.\n",
            "\n",
            "    :param label_offset: (optional)\n",
            "        The x and y offsets of the label text measured in points.\n",
            "\n",
            "    :param shift: (optional)\n",
            "        The vertical \"shift\" of the plate measured in model units. This will\n",
            "        move the bottom of the panel by ``shift`` units.\n",
            "\n",
            "    :param position: (optional)\n",
            "        One of ``\"bottom left\"`` or ``\"bottom right\"``.\n",
            "\n",
            "    :param rect_params: (optional)\n",
            "        A dictionary of parameters to pass to the\n",
            "        :class:`matplotlib.patches.Rectangle` constructor.\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, rect, label=None, label_offset=[5, 5], shift=0,\n",
            "                 position=\"bottom left\", rect_params={}, bbox={}):\n",
            "        self.rect = rect\n",
            "        self.label = label\n",
            "        self.label_offset = label_offset\n",
            "        self.shift = shift\n",
            "        self.rect_params = dict(rect_params)\n",
            "        self.bbox = dict(bbox)\n",
            "        self.position = position\n",
            "\n",
            "    def render(self, ctx):\n",
            "        \"\"\"\n",
            "        Render the plate in the given axes.\n",
            "\n",
            "        :param ctx:\n",
            "            The :class:`_rendering_context` object.\n",
            "\n",
            "        \"\"\"\n",
            "        ax = ctx.ax()\n",
            "\n",
            "        s = np.array([0, self.shift])\n",
            "        r = np.atleast_1d(self.rect)\n",
            "        bl = ctx.convert(*(r[:2] + s))\n",
            "        tr = ctx.convert(*(r[:2] + r[2:]))\n",
            "        r = np.concatenate([bl, tr - bl])\n",
            "\n",
            "        p = self.rect_params\n",
            "        p[\"ec\"] = _pop_multiple(p, \"k\", \"ec\", \"edgecolor\")\n",
            "        p[\"fc\"] = _pop_multiple(p, \"none\", \"fc\", \"facecolor\")\n",
            "        p[\"lw\"] = _pop_multiple(p, ctx.line_width, \"lw\", \"linewidth\")\n",
            "\n",
            "        rect = Rectangle(r[:2], *r[2:], **p)\n",
            "        ax.add_artist(rect)\n",
            "\n",
            "        if self.label is not None:\n",
            "            offset = np.array(self.label_offset)\n",
            "            if self.position == \"bottom left\":\n",
            "                pos = r[:2]\n",
            "                ha = \"left\"\n",
            "            elif self.position == \"bottom right\":\n",
            "                pos = r[:2]\n",
            "                pos[0] += r[2]\n",
            "                ha = \"right\"\n",
            "                offset[0] -= 2 * offset[0]\n",
            "            else:\n",
            "                raise RuntimeError(\"Unknown positioning string: {0}\"\n",
            "                                   .format(self.position))\n",
            "\n",
            "            ax.annotate(self.label, pos, xycoords=\"data\",\n",
            "                        xytext=offset, textcoords=\"offset points\",\n",
            "                        bbox=self.bbox,\n",
            "                        horizontalalignment=ha)\n",
            "\n",
            "        return rect\n",
            "\n",
            "\n",
            "class _rendering_context(object):\n",
            "    \"\"\"\n",
            "    :param shape:\n",
            "        The number of rows and columns in the grid.\n",
            "\n",
            "    :param origin:\n",
            "        The coordinates of the bottom left corner of the plot.\n",
            "\n",
            "    :param grid_unit:\n",
            "        The size of the grid spacing measured in centimeters.\n",
            "\n",
            "    :param node_unit:\n",
            "        The base unit for the node size. This is a number in centimeters that\n",
            "        sets the default diameter of the nodes.\n",
            "\n",
            "    :param observed_style:\n",
            "        How should the \"observed\" nodes be indicated? This must be one of:\n",
            "        ``\"shaded\"``, ``\"inner\"`` or ``\"outer\"`` where ``inner`` and\n",
            "        ``outer`` nodes are shown as double circles with the second circle\n",
            "        plotted inside or outside of the standard one, respectively.\n",
            "\n",
            "    :param node_ec:\n",
            "        The default edge color for the nodes.\n",
            "\n",
            "    :param directed:\n",
            "        Should the edges be directed by default?\n",
            "\n",
            "    :param aspect:\n",
            "        The default aspect ratio for the nodes.\n",
            "\n",
            "    :param label_params:\n",
            "        Default node label parameters.\n",
            "\n",
            "    \"\"\"\n",
            "    def __init__(self, **kwargs):\n",
            "        # Save the style defaults.\n",
            "        self.line_width = kwargs.get(\"line_width\", 1.0)\n",
            "\n",
            "        # Make sure that the observed node style is one that we recognize.\n",
            "        self.observed_style = kwargs.get(\"observed_style\", \"shaded\").lower()\n",
            "        styles = [\"shaded\", \"inner\", \"outer\"]\n",
            "        assert self.observed_style in styles, \\\n",
            "            \"Unrecognized observed node style: {0}\\n\".format(\n",
            "                self.observed_style) \\\n",
            "            + \"\\tOptions are: {0}\".format(\", \".join(styles))\n",
            "\n",
            "        # Set up the figure and grid dimensions.\n",
            "        self.shape = np.array(kwargs.get(\"shape\", [1, 1]))\n",
            "        self.origin = np.array(kwargs.get(\"origin\", [0, 0]))\n",
            "        self.grid_unit = kwargs.get(\"grid_unit\", 2.0)\n",
            "        self.figsize = self.grid_unit * self.shape / 2.54\n",
            "\n",
            "        self.node_unit = kwargs.get(\"node_unit\", 1.0)\n",
            "        self.node_ec = kwargs.get(\"node_ec\", \"k\")\n",
            "        self.directed = kwargs.get(\"directed\", True)\n",
            "        self.aspect = kwargs.get(\"aspect\", 1.0)\n",
            "        self.label_params = dict(kwargs.get(\"label_params\", {}))\n",
            "\n",
            "        # Initialize the figure to ``None`` to handle caching later.\n",
            "        self._figure = None\n",
            "        self._ax = None\n",
            "\n",
            "    def figure(self):\n",
            "        if self._figure is not None:\n",
            "            return self._figure\n",
            "        self._figure = plt.figure(figsize=self.figsize)\n",
            "        return self._figure\n",
            "\n",
            "    def ax(self):\n",
            "        if self._ax is not None:\n",
            "            return self._ax\n",
            "\n",
            "        # Add a new axis object if it doesn't exist.\n",
            "        self._ax = self.figure().add_axes((0, 0, 1, 1), frameon=False,\n",
            "                                          xticks=[], yticks=[])\n",
            "\n",
            "        # Set the bounds.\n",
            "        l0 = self.convert(*self.origin)\n",
            "        l1 = self.convert(*(self.origin + self.shape))\n",
            "        self._ax.set_xlim(l0[0], l1[0])\n",
            "        self._ax.set_ylim(l0[1], l1[1])\n",
            "\n",
            "        return self._ax\n",
            "\n",
            "    def convert(self, *xy):\n",
            "        \"\"\"\n",
            "        Convert from model coordinates to plot coordinates.\n",
            "\n",
            "        \"\"\"\n",
            "        assert len(xy) == 2\n",
            "        return self.grid_unit * (np.atleast_1d(xy) - self.origin)\n",
            "\n",
            "\n",
            "def _pop_multiple(d, default, *args):\n",
            "    \"\"\"\n",
            "    A helper function for dealing with the way that matplotlib annoyingly\n",
            "    allows multiple keyword arguments. For example, ``edgecolor`` and ``ec``\n",
            "    are generally equivalent but no exception is thrown if they are both\n",
            "    used.\n",
            "\n",
            "    *Note: This function does throw a :class:`ValueError` if more than one\n",
            "    of the equivalent arguments are provided.*\n",
            "\n",
            "    :param d:\n",
            "        A :class:`dict`-like object to \"pop\" from.\n",
            "\n",
            "    :param default:\n",
            "        The default value to return if none of the arguments are provided.\n",
            "\n",
            "    :param *args:\n",
            "        The arguments to try to retrieve.\n",
            "\n",
            "    \"\"\"\n",
            "    assert len(args) > 0, \"You must provide at least one argument to 'pop'.\"\n",
            "\n",
            "    results = []\n",
            "    for k in args:\n",
            "        try:\n",
            "            results.append((k, d.pop(k)))\n",
            "        except KeyError:\n",
            "            pass\n",
            "\n",
            "    if len(results) > 1:\n",
            "        raise TypeError(\"The arguments ({0}) are equivalent, you can only \"\n",
            "                        .format(\", \".join([k for k, v in results]))\n",
            "                        + \"provide one of them.\")\n",
            "\n",
            "    if len(results) == 0:\n",
            "        return default\n",
            "\n",
            "    return results[0][1]\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Pythonic command-line interface parser that will make you smile.\n",
            "\n",
            " * http://docopt.org\n",
            " * Repository and issue-tracker: https://github.com/docopt/docopt\n",
            " * Licensed under terms of MIT license (see LICENSE-MIT)\n",
            " * Copyright (c) 2013 Vladimir Keleshev, vladimir@keleshev.com\n",
            "\n",
            "\"\"\"\n",
            "import sys\n",
            "import re\n",
            "\n",
            "\n",
            "__all__ = ['docopt']\n",
            "__version__ = '0.6.2'\n",
            "\n",
            "\n",
            "class DocoptLanguageError(Exception):\n",
            "\n",
            "    \"\"\"Error in construction of usage-message by developer.\"\"\"\n",
            "\n",
            "\n",
            "class DocoptExit(SystemExit):\n",
            "\n",
            "    \"\"\"Exit in case user invoked program with incorrect arguments.\"\"\"\n",
            "\n",
            "    usage = ''\n",
            "\n",
            "    def __init__(self, message=''):\n",
            "        SystemExit.__init__(self, (message + '\\n' + self.usage).strip())\n",
            "\n",
            "\n",
            "class Pattern(object):\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        return repr(self) == repr(other)\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash(repr(self))\n",
            "\n",
            "    def fix(self):\n",
            "        self.fix_identities()\n",
            "        self.fix_repeating_arguments()\n",
            "        return self\n",
            "\n",
            "    def fix_identities(self, uniq=None):\n",
            "        \"\"\"Make pattern-tree tips point to same object if they are equal.\"\"\"\n",
            "        if not hasattr(self, 'children'):\n",
            "            return self\n",
            "        uniq = list(set(self.flat())) if uniq is None else uniq\n",
            "        for i, c in enumerate(self.children):\n",
            "            if not hasattr(c, 'children'):\n",
            "                assert c in uniq\n",
            "                self.children[i] = uniq[uniq.index(c)]\n",
            "            else:\n",
            "                c.fix_identities(uniq)\n",
            "\n",
            "    def fix_repeating_arguments(self):\n",
            "        \"\"\"Fix elements that should accumulate/increment values.\"\"\"\n",
            "        either = [list(c.children) for c in self.either.children]\n",
            "        for case in either:\n",
            "            for e in [c for c in case if case.count(c) > 1]:\n",
            "                if type(e) is Argument or type(e) is Option and e.argcount:\n",
            "                    if e.value is None:\n",
            "                        e.value = []\n",
            "                    elif type(e.value) is not list:\n",
            "                        e.value = e.value.split()\n",
            "                if type(e) is Command or type(e) is Option and e.argcount == 0:\n",
            "                    e.value = 0\n",
            "        return self\n",
            "\n",
            "    @property\n",
            "    def either(self):\n",
            "        \"\"\"Transform pattern into an equivalent, with only top-level Either.\"\"\"\n",
            "        # Currently the pattern will not be equivalent, but more \"narrow\",\n",
            "        # although good enough to reason about list arguments.\n",
            "        ret = []\n",
            "        groups = [[self]]\n",
            "        while groups:\n",
            "            children = groups.pop(0)\n",
            "            types = [type(c) for c in children]\n",
            "            if Either in types:\n",
            "                either = [c for c in children if type(c) is Either][0]\n",
            "                children.pop(children.index(either))\n",
            "                for c in either.children:\n",
            "                    groups.append([c] + children)\n",
            "            elif Required in types:\n",
            "                required = [c for c in children if type(c) is Required][0]\n",
            "                children.pop(children.index(required))\n",
            "                groups.append(list(required.children) + children)\n",
            "            elif Optional in types:\n",
            "                optional = [c for c in children if type(c) is Optional][0]\n",
            "                children.pop(children.index(optional))\n",
            "                groups.append(list(optional.children) + children)\n",
            "            elif AnyOptions in types:\n",
            "                optional = [c for c in children if type(c) is AnyOptions][0]\n",
            "                children.pop(children.index(optional))\n",
            "                groups.append(list(optional.children) + children)\n",
            "            elif OneOrMore in types:\n",
            "                oneormore = [c for c in children if type(c) is OneOrMore][0]\n",
            "                children.pop(children.index(oneormore))\n",
            "                groups.append(list(oneormore.children) * 2 + children)\n",
            "            else:\n",
            "                ret.append(children)\n",
            "        return Either(*[Required(*e) for e in ret])\n",
            "\n",
            "\n",
            "class ChildPattern(Pattern):\n",
            "\n",
            "    def __init__(self, name, value=None):\n",
            "        self.name = name\n",
            "        self.value = value\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '%s(%r, %r)' % (self.__class__.__name__, self.name, self.value)\n",
            "\n",
            "    def flat(self, *types):\n",
            "        return [self] if not types or type(self) in types else []\n",
            "\n",
            "    def match(self, left, collected=None):\n",
            "        collected = [] if collected is None else collected\n",
            "        pos, match = self.single_match(left)\n",
            "        if match is None:\n",
            "            return False, left, collected\n",
            "        left_ = left[:pos] + left[pos + 1:]\n",
            "        same_name = [a for a in collected if a.name == self.name]\n",
            "        if type(self.value) in (int, list):\n",
            "            if type(self.value) is int:\n",
            "                increment = 1\n",
            "            else:\n",
            "                increment = ([match.value] if type(match.value) is str\n",
            "                             else match.value)\n",
            "            if not same_name:\n",
            "                match.value = increment\n",
            "                return True, left_, collected + [match]\n",
            "            same_name[0].value += increment\n",
            "            return True, left_, collected\n",
            "        return True, left_, collected + [match]\n",
            "\n",
            "\n",
            "class ParentPattern(Pattern):\n",
            "\n",
            "    def __init__(self, *children):\n",
            "        self.children = list(children)\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '%s(%s)' % (self.__class__.__name__,\n",
            "                           ', '.join(repr(a) for a in self.children))\n",
            "\n",
            "    def flat(self, *types):\n",
            "        if type(self) in types:\n",
            "            return [self]\n",
            "        return sum([c.flat(*types) for c in self.children], [])\n",
            "\n",
            "\n",
            "class Argument(ChildPattern):\n",
            "\n",
            "    def single_match(self, left):\n",
            "        for n, p in enumerate(left):\n",
            "            if type(p) is Argument:\n",
            "                return n, Argument(self.name, p.value)\n",
            "        return None, None\n",
            "\n",
            "    @classmethod\n",
            "    def parse(class_, source):\n",
            "        name = re.findall('(<\\S*?>)', source)[0]\n",
            "        value = re.findall('\\[default: (.*)\\]', source, flags=re.I)\n",
            "        return class_(name, value[0] if value else None)\n",
            "\n",
            "\n",
            "class Command(Argument):\n",
            "\n",
            "    def __init__(self, name, value=False):\n",
            "        self.name = name\n",
            "        self.value = value\n",
            "\n",
            "    def single_match(self, left):\n",
            "        for n, p in enumerate(left):\n",
            "            if type(p) is Argument:\n",
            "                if p.value == self.name:\n",
            "                    return n, Command(self.name, True)\n",
            "                else:\n",
            "                    break\n",
            "        return None, None\n",
            "\n",
            "\n",
            "class Option(ChildPattern):\n",
            "\n",
            "    def __init__(self, short=None, long=None, argcount=0, value=False):\n",
            "        assert argcount in (0, 1)\n",
            "        self.short, self.long = short, long\n",
            "        self.argcount, self.value = argcount, value\n",
            "        self.value = None if value is False and argcount else value\n",
            "\n",
            "    @classmethod\n",
            "    def parse(class_, option_description):\n",
            "        short, long, argcount, value = None, None, 0, False\n",
            "        options, _, description = option_description.strip().partition('  ')\n",
            "        options = options.replace(',', ' ').replace('=', ' ')\n",
            "        for s in options.split():\n",
            "            if s.startswith('--'):\n",
            "                long = s\n",
            "            elif s.startswith('-'):\n",
            "                short = s\n",
            "            else:\n",
            "                argcount = 1\n",
            "        if argcount:\n",
            "            matched = re.findall('\\[default: (.*)\\]', description, flags=re.I)\n",
            "            value = matched[0] if matched else None\n",
            "        return class_(short, long, argcount, value)\n",
            "\n",
            "    def single_match(self, left):\n",
            "        for n, p in enumerate(left):\n",
            "            if self.name == p.name:\n",
            "                return n, p\n",
            "        return None, None\n",
            "\n",
            "    @property\n",
            "    def name(self):\n",
            "        return self.long or self.short\n",
            "\n",
            "    def __repr__(self):\n",
            "        return 'Option(%r, %r, %r, %r)' % (self.short, self.long,\n",
            "                                           self.argcount, self.value)\n",
            "\n",
            "\n",
            "class Required(ParentPattern):\n",
            "\n",
            "    def match(self, left, collected=None):\n",
            "        collected = [] if collected is None else collected\n",
            "        l = left\n",
            "        c = collected\n",
            "        for p in self.children:\n",
            "            matched, l, c = p.match(l, c)\n",
            "            if not matched:\n",
            "                return False, left, collected\n",
            "        return True, l, c\n",
            "\n",
            "\n",
            "class Optional(ParentPattern):\n",
            "\n",
            "    def match(self, left, collected=None):\n",
            "        collected = [] if collected is None else collected\n",
            "        for p in self.children:\n",
            "            m, left, collected = p.match(left, collected)\n",
            "        return True, left, collected\n",
            "\n",
            "\n",
            "class AnyOptions(Optional):\n",
            "\n",
            "    \"\"\"Marker/placeholder for [options] shortcut.\"\"\"\n",
            "\n",
            "\n",
            "class OneOrMore(ParentPattern):\n",
            "\n",
            "    def match(self, left, collected=None):\n",
            "        assert len(self.children) == 1\n",
            "        collected = [] if collected is None else collected\n",
            "        l = left\n",
            "        c = collected\n",
            "        l_ = None\n",
            "        matched = True\n",
            "        times = 0\n",
            "        while matched:\n",
            "            # could it be that something didn't match but changed l or c?\n",
            "            matched, l, c = self.children[0].match(l, c)\n",
            "            times += 1 if matched else 0\n",
            "            if l_ == l:\n",
            "                break\n",
            "            l_ = l\n",
            "        if times >= 1:\n",
            "            return True, l, c\n",
            "        return False, left, collected\n",
            "\n",
            "\n",
            "class Either(ParentPattern):\n",
            "\n",
            "    def match(self, left, collected=None):\n",
            "        collected = [] if collected is None else collected\n",
            "        outcomes = []\n",
            "        for p in self.children:\n",
            "            matched, _, _ = outcome = p.match(left, collected)\n",
            "            if matched:\n",
            "                outcomes.append(outcome)\n",
            "        if outcomes:\n",
            "            return min(outcomes, key=lambda outcome: len(outcome[1]))\n",
            "        return False, left, collected\n",
            "\n",
            "\n",
            "class TokenStream(list):\n",
            "\n",
            "    def __init__(self, source, error):\n",
            "        self += source.split() if hasattr(source, 'split') else source\n",
            "        self.error = error\n",
            "\n",
            "    def move(self):\n",
            "        return self.pop(0) if len(self) else None\n",
            "\n",
            "    def current(self):\n",
            "        return self[0] if len(self) else None\n",
            "\n",
            "\n",
            "def parse_long(tokens, options):\n",
            "    \"\"\"long ::= '--' chars [ ( ' ' | '=' ) chars ] ;\"\"\"\n",
            "    long, eq, value = tokens.move().partition('=')\n",
            "    assert long.startswith('--')\n",
            "    value = None if eq == value == '' else value\n",
            "    similar = [o for o in options if o.long == long]\n",
            "    if tokens.error is DocoptExit and similar == []:  # if no exact match\n",
            "        similar = [o for o in options if o.long and o.long.startswith(long)]\n",
            "    if len(similar) > 1:  # might be simply specified ambiguously 2+ times?\n",
            "        raise tokens.error('%s is not a unique prefix: %s?' %\n",
            "                           (long, ', '.join(o.long for o in similar)))\n",
            "    elif len(similar) < 1:\n",
            "        argcount = 1 if eq == '=' else 0\n",
            "        o = Option(None, long, argcount)\n",
            "        options.append(o)\n",
            "        if tokens.error is DocoptExit:\n",
            "            o = Option(None, long, argcount, value if argcount else True)\n",
            "    else:\n",
            "        o = Option(similar[0].short, similar[0].long,\n",
            "                   similar[0].argcount, similar[0].value)\n",
            "        if o.argcount == 0:\n",
            "            if value is not None:\n",
            "                raise tokens.error('%s must not have an argument' % o.long)\n",
            "        else:\n",
            "            if value is None:\n",
            "                if tokens.current() is None:\n",
            "                    raise tokens.error('%s requires argument' % o.long)\n",
            "                value = tokens.move()\n",
            "        if tokens.error is DocoptExit:\n",
            "            o.value = value if value is not None else True\n",
            "    return [o]\n",
            "\n",
            "\n",
            "def parse_shorts(tokens, options):\n",
            "    \"\"\"shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;\"\"\"\n",
            "    token = tokens.move()\n",
            "    assert token.startswith('-') and not token.startswith('--')\n",
            "    left = token.lstrip('-')\n",
            "    parsed = []\n",
            "    while left != '':\n",
            "        short, left = '-' + left[0], left[1:]\n",
            "        similar = [o for o in options if o.short == short]\n",
            "        if len(similar) > 1:\n",
            "            raise tokens.error('%s is specified ambiguously %d times' %\n",
            "                               (short, len(similar)))\n",
            "        elif len(similar) < 1:\n",
            "            o = Option(short, None, 0)\n",
            "            options.append(o)\n",
            "            if tokens.error is DocoptExit:\n",
            "                o = Option(short, None, 0, True)\n",
            "        else:  # why copying is necessary here?\n",
            "            o = Option(short, similar[0].long,\n",
            "                       similar[0].argcount, similar[0].value)\n",
            "            value = None\n",
            "            if o.argcount != 0:\n",
            "                if left == '':\n",
            "                    if tokens.current() is None:\n",
            "                        raise tokens.error('%s requires argument' % short)\n",
            "                    value = tokens.move()\n",
            "                else:\n",
            "                    value = left\n",
            "                    left = ''\n",
            "            if tokens.error is DocoptExit:\n",
            "                o.value = value if value is not None else True\n",
            "        parsed.append(o)\n",
            "    return parsed\n",
            "\n",
            "\n",
            "def parse_pattern(source, options):\n",
            "    tokens = TokenStream(re.sub(r'([\\[\\]\\(\\)\\|]|\\.\\.\\.)', r' \\1 ', source),\n",
            "                         DocoptLanguageError)\n",
            "    result = parse_expr(tokens, options)\n",
            "    if tokens.current() is not None:\n",
            "        raise tokens.error('unexpected ending: %r' % ' '.join(tokens))\n",
            "    return Required(*result)\n",
            "\n",
            "\n",
            "def parse_expr(tokens, options):\n",
            "    \"\"\"expr ::= seq ( '|' seq )* ;\"\"\"\n",
            "    seq = parse_seq(tokens, options)\n",
            "    if tokens.current() != '|':\n",
            "        return seq\n",
            "    result = [Required(*seq)] if len(seq) > 1 else seq\n",
            "    while tokens.current() == '|':\n",
            "        tokens.move()\n",
            "        seq = parse_seq(tokens, options)\n",
            "        result += [Required(*seq)] if len(seq) > 1 else seq\n",
            "    return [Either(*result)] if len(result) > 1 else result\n",
            "\n",
            "\n",
            "def parse_seq(tokens, options):\n",
            "    \"\"\"seq ::= ( atom [ '...' ] )* ;\"\"\"\n",
            "    result = []\n",
            "    while tokens.current() not in [None, ']', ')', '|']:\n",
            "        atom = parse_atom(tokens, options)\n",
            "        if tokens.current() == '...':\n",
            "            atom = [OneOrMore(*atom)]\n",
            "            tokens.move()\n",
            "        result += atom\n",
            "    return result\n",
            "\n",
            "\n",
            "def parse_atom(tokens, options):\n",
            "    \"\"\"atom ::= '(' expr ')' | '[' expr ']' | 'options'\n",
            "             | long | shorts | argument | command ;\n",
            "    \"\"\"\n",
            "    token = tokens.current()\n",
            "    result = []\n",
            "    if token in '([':\n",
            "        tokens.move()\n",
            "        matching, pattern = {'(': [')', Required], '[': [']', Optional]}[token]\n",
            "        result = pattern(*parse_expr(tokens, options))\n",
            "        if tokens.move() != matching:\n",
            "            raise tokens.error(\"unmatched '%s'\" % token)\n",
            "        return [result]\n",
            "    elif token == 'options':\n",
            "        tokens.move()\n",
            "        return [AnyOptions()]\n",
            "    elif token.startswith('--') and token != '--':\n",
            "        return parse_long(tokens, options)\n",
            "    elif token.startswith('-') and token not in ('-', '--'):\n",
            "        return parse_shorts(tokens, options)\n",
            "    elif token.startswith('<') and token.endswith('>') or token.isupper():\n",
            "        return [Argument(tokens.move())]\n",
            "    else:\n",
            "        return [Command(tokens.move())]\n",
            "\n",
            "\n",
            "def parse_argv(tokens, options, options_first=False):\n",
            "    \"\"\"Parse command-line argument vector.\n",
            "\n",
            "    If options_first:\n",
            "        argv ::= [ long | shorts ]* [ argument ]* [ '--' [ argument ]* ] ;\n",
            "    else:\n",
            "        argv ::= [ long | shorts | argument ]* [ '--' [ argument ]* ] ;\n",
            "\n",
            "    \"\"\"\n",
            "    parsed = []\n",
            "    while tokens.current() is not None:\n",
            "        if tokens.current() == '--':\n",
            "            return parsed + [Argument(None, v) for v in tokens]\n",
            "        elif tokens.current().startswith('--'):\n",
            "            parsed += parse_long(tokens, options)\n",
            "        elif tokens.current().startswith('-') and tokens.current() != '-':\n",
            "            parsed += parse_shorts(tokens, options)\n",
            "        elif options_first:\n",
            "            return parsed + [Argument(None, v) for v in tokens]\n",
            "        else:\n",
            "            parsed.append(Argument(None, tokens.move()))\n",
            "    return parsed\n",
            "\n",
            "\n",
            "def parse_defaults(doc):\n",
            "    # in python < 2.7 you can't pass flags=re.MULTILINE\n",
            "    split = re.split('\\n *(<\\S+?>|-\\S+?)', doc)[1:]\n",
            "    split = [s1 + s2 for s1, s2 in zip(split[::2], split[1::2])]\n",
            "    options = [Option.parse(s) for s in split if s.startswith('-')]\n",
            "    #arguments = [Argument.parse(s) for s in split if s.startswith('<')]\n",
            "    #return options, arguments\n",
            "    return options\n",
            "\n",
            "\n",
            "def printable_usage(doc):\n",
            "    # in python < 2.7 you can't pass flags=re.IGNORECASE\n",
            "    usage_split = re.split(r'([Uu][Ss][Aa][Gg][Ee]:)', doc)\n",
            "    if len(usage_split) < 3:\n",
            "        raise DocoptLanguageError('\"usage:\" (case-insensitive) not found.')\n",
            "    if len(usage_split) > 3:\n",
            "        raise DocoptLanguageError('More than one \"usage:\" (case-insensitive).')\n",
            "    return re.split(r'\\n\\s*\\n', ''.join(usage_split[1:]))[0].strip()\n",
            "\n",
            "\n",
            "def formal_usage(printable_usage):\n",
            "    pu = printable_usage.split()[1:]  # split and drop \"usage:\"\n",
            "    return '( ' + ' '.join(') | (' if s == pu[0] else s for s in pu[1:]) + ' )'\n",
            "\n",
            "\n",
            "def extras(help, version, options, doc):\n",
            "    if help and any((o.name in ('-h', '--help')) and o.value for o in options):\n",
            "        print(doc.strip(\"\\n\"))\n",
            "        sys.exit()\n",
            "    if version and any(o.name == '--version' and o.value for o in options):\n",
            "        print(version)\n",
            "        sys.exit()\n",
            "\n",
            "\n",
            "class Dict(dict):\n",
            "    def __repr__(self):\n",
            "        return '{%s}' % ',\\n '.join('%r: %r' % i for i in sorted(self.items()))\n",
            "\n",
            "\n",
            "def docopt(doc, argv=None, help=True, version=None, options_first=False):\n",
            "    \"\"\"Parse `argv` based on command-line interface described in `doc`.\n",
            "\n",
            "    `docopt` creates your command-line interface based on its\n",
            "    description that you pass as `doc`. Such description can contain\n",
            "    --options, <positional-argument>, commands, which could be\n",
            "    [optional], (required), (mutually | exclusive) or repeated...\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    doc : str\n",
            "        Description of your command-line interface.\n",
            "    argv : list of str, optional\n",
            "        Argument vector to be parsed. sys.argv[1:] is used if not\n",
            "        provided.\n",
            "    help : bool (default: True)\n",
            "        Set to False to disable automatic help on -h or --help\n",
            "        options.\n",
            "    version : any object\n",
            "        If passed, the object will be printed if --version is in\n",
            "        `argv`.\n",
            "    options_first : bool (default: False)\n",
            "        Set to True to require options preceed positional arguments,\n",
            "        i.e. to forbid options and positional arguments intermix.\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    args : dict\n",
            "        A dictionary, where keys are names of command-line elements\n",
            "        such as e.g. \"--verbose\" and \"<path>\", and values are the\n",
            "        parsed values of those elements.\n",
            "\n",
            "    Example\n",
            "    -------\n",
            "    >>> from docopt import docopt\n",
            "    >>> doc = '''\n",
            "    Usage:\n",
            "        my_program tcp <host> <port> [--timeout=<seconds>]\n",
            "        my_program serial <port> [--baud=<n>] [--timeout=<seconds>]\n",
            "        my_program (-h | --help | --version)\n",
            "\n",
            "    Options:\n",
            "        -h, --help  Show this screen and exit.\n",
            "        --baud=<n>  Baudrate [default: 9600]\n",
            "    '''\n",
            "    >>> argv = ['tcp', '127.0.0.1', '80', '--timeout', '30']\n",
            "    >>> docopt(doc, argv)\n",
            "    {'--baud': '9600',\n",
            "     '--help': False,\n",
            "     '--timeout': '30',\n",
            "     '--version': False,\n",
            "     '<host>': '127.0.0.1',\n",
            "     '<port>': '80',\n",
            "     'serial': False,\n",
            "     'tcp': True}\n",
            "\n",
            "    See also\n",
            "    --------\n",
            "    * For video introduction see http://docopt.org\n",
            "    * Full documentation is available in README.rst as well as online\n",
            "      at https://github.com/docopt/docopt#readme\n",
            "\n",
            "    \"\"\"\n",
            "    if argv is None:\n",
            "        argv = sys.argv[1:]\n",
            "    DocoptExit.usage = printable_usage(doc)\n",
            "    options = parse_defaults(doc)\n",
            "    pattern = parse_pattern(formal_usage(DocoptExit.usage), options)\n",
            "    # [default] syntax for argument is disabled\n",
            "    #for a in pattern.flat(Argument):\n",
            "    #    same_name = [d for d in arguments if d.name == a.name]\n",
            "    #    if same_name:\n",
            "    #        a.value = same_name[0].value\n",
            "    argv = parse_argv(TokenStream(argv, DocoptExit), list(options),\n",
            "                      options_first)\n",
            "    pattern_options = set(pattern.flat(Option))\n",
            "    for ao in pattern.flat(AnyOptions):\n",
            "        doc_options = parse_defaults(doc)\n",
            "        ao.children = list(set(doc_options) - pattern_options)\n",
            "        #if any_options:\n",
            "        #    ao.children += [Option(o.short, o.long, o.argcount)\n",
            "        #                    for o in argv if type(o) is Option]\n",
            "    extras(help, version, argv, doc)\n",
            "    matched, left, collected = pattern.fix().match(argv)\n",
            "    if matched and left == []:  # better error message if left?\n",
            "        return Dict((a.name, a.value) for a in (pattern.flat() + collected))\n",
            "    raise DocoptExit()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "import struct\n",
            "\n",
            "_UNIT_KM = -3\n",
            "_UNIT_100M = -2\n",
            "_UNIT_10M = -1\n",
            "_UNIT_1M = 0\n",
            "_UNIT_10CM = 1\n",
            "_UNIT_CM = 2\n",
            "_UNIT_MM = 3\n",
            "_UNIT_0_1MM = 4\n",
            "_UNIT_0_01MM = 5\n",
            "_UNIT_UM = 6\n",
            "_UNIT_INCH = 6\n",
            "\n",
            "_TIFF_TYPE_SIZES = {\n",
            "  1: 1,\n",
            "  2: 1,\n",
            "  3: 2,\n",
            "  4: 4,\n",
            "  5: 8,\n",
            "  6: 1,\n",
            "  7: 1,\n",
            "  8: 2,\n",
            "  9: 4,\n",
            "  10: 8,\n",
            "  11: 4,\n",
            "  12: 8,\n",
            "}\n",
            "\n",
            "\n",
            "def _convertToDPI(density, unit):\n",
            "    if unit == _UNIT_KM:\n",
            "        return int(density * 0.0000254 + 0.5)\n",
            "    elif unit == _UNIT_100M:\n",
            "        return int(density * 0.000254 + 0.5)\n",
            "    elif unit == _UNIT_10M:\n",
            "        return int(density * 0.00254 + 0.5)\n",
            "    elif unit == _UNIT_1M:\n",
            "        return int(density * 0.0254 + 0.5)\n",
            "    elif unit == _UNIT_10CM:\n",
            "        return int(density * 0.254 + 0.5)\n",
            "    elif unit == _UNIT_CM:\n",
            "        return int(density * 2.54 + 0.5)\n",
            "    elif unit == _UNIT_MM:\n",
            "        return int(density * 25.4 + 0.5)\n",
            "    elif unit == _UNIT_0_1MM:\n",
            "        return density * 254\n",
            "    elif unit == _UNIT_0_01MM:\n",
            "        return density * 2540\n",
            "    elif unit == _UNIT_UM:\n",
            "        return density * 25400\n",
            "    return density\n",
            "\n",
            "\n",
            "def get(filepath):\n",
            "    \"\"\"\n",
            "    Return (width, height) for a given img file content\n",
            "    no requirements\n",
            "    \"\"\"\n",
            "    height = -1\n",
            "    width = -1\n",
            "\n",
            "    with open(filepath, 'rb') as fhandle:\n",
            "        head = fhandle.read(24)\n",
            "        size = len(head)\n",
            "        # handle GIFs\n",
            "        if size >= 10 and head[:6] in (b'GIF87a', b'GIF89a'):\n",
            "            # Check to see if content_type is correct\n",
            "            try:\n",
            "                width, height = struct.unpack(\"<hh\", head[6:10])\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid GIF file\")\n",
            "        # see png edition spec bytes are below chunk length then and finally the\n",
            "        elif size >= 24 and head.startswith(b'\\211PNG\\r\\n\\032\\n') and head[12:16] == b'IHDR':\n",
            "            try:\n",
            "                width, height = struct.unpack(\">LL\", head[16:24])\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid PNG file\")\n",
            "        # Maybe this is for an older PNG version.\n",
            "        elif size >= 16 and head.startswith(b'\\211PNG\\r\\n\\032\\n'):\n",
            "            # Check to see if we have the right content type\n",
            "            try:\n",
            "                width, height = struct.unpack(\">LL\", head[8:16])\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid PNG file\")\n",
            "        # handle JPEGs\n",
            "        elif size >= 2 and head.startswith(b'\\377\\330'):\n",
            "            try:\n",
            "                fhandle.seek(0)  # Read 0xff next\n",
            "                size = 2\n",
            "                ftype = 0\n",
            "                while not 0xc0 <= ftype <= 0xcf or ftype in [0xc4, 0xc8, 0xcc]:\n",
            "                    fhandle.seek(size, 1)\n",
            "                    byte = fhandle.read(1)\n",
            "                    while ord(byte) == 0xff:\n",
            "                        byte = fhandle.read(1)\n",
            "                    ftype = ord(byte)\n",
            "                    size = struct.unpack('>H', fhandle.read(2))[0] - 2\n",
            "                # We are at a SOFn block\n",
            "                fhandle.seek(1, 1)  # Skip `precision' byte.\n",
            "                height, width = struct.unpack('>HH', fhandle.read(4))\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid JPEG file\")\n",
            "        # handle JPEG2000s\n",
            "        elif size >= 12 and head.startswith(b'\\x00\\x00\\x00\\x0cjP  \\r\\n\\x87\\n'):\n",
            "            fhandle.seek(48)\n",
            "            try:\n",
            "                height, width = struct.unpack('>LL', fhandle.read(8))\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid JPEG2000 file\")\n",
            "        # handle big endien TIFF\n",
            "        elif size >= 8 and head.startswith(b\"\\x4d\\x4d\\x00\\x2a\"):\n",
            "            offset = struct.unpack('>L', head[4:8])[0]\n",
            "            fhandle.seek(offset)\n",
            "            ifdsize = struct.unpack(\">H\", fhandle.read(2))[0]\n",
            "            for i in range(ifdsize):\n",
            "                tag, datatype, count, data = struct.unpack(\">HHLL\", fhandle.read(12))\n",
            "                if tag == 256:\n",
            "                    if datatype == 3:\n",
            "                        width = int(data / 65536)\n",
            "                    elif datatype == 4:\n",
            "                        width = data\n",
            "                    else:\n",
            "                        raise ValueError(\"Invalid TIFF file: width column data type should be SHORT/LONG.\")\n",
            "                elif tag == 257:\n",
            "                    if datatype == 3:\n",
            "                        height = int(data / 65536)\n",
            "                    elif datatype == 4:\n",
            "                        height = data\n",
            "                    else:\n",
            "                        raise ValueError(\"Invalid TIFF file: height column data type should be SHORT/LONG.\")\n",
            "                if width != -1 and height != -1:\n",
            "                    break\n",
            "            if width == -1 or height == -1:\n",
            "                raise ValueError(\"Invalid TIFF file: width and/or height IDS entries are missing.\")\n",
            "        elif size >= 8 and head.startswith(b\"\\x49\\x49\\x2a\\x00\"):\n",
            "            offset = struct.unpack('<L', head[4:8])[0]\n",
            "            fhandle.seek(offset)\n",
            "            ifdsize = struct.unpack(\"<H\", fhandle.read(2))[0]\n",
            "            for i in range(ifdsize):\n",
            "                tag, datatype, count, data = struct.unpack(\"<HHLL\", fhandle.read(12))\n",
            "                if tag == 256:\n",
            "                    width = data\n",
            "                elif tag == 257:\n",
            "                    height = data\n",
            "                if width != -1 and height != -1:\n",
            "                    break\n",
            "            if width == -1 or height == -1:\n",
            "                raise ValueError(\"Invalid TIFF file: width and/or height IDS entries are missing.\")\n",
            "\n",
            "    return width, height\n",
            "\n",
            "\n",
            "def getDPI(filepath):\n",
            "    \"\"\"\n",
            "    Return (width, height) for a given img file content\n",
            "    no requirements\n",
            "    \"\"\"\n",
            "    xDPI = -1\n",
            "    yDPI = -1\n",
            "    with open(filepath, 'rb') as fhandle:\n",
            "        head = fhandle.read(24)\n",
            "        size = len(head)\n",
            "        # handle GIFs\n",
            "        # GIFs doesn't have density\n",
            "        if size >= 10 and head[:6] in (b'GIF87a', b'GIF89a'):\n",
            "            pass\n",
            "        # see png edition spec bytes are below chunk length then and finally the\n",
            "        elif size >= 24 and head.startswith(b'\\211PNG\\r\\n\\032\\n'):\n",
            "            chunkOffset = 8\n",
            "            chunk = head[8:]\n",
            "            while True:\n",
            "                chunkType = chunk[4:8]\n",
            "                if chunkType == b'pHYs':\n",
            "                    try:\n",
            "                        xDensity, yDensity, unit = struct.unpack(\">LLB\", chunk[8:])\n",
            "                    except struct.error:\n",
            "                        raise ValueError(\"Invalid PNG file\")\n",
            "                    if unit:\n",
            "                        xDPI = _convertToDPI(xDensity, _UNIT_1M)\n",
            "                        yDPI = _convertToDPI(yDensity, _UNIT_1M)\n",
            "                    else:  # no unit\n",
            "                        xDPI = xDensity\n",
            "                        yDPI = yDensity\n",
            "                    break\n",
            "                elif chunkType == b'IDAT':\n",
            "                    break\n",
            "                else:\n",
            "                    try:\n",
            "                        dataSize, = struct.unpack(\">L\", chunk[0:4])\n",
            "                    except struct.error:\n",
            "                        raise ValueError(\"Invalid PNG file\")\n",
            "                    chunkOffset += dataSize + 12\n",
            "                    fhandle.seek(chunkOffset)\n",
            "                    chunk = fhandle.read(17)\n",
            "        # handle JPEGs\n",
            "        elif size >= 2 and head.startswith(b'\\377\\330'):\n",
            "            try:\n",
            "                fhandle.seek(0)  # Read 0xff next\n",
            "                size = 2\n",
            "                ftype = 0\n",
            "                while not 0xc0 <= ftype <= 0xcf:\n",
            "                    if ftype == 0xe0:  # APP0 marker\n",
            "                        fhandle.seek(7, 1)\n",
            "                        unit, xDensity, yDensity = struct.unpack(\">BHH\", fhandle.read(5))\n",
            "                        if unit == 1 or unit == 0:\n",
            "                            xDPI = xDensity\n",
            "                            yDPI = yDensity\n",
            "                        elif unit == 2:\n",
            "                            xDPI = _convertToDPI(xDensity, _UNIT_CM)\n",
            "                            yDPI = _convertToDPI(yDensity, _UNIT_CM)\n",
            "                        break\n",
            "                    fhandle.seek(size, 1)\n",
            "                    byte = fhandle.read(1)\n",
            "                    while ord(byte) == 0xff:\n",
            "                        byte = fhandle.read(1)\n",
            "                    ftype = ord(byte)\n",
            "                    size = struct.unpack('>H', fhandle.read(2))[0] - 2\n",
            "            except struct.error:\n",
            "                raise ValueError(\"Invalid JPEG file\")\n",
            "        # handle JPEG2000s\n",
            "        elif size >= 12 and head.startswith(b'\\x00\\x00\\x00\\x0cjP  \\r\\n\\x87\\n'):\n",
            "            fhandle.seek(32)\n",
            "            # skip JP2 image header box\n",
            "            headerSize = struct.unpack('>L', fhandle.read(4))[0] - 8\n",
            "            fhandle.seek(4, 1)\n",
            "            foundResBox = False\n",
            "            try:\n",
            "                while headerSize > 0:\n",
            "                    print(\"headerSize\", headerSize)\n",
            "                    boxHeader = fhandle.read(8)\n",
            "                    boxType = boxHeader[4:]\n",
            "                    print(boxType)\n",
            "                    if boxType == 'res ':  # find resolution super box\n",
            "                        foundResBox = True\n",
            "                        headerSize -= 8\n",
            "                        print(\"found res super box\")\n",
            "                        break\n",
            "                    print(\"@1\", boxHeader)\n",
            "                    boxSize, = struct.unpack('>L', boxHeader[:4])\n",
            "                    print(\"boxSize\", boxSize)\n",
            "                    fhandle.seek(boxSize - 8, 1)\n",
            "                    headerSize -= boxSize\n",
            "                if foundResBox:\n",
            "                    while headerSize > 0:\n",
            "                        boxHeader = fhandle.read(8)\n",
            "                        boxType = boxHeader[4:]\n",
            "                        print(boxType)\n",
            "                        if boxType == 'resd':  # Display resolution box\n",
            "                            print(\"@2\")\n",
            "                            yDensity, xDensity, yUnit, xUnit = struct.unpack(\">HHBB\", fhandle.read(10))\n",
            "                            xDPI = _convertToDPI(xDensity, xUnit)\n",
            "                            yDPI = _convertToDPI(yDensity, yUnit)\n",
            "                            break\n",
            "                        boxSize, = struct.unpack('>L', boxHeader[:4])\n",
            "                        print(\"boxSize\", boxSize)\n",
            "                        fhandle.seek(boxSize - 8, 1)\n",
            "                        headerSize -= boxSize\n",
            "            except struct.error as e:\n",
            "                print(e)\n",
            "                raise ValueError(\"Invalid JPEG2000 file\")\n",
            "    return xDPI, yDPI\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Module for reading and writing bzip2-compressed files.\n",
            "\n",
            "This module contains a backport of Python 3.4's bz2.open() function and\n",
            "BZ2File class, adapted to work with earlier versions of Python.\n",
            "\"\"\"\n",
            "\n",
            "__all__ = [\"BZ2File\", \"open\"]\n",
            "\n",
            "__author__ = \"Nadeem Vawda <nadeem.vawda@gmail.com>\"\n",
            "\n",
            "import io\n",
            "import sys\n",
            "import warnings\n",
            "\n",
            "try:\n",
            "    from threading import RLock\n",
            "except ImportError:\n",
            "    from dummy_threading import RLock\n",
            "\n",
            "from bz2 import BZ2Compressor, BZ2Decompressor\n",
            "\n",
            "\n",
            "_MODE_CLOSED   = 0\n",
            "_MODE_READ     = 1\n",
            "_MODE_READ_EOF = 2\n",
            "_MODE_WRITE    = 3\n",
            "\n",
            "_BUFFER_SIZE = 8192\n",
            "\n",
            "_STR_TYPES = (str, unicode) if (str is bytes) else (str, bytes)\n",
            "\n",
            "# The 'x' mode for open() was introduced in Python 3.3.\n",
            "_HAS_OPEN_X_MODE = sys.version_info[:2] >= (3, 3)\n",
            "\n",
            "_builtin_open = open\n",
            "\n",
            "\n",
            "class BZ2File(io.BufferedIOBase):\n",
            "\n",
            "    \"\"\"A file object providing transparent bzip2 (de)compression.\n",
            "\n",
            "    A BZ2File can act as a wrapper for an existing file object, or refer\n",
            "    directly to a named file on disk.\n",
            "\n",
            "    Note that BZ2File provides a *binary* file interface - data read is\n",
            "    returned as bytes, and data to be written should be given as bytes.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, filename, mode=\"r\", buffering=None, compresslevel=9):\n",
            "        \"\"\"Open a bzip2-compressed file.\n",
            "\n",
            "        If filename is a str, bytes or unicode object, it gives the name\n",
            "        of the file to be opened. Otherwise, it should be a file object,\n",
            "        which will be used to read or write the compressed data.\n",
            "\n",
            "        mode can be 'r' for reading (default), 'w' for (over)writing,\n",
            "        'x' for creating exclusively, or 'a' for appending. These can\n",
            "        equivalently be given as 'rb', 'wb', 'xb', and 'ab'.\n",
            "\n",
            "        buffering is ignored. Its use is deprecated.\n",
            "\n",
            "        If mode is 'w', 'x' or 'a', compresslevel can be a number between 1\n",
            "        and 9 specifying the level of compression: 1 produces the least\n",
            "        compression, and 9 (default) produces the most compression.\n",
            "\n",
            "        If mode is 'r', the input file may be the concatenation of\n",
            "        multiple compressed streams.\n",
            "        \"\"\"\n",
            "        # This lock must be recursive, so that BufferedIOBase's\n",
            "        # readline(), readlines() and writelines() don't deadlock.\n",
            "        self._lock = RLock()\n",
            "        self._fp = None\n",
            "        self._closefp = False\n",
            "        self._mode = _MODE_CLOSED\n",
            "        self._pos = 0\n",
            "        self._size = -1\n",
            "\n",
            "        if buffering is not None:\n",
            "            warnings.warn(\"Use of 'buffering' argument is deprecated\",\n",
            "                          DeprecationWarning)\n",
            "\n",
            "        if not (1 <= compresslevel <= 9):\n",
            "            raise ValueError(\"compresslevel must be between 1 and 9\")\n",
            "\n",
            "        if mode in (\"\", \"r\", \"rb\"):\n",
            "            mode = \"rb\"\n",
            "            mode_code = _MODE_READ\n",
            "            self._decompressor = BZ2Decompressor()\n",
            "            self._buffer = b\"\"\n",
            "            self._buffer_offset = 0\n",
            "        elif mode in (\"w\", \"wb\"):\n",
            "            mode = \"wb\"\n",
            "            mode_code = _MODE_WRITE\n",
            "            self._compressor = BZ2Compressor(compresslevel)\n",
            "        elif mode in (\"x\", \"xb\") and _HAS_OPEN_X_MODE:\n",
            "            mode = \"xb\"\n",
            "            mode_code = _MODE_WRITE\n",
            "            self._compressor = BZ2Compressor(compresslevel)\n",
            "        elif mode in (\"a\", \"ab\"):\n",
            "            mode = \"ab\"\n",
            "            mode_code = _MODE_WRITE\n",
            "            self._compressor = BZ2Compressor(compresslevel)\n",
            "        else:\n",
            "            raise ValueError(\"Invalid mode: %r\" % (mode,))\n",
            "\n",
            "        if isinstance(filename, _STR_TYPES):\n",
            "            self._fp = _builtin_open(filename, mode)\n",
            "            self._closefp = True\n",
            "            self._mode = mode_code\n",
            "        elif hasattr(filename, \"read\") or hasattr(filename, \"write\"):\n",
            "            self._fp = filename\n",
            "            self._mode = mode_code\n",
            "        else:\n",
            "            raise TypeError(\"filename must be a %s or %s object, or a file\" %\n",
            "                            (_STR_TYPES[0].__name__, _STR_TYPES[1].__name__))\n",
            "\n",
            "    def close(self):\n",
            "        \"\"\"Flush and close the file.\n",
            "\n",
            "        May be called more than once without error. Once the file is\n",
            "        closed, any other operation on it will raise a ValueError.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            if self._mode == _MODE_CLOSED:\n",
            "                return\n",
            "            try:\n",
            "                if self._mode in (_MODE_READ, _MODE_READ_EOF):\n",
            "                    self._decompressor = None\n",
            "                elif self._mode == _MODE_WRITE:\n",
            "                    self._fp.write(self._compressor.flush())\n",
            "                    self._compressor = None\n",
            "            finally:\n",
            "                try:\n",
            "                    if self._closefp:\n",
            "                        self._fp.close()\n",
            "                finally:\n",
            "                    self._fp = None\n",
            "                    self._closefp = False\n",
            "                    self._mode = _MODE_CLOSED\n",
            "                    self._buffer = b\"\"\n",
            "                    self._buffer_offset = 0\n",
            "\n",
            "    @property\n",
            "    def closed(self):\n",
            "        \"\"\"True if this file is closed.\"\"\"\n",
            "        return self._mode == _MODE_CLOSED\n",
            "\n",
            "    def fileno(self):\n",
            "        \"\"\"Return the file descriptor for the underlying file.\"\"\"\n",
            "        self._check_not_closed()\n",
            "        return self._fp.fileno()\n",
            "\n",
            "    def seekable(self):\n",
            "        \"\"\"Return whether the file supports seeking.\"\"\"\n",
            "        return self.readable() and (self._fp.seekable()\n",
            "                                    if hasattr(self._fp, \"seekable\")\n",
            "                                    else hasattr(self._fp, \"seek\"))\n",
            "\n",
            "    def readable(self):\n",
            "        \"\"\"Return whether the file was opened for reading.\"\"\"\n",
            "        self._check_not_closed()\n",
            "        return self._mode in (_MODE_READ, _MODE_READ_EOF)\n",
            "\n",
            "    def writable(self):\n",
            "        \"\"\"Return whether the file was opened for writing.\"\"\"\n",
            "        self._check_not_closed()\n",
            "        return self._mode == _MODE_WRITE\n",
            "\n",
            "    # Mode-checking helper functions.\n",
            "\n",
            "    def _check_not_closed(self):\n",
            "        if self.closed:\n",
            "            raise ValueError(\"I/O operation on closed file\")\n",
            "\n",
            "    def _check_can_read(self):\n",
            "        if self._mode not in (_MODE_READ, _MODE_READ_EOF):\n",
            "            self._check_not_closed()\n",
            "            raise io.UnsupportedOperation(\"File not open for reading\")\n",
            "\n",
            "    def _check_can_write(self):\n",
            "        if self._mode != _MODE_WRITE:\n",
            "            self._check_not_closed()\n",
            "            raise io.UnsupportedOperation(\"File not open for writing\")\n",
            "\n",
            "    def _check_can_seek(self):\n",
            "        if self._mode not in (_MODE_READ, _MODE_READ_EOF):\n",
            "            self._check_not_closed()\n",
            "            raise io.UnsupportedOperation(\"Seeking is only supported \"\n",
            "                                          \"on files open for reading\")\n",
            "        if hasattr(self._fp, \"seekable\") and not self._fp.seekable():\n",
            "            raise io.UnsupportedOperation(\"The underlying file object \"\n",
            "                                          \"does not support seeking\")\n",
            "\n",
            "    # Fill the readahead buffer if it is empty. Returns False on EOF.\n",
            "    def _fill_buffer(self):\n",
            "        if self._mode == _MODE_READ_EOF:\n",
            "            return False\n",
            "        # Depending on the input data, our call to the decompressor may not\n",
            "        # return any data. In this case, try again after reading another block.\n",
            "        while self._buffer_offset == len(self._buffer):\n",
            "            rawblock = (self._decompressor.unused_data or\n",
            "                        self._fp.read(_BUFFER_SIZE))\n",
            "\n",
            "            if not rawblock:\n",
            "                try:\n",
            "                    self._decompressor.decompress(b\"\")\n",
            "                except EOFError:\n",
            "                    # End-of-stream marker and end of file. We're good.\n",
            "                    self._mode = _MODE_READ_EOF\n",
            "                    self._size = self._pos\n",
            "                    return False\n",
            "                else:\n",
            "                    # Problem - we were expecting more compressed data.\n",
            "                    raise EOFError(\"Compressed file ended before the \"\n",
            "                                   \"end-of-stream marker was reached\")\n",
            "\n",
            "            try:\n",
            "                self._buffer = self._decompressor.decompress(rawblock)\n",
            "            except EOFError:\n",
            "                # Continue to next stream.\n",
            "                self._decompressor = BZ2Decompressor()\n",
            "                try:\n",
            "                    self._buffer = self._decompressor.decompress(rawblock)\n",
            "                except IOError:\n",
            "                    # Trailing data isn't a valid bzip2 stream. We're done here.\n",
            "                    self._mode = _MODE_READ_EOF\n",
            "                    self._size = self._pos\n",
            "                    return False\n",
            "            self._buffer_offset = 0\n",
            "        return True\n",
            "\n",
            "    # Read data until EOF.\n",
            "    # If return_data is false, consume the data without returning it.\n",
            "    def _read_all(self, return_data=True):\n",
            "        # The loop assumes that _buffer_offset is 0. Ensure that this is true.\n",
            "        self._buffer = self._buffer[self._buffer_offset:]\n",
            "        self._buffer_offset = 0\n",
            "\n",
            "        blocks = []\n",
            "        while self._fill_buffer():\n",
            "            if return_data:\n",
            "                blocks.append(self._buffer)\n",
            "            self._pos += len(self._buffer)\n",
            "            self._buffer = b\"\"\n",
            "        if return_data:\n",
            "            return b\"\".join(blocks)\n",
            "\n",
            "    # Read a block of up to n bytes.\n",
            "    # If return_data is false, consume the data without returning it.\n",
            "    def _read_block(self, n, return_data=True):\n",
            "        # If we have enough data buffered, return immediately.\n",
            "        end = self._buffer_offset + n\n",
            "        if end <= len(self._buffer):\n",
            "            data = self._buffer[self._buffer_offset : end]\n",
            "            self._buffer_offset = end\n",
            "            self._pos += len(data)\n",
            "            return data if return_data else None\n",
            "\n",
            "        # The loop assumes that _buffer_offset is 0. Ensure that this is true.\n",
            "        self._buffer = self._buffer[self._buffer_offset:]\n",
            "        self._buffer_offset = 0\n",
            "\n",
            "        blocks = []\n",
            "        while n > 0 and self._fill_buffer():\n",
            "            if n < len(self._buffer):\n",
            "                data = self._buffer[:n]\n",
            "                self._buffer_offset = n\n",
            "            else:\n",
            "                data = self._buffer\n",
            "                self._buffer = b\"\"\n",
            "            if return_data:\n",
            "                blocks.append(data)\n",
            "            self._pos += len(data)\n",
            "            n -= len(data)\n",
            "        if return_data:\n",
            "            return b\"\".join(blocks)\n",
            "\n",
            "    def peek(self, n=0):\n",
            "        \"\"\"Return buffered data without advancing the file position.\n",
            "\n",
            "        Always returns at least one byte of data, unless at EOF.\n",
            "        The exact number of bytes returned is unspecified.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._check_can_read()\n",
            "            if not self._fill_buffer():\n",
            "                return b\"\"\n",
            "            return self._buffer[self._buffer_offset:]\n",
            "\n",
            "    def read(self, size=-1):\n",
            "        \"\"\"Read up to size uncompressed bytes from the file.\n",
            "\n",
            "        If size is negative or omitted, read until EOF is reached.\n",
            "        Returns b'' if the file is already at EOF.\n",
            "        \"\"\"\n",
            "        if size is None:\n",
            "            raise TypeError()\n",
            "        with self._lock:\n",
            "            self._check_can_read()\n",
            "            if size == 0:\n",
            "                return b\"\"\n",
            "            elif size < 0:\n",
            "                return self._read_all()\n",
            "            else:\n",
            "                return self._read_block(size)\n",
            "\n",
            "    def read1(self, size=-1):\n",
            "        \"\"\"Read up to size uncompressed bytes, while trying to avoid\n",
            "        making multiple reads from the underlying stream.\n",
            "\n",
            "        Returns b'' if the file is at EOF.\n",
            "        \"\"\"\n",
            "        # Usually, read1() calls _fp.read() at most once. However, sometimes\n",
            "        # this does not give enough data for the decompressor to make progress.\n",
            "        # In this case we make multiple reads, to avoid returning b\"\".\n",
            "        with self._lock:\n",
            "            self._check_can_read()\n",
            "            if (size == 0 or\n",
            "                # Only call _fill_buffer() if the buffer is actually empty.\n",
            "                # This gives a significant speedup if *size* is small.\n",
            "                (self._buffer_offset == len(self._buffer) and not self._fill_buffer())):\n",
            "                return b\"\"\n",
            "            if size > 0:\n",
            "                data = self._buffer[self._buffer_offset :\n",
            "                                    self._buffer_offset + size]\n",
            "                self._buffer_offset += len(data)\n",
            "            else:\n",
            "                data = self._buffer[self._buffer_offset:]\n",
            "                self._buffer = b\"\"\n",
            "                self._buffer_offset = 0\n",
            "            self._pos += len(data)\n",
            "            return data\n",
            "\n",
            "    def readinto(self, b):\n",
            "        \"\"\"Read up to len(b) bytes into b.\n",
            "\n",
            "        Returns the number of bytes read (0 for EOF).\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            return io.BufferedIOBase.readinto(self, b)\n",
            "\n",
            "    def readline(self, size=-1):\n",
            "        \"\"\"Read a line of uncompressed bytes from the file.\n",
            "\n",
            "        The terminating newline (if present) is retained. If size is\n",
            "        non-negative, no more than size bytes will be read (in which\n",
            "        case the line may be incomplete). Returns b'' if already at EOF.\n",
            "        \"\"\"\n",
            "        if not isinstance(size, int):\n",
            "            if not hasattr(size, \"__index__\"):\n",
            "                raise TypeError(\"Integer argument expected\")\n",
            "            size = size.__index__()\n",
            "        with self._lock:\n",
            "            self._check_can_read()\n",
            "            # Shortcut for the common case - the whole line is in the buffer.\n",
            "            if size < 0:\n",
            "                end = self._buffer.find(b\"\\n\", self._buffer_offset) + 1\n",
            "                if end > 0:\n",
            "                    line = self._buffer[self._buffer_offset : end]\n",
            "                    self._buffer_offset = end\n",
            "                    self._pos += len(line)\n",
            "                    return line\n",
            "            return io.BufferedIOBase.readline(self, size)\n",
            "\n",
            "    def readlines(self, size=-1):\n",
            "        \"\"\"Read a list of lines of uncompressed bytes from the file.\n",
            "\n",
            "        size can be specified to control the number of lines read: no\n",
            "        further lines will be read once the total size of the lines read\n",
            "        so far equals or exceeds size.\n",
            "        \"\"\"\n",
            "        if not isinstance(size, int):\n",
            "            if not hasattr(size, \"__index__\"):\n",
            "                raise TypeError(\"Integer argument expected\")\n",
            "            size = size.__index__()\n",
            "        with self._lock:\n",
            "            return io.BufferedIOBase.readlines(self, size)\n",
            "\n",
            "    def write(self, data):\n",
            "        \"\"\"Write a byte string to the file.\n",
            "\n",
            "        Returns the number of uncompressed bytes written, which is\n",
            "        always len(data). Note that due to buffering, the file on disk\n",
            "        may not reflect the data written until close() is called.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._check_can_write()\n",
            "            compressed = self._compressor.compress(data)\n",
            "            self._fp.write(compressed)\n",
            "            self._pos += len(data)\n",
            "            return len(data)\n",
            "\n",
            "    def writelines(self, seq):\n",
            "        \"\"\"Write a sequence of byte strings to the file.\n",
            "\n",
            "        Returns the number of uncompressed bytes written.\n",
            "        seq can be any iterable yielding byte strings.\n",
            "\n",
            "        Line separators are not added between the written byte strings.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            return io.BufferedIOBase.writelines(self, seq)\n",
            "\n",
            "    # Rewind the file to the beginning of the data stream.\n",
            "    def _rewind(self):\n",
            "        self._fp.seek(0, 0)\n",
            "        self._mode = _MODE_READ\n",
            "        self._pos = 0\n",
            "        self._decompressor = BZ2Decompressor()\n",
            "        self._buffer = b\"\"\n",
            "        self._buffer_offset = 0\n",
            "\n",
            "    def seek(self, offset, whence=0):\n",
            "        \"\"\"Change the file position.\n",
            "\n",
            "        The new position is specified by offset, relative to the\n",
            "        position indicated by whence. Values for whence are:\n",
            "\n",
            "            0: start of stream (default); offset must not be negative\n",
            "            1: current stream position\n",
            "            2: end of stream; offset must not be positive\n",
            "\n",
            "        Returns the new file position.\n",
            "\n",
            "        Note that seeking is emulated, so depending on the parameters,\n",
            "        this operation may be extremely slow.\n",
            "        \"\"\"\n",
            "        with self._lock:\n",
            "            self._check_can_seek()\n",
            "\n",
            "            # Recalculate offset as an absolute file position.\n",
            "            if whence == 0:\n",
            "                pass\n",
            "            elif whence == 1:\n",
            "                offset = self._pos + offset\n",
            "            elif whence == 2:\n",
            "                # Seeking relative to EOF - we need to know the file's size.\n",
            "                if self._size < 0:\n",
            "                    self._read_all(return_data=False)\n",
            "                offset = self._size + offset\n",
            "            else:\n",
            "                raise ValueError(\"Invalid value for whence: %s\" % (whence,))\n",
            "\n",
            "            # Make it so that offset is the number of bytes to skip forward.\n",
            "            if offset < self._pos:\n",
            "                self._rewind()\n",
            "            else:\n",
            "                offset -= self._pos\n",
            "\n",
            "            # Read and discard data until we reach the desired position.\n",
            "            self._read_block(offset, return_data=False)\n",
            "\n",
            "            return self._pos\n",
            "\n",
            "    def tell(self):\n",
            "        \"\"\"Return the current file position.\"\"\"\n",
            "        with self._lock:\n",
            "            self._check_not_closed()\n",
            "            return self._pos\n",
            "\n",
            "\n",
            "def open(filename, mode=\"rb\", compresslevel=9,\n",
            "         encoding=None, errors=None, newline=None):\n",
            "    \"\"\"Open a bzip2-compressed file in binary or text mode.\n",
            "\n",
            "    The filename argument can be an actual filename (a str, bytes or unicode\n",
            "    object), or an existing file object to read from or write to.\n",
            "\n",
            "    The mode argument can be \"r\", \"rb\", \"w\", \"wb\", \"x\", \"xb\", \"a\" or\n",
            "    \"ab\" for binary mode, or \"rt\", \"wt\", \"xt\" or \"at\" for text mode.\n",
            "    The default mode is \"rb\", and the default compresslevel is 9.\n",
            "\n",
            "    For binary mode, this function is equivalent to the BZ2File\n",
            "    constructor: BZ2File(filename, mode, compresslevel). In this case,\n",
            "    the encoding, errors and newline arguments must not be provided.\n",
            "\n",
            "    For text mode, a BZ2File object is created, and wrapped in an\n",
            "    io.TextIOWrapper instance with the specified encoding, error\n",
            "    handling behavior, and line ending(s).\n",
            "\n",
            "    \"\"\"\n",
            "    if \"t\" in mode:\n",
            "        if \"b\" in mode:\n",
            "            raise ValueError(\"Invalid mode: %r\" % (mode,))\n",
            "    else:\n",
            "        if encoding is not None:\n",
            "            raise ValueError(\"Argument 'encoding' not supported in binary mode\")\n",
            "        if errors is not None:\n",
            "            raise ValueError(\"Argument 'errors' not supported in binary mode\")\n",
            "        if newline is not None:\n",
            "            raise ValueError(\"Argument 'newline' not supported in binary mode\")\n",
            "\n",
            "    bz_mode = mode.replace(\"t\", \"\")\n",
            "    binary_file = BZ2File(filename, bz_mode, compresslevel=compresslevel)\n",
            "\n",
            "    if \"t\" in mode:\n",
            "        return io.TextIOWrapper(binary_file, encoding, errors, newline)\n",
            "    else:\n",
            "        return binary_file\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "__all__ = [\"generic\"]\n",
            "try:\n",
            "    from types import ClassType, InstanceType\n",
            "    classtypes = type, ClassType\n",
            "except ImportError:\n",
            "    classtypes = type\n",
            "    InstanceType = None\n",
            "\n",
            "def generic(func):\n",
            "    \"\"\"Create a simple generic function\"\"\"\n",
            "\n",
            "    _sentinel = object()\n",
            "\n",
            "    def _by_class(*args, **kw):\n",
            "        cls = args[0].__class__\n",
            "        for t in type(cls.__name__, (cls,object), {}).__mro__:\n",
            "            f = _gbt(t, _sentinel)\n",
            "            if f is not _sentinel:\n",
            "                return f(*args, **kw)\n",
            "        else:\n",
            "            return func(*args, **kw)\n",
            "\n",
            "    _by_type = {object: func, InstanceType: _by_class}\n",
            "    _gbt = _by_type.get\n",
            "\n",
            "    def when_type(*types):\n",
            "        \"\"\"Decorator to add a method that will be called for the given types\"\"\"\n",
            "        for t in types:\n",
            "            if not isinstance(t, classtypes):\n",
            "                raise TypeError(\n",
            "                    \"%r is not a type or class\" % (t,)\n",
            "                )\n",
            "        def decorate(f):\n",
            "            for t in types:\n",
            "                if _by_type.setdefault(t,f) is not f:\n",
            "                    raise TypeError(\n",
            "                        \"%r already has method for type %r\" % (func, t)\n",
            "                    )\n",
            "            return f\n",
            "        return decorate\n",
            "\n",
            "    _by_object = {}\n",
            "    _gbo = _by_object.get\n",
            "\n",
            "    def when_object(*obs):\n",
            "        \"\"\"Decorator to add a method to be called for the given object(s)\"\"\"\n",
            "        def decorate(f):\n",
            "            for o in obs:\n",
            "                if _by_object.setdefault(id(o), (o,f))[1] is not f:\n",
            "                    raise TypeError(\n",
            "                        \"%r already has method for object %r\" % (func, o)\n",
            "                    )\n",
            "            return f\n",
            "        return decorate\n",
            "\n",
            "\n",
            "    def dispatch(*args, **kw):\n",
            "        f = _gbo(id(args[0]), _sentinel)\n",
            "        if f is _sentinel:\n",
            "            for t in type(args[0]).__mro__:\n",
            "                f = _gbt(t, _sentinel)\n",
            "                if f is not _sentinel:\n",
            "                    return f(*args, **kw)\n",
            "            else:\n",
            "                return func(*args, **kw)\n",
            "        else:\n",
            "            return f[1](*args, **kw)\n",
            "\n",
            "    dispatch.__name__       = func.__name__\n",
            "    dispatch.__dict__       = func.__dict__.copy()\n",
            "    dispatch.__doc__        = func.__doc__\n",
            "    dispatch.__module__     = func.__module__\n",
            "\n",
            "    dispatch.when_type = when_type\n",
            "    dispatch.when_object = when_object\n",
            "    dispatch.default = func\n",
            "    dispatch.has_object = lambda o: id(o) in _by_object\n",
            "    dispatch.has_type   = lambda t: t in _by_type\n",
            "    return dispatch\n",
            "\n",
            "\n",
            "\n",
            "def test_suite():\n",
            "    import doctest\n",
            "    return doctest.DocFileSuite(\n",
            "        'README.txt',\n",
            "        optionflags=doctest.ELLIPSIS|doctest.REPORT_ONLY_FIRST_FAILURE,\n",
            "    )\n",
            "\n",
            "if __name__=='__main__':\n",
            "    import unittest\n",
            "    r = unittest.TextTestRunner()\n",
            "    r.run(test_suite())\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Launch the root jupyter command\n",
            "\n",
            "Alias to jupyter_core\n",
            "\"\"\"\n",
            "\n",
            "__version__ = '1.0.0'\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    from runpy import run_module\n",
            "    run_module('jupyter_core')\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "import abc\n",
            "from abc import abstractmethod, abstractproperty\n",
            "import collections\n",
            "import contextlib\n",
            "import functools\n",
            "import re as stdlib_re  # Avoid confusion with the re we export.\n",
            "import sys\n",
            "import types\n",
            "try:\n",
            "    import collections.abc as collections_abc\n",
            "except ImportError:\n",
            "    import collections as collections_abc  # Fallback for PY3.2.\n",
            "if sys.version_info[:2] >= (3, 6):\n",
            "    import _collections_abc  # Needed for private function _check_methods # noqa\n",
            "try:\n",
            "    from types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType\n",
            "except ImportError:\n",
            "    WrapperDescriptorType = type(object.__init__)\n",
            "    MethodWrapperType = type(object().__str__)\n",
            "    MethodDescriptorType = type(str.join)\n",
            "\n",
            "\n",
            "# Please keep __all__ alphabetized within each category.\n",
            "__all__ = [\n",
            "    # Super-special typing primitives.\n",
            "    'Any',\n",
            "    'Callable',\n",
            "    'ClassVar',\n",
            "    'Generic',\n",
            "    'Optional',\n",
            "    'Tuple',\n",
            "    'Type',\n",
            "    'TypeVar',\n",
            "    'Union',\n",
            "\n",
            "    # ABCs (from collections.abc).\n",
            "    'AbstractSet',  # collections.abc.Set.\n",
            "    'GenericMeta',  # subclass of abc.ABCMeta and a metaclass\n",
            "                    # for 'Generic' and ABCs below.\n",
            "    'ByteString',\n",
            "    'Container',\n",
            "    'ContextManager',\n",
            "    'Hashable',\n",
            "    'ItemsView',\n",
            "    'Iterable',\n",
            "    'Iterator',\n",
            "    'KeysView',\n",
            "    'Mapping',\n",
            "    'MappingView',\n",
            "    'MutableMapping',\n",
            "    'MutableSequence',\n",
            "    'MutableSet',\n",
            "    'Sequence',\n",
            "    'Sized',\n",
            "    'ValuesView',\n",
            "    # The following are added depending on presence\n",
            "    # of their non-generic counterparts in stdlib:\n",
            "    # Awaitable,\n",
            "    # AsyncIterator,\n",
            "    # AsyncIterable,\n",
            "    # Coroutine,\n",
            "    # Collection,\n",
            "    # AsyncGenerator,\n",
            "    # AsyncContextManager\n",
            "\n",
            "    # Structural checks, a.k.a. protocols.\n",
            "    'Reversible',\n",
            "    'SupportsAbs',\n",
            "    'SupportsBytes',\n",
            "    'SupportsComplex',\n",
            "    'SupportsFloat',\n",
            "    'SupportsIndex',\n",
            "    'SupportsInt',\n",
            "    'SupportsRound',\n",
            "\n",
            "    # Concrete collection types.\n",
            "    'Counter',\n",
            "    'Deque',\n",
            "    'Dict',\n",
            "    'DefaultDict',\n",
            "    'List',\n",
            "    'Set',\n",
            "    'FrozenSet',\n",
            "    'NamedTuple',  # Not really a type.\n",
            "    'Generator',\n",
            "\n",
            "    # One-off things.\n",
            "    'AnyStr',\n",
            "    'cast',\n",
            "    'get_type_hints',\n",
            "    'NewType',\n",
            "    'no_type_check',\n",
            "    'no_type_check_decorator',\n",
            "    'NoReturn',\n",
            "    'overload',\n",
            "    'Text',\n",
            "    'TYPE_CHECKING',\n",
            "]\n",
            "\n",
            "# The pseudo-submodules 're' and 'io' are part of the public\n",
            "# namespace, but excluded from __all__ because they might stomp on\n",
            "# legitimate imports of those modules.\n",
            "\n",
            "\n",
            "def _qualname(x):\n",
            "    if sys.version_info[:2] >= (3, 3):\n",
            "        return x.__qualname__\n",
            "    else:\n",
            "        # Fall back to just name.\n",
            "        return x.__name__\n",
            "\n",
            "\n",
            "def _trim_name(nm):\n",
            "    whitelist = ('_TypeAlias', '_ForwardRef', '_TypingBase', '_FinalTypingBase')\n",
            "    if nm.startswith('_') and nm not in whitelist:\n",
            "        nm = nm[1:]\n",
            "    return nm\n",
            "\n",
            "\n",
            "class TypingMeta(type):\n",
            "    \"\"\"Metaclass for most types defined in typing module\n",
            "    (not a part of public API).\n",
            "\n",
            "    This overrides __new__() to require an extra keyword parameter\n",
            "    '_root', which serves as a guard against naive subclassing of the\n",
            "    typing classes.  Any legitimate class defined using a metaclass\n",
            "    derived from TypingMeta must pass _root=True.\n",
            "\n",
            "    This also defines a dummy constructor (all the work for most typing\n",
            "    constructs is done in __new__) and a nicer repr().\n",
            "    \"\"\"\n",
            "\n",
            "    _is_protocol = False\n",
            "\n",
            "    def __new__(cls, name, bases, namespace, *, _root=False):\n",
            "        if not _root:\n",
            "            raise TypeError(\"Cannot subclass %s\" %\n",
            "                            (', '.join(map(_type_repr, bases)) or '()'))\n",
            "        return super().__new__(cls, name, bases, namespace)\n",
            "\n",
            "    def __init__(self, *args, **kwds):\n",
            "        pass\n",
            "\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        \"\"\"Override this in subclasses to interpret forward references.\n",
            "\n",
            "        For example, List['C'] is internally stored as\n",
            "        List[_ForwardRef('C')], which should evaluate to List[C],\n",
            "        where C is an object found in globalns or localns (searching\n",
            "        localns first, of course).\n",
            "        \"\"\"\n",
            "        return self\n",
            "\n",
            "    def _get_type_vars(self, tvars):\n",
            "        pass\n",
            "\n",
            "    def __repr__(self):\n",
            "        qname = _trim_name(_qualname(self))\n",
            "        return '%s.%s' % (self.__module__, qname)\n",
            "\n",
            "\n",
            "class _TypingBase(metaclass=TypingMeta, _root=True):\n",
            "    \"\"\"Internal indicator of special typing constructs.\"\"\"\n",
            "\n",
            "    __slots__ = ('__weakref__',)\n",
            "\n",
            "    def __init__(self, *args, **kwds):\n",
            "        pass\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        \"\"\"Constructor.\n",
            "\n",
            "        This only exists to give a better error message in case\n",
            "        someone tries to subclass a special typing object (not a good idea).\n",
            "        \"\"\"\n",
            "        if (len(args) == 3 and\n",
            "                isinstance(args[0], str) and\n",
            "                isinstance(args[1], tuple)):\n",
            "            # Close enough.\n",
            "            raise TypeError(\"Cannot subclass %r\" % cls)\n",
            "        return super().__new__(cls)\n",
            "\n",
            "    # Things that are not classes also need these.\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        return self\n",
            "\n",
            "    def _get_type_vars(self, tvars):\n",
            "        pass\n",
            "\n",
            "    def __repr__(self):\n",
            "        cls = type(self)\n",
            "        qname = _trim_name(_qualname(cls))\n",
            "        return '%s.%s' % (cls.__module__, qname)\n",
            "\n",
            "    def __call__(self, *args, **kwds):\n",
            "        raise TypeError(\"Cannot instantiate %r\" % type(self))\n",
            "\n",
            "\n",
            "class _FinalTypingBase(_TypingBase, _root=True):\n",
            "    \"\"\"Internal mix-in class to prevent instantiation.\n",
            "\n",
            "    Prevents instantiation unless _root=True is given in class call.\n",
            "    It is used to create pseudo-singleton instances Any, Union, Optional, etc.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, _root=False, **kwds):\n",
            "        self = super().__new__(cls, *args, **kwds)\n",
            "        if _root is True:\n",
            "            return self\n",
            "        raise TypeError(\"Cannot instantiate %r\" % cls)\n",
            "\n",
            "    def __reduce__(self):\n",
            "        return _trim_name(type(self).__name__)\n",
            "\n",
            "\n",
            "class _ForwardRef(_TypingBase, _root=True):\n",
            "    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n",
            "\n",
            "    __slots__ = ('__forward_arg__', '__forward_code__',\n",
            "                 '__forward_evaluated__', '__forward_value__')\n",
            "\n",
            "    def __init__(self, arg):\n",
            "        super().__init__(arg)\n",
            "        if not isinstance(arg, str):\n",
            "            raise TypeError('Forward reference must be a string -- got %r' % (arg,))\n",
            "        try:\n",
            "            code = compile(arg, '<string>', 'eval')\n",
            "        except SyntaxError:\n",
            "            raise SyntaxError('Forward reference must be an expression -- got %r' %\n",
            "                              (arg,))\n",
            "        self.__forward_arg__ = arg\n",
            "        self.__forward_code__ = code\n",
            "        self.__forward_evaluated__ = False\n",
            "        self.__forward_value__ = None\n",
            "\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        if not self.__forward_evaluated__ or localns is not globalns:\n",
            "            if globalns is None and localns is None:\n",
            "                globalns = localns = {}\n",
            "            elif globalns is None:\n",
            "                globalns = localns\n",
            "            elif localns is None:\n",
            "                localns = globalns\n",
            "            self.__forward_value__ = _type_check(\n",
            "                eval(self.__forward_code__, globalns, localns),\n",
            "                \"Forward references must evaluate to types.\")\n",
            "            self.__forward_evaluated__ = True\n",
            "        return self.__forward_value__\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if not isinstance(other, _ForwardRef):\n",
            "            return NotImplemented\n",
            "        return (self.__forward_arg__ == other.__forward_arg__ and\n",
            "                self.__forward_value__ == other.__forward_value__)\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash((self.__forward_arg__, self.__forward_value__))\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        raise TypeError(\"Forward references cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        raise TypeError(\"Forward references cannot be used with issubclass().\")\n",
            "\n",
            "    def __repr__(self):\n",
            "        return '_ForwardRef(%r)' % (self.__forward_arg__,)\n",
            "\n",
            "\n",
            "class _TypeAlias(_TypingBase, _root=True):\n",
            "    \"\"\"Internal helper class for defining generic variants of concrete types.\n",
            "\n",
            "    Note that this is not a type; let's call it a pseudo-type.  It cannot\n",
            "    be used in instance and subclass checks in parameterized form, i.e.\n",
            "    ``isinstance(42, Match[str])`` raises ``TypeError`` instead of returning\n",
            "    ``False``.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ('name', 'type_var', 'impl_type', 'type_checker')\n",
            "\n",
            "    def __init__(self, name, type_var, impl_type, type_checker):\n",
            "        \"\"\"Initializer.\n",
            "\n",
            "        Args:\n",
            "            name: The name, e.g. 'Pattern'.\n",
            "            type_var: The type parameter, e.g. AnyStr, or the\n",
            "                specific type, e.g. str.\n",
            "            impl_type: The implementation type.\n",
            "            type_checker: Function that takes an impl_type instance.\n",
            "                and returns a value that should be a type_var instance.\n",
            "        \"\"\"\n",
            "        assert isinstance(name, str), repr(name)\n",
            "        assert isinstance(impl_type, type), repr(impl_type)\n",
            "        assert not isinstance(impl_type, TypingMeta), repr(impl_type)\n",
            "        assert isinstance(type_var, (type, _TypingBase)), repr(type_var)\n",
            "        self.name = name\n",
            "        self.type_var = type_var\n",
            "        self.impl_type = impl_type\n",
            "        self.type_checker = type_checker\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"%s[%s]\" % (self.name, _type_repr(self.type_var))\n",
            "\n",
            "    def __getitem__(self, parameter):\n",
            "        if not isinstance(self.type_var, TypeVar):\n",
            "            raise TypeError(\"%s cannot be further parameterized.\" % self)\n",
            "        if self.type_var.__constraints__ and isinstance(parameter, type):\n",
            "            if not issubclass(parameter, self.type_var.__constraints__):\n",
            "                raise TypeError(\"%s is not a valid substitution for %s.\" %\n",
            "                                (parameter, self.type_var))\n",
            "        if isinstance(parameter, TypeVar) and parameter is not self.type_var:\n",
            "            raise TypeError(\"%s cannot be re-parameterized.\" % self)\n",
            "        return self.__class__(self.name, parameter,\n",
            "                              self.impl_type, self.type_checker)\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if not isinstance(other, _TypeAlias):\n",
            "            return NotImplemented\n",
            "        return self.name == other.name and self.type_var == other.type_var\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash((self.name, self.type_var))\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        if not isinstance(self.type_var, TypeVar):\n",
            "            raise TypeError(\"Parameterized type aliases cannot be used \"\n",
            "                            \"with isinstance().\")\n",
            "        return isinstance(obj, self.impl_type)\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        if not isinstance(self.type_var, TypeVar):\n",
            "            raise TypeError(\"Parameterized type aliases cannot be used \"\n",
            "                            \"with issubclass().\")\n",
            "        return issubclass(cls, self.impl_type)\n",
            "\n",
            "\n",
            "def _get_type_vars(types, tvars):\n",
            "    for t in types:\n",
            "        if isinstance(t, TypingMeta) or isinstance(t, _TypingBase):\n",
            "            t._get_type_vars(tvars)\n",
            "\n",
            "\n",
            "def _type_vars(types):\n",
            "    tvars = []\n",
            "    _get_type_vars(types, tvars)\n",
            "    return tuple(tvars)\n",
            "\n",
            "\n",
            "def _eval_type(t, globalns, localns):\n",
            "    if isinstance(t, TypingMeta) or isinstance(t, _TypingBase):\n",
            "        return t._eval_type(globalns, localns)\n",
            "    return t\n",
            "\n",
            "\n",
            "def _type_check(arg, msg):\n",
            "    \"\"\"Check that the argument is a type, and return it (internal helper).\n",
            "\n",
            "    As a special case, accept None and return type(None) instead.\n",
            "    Also, _TypeAlias instances (e.g. Match, Pattern) are acceptable.\n",
            "\n",
            "    The msg argument is a human-readable error message, e.g.\n",
            "\n",
            "        \"Union[arg, ...]: arg should be a type.\"\n",
            "\n",
            "    We append the repr() of the actual value (truncated to 100 chars).\n",
            "    \"\"\"\n",
            "    if arg is None:\n",
            "        return type(None)\n",
            "    if isinstance(arg, str):\n",
            "        arg = _ForwardRef(arg)\n",
            "    if (\n",
            "        isinstance(arg, _TypingBase) and type(arg).__name__ == '_ClassVar' or\n",
            "        not isinstance(arg, (type, _TypingBase)) and not callable(arg)\n",
            "    ):\n",
            "        raise TypeError(msg + \" Got %.100r.\" % (arg,))\n",
            "    # Bare Union etc. are not valid as type arguments\n",
            "    if (\n",
            "        type(arg).__name__ in ('_Union', '_Optional') and\n",
            "        not getattr(arg, '__origin__', None) or\n",
            "        isinstance(arg, TypingMeta) and arg._gorg in (Generic, _Protocol)\n",
            "    ):\n",
            "        raise TypeError(\"Plain %s is not valid as type argument\" % arg)\n",
            "    return arg\n",
            "\n",
            "\n",
            "def _type_repr(obj):\n",
            "    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n",
            "\n",
            "    If obj is a type, we return a shorter version than the default\n",
            "    type.__repr__, based on the module and qualified name, which is\n",
            "    typically enough to uniquely identify a type.  For everything\n",
            "    else, we fall back on repr(obj).\n",
            "    \"\"\"\n",
            "    if isinstance(obj, type) and not isinstance(obj, TypingMeta):\n",
            "        if obj.__module__ == 'builtins':\n",
            "            return _qualname(obj)\n",
            "        return '%s.%s' % (obj.__module__, _qualname(obj))\n",
            "    if obj is ...:\n",
            "        return('...')\n",
            "    if isinstance(obj, types.FunctionType):\n",
            "        return obj.__name__\n",
            "    return repr(obj)\n",
            "\n",
            "\n",
            "class _Any(_FinalTypingBase, _root=True):\n",
            "    \"\"\"Special type indicating an unconstrained type.\n",
            "\n",
            "    - Any is compatible with every type.\n",
            "    - Any assumed to have all methods.\n",
            "    - All values assumed to be instances of Any.\n",
            "\n",
            "    Note that all the above statements are true from the point of view of\n",
            "    static type checkers. At runtime, Any should not be used with instance\n",
            "    or class checks.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        raise TypeError(\"Any cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        raise TypeError(\"Any cannot be used with issubclass().\")\n",
            "\n",
            "\n",
            "Any = _Any(_root=True)\n",
            "\n",
            "\n",
            "class _NoReturn(_FinalTypingBase, _root=True):\n",
            "    \"\"\"Special type indicating functions that never return.\n",
            "    Example::\n",
            "\n",
            "      from typing import NoReturn\n",
            "\n",
            "      def stop() -> NoReturn:\n",
            "          raise Exception('no way')\n",
            "\n",
            "    This type is invalid in other positions, e.g., ``List[NoReturn]``\n",
            "    will fail in static type checkers.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        raise TypeError(\"NoReturn cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        raise TypeError(\"NoReturn cannot be used with issubclass().\")\n",
            "\n",
            "\n",
            "NoReturn = _NoReturn(_root=True)\n",
            "\n",
            "\n",
            "class TypeVar(_TypingBase, _root=True):\n",
            "    \"\"\"Type variable.\n",
            "\n",
            "    Usage::\n",
            "\n",
            "      T = TypeVar('T')  # Can be anything\n",
            "      A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
            "\n",
            "    Type variables exist primarily for the benefit of static type\n",
            "    checkers.  They serve as the parameters for generic types as well\n",
            "    as for generic function definitions.  See class Generic for more\n",
            "    information on generic types.  Generic functions work as follows:\n",
            "\n",
            "      def repeat(x: T, n: int) -> List[T]:\n",
            "          '''Return a list containing n references to x.'''\n",
            "          return [x]*n\n",
            "\n",
            "      def longest(x: A, y: A) -> A:\n",
            "          '''Return the longest of two strings.'''\n",
            "          return x if len(x) >= len(y) else y\n",
            "\n",
            "    The latter example's signature is essentially the overloading\n",
            "    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
            "    that if the arguments are instances of some subclass of str,\n",
            "    the return type is still plain str.\n",
            "\n",
            "    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
            "\n",
            "    Type variables defined with covariant=True or contravariant=True\n",
            "    can be used do declare covariant or contravariant generic types.\n",
            "    See PEP 484 for more details. By default generic types are invariant\n",
            "    in all type variables.\n",
            "\n",
            "    Type variables can be introspected. e.g.:\n",
            "\n",
            "      T.__name__ == 'T'\n",
            "      T.__constraints__ == ()\n",
            "      T.__covariant__ == False\n",
            "      T.__contravariant__ = False\n",
            "      A.__constraints__ == (str, bytes)\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ('__name__', '__bound__', '__constraints__',\n",
            "                 '__covariant__', '__contravariant__')\n",
            "\n",
            "    def __init__(self, name, *constraints, bound=None,\n",
            "                 covariant=False, contravariant=False):\n",
            "        super().__init__(name, *constraints, bound=bound,\n",
            "                         covariant=covariant, contravariant=contravariant)\n",
            "        self.__name__ = name\n",
            "        if covariant and contravariant:\n",
            "            raise ValueError(\"Bivariant types are not supported.\")\n",
            "        self.__covariant__ = bool(covariant)\n",
            "        self.__contravariant__ = bool(contravariant)\n",
            "        if constraints and bound is not None:\n",
            "            raise TypeError(\"Constraints cannot be combined with bound=...\")\n",
            "        if constraints and len(constraints) == 1:\n",
            "            raise TypeError(\"A single constraint is not allowed\")\n",
            "        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n",
            "        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n",
            "        if bound:\n",
            "            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n",
            "        else:\n",
            "            self.__bound__ = None\n",
            "\n",
            "    def _get_type_vars(self, tvars):\n",
            "        if self not in tvars:\n",
            "            tvars.append(self)\n",
            "\n",
            "    def __repr__(self):\n",
            "        if self.__covariant__:\n",
            "            prefix = '+'\n",
            "        elif self.__contravariant__:\n",
            "            prefix = '-'\n",
            "        else:\n",
            "            prefix = '~'\n",
            "        return prefix + self.__name__\n",
            "\n",
            "    def __instancecheck__(self, instance):\n",
            "        raise TypeError(\"Type variables cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        raise TypeError(\"Type variables cannot be used with issubclass().\")\n",
            "\n",
            "\n",
            "# Some unconstrained type variables.  These are used by the container types.\n",
            "# (These are not for export.)\n",
            "T = TypeVar('T')  # Any type.\n",
            "KT = TypeVar('KT')  # Key type.\n",
            "VT = TypeVar('VT')  # Value type.\n",
            "T_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\n",
            "V_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\n",
            "VT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\n",
            "T_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n",
            "\n",
            "# A useful type variable with constraints.  This represents string types.\n",
            "# (This one *is* for export!)\n",
            "AnyStr = TypeVar('AnyStr', bytes, str)\n",
            "\n",
            "\n",
            "def _replace_arg(arg, tvars, args):\n",
            "    \"\"\"An internal helper function: replace arg if it is a type variable\n",
            "    found in tvars with corresponding substitution from args or\n",
            "    with corresponding substitution sub-tree if arg is a generic type.\n",
            "    \"\"\"\n",
            "\n",
            "    if tvars is None:\n",
            "        tvars = []\n",
            "    if hasattr(arg, '_subs_tree') and isinstance(arg, (GenericMeta, _TypingBase)):\n",
            "        return arg._subs_tree(tvars, args)\n",
            "    if isinstance(arg, TypeVar):\n",
            "        for i, tvar in enumerate(tvars):\n",
            "            if arg == tvar:\n",
            "                return args[i]\n",
            "    return arg\n",
            "\n",
            "\n",
            "# Special typing constructs Union, Optional, Generic, Callable and Tuple\n",
            "# use three special attributes for internal bookkeeping of generic types:\n",
            "# * __parameters__ is a tuple of unique free type parameters of a generic\n",
            "#   type, for example, Dict[T, T].__parameters__ == (T,);\n",
            "# * __origin__ keeps a reference to a type that was subscripted,\n",
            "#   e.g., Union[T, int].__origin__ == Union;\n",
            "# * __args__ is a tuple of all arguments used in subscripting,\n",
            "#   e.g., Dict[T, int].__args__ == (T, int).\n",
            "\n",
            "\n",
            "def _subs_tree(cls, tvars=None, args=None):\n",
            "    \"\"\"An internal helper function: calculate substitution tree\n",
            "    for generic cls after replacing its type parameters with\n",
            "    substitutions in tvars -> args (if any).\n",
            "    Repeat the same following __origin__'s.\n",
            "\n",
            "    Return a list of arguments with all possible substitutions\n",
            "    performed. Arguments that are generic classes themselves are represented\n",
            "    as tuples (so that no new classes are created by this function).\n",
            "    For example: _subs_tree(List[Tuple[int, T]][str]) == [(Tuple, int, str)]\n",
            "    \"\"\"\n",
            "\n",
            "    if cls.__origin__ is None:\n",
            "        return cls\n",
            "    # Make of chain of origins (i.e. cls -> cls.__origin__)\n",
            "    current = cls.__origin__\n",
            "    orig_chain = []\n",
            "    while current.__origin__ is not None:\n",
            "        orig_chain.append(current)\n",
            "        current = current.__origin__\n",
            "    # Replace type variables in __args__ if asked ...\n",
            "    tree_args = []\n",
            "    for arg in cls.__args__:\n",
            "        tree_args.append(_replace_arg(arg, tvars, args))\n",
            "    # ... then continue replacing down the origin chain.\n",
            "    for ocls in orig_chain:\n",
            "        new_tree_args = []\n",
            "        for arg in ocls.__args__:\n",
            "            new_tree_args.append(_replace_arg(arg, ocls.__parameters__, tree_args))\n",
            "        tree_args = new_tree_args\n",
            "    return tree_args\n",
            "\n",
            "\n",
            "def _remove_dups_flatten(parameters):\n",
            "    \"\"\"An internal helper for Union creation and substitution: flatten Union's\n",
            "    among parameters, then remove duplicates and strict subclasses.\n",
            "    \"\"\"\n",
            "\n",
            "    # Flatten out Union[Union[...], ...].\n",
            "    params = []\n",
            "    for p in parameters:\n",
            "        if isinstance(p, _Union) and p.__origin__ is Union:\n",
            "            params.extend(p.__args__)\n",
            "        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n",
            "            params.extend(p[1:])\n",
            "        else:\n",
            "            params.append(p)\n",
            "    # Weed out strict duplicates, preserving the first of each occurrence.\n",
            "    all_params = set(params)\n",
            "    if len(all_params) < len(params):\n",
            "        new_params = []\n",
            "        for t in params:\n",
            "            if t in all_params:\n",
            "                new_params.append(t)\n",
            "                all_params.remove(t)\n",
            "        params = new_params\n",
            "        assert not all_params, all_params\n",
            "    # Weed out subclasses.\n",
            "    # E.g. Union[int, Employee, Manager] == Union[int, Employee].\n",
            "    # If object is present it will be sole survivor among proper classes.\n",
            "    # Never discard type variables.\n",
            "    # (In particular, Union[str, AnyStr] != AnyStr.)\n",
            "    all_params = set(params)\n",
            "    for t1 in params:\n",
            "        if not isinstance(t1, type):\n",
            "            continue\n",
            "        if any(isinstance(t2, type) and issubclass(t1, t2)\n",
            "               for t2 in all_params - {t1}\n",
            "               if not (isinstance(t2, GenericMeta) and\n",
            "                       t2.__origin__ is not None)):\n",
            "            all_params.remove(t1)\n",
            "    return tuple(t for t in params if t in all_params)\n",
            "\n",
            "\n",
            "def _check_generic(cls, parameters):\n",
            "    # Check correct count for parameters of a generic cls (internal helper).\n",
            "    if not cls.__parameters__:\n",
            "        raise TypeError(\"%s is not a generic class\" % repr(cls))\n",
            "    alen = len(parameters)\n",
            "    elen = len(cls.__parameters__)\n",
            "    if alen != elen:\n",
            "        raise TypeError(\"Too %s parameters for %s; actual %s, expected %s\" %\n",
            "                        (\"many\" if alen > elen else \"few\", repr(cls), alen, elen))\n",
            "\n",
            "\n",
            "_cleanups = []\n",
            "\n",
            "\n",
            "def _tp_cache(func):\n",
            "    \"\"\"Internal wrapper caching __getitem__ of generic types with a fallback to\n",
            "    original function for non-hashable arguments.\n",
            "    \"\"\"\n",
            "\n",
            "    cached = functools.lru_cache()(func)\n",
            "    _cleanups.append(cached.cache_clear)\n",
            "\n",
            "    @functools.wraps(func)\n",
            "    def inner(*args, **kwds):\n",
            "        try:\n",
            "            return cached(*args, **kwds)\n",
            "        except TypeError:\n",
            "            pass  # All real errors (not unhashable args) are raised below.\n",
            "        return func(*args, **kwds)\n",
            "    return inner\n",
            "\n",
            "\n",
            "class _Union(_FinalTypingBase, _root=True):\n",
            "    \"\"\"Union type; Union[X, Y] means either X or Y.\n",
            "\n",
            "    To define a union, use e.g. Union[int, str].  Details:\n",
            "\n",
            "    - The arguments must be types and there must be at least one.\n",
            "\n",
            "    - None as an argument is a special case and is replaced by\n",
            "      type(None).\n",
            "\n",
            "    - Unions of unions are flattened, e.g.::\n",
            "\n",
            "        Union[Union[int, str], float] == Union[int, str, float]\n",
            "\n",
            "    - Unions of a single argument vanish, e.g.::\n",
            "\n",
            "        Union[int] == int  # The constructor actually returns int\n",
            "\n",
            "    - Redundant arguments are skipped, e.g.::\n",
            "\n",
            "        Union[int, str, int] == Union[int, str]\n",
            "\n",
            "    - When comparing unions, the argument order is ignored, e.g.::\n",
            "\n",
            "        Union[int, str] == Union[str, int]\n",
            "\n",
            "    - When two arguments have a subclass relationship, the least\n",
            "      derived argument is kept, e.g.::\n",
            "\n",
            "        class Employee: pass\n",
            "        class Manager(Employee): pass\n",
            "        Union[int, Employee, Manager] == Union[int, Employee]\n",
            "        Union[Manager, int, Employee] == Union[int, Employee]\n",
            "        Union[Employee, Manager] == Employee\n",
            "\n",
            "    - Similar for object::\n",
            "\n",
            "        Union[int, object] == object\n",
            "\n",
            "    - You cannot subclass or instantiate a union.\n",
            "\n",
            "    - You can use Optional[X] as a shorthand for Union[X, None].\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ('__parameters__', '__args__', '__origin__', '__tree_hash__')\n",
            "\n",
            "    def __new__(cls, parameters=None, origin=None, *args, _root=False):\n",
            "        self = super().__new__(cls, parameters, origin, *args, _root=_root)\n",
            "        if origin is None:\n",
            "            self.__parameters__ = None\n",
            "            self.__args__ = None\n",
            "            self.__origin__ = None\n",
            "            self.__tree_hash__ = hash(frozenset(('Union',)))\n",
            "            return self\n",
            "        if not isinstance(parameters, tuple):\n",
            "            raise TypeError(\"Expected parameters=<tuple>\")\n",
            "        if origin is Union:\n",
            "            parameters = _remove_dups_flatten(parameters)\n",
            "            # It's not a union if there's only one type left.\n",
            "            if len(parameters) == 1:\n",
            "                return parameters[0]\n",
            "        self.__parameters__ = _type_vars(parameters)\n",
            "        self.__args__ = parameters\n",
            "        self.__origin__ = origin\n",
            "        # Pre-calculate the __hash__ on instantiation.\n",
            "        # This improves speed for complex substitutions.\n",
            "        subs_tree = self._subs_tree()\n",
            "        if isinstance(subs_tree, tuple):\n",
            "            self.__tree_hash__ = hash(frozenset(subs_tree))\n",
            "        else:\n",
            "            self.__tree_hash__ = hash(subs_tree)\n",
            "        return self\n",
            "\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        if self.__args__ is None:\n",
            "            return self\n",
            "        ev_args = tuple(_eval_type(t, globalns, localns) for t in self.__args__)\n",
            "        ev_origin = _eval_type(self.__origin__, globalns, localns)\n",
            "        if ev_args == self.__args__ and ev_origin == self.__origin__:\n",
            "            # Everything is already evaluated.\n",
            "            return self\n",
            "        return self.__class__(ev_args, ev_origin, _root=True)\n",
            "\n",
            "    def _get_type_vars(self, tvars):\n",
            "        if self.__origin__ and self.__parameters__:\n",
            "            _get_type_vars(self.__parameters__, tvars)\n",
            "\n",
            "    def __repr__(self):\n",
            "        if self.__origin__ is None:\n",
            "            return super().__repr__()\n",
            "        tree = self._subs_tree()\n",
            "        if not isinstance(tree, tuple):\n",
            "            return repr(tree)\n",
            "        return tree[0]._tree_repr(tree)\n",
            "\n",
            "    def _tree_repr(self, tree):\n",
            "        arg_list = []\n",
            "        for arg in tree[1:]:\n",
            "            if not isinstance(arg, tuple):\n",
            "                arg_list.append(_type_repr(arg))\n",
            "            else:\n",
            "                arg_list.append(arg[0]._tree_repr(arg))\n",
            "        return super().__repr__() + '[%s]' % ', '.join(arg_list)\n",
            "\n",
            "    @_tp_cache\n",
            "    def __getitem__(self, parameters):\n",
            "        if parameters == ():\n",
            "            raise TypeError(\"Cannot take a Union of no types.\")\n",
            "        if not isinstance(parameters, tuple):\n",
            "            parameters = (parameters,)\n",
            "        if self.__origin__ is None:\n",
            "            msg = \"Union[arg, ...]: each arg must be a type.\"\n",
            "        else:\n",
            "            msg = \"Parameters to generic types must be types.\"\n",
            "        parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "        if self is not Union:\n",
            "            _check_generic(self, parameters)\n",
            "        return self.__class__(parameters, origin=self, _root=True)\n",
            "\n",
            "    def _subs_tree(self, tvars=None, args=None):\n",
            "        if self is Union:\n",
            "            return Union  # Nothing to substitute\n",
            "        tree_args = _subs_tree(self, tvars, args)\n",
            "        tree_args = _remove_dups_flatten(tree_args)\n",
            "        if len(tree_args) == 1:\n",
            "            return tree_args[0]  # Union of a single type is that type\n",
            "        return (Union,) + tree_args\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if isinstance(other, _Union):\n",
            "            return self.__tree_hash__ == other.__tree_hash__\n",
            "        elif self is not Union:\n",
            "            return self._subs_tree() == other\n",
            "        else:\n",
            "            return self is other\n",
            "\n",
            "    def __hash__(self):\n",
            "        return self.__tree_hash__\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        raise TypeError(\"Unions cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        raise TypeError(\"Unions cannot be used with issubclass().\")\n",
            "\n",
            "\n",
            "Union = _Union(_root=True)\n",
            "\n",
            "\n",
            "class _Optional(_FinalTypingBase, _root=True):\n",
            "    \"\"\"Optional type.\n",
            "\n",
            "    Optional[X] is equivalent to Union[X, None].\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    @_tp_cache\n",
            "    def __getitem__(self, arg):\n",
            "        arg = _type_check(arg, \"Optional[t] requires a single type.\")\n",
            "        return Union[arg, type(None)]\n",
            "\n",
            "\n",
            "Optional = _Optional(_root=True)\n",
            "\n",
            "\n",
            "def _next_in_mro(cls):\n",
            "    \"\"\"Helper for Generic.__new__.\n",
            "\n",
            "    Returns the class after the last occurrence of Generic or\n",
            "    Generic[...] in cls.__mro__.\n",
            "    \"\"\"\n",
            "    next_in_mro = object\n",
            "    # Look for the last occurrence of Generic or Generic[...].\n",
            "    for i, c in enumerate(cls.__mro__[:-1]):\n",
            "        if isinstance(c, GenericMeta) and c._gorg is Generic:\n",
            "            next_in_mro = cls.__mro__[i + 1]\n",
            "    return next_in_mro\n",
            "\n",
            "\n",
            "def _make_subclasshook(cls):\n",
            "    \"\"\"Construct a __subclasshook__ callable that incorporates\n",
            "    the associated __extra__ class in subclass checks performed\n",
            "    against cls.\n",
            "    \"\"\"\n",
            "    if isinstance(cls.__extra__, abc.ABCMeta):\n",
            "        # The logic mirrors that of ABCMeta.__subclasscheck__.\n",
            "        # Registered classes need not be checked here because\n",
            "        # cls and its extra share the same _abc_registry.\n",
            "        def __extrahook__(subclass):\n",
            "            res = cls.__extra__.__subclasshook__(subclass)\n",
            "            if res is not NotImplemented:\n",
            "                return res\n",
            "            if cls.__extra__ in subclass.__mro__:\n",
            "                return True\n",
            "            for scls in cls.__extra__.__subclasses__():\n",
            "                if isinstance(scls, GenericMeta):\n",
            "                    continue\n",
            "                if issubclass(subclass, scls):\n",
            "                    return True\n",
            "            return NotImplemented\n",
            "    else:\n",
            "        # For non-ABC extras we'll just call issubclass().\n",
            "        def __extrahook__(subclass):\n",
            "            if cls.__extra__ and issubclass(subclass, cls.__extra__):\n",
            "                return True\n",
            "            return NotImplemented\n",
            "    return __extrahook__\n",
            "\n",
            "\n",
            "def _no_slots_copy(dct):\n",
            "    \"\"\"Internal helper: copy class __dict__ and clean slots class variables.\n",
            "    (They will be re-created if necessary by normal class machinery.)\n",
            "    \"\"\"\n",
            "    dict_copy = dict(dct)\n",
            "    if '__slots__' in dict_copy:\n",
            "        for slot in dict_copy['__slots__']:\n",
            "            dict_copy.pop(slot, None)\n",
            "    return dict_copy\n",
            "\n",
            "\n",
            "class GenericMeta(TypingMeta, abc.ABCMeta):\n",
            "    \"\"\"Metaclass for generic types.\n",
            "\n",
            "    This is a metaclass for typing.Generic and generic ABCs defined in\n",
            "    typing module. User defined subclasses of GenericMeta can override\n",
            "    __new__ and invoke super().__new__. Note that GenericMeta.__new__\n",
            "    has strict rules on what is allowed in its bases argument:\n",
            "    * plain Generic is disallowed in bases;\n",
            "    * Generic[...] should appear in bases at most once;\n",
            "    * if Generic[...] is present, then it should list all type variables\n",
            "      that appear in other bases.\n",
            "    In addition, type of all generic bases is erased, e.g., C[int] is\n",
            "    stripped to plain C.\n",
            "    \"\"\"\n",
            "\n",
            "    def __new__(cls, name, bases, namespace,\n",
            "                tvars=None, args=None, origin=None, extra=None, orig_bases=None):\n",
            "        \"\"\"Create a new generic class. GenericMeta.__new__ accepts\n",
            "        keyword arguments that are used for internal bookkeeping, therefore\n",
            "        an override should pass unused keyword arguments to super().\n",
            "        \"\"\"\n",
            "        if tvars is not None:\n",
            "            # Called from __getitem__() below.\n",
            "            assert origin is not None\n",
            "            assert all(isinstance(t, TypeVar) for t in tvars), tvars\n",
            "        else:\n",
            "            # Called from class statement.\n",
            "            assert tvars is None, tvars\n",
            "            assert args is None, args\n",
            "            assert origin is None, origin\n",
            "\n",
            "            # Get the full set of tvars from the bases.\n",
            "            tvars = _type_vars(bases)\n",
            "            # Look for Generic[T1, ..., Tn].\n",
            "            # If found, tvars must be a subset of it.\n",
            "            # If not found, tvars is it.\n",
            "            # Also check for and reject plain Generic,\n",
            "            # and reject multiple Generic[...].\n",
            "            gvars = None\n",
            "            for base in bases:\n",
            "                if base is Generic:\n",
            "                    raise TypeError(\"Cannot inherit from plain Generic\")\n",
            "                if (isinstance(base, GenericMeta) and\n",
            "                        base.__origin__ is Generic):\n",
            "                    if gvars is not None:\n",
            "                        raise TypeError(\n",
            "                            \"Cannot inherit from Generic[...] multiple types.\")\n",
            "                    gvars = base.__parameters__\n",
            "            if gvars is None:\n",
            "                gvars = tvars\n",
            "            else:\n",
            "                tvarset = set(tvars)\n",
            "                gvarset = set(gvars)\n",
            "                if not tvarset <= gvarset:\n",
            "                    raise TypeError(\n",
            "                        \"Some type variables (%s) \"\n",
            "                        \"are not listed in Generic[%s]\" %\n",
            "                        (\", \".join(str(t) for t in tvars if t not in gvarset),\n",
            "                         \", \".join(str(g) for g in gvars)))\n",
            "                tvars = gvars\n",
            "\n",
            "        initial_bases = bases\n",
            "        if extra is not None and type(extra) is abc.ABCMeta and extra not in bases:\n",
            "            bases = (extra,) + bases\n",
            "        bases = tuple(b._gorg if isinstance(b, GenericMeta) else b for b in bases)\n",
            "\n",
            "        # remove bare Generic from bases if there are other generic bases\n",
            "        if any(isinstance(b, GenericMeta) and b is not Generic for b in bases):\n",
            "            bases = tuple(b for b in bases if b is not Generic)\n",
            "        namespace.update({'__origin__': origin, '__extra__': extra,\n",
            "                          '_gorg': None if not origin else origin._gorg})\n",
            "        self = super().__new__(cls, name, bases, namespace, _root=True)\n",
            "        super(GenericMeta, self).__setattr__('_gorg',\n",
            "                                             self if not origin else origin._gorg)\n",
            "        self.__parameters__ = tvars\n",
            "        # Be prepared that GenericMeta will be subclassed by TupleMeta\n",
            "        # and CallableMeta, those two allow ..., (), or [] in __args___.\n",
            "        self.__args__ = tuple(... if a is _TypingEllipsis else\n",
            "                              () if a is _TypingEmpty else\n",
            "                              a for a in args) if args else None\n",
            "        # Speed hack (https://github.com/python/typing/issues/196).\n",
            "        self.__next_in_mro__ = _next_in_mro(self)\n",
            "        # Preserve base classes on subclassing (__bases__ are type erased now).\n",
            "        if orig_bases is None:\n",
            "            self.__orig_bases__ = initial_bases\n",
            "\n",
            "        # This allows unparameterized generic collections to be used\n",
            "        # with issubclass() and isinstance() in the same way as their\n",
            "        # collections.abc counterparts (e.g., isinstance([], Iterable)).\n",
            "        if (\n",
            "            '__subclasshook__' not in namespace and extra or\n",
            "            # allow overriding\n",
            "            getattr(self.__subclasshook__, '__name__', '') == '__extrahook__'\n",
            "        ):\n",
            "            self.__subclasshook__ = _make_subclasshook(self)\n",
            "        if isinstance(extra, abc.ABCMeta):\n",
            "            self._abc_registry = extra._abc_registry\n",
            "            self._abc_cache = extra._abc_cache\n",
            "        elif origin is not None:\n",
            "            self._abc_registry = origin._abc_registry\n",
            "            self._abc_cache = origin._abc_cache\n",
            "\n",
            "        if origin and hasattr(origin, '__qualname__'):  # Fix for Python 3.2.\n",
            "            self.__qualname__ = origin.__qualname__\n",
            "        self.__tree_hash__ = (hash(self._subs_tree()) if origin else\n",
            "                              super(GenericMeta, self).__hash__())\n",
            "        return self\n",
            "\n",
            "    # _abc_negative_cache and _abc_negative_cache_version\n",
            "    # realised as descriptors, since GenClass[t1, t2, ...] always\n",
            "    # share subclass info with GenClass.\n",
            "    # This is an important memory optimization.\n",
            "    @property\n",
            "    def _abc_negative_cache(self):\n",
            "        if isinstance(self.__extra__, abc.ABCMeta):\n",
            "            return self.__extra__._abc_negative_cache\n",
            "        return self._gorg._abc_generic_negative_cache\n",
            "\n",
            "    @_abc_negative_cache.setter\n",
            "    def _abc_negative_cache(self, value):\n",
            "        if self.__origin__ is None:\n",
            "            if isinstance(self.__extra__, abc.ABCMeta):\n",
            "                self.__extra__._abc_negative_cache = value\n",
            "            else:\n",
            "                self._abc_generic_negative_cache = value\n",
            "\n",
            "    @property\n",
            "    def _abc_negative_cache_version(self):\n",
            "        if isinstance(self.__extra__, abc.ABCMeta):\n",
            "            return self.__extra__._abc_negative_cache_version\n",
            "        return self._gorg._abc_generic_negative_cache_version\n",
            "\n",
            "    @_abc_negative_cache_version.setter\n",
            "    def _abc_negative_cache_version(self, value):\n",
            "        if self.__origin__ is None:\n",
            "            if isinstance(self.__extra__, abc.ABCMeta):\n",
            "                self.__extra__._abc_negative_cache_version = value\n",
            "            else:\n",
            "                self._abc_generic_negative_cache_version = value\n",
            "\n",
            "    def _get_type_vars(self, tvars):\n",
            "        if self.__origin__ and self.__parameters__:\n",
            "            _get_type_vars(self.__parameters__, tvars)\n",
            "\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        ev_origin = (self.__origin__._eval_type(globalns, localns)\n",
            "                     if self.__origin__ else None)\n",
            "        ev_args = tuple(_eval_type(a, globalns, localns) for a\n",
            "                        in self.__args__) if self.__args__ else None\n",
            "        if ev_origin == self.__origin__ and ev_args == self.__args__:\n",
            "            return self\n",
            "        return self.__class__(self.__name__,\n",
            "                              self.__bases__,\n",
            "                              _no_slots_copy(self.__dict__),\n",
            "                              tvars=_type_vars(ev_args) if ev_args else None,\n",
            "                              args=ev_args,\n",
            "                              origin=ev_origin,\n",
            "                              extra=self.__extra__,\n",
            "                              orig_bases=self.__orig_bases__)\n",
            "\n",
            "    def __repr__(self):\n",
            "        if self.__origin__ is None:\n",
            "            return super().__repr__()\n",
            "        return self._tree_repr(self._subs_tree())\n",
            "\n",
            "    def _tree_repr(self, tree):\n",
            "        arg_list = []\n",
            "        for arg in tree[1:]:\n",
            "            if arg == ():\n",
            "                arg_list.append('()')\n",
            "            elif not isinstance(arg, tuple):\n",
            "                arg_list.append(_type_repr(arg))\n",
            "            else:\n",
            "                arg_list.append(arg[0]._tree_repr(arg))\n",
            "        return super().__repr__() + '[%s]' % ', '.join(arg_list)\n",
            "\n",
            "    def _subs_tree(self, tvars=None, args=None):\n",
            "        if self.__origin__ is None:\n",
            "            return self\n",
            "        tree_args = _subs_tree(self, tvars, args)\n",
            "        return (self._gorg,) + tuple(tree_args)\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if not isinstance(other, GenericMeta):\n",
            "            return NotImplemented\n",
            "        if self.__origin__ is None or other.__origin__ is None:\n",
            "            return self is other\n",
            "        return self.__tree_hash__ == other.__tree_hash__\n",
            "\n",
            "    def __hash__(self):\n",
            "        return self.__tree_hash__\n",
            "\n",
            "    @_tp_cache\n",
            "    def __getitem__(self, params):\n",
            "        if not isinstance(params, tuple):\n",
            "            params = (params,)\n",
            "        if not params and self._gorg is not Tuple:\n",
            "            raise TypeError(\n",
            "                \"Parameter list to %s[...] cannot be empty\" % _qualname(self))\n",
            "        msg = \"Parameters to generic types must be types.\"\n",
            "        params = tuple(_type_check(p, msg) for p in params)\n",
            "        if self is Generic:\n",
            "            # Generic can only be subscripted with unique type variables.\n",
            "            if not all(isinstance(p, TypeVar) for p in params):\n",
            "                raise TypeError(\n",
            "                    \"Parameters to Generic[...] must all be type variables\")\n",
            "            if len(set(params)) != len(params):\n",
            "                raise TypeError(\n",
            "                    \"Parameters to Generic[...] must all be unique\")\n",
            "            tvars = params\n",
            "            args = params\n",
            "        elif self in (Tuple, Callable):\n",
            "            tvars = _type_vars(params)\n",
            "            args = params\n",
            "        elif self is _Protocol:\n",
            "            # _Protocol is internal, don't check anything.\n",
            "            tvars = params\n",
            "            args = params\n",
            "        elif self.__origin__ in (Generic, _Protocol):\n",
            "            # Can't subscript Generic[...] or _Protocol[...].\n",
            "            raise TypeError(\"Cannot subscript already-subscripted %s\" %\n",
            "                            repr(self))\n",
            "        else:\n",
            "            # Subscripting a regular Generic subclass.\n",
            "            _check_generic(self, params)\n",
            "            tvars = _type_vars(params)\n",
            "            args = params\n",
            "\n",
            "        prepend = (self,) if self.__origin__ is None else ()\n",
            "        return self.__class__(self.__name__,\n",
            "                              prepend + self.__bases__,\n",
            "                              _no_slots_copy(self.__dict__),\n",
            "                              tvars=tvars,\n",
            "                              args=args,\n",
            "                              origin=self,\n",
            "                              extra=self.__extra__,\n",
            "                              orig_bases=self.__orig_bases__)\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        if self.__origin__ is not None:\n",
            "            if sys._getframe(1).f_globals['__name__'] not in ['abc', 'functools']:\n",
            "                raise TypeError(\"Parameterized generics cannot be used with class \"\n",
            "                                \"or instance checks\")\n",
            "            return False\n",
            "        if self is Generic:\n",
            "            raise TypeError(\"Class %r cannot be used with class \"\n",
            "                            \"or instance checks\" % self)\n",
            "        return super().__subclasscheck__(cls)\n",
            "\n",
            "    def __instancecheck__(self, instance):\n",
            "        # Since we extend ABC.__subclasscheck__ and\n",
            "        # ABC.__instancecheck__ inlines the cache checking done by the\n",
            "        # latter, we must extend __instancecheck__ too. For simplicity\n",
            "        # we just skip the cache check -- instance checks for generic\n",
            "        # classes are supposed to be rare anyways.\n",
            "        return issubclass(instance.__class__, self)\n",
            "\n",
            "    def __setattr__(self, attr, value):\n",
            "        # We consider all the subscripted generics as proxies for original class\n",
            "        if (\n",
            "            attr.startswith('__') and attr.endswith('__') or\n",
            "            attr.startswith('_abc_') or\n",
            "            self._gorg is None  # The class is not fully created, see #typing/506\n",
            "        ):\n",
            "            super(GenericMeta, self).__setattr__(attr, value)\n",
            "        else:\n",
            "            super(GenericMeta, self._gorg).__setattr__(attr, value)\n",
            "\n",
            "\n",
            "# Prevent checks for Generic to crash when defining Generic.\n",
            "Generic = None\n",
            "\n",
            "\n",
            "def _generic_new(base_cls, cls, *args, **kwds):\n",
            "    # Assure type is erased on instantiation,\n",
            "    # but attempt to store it in __orig_class__\n",
            "    if cls.__origin__ is None:\n",
            "        if (base_cls.__new__ is object.__new__ and\n",
            "                cls.__init__ is not object.__init__):\n",
            "            return base_cls.__new__(cls)\n",
            "        else:\n",
            "            return base_cls.__new__(cls, *args, **kwds)\n",
            "    else:\n",
            "        origin = cls._gorg\n",
            "        if (base_cls.__new__ is object.__new__ and\n",
            "                cls.__init__ is not object.__init__):\n",
            "            obj = base_cls.__new__(origin)\n",
            "        else:\n",
            "            obj = base_cls.__new__(origin, *args, **kwds)\n",
            "        try:\n",
            "            obj.__orig_class__ = cls\n",
            "        except AttributeError:\n",
            "            pass\n",
            "        obj.__init__(*args, **kwds)\n",
            "        return obj\n",
            "\n",
            "\n",
            "class Generic(metaclass=GenericMeta):\n",
            "    \"\"\"Abstract base class for generic types.\n",
            "\n",
            "    A generic type is typically declared by inheriting from\n",
            "    this class parameterized with one or more type variables.\n",
            "    For example, a generic mapping type might be defined as::\n",
            "\n",
            "      class Mapping(Generic[KT, VT]):\n",
            "          def __getitem__(self, key: KT) -> VT:\n",
            "              ...\n",
            "          # Etc.\n",
            "\n",
            "    This class can then be used as follows::\n",
            "\n",
            "      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n",
            "          try:\n",
            "              return mapping[key]\n",
            "          except KeyError:\n",
            "              return default\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Generic:\n",
            "            raise TypeError(\"Type Generic cannot be instantiated; \"\n",
            "                            \"it can be used only as a base class\")\n",
            "        return _generic_new(cls.__next_in_mro__, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class _TypingEmpty:\n",
            "    \"\"\"Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n",
            "    to allow empty list/tuple in specific places, without allowing them\n",
            "    to sneak in where prohibited.\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "class _TypingEllipsis:\n",
            "    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n",
            "\n",
            "\n",
            "class TupleMeta(GenericMeta):\n",
            "    \"\"\"Metaclass for Tuple (internal).\"\"\"\n",
            "\n",
            "    @_tp_cache\n",
            "    def __getitem__(self, parameters):\n",
            "        if self.__origin__ is not None or self._gorg is not Tuple:\n",
            "            # Normal generic rules apply if this is not the first subscription\n",
            "            # or a subscription of a subclass.\n",
            "            return super().__getitem__(parameters)\n",
            "        if parameters == ():\n",
            "            return super().__getitem__((_TypingEmpty,))\n",
            "        if not isinstance(parameters, tuple):\n",
            "            parameters = (parameters,)\n",
            "        if len(parameters) == 2 and parameters[1] is ...:\n",
            "            msg = \"Tuple[t, ...]: t must be a type.\"\n",
            "            p = _type_check(parameters[0], msg)\n",
            "            return super().__getitem__((p, _TypingEllipsis))\n",
            "        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n",
            "        parameters = tuple(_type_check(p, msg) for p in parameters)\n",
            "        return super().__getitem__(parameters)\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        if self.__args__ is None:\n",
            "            return isinstance(obj, tuple)\n",
            "        raise TypeError(\"Parameterized Tuple cannot be used \"\n",
            "                        \"with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        if self.__args__ is None:\n",
            "            return issubclass(cls, tuple)\n",
            "        raise TypeError(\"Parameterized Tuple cannot be used \"\n",
            "                        \"with issubclass().\")\n",
            "\n",
            "\n",
            "class Tuple(tuple, extra=tuple, metaclass=TupleMeta):\n",
            "    \"\"\"Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n",
            "\n",
            "    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
            "    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
            "    of an int, a float and a string.\n",
            "\n",
            "    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Tuple:\n",
            "            raise TypeError(\"Type Tuple cannot be instantiated; \"\n",
            "                            \"use tuple() instead\")\n",
            "        return _generic_new(tuple, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class CallableMeta(GenericMeta):\n",
            "    \"\"\"Metaclass for Callable (internal).\"\"\"\n",
            "\n",
            "    def __repr__(self):\n",
            "        if self.__origin__ is None:\n",
            "            return super().__repr__()\n",
            "        return self._tree_repr(self._subs_tree())\n",
            "\n",
            "    def _tree_repr(self, tree):\n",
            "        if self._gorg is not Callable:\n",
            "            return super()._tree_repr(tree)\n",
            "        # For actual Callable (not its subclass) we override\n",
            "        # super()._tree_repr() for nice formatting.\n",
            "        arg_list = []\n",
            "        for arg in tree[1:]:\n",
            "            if not isinstance(arg, tuple):\n",
            "                arg_list.append(_type_repr(arg))\n",
            "            else:\n",
            "                arg_list.append(arg[0]._tree_repr(arg))\n",
            "        if arg_list[0] == '...':\n",
            "            return repr(tree[0]) + '[..., %s]' % arg_list[1]\n",
            "        return (repr(tree[0]) +\n",
            "                '[[%s], %s]' % (', '.join(arg_list[:-1]), arg_list[-1]))\n",
            "\n",
            "    def __getitem__(self, parameters):\n",
            "        \"\"\"A thin wrapper around __getitem_inner__ to provide the latter\n",
            "        with hashable arguments to improve speed.\n",
            "        \"\"\"\n",
            "\n",
            "        if self.__origin__ is not None or self._gorg is not Callable:\n",
            "            return super().__getitem__(parameters)\n",
            "        if not isinstance(parameters, tuple) or len(parameters) != 2:\n",
            "            raise TypeError(\"Callable must be used as \"\n",
            "                            \"Callable[[arg, ...], result].\")\n",
            "        args, result = parameters\n",
            "        if args is Ellipsis:\n",
            "            parameters = (Ellipsis, result)\n",
            "        else:\n",
            "            if not isinstance(args, list):\n",
            "                raise TypeError(\"Callable[args, result]: args must be a list.\"\n",
            "                                \" Got %.100r.\" % (args,))\n",
            "            parameters = (tuple(args), result)\n",
            "        return self.__getitem_inner__(parameters)\n",
            "\n",
            "    @_tp_cache\n",
            "    def __getitem_inner__(self, parameters):\n",
            "        args, result = parameters\n",
            "        msg = \"Callable[args, result]: result must be a type.\"\n",
            "        result = _type_check(result, msg)\n",
            "        if args is Ellipsis:\n",
            "            return super().__getitem__((_TypingEllipsis, result))\n",
            "        msg = \"Callable[[arg, ...], result]: each arg must be a type.\"\n",
            "        args = tuple(_type_check(arg, msg) for arg in args)\n",
            "        parameters = args + (result,)\n",
            "        return super().__getitem__(parameters)\n",
            "\n",
            "\n",
            "class Callable(extra=collections_abc.Callable, metaclass=CallableMeta):\n",
            "    \"\"\"Callable type; Callable[[int], str] is a function of (int) -> str.\n",
            "\n",
            "    The subscription syntax must always be used with exactly two\n",
            "    values: the argument list and the return type.  The argument list\n",
            "    must be a list of types or ellipsis; the return type must be a single type.\n",
            "\n",
            "    There is no syntax to indicate optional or keyword arguments,\n",
            "    such function types are rarely used as callback types.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Callable:\n",
            "            raise TypeError(\"Type Callable cannot be instantiated; \"\n",
            "                            \"use a non-abstract subclass instead\")\n",
            "        return _generic_new(cls.__next_in_mro__, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class _ClassVar(_FinalTypingBase, _root=True):\n",
            "    \"\"\"Special type construct to mark class variables.\n",
            "\n",
            "    An annotation wrapped in ClassVar indicates that a given\n",
            "    attribute is intended to be used as a class variable and\n",
            "    should not be set on instances of that class. Usage::\n",
            "\n",
            "      class Starship:\n",
            "          stats: ClassVar[Dict[str, int]] = {} # class variable\n",
            "          damage: int = 10                     # instance variable\n",
            "\n",
            "    ClassVar accepts only types and cannot be further subscribed.\n",
            "\n",
            "    Note that ClassVar is not a class itself, and should not\n",
            "    be used with isinstance() or issubclass().\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ('__type__',)\n",
            "\n",
            "    def __init__(self, tp=None, **kwds):\n",
            "        self.__type__ = tp\n",
            "\n",
            "    def __getitem__(self, item):\n",
            "        cls = type(self)\n",
            "        if self.__type__ is None:\n",
            "            return cls(_type_check(item,\n",
            "                       '{} accepts only single type.'.format(cls.__name__[1:])),\n",
            "                       _root=True)\n",
            "        raise TypeError('{} cannot be further subscripted'\n",
            "                        .format(cls.__name__[1:]))\n",
            "\n",
            "    def _eval_type(self, globalns, localns):\n",
            "        new_tp = _eval_type(self.__type__, globalns, localns)\n",
            "        if new_tp == self.__type__:\n",
            "            return self\n",
            "        return type(self)(new_tp, _root=True)\n",
            "\n",
            "    def __repr__(self):\n",
            "        r = super().__repr__()\n",
            "        if self.__type__ is not None:\n",
            "            r += '[{}]'.format(_type_repr(self.__type__))\n",
            "        return r\n",
            "\n",
            "    def __hash__(self):\n",
            "        return hash((type(self).__name__, self.__type__))\n",
            "\n",
            "    def __eq__(self, other):\n",
            "        if not isinstance(other, _ClassVar):\n",
            "            return NotImplemented\n",
            "        if self.__type__ is not None:\n",
            "            return self.__type__ == other.__type__\n",
            "        return self is other\n",
            "\n",
            "\n",
            "ClassVar = _ClassVar(_root=True)\n",
            "\n",
            "\n",
            "def cast(typ, val):\n",
            "    \"\"\"Cast a value to a type.\n",
            "\n",
            "    This returns the value unchanged.  To the type checker this\n",
            "    signals that the return value has the designated type, but at\n",
            "    runtime we intentionally don't check anything (we want this\n",
            "    to be as fast as possible).\n",
            "    \"\"\"\n",
            "    return val\n",
            "\n",
            "\n",
            "def _get_defaults(func):\n",
            "    \"\"\"Internal helper to extract the default arguments, by name.\"\"\"\n",
            "    try:\n",
            "        code = func.__code__\n",
            "    except AttributeError:\n",
            "        # Some built-in functions don't have __code__, __defaults__, etc.\n",
            "        return {}\n",
            "    pos_count = code.co_argcount\n",
            "    arg_names = code.co_varnames\n",
            "    arg_names = arg_names[:pos_count]\n",
            "    defaults = func.__defaults__ or ()\n",
            "    kwdefaults = func.__kwdefaults__\n",
            "    res = dict(kwdefaults) if kwdefaults else {}\n",
            "    pos_offset = pos_count - len(defaults)\n",
            "    for name, value in zip(arg_names[pos_offset:], defaults):\n",
            "        assert name not in res\n",
            "        res[name] = value\n",
            "    return res\n",
            "\n",
            "\n",
            "_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n",
            "                  types.MethodType, types.ModuleType,\n",
            "                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n",
            "\n",
            "\n",
            "def get_type_hints(obj, globalns=None, localns=None):\n",
            "    \"\"\"Return type hints for an object.\n",
            "\n",
            "    This is often the same as obj.__annotations__, but it handles\n",
            "    forward references encoded as string literals, and if necessary\n",
            "    adds Optional[t] if a default value equal to None is set.\n",
            "\n",
            "    The argument may be a module, class, method, or function. The annotations\n",
            "    are returned as a dictionary. For classes, annotations include also\n",
            "    inherited members.\n",
            "\n",
            "    TypeError is raised if the argument is not of a type that can contain\n",
            "    annotations, and an empty dictionary is returned if no annotations are\n",
            "    present.\n",
            "\n",
            "    BEWARE -- the behavior of globalns and localns is counterintuitive\n",
            "    (unless you are familiar with how eval() and exec() work).  The\n",
            "    search order is locals first, then globals.\n",
            "\n",
            "    - If no dict arguments are passed, an attempt is made to use the\n",
            "      globals from obj (or the respective module's globals for classes),\n",
            "      and these are also used as the locals.  If the object does not appear\n",
            "      to have globals, an empty dictionary is used.\n",
            "\n",
            "    - If one dict argument is passed, it is used for both globals and\n",
            "      locals.\n",
            "\n",
            "    - If two dict arguments are passed, they specify globals and\n",
            "      locals, respectively.\n",
            "    \"\"\"\n",
            "\n",
            "    if getattr(obj, '__no_type_check__', None):\n",
            "        return {}\n",
            "    # Classes require a special treatment.\n",
            "    if isinstance(obj, type):\n",
            "        hints = {}\n",
            "        for base in reversed(obj.__mro__):\n",
            "            if globalns is None:\n",
            "                base_globals = sys.modules[base.__module__].__dict__\n",
            "            else:\n",
            "                base_globals = globalns\n",
            "            ann = base.__dict__.get('__annotations__', {})\n",
            "            for name, value in ann.items():\n",
            "                if value is None:\n",
            "                    value = type(None)\n",
            "                if isinstance(value, str):\n",
            "                    value = _ForwardRef(value)\n",
            "                value = _eval_type(value, base_globals, localns)\n",
            "                hints[name] = value\n",
            "        return hints\n",
            "\n",
            "    if globalns is None:\n",
            "        if isinstance(obj, types.ModuleType):\n",
            "            globalns = obj.__dict__\n",
            "        else:\n",
            "            globalns = getattr(obj, '__globals__', {})\n",
            "        if localns is None:\n",
            "            localns = globalns\n",
            "    elif localns is None:\n",
            "        localns = globalns\n",
            "    hints = getattr(obj, '__annotations__', None)\n",
            "    if hints is None:\n",
            "        # Return empty annotations for something that _could_ have them.\n",
            "        if isinstance(obj, _allowed_types):\n",
            "            return {}\n",
            "        else:\n",
            "            raise TypeError('{!r} is not a module, class, method, '\n",
            "                            'or function.'.format(obj))\n",
            "    defaults = _get_defaults(obj)\n",
            "    hints = dict(hints)\n",
            "    for name, value in hints.items():\n",
            "        if value is None:\n",
            "            value = type(None)\n",
            "        if isinstance(value, str):\n",
            "            value = _ForwardRef(value)\n",
            "        value = _eval_type(value, globalns, localns)\n",
            "        if name in defaults and defaults[name] is None:\n",
            "            value = Optional[value]\n",
            "        hints[name] = value\n",
            "    return hints\n",
            "\n",
            "\n",
            "def no_type_check(arg):\n",
            "    \"\"\"Decorator to indicate that annotations are not type hints.\n",
            "\n",
            "    The argument must be a class or function; if it is a class, it\n",
            "    applies recursively to all methods and classes defined in that class\n",
            "    (but not to methods defined in its superclasses or subclasses).\n",
            "\n",
            "    This mutates the function(s) or class(es) in place.\n",
            "    \"\"\"\n",
            "    if isinstance(arg, type):\n",
            "        arg_attrs = arg.__dict__.copy()\n",
            "        for attr, val in arg.__dict__.items():\n",
            "            if val in arg.__bases__ + (arg,):\n",
            "                arg_attrs.pop(attr)\n",
            "        for obj in arg_attrs.values():\n",
            "            if isinstance(obj, types.FunctionType):\n",
            "                obj.__no_type_check__ = True\n",
            "            if isinstance(obj, type):\n",
            "                no_type_check(obj)\n",
            "    try:\n",
            "        arg.__no_type_check__ = True\n",
            "    except TypeError:  # built-in classes\n",
            "        pass\n",
            "    return arg\n",
            "\n",
            "\n",
            "def no_type_check_decorator(decorator):\n",
            "    \"\"\"Decorator to give another decorator the @no_type_check effect.\n",
            "\n",
            "    This wraps the decorator with something that wraps the decorated\n",
            "    function in @no_type_check.\n",
            "    \"\"\"\n",
            "\n",
            "    @functools.wraps(decorator)\n",
            "    def wrapped_decorator(*args, **kwds):\n",
            "        func = decorator(*args, **kwds)\n",
            "        func = no_type_check(func)\n",
            "        return func\n",
            "\n",
            "    return wrapped_decorator\n",
            "\n",
            "\n",
            "def _overload_dummy(*args, **kwds):\n",
            "    \"\"\"Helper for @overload to raise when called.\"\"\"\n",
            "    raise NotImplementedError(\n",
            "        \"You should not call an overloaded function. \"\n",
            "        \"A series of @overload-decorated functions \"\n",
            "        \"outside a stub module should always be followed \"\n",
            "        \"by an implementation that is not @overload-ed.\")\n",
            "\n",
            "\n",
            "def overload(func):\n",
            "    \"\"\"Decorator for overloaded functions/methods.\n",
            "\n",
            "    In a stub file, place two or more stub definitions for the same\n",
            "    function in a row, each decorated with @overload.  For example:\n",
            "\n",
            "      @overload\n",
            "      def utf8(value: None) -> None: ...\n",
            "      @overload\n",
            "      def utf8(value: bytes) -> bytes: ...\n",
            "      @overload\n",
            "      def utf8(value: str) -> bytes: ...\n",
            "\n",
            "    In a non-stub file (i.e. a regular .py file), do the same but\n",
            "    follow it with an implementation.  The implementation should *not*\n",
            "    be decorated with @overload.  For example:\n",
            "\n",
            "      @overload\n",
            "      def utf8(value: None) -> None: ...\n",
            "      @overload\n",
            "      def utf8(value: bytes) -> bytes: ...\n",
            "      @overload\n",
            "      def utf8(value: str) -> bytes: ...\n",
            "      def utf8(value):\n",
            "          # implementation goes here\n",
            "    \"\"\"\n",
            "    return _overload_dummy\n",
            "\n",
            "\n",
            "class _ProtocolMeta(GenericMeta):\n",
            "    \"\"\"Internal metaclass for _Protocol.\n",
            "\n",
            "    This exists so _Protocol classes can be generic without deriving\n",
            "    from Generic.\n",
            "    \"\"\"\n",
            "\n",
            "    def __instancecheck__(self, obj):\n",
            "        if _Protocol not in self.__bases__:\n",
            "            return super().__instancecheck__(obj)\n",
            "        raise TypeError(\"Protocols cannot be used with isinstance().\")\n",
            "\n",
            "    def __subclasscheck__(self, cls):\n",
            "        if not self._is_protocol:\n",
            "            # No structural checks since this isn't a protocol.\n",
            "            return NotImplemented\n",
            "\n",
            "        if self is _Protocol:\n",
            "            # Every class is a subclass of the empty protocol.\n",
            "            return True\n",
            "\n",
            "        # Find all attributes defined in the protocol.\n",
            "        attrs = self._get_protocol_attrs()\n",
            "\n",
            "        for attr in attrs:\n",
            "            if not any(attr in d.__dict__ for d in cls.__mro__):\n",
            "                return False\n",
            "        return True\n",
            "\n",
            "    def _get_protocol_attrs(self):\n",
            "        # Get all Protocol base classes.\n",
            "        protocol_bases = []\n",
            "        for c in self.__mro__:\n",
            "            if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':\n",
            "                protocol_bases.append(c)\n",
            "\n",
            "        # Get attributes included in protocol.\n",
            "        attrs = set()\n",
            "        for base in protocol_bases:\n",
            "            for attr in base.__dict__.keys():\n",
            "                # Include attributes not defined in any non-protocol bases.\n",
            "                for c in self.__mro__:\n",
            "                    if (c is not base and attr in c.__dict__ and\n",
            "                            not getattr(c, '_is_protocol', False)):\n",
            "                        break\n",
            "                else:\n",
            "                    if (not attr.startswith('_abc_') and\n",
            "                            attr != '__abstractmethods__' and\n",
            "                            attr != '__annotations__' and\n",
            "                            attr != '__weakref__' and\n",
            "                            attr != '_is_protocol' and\n",
            "                            attr != '_gorg' and\n",
            "                            attr != '__dict__' and\n",
            "                            attr != '__args__' and\n",
            "                            attr != '__slots__' and\n",
            "                            attr != '_get_protocol_attrs' and\n",
            "                            attr != '__next_in_mro__' and\n",
            "                            attr != '__parameters__' and\n",
            "                            attr != '__origin__' and\n",
            "                            attr != '__orig_bases__' and\n",
            "                            attr != '__extra__' and\n",
            "                            attr != '__tree_hash__' and\n",
            "                            attr != '__module__'):\n",
            "                        attrs.add(attr)\n",
            "\n",
            "        return attrs\n",
            "\n",
            "\n",
            "class _Protocol(metaclass=_ProtocolMeta):\n",
            "    \"\"\"Internal base class for protocol classes.\n",
            "\n",
            "    This implements a simple-minded structural issubclass check\n",
            "    (similar but more general than the one-offs in collections.abc\n",
            "    such as Hashable).\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    _is_protocol = True\n",
            "\n",
            "\n",
            "# Various ABCs mimicking those in collections.abc.\n",
            "# A few are simply re-exported for completeness.\n",
            "\n",
            "Hashable = collections_abc.Hashable  # Not generic.\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'Awaitable'):\n",
            "    class Awaitable(Generic[T_co], extra=collections_abc.Awaitable):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('Awaitable')\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'Coroutine'):\n",
            "    class Coroutine(Awaitable[V_co], Generic[T_co, T_contra, V_co],\n",
            "                    extra=collections_abc.Coroutine):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('Coroutine')\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'AsyncIterable'):\n",
            "\n",
            "    class AsyncIterable(Generic[T_co], extra=collections_abc.AsyncIterable):\n",
            "        __slots__ = ()\n",
            "\n",
            "    class AsyncIterator(AsyncIterable[T_co],\n",
            "                        extra=collections_abc.AsyncIterator):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('AsyncIterable')\n",
            "    __all__.append('AsyncIterator')\n",
            "\n",
            "\n",
            "class Iterable(Generic[T_co], extra=collections_abc.Iterable):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class Iterator(Iterable[T_co], extra=collections_abc.Iterator):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class SupportsInt(_Protocol):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __int__(self) -> int:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsFloat(_Protocol):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __float__(self) -> float:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsComplex(_Protocol):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __complex__(self) -> complex:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsBytes(_Protocol):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __bytes__(self) -> bytes:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsIndex(_Protocol):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __index__(self) -> int:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsAbs(_Protocol[T_co]):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __abs__(self) -> T_co:\n",
            "        pass\n",
            "\n",
            "\n",
            "class SupportsRound(_Protocol[T_co]):\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def __round__(self, ndigits: int = 0) -> T_co:\n",
            "        pass\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'Reversible'):\n",
            "    class Reversible(Iterable[T_co], extra=collections_abc.Reversible):\n",
            "        __slots__ = ()\n",
            "else:\n",
            "    class Reversible(_Protocol[T_co]):\n",
            "        __slots__ = ()\n",
            "\n",
            "        @abstractmethod\n",
            "        def __reversed__(self) -> 'Iterator[T_co]':\n",
            "            pass\n",
            "\n",
            "\n",
            "Sized = collections_abc.Sized  # Not generic.\n",
            "\n",
            "\n",
            "class Container(Generic[T_co], extra=collections_abc.Container):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'Collection'):\n",
            "    class Collection(Sized, Iterable[T_co], Container[T_co],\n",
            "                     extra=collections_abc.Collection):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('Collection')\n",
            "\n",
            "\n",
            "# Callable was defined earlier.\n",
            "\n",
            "if hasattr(collections_abc, 'Collection'):\n",
            "    class AbstractSet(Collection[T_co],\n",
            "                      extra=collections_abc.Set):\n",
            "        __slots__ = ()\n",
            "else:\n",
            "    class AbstractSet(Sized, Iterable[T_co], Container[T_co],\n",
            "                      extra=collections_abc.Set):\n",
            "        __slots__ = ()\n",
            "\n",
            "\n",
            "class MutableSet(AbstractSet[T], extra=collections_abc.MutableSet):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "# NOTE: It is only covariant in the value type.\n",
            "if hasattr(collections_abc, 'Collection'):\n",
            "    class Mapping(Collection[KT], Generic[KT, VT_co],\n",
            "                  extra=collections_abc.Mapping):\n",
            "        __slots__ = ()\n",
            "else:\n",
            "    class Mapping(Sized, Iterable[KT], Container[KT], Generic[KT, VT_co],\n",
            "                  extra=collections_abc.Mapping):\n",
            "        __slots__ = ()\n",
            "\n",
            "\n",
            "class MutableMapping(Mapping[KT, VT], extra=collections_abc.MutableMapping):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'Reversible'):\n",
            "    if hasattr(collections_abc, 'Collection'):\n",
            "        class Sequence(Reversible[T_co], Collection[T_co],\n",
            "                       extra=collections_abc.Sequence):\n",
            "            __slots__ = ()\n",
            "    else:\n",
            "        class Sequence(Sized, Reversible[T_co], Container[T_co],\n",
            "                       extra=collections_abc.Sequence):\n",
            "            __slots__ = ()\n",
            "else:\n",
            "    class Sequence(Sized, Iterable[T_co], Container[T_co],\n",
            "                   extra=collections_abc.Sequence):\n",
            "        __slots__ = ()\n",
            "\n",
            "\n",
            "class MutableSequence(Sequence[T], extra=collections_abc.MutableSequence):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class ByteString(Sequence[int], extra=collections_abc.ByteString):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class List(list, MutableSequence[T], extra=list):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is List:\n",
            "            raise TypeError(\"Type List cannot be instantiated; \"\n",
            "                            \"use list() instead\")\n",
            "        return _generic_new(list, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class Deque(collections.deque, MutableSequence[T], extra=collections.deque):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Deque:\n",
            "            return collections.deque(*args, **kwds)\n",
            "        return _generic_new(collections.deque, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class Set(set, MutableSet[T], extra=set):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Set:\n",
            "            raise TypeError(\"Type Set cannot be instantiated; \"\n",
            "                            \"use set() instead\")\n",
            "        return _generic_new(set, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class FrozenSet(frozenset, AbstractSet[T_co], extra=frozenset):\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is FrozenSet:\n",
            "            raise TypeError(\"Type FrozenSet cannot be instantiated; \"\n",
            "                            \"use frozenset() instead\")\n",
            "        return _generic_new(frozenset, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class MappingView(Sized, Iterable[T_co], extra=collections_abc.MappingView):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class KeysView(MappingView[KT], AbstractSet[KT],\n",
            "               extra=collections_abc.KeysView):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class ItemsView(MappingView[Tuple[KT, VT_co]],\n",
            "                AbstractSet[Tuple[KT, VT_co]],\n",
            "                Generic[KT, VT_co],\n",
            "                extra=collections_abc.ItemsView):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "class ValuesView(MappingView[VT_co], extra=collections_abc.ValuesView):\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "if hasattr(contextlib, 'AbstractContextManager'):\n",
            "    class ContextManager(Generic[T_co], extra=contextlib.AbstractContextManager):\n",
            "        __slots__ = ()\n",
            "else:\n",
            "    class ContextManager(Generic[T_co]):\n",
            "        __slots__ = ()\n",
            "\n",
            "        def __enter__(self):\n",
            "            return self\n",
            "\n",
            "        @abc.abstractmethod\n",
            "        def __exit__(self, exc_type, exc_value, traceback):\n",
            "            return None\n",
            "\n",
            "        @classmethod\n",
            "        def __subclasshook__(cls, C):\n",
            "            if cls is ContextManager:\n",
            "                # In Python 3.6+, it is possible to set a method to None to\n",
            "                # explicitly indicate that the class does not implement an ABC\n",
            "                # (https://bugs.python.org/issue25958), but we do not support\n",
            "                # that pattern here because this fallback class is only used\n",
            "                # in Python 3.5 and earlier.\n",
            "                if (any(\"__enter__\" in B.__dict__ for B in C.__mro__) and\n",
            "                    any(\"__exit__\" in B.__dict__ for B in C.__mro__)):\n",
            "                    return True\n",
            "            return NotImplemented\n",
            "\n",
            "\n",
            "if hasattr(contextlib, 'AbstractAsyncContextManager'):\n",
            "    class AsyncContextManager(Generic[T_co],\n",
            "                              extra=contextlib.AbstractAsyncContextManager):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('AsyncContextManager')\n",
            "elif sys.version_info[:2] >= (3, 5):\n",
            "    exec(\"\"\"\n",
            "class AsyncContextManager(Generic[T_co]):\n",
            "    __slots__ = ()\n",
            "\n",
            "    async def __aenter__(self):\n",
            "        return self\n",
            "\n",
            "    @abc.abstractmethod\n",
            "    async def __aexit__(self, exc_type, exc_value, traceback):\n",
            "        return None\n",
            "\n",
            "    @classmethod\n",
            "    def __subclasshook__(cls, C):\n",
            "        if cls is AsyncContextManager:\n",
            "            if sys.version_info[:2] >= (3, 6):\n",
            "                return _collections_abc._check_methods(C, \"__aenter__\", \"__aexit__\")\n",
            "            if (any(\"__aenter__\" in B.__dict__ for B in C.__mro__) and\n",
            "                    any(\"__aexit__\" in B.__dict__ for B in C.__mro__)):\n",
            "                return True\n",
            "        return NotImplemented\n",
            "\n",
            "__all__.append('AsyncContextManager')\n",
            "\"\"\")\n",
            "\n",
            "\n",
            "class Dict(dict, MutableMapping[KT, VT], extra=dict):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Dict:\n",
            "            raise TypeError(\"Type Dict cannot be instantiated; \"\n",
            "                            \"use dict() instead\")\n",
            "        return _generic_new(dict, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class DefaultDict(collections.defaultdict, MutableMapping[KT, VT],\n",
            "                  extra=collections.defaultdict):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is DefaultDict:\n",
            "            return collections.defaultdict(*args, **kwds)\n",
            "        return _generic_new(collections.defaultdict, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "class Counter(collections.Counter, Dict[T, int], extra=collections.Counter):\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Counter:\n",
            "            return collections.Counter(*args, **kwds)\n",
            "        return _generic_new(collections.Counter, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "if hasattr(collections, 'ChainMap'):\n",
            "    # ChainMap only exists in 3.3+\n",
            "    __all__.append('ChainMap')\n",
            "\n",
            "    class ChainMap(collections.ChainMap, MutableMapping[KT, VT],\n",
            "                   extra=collections.ChainMap):\n",
            "\n",
            "        __slots__ = ()\n",
            "\n",
            "        def __new__(cls, *args, **kwds):\n",
            "            if cls._gorg is ChainMap:\n",
            "                return collections.ChainMap(*args, **kwds)\n",
            "            return _generic_new(collections.ChainMap, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "# Determine what base class to use for Generator.\n",
            "if hasattr(collections_abc, 'Generator'):\n",
            "    # Sufficiently recent versions of 3.5 have a Generator ABC.\n",
            "    _G_base = collections_abc.Generator\n",
            "else:\n",
            "    # Fall back on the exact type.\n",
            "    _G_base = types.GeneratorType\n",
            "\n",
            "\n",
            "class Generator(Iterator[T_co], Generic[T_co, T_contra, V_co],\n",
            "                extra=_G_base):\n",
            "    __slots__ = ()\n",
            "\n",
            "    def __new__(cls, *args, **kwds):\n",
            "        if cls._gorg is Generator:\n",
            "            raise TypeError(\"Type Generator cannot be instantiated; \"\n",
            "                            \"create a subclass instead\")\n",
            "        return _generic_new(_G_base, cls, *args, **kwds)\n",
            "\n",
            "\n",
            "if hasattr(collections_abc, 'AsyncGenerator'):\n",
            "    class AsyncGenerator(AsyncIterator[T_co], Generic[T_co, T_contra],\n",
            "                         extra=collections_abc.AsyncGenerator):\n",
            "        __slots__ = ()\n",
            "\n",
            "    __all__.append('AsyncGenerator')\n",
            "\n",
            "\n",
            "# Internal type variable used for Type[].\n",
            "CT_co = TypeVar('CT_co', covariant=True, bound=type)\n",
            "\n",
            "\n",
            "# This is not a real generic class.  Don't use outside annotations.\n",
            "class Type(Generic[CT_co], extra=type):\n",
            "    \"\"\"A special construct usable to annotate class objects.\n",
            "\n",
            "    For example, suppose we have the following classes::\n",
            "\n",
            "      class User: ...  # Abstract base for User classes\n",
            "      class BasicUser(User): ...\n",
            "      class ProUser(User): ...\n",
            "      class TeamUser(User): ...\n",
            "\n",
            "    And a function that takes a class argument that's a subclass of\n",
            "    User and returns an instance of the corresponding class::\n",
            "\n",
            "      U = TypeVar('U', bound=User)\n",
            "      def new_user(user_class: Type[U]) -> U:\n",
            "          user = user_class()\n",
            "          # (Here we could write the user object to a database)\n",
            "          return user\n",
            "\n",
            "      joe = new_user(BasicUser)\n",
            "\n",
            "    At this point the type checker knows that joe has type BasicUser.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "\n",
            "def _make_nmtuple(name, types):\n",
            "    msg = \"NamedTuple('Name', [(f0, t0), (f1, t1), ...]); each t must be a type\"\n",
            "    types = [(n, _type_check(t, msg)) for n, t in types]\n",
            "    nm_tpl = collections.namedtuple(name, [n for n, t in types])\n",
            "    # Prior to PEP 526, only _field_types attribute was assigned.\n",
            "    # Now, both __annotations__ and _field_types are used to maintain compatibility.\n",
            "    nm_tpl.__annotations__ = nm_tpl._field_types = collections.OrderedDict(types)\n",
            "    try:\n",
            "        nm_tpl.__module__ = sys._getframe(2).f_globals.get('__name__', '__main__')\n",
            "    except (AttributeError, ValueError):\n",
            "        pass\n",
            "    return nm_tpl\n",
            "\n",
            "\n",
            "_PY36 = sys.version_info[:2] >= (3, 6)\n",
            "\n",
            "# attributes prohibited to set in NamedTuple class syntax\n",
            "_prohibited = ('__new__', '__init__', '__slots__', '__getnewargs__',\n",
            "               '_fields', '_field_defaults', '_field_types',\n",
            "               '_make', '_replace', '_asdict', '_source')\n",
            "\n",
            "_special = ('__module__', '__name__', '__qualname__', '__annotations__')\n",
            "\n",
            "\n",
            "class NamedTupleMeta(type):\n",
            "\n",
            "    def __new__(cls, typename, bases, ns):\n",
            "        if ns.get('_root', False):\n",
            "            return super().__new__(cls, typename, bases, ns)\n",
            "        if not _PY36:\n",
            "            raise TypeError(\"Class syntax for NamedTuple is only supported\"\n",
            "                            \" in Python 3.6+\")\n",
            "        types = ns.get('__annotations__', {})\n",
            "        nm_tpl = _make_nmtuple(typename, types.items())\n",
            "        defaults = []\n",
            "        defaults_dict = {}\n",
            "        for field_name in types:\n",
            "            if field_name in ns:\n",
            "                default_value = ns[field_name]\n",
            "                defaults.append(default_value)\n",
            "                defaults_dict[field_name] = default_value\n",
            "            elif defaults:\n",
            "                raise TypeError(\"Non-default namedtuple field {field_name} cannot \"\n",
            "                                \"follow default field(s) {default_names}\"\n",
            "                                .format(field_name=field_name,\n",
            "                                        default_names=', '.join(defaults_dict.keys())))\n",
            "        nm_tpl.__new__.__annotations__ = collections.OrderedDict(types)\n",
            "        nm_tpl.__new__.__defaults__ = tuple(defaults)\n",
            "        nm_tpl._field_defaults = defaults_dict\n",
            "        # update from user namespace without overriding special namedtuple attributes\n",
            "        for key in ns:\n",
            "            if key in _prohibited:\n",
            "                raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n",
            "            elif key not in _special and key not in nm_tpl._fields:\n",
            "                setattr(nm_tpl, key, ns[key])\n",
            "        return nm_tpl\n",
            "\n",
            "\n",
            "class NamedTuple(metaclass=NamedTupleMeta):\n",
            "    \"\"\"Typed version of namedtuple.\n",
            "\n",
            "    Usage in Python versions >= 3.6::\n",
            "\n",
            "        class Employee(NamedTuple):\n",
            "            name: str\n",
            "            id: int\n",
            "\n",
            "    This is equivalent to::\n",
            "\n",
            "        Employee = collections.namedtuple('Employee', ['name', 'id'])\n",
            "\n",
            "    The resulting class has extra __annotations__ and _field_types\n",
            "    attributes, giving an ordered dict mapping field names to types.\n",
            "    __annotations__ should be preferred, while _field_types\n",
            "    is kept to maintain pre PEP 526 compatibility. (The field names\n",
            "    are in the _fields attribute, which is part of the namedtuple\n",
            "    API.) Alternative equivalent keyword syntax is also accepted::\n",
            "\n",
            "        Employee = NamedTuple('Employee', name=str, id=int)\n",
            "\n",
            "    In Python versions <= 3.5 use::\n",
            "\n",
            "        Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n",
            "    \"\"\"\n",
            "    _root = True\n",
            "\n",
            "    def __new__(self, typename, fields=None, **kwargs):\n",
            "        if kwargs and not _PY36:\n",
            "            raise TypeError(\"Keyword syntax for NamedTuple is only supported\"\n",
            "                            \" in Python 3.6+\")\n",
            "        if fields is None:\n",
            "            fields = kwargs.items()\n",
            "        elif kwargs:\n",
            "            raise TypeError(\"Either list of fields or keywords\"\n",
            "                            \" can be provided to NamedTuple, not both\")\n",
            "        return _make_nmtuple(typename, fields)\n",
            "\n",
            "\n",
            "def NewType(name, tp):\n",
            "    \"\"\"NewType creates simple unique types with almost zero\n",
            "    runtime overhead. NewType(name, tp) is considered a subtype of tp\n",
            "    by static type checkers. At runtime, NewType(name, tp) returns\n",
            "    a dummy function that simply returns its argument. Usage::\n",
            "\n",
            "        UserId = NewType('UserId', int)\n",
            "\n",
            "        def name_by_id(user_id: UserId) -> str:\n",
            "            ...\n",
            "\n",
            "        UserId('user')          # Fails type check\n",
            "\n",
            "        name_by_id(42)          # Fails type check\n",
            "        name_by_id(UserId(42))  # OK\n",
            "\n",
            "        num = UserId(5) + 1     # type: int\n",
            "    \"\"\"\n",
            "\n",
            "    def new_type(x):\n",
            "        return x\n",
            "\n",
            "    new_type.__name__ = name\n",
            "    new_type.__supertype__ = tp\n",
            "    return new_type\n",
            "\n",
            "\n",
            "# Python-version-specific alias (Python 2: unicode; Python 3: str)\n",
            "Text = str\n",
            "\n",
            "\n",
            "# Constant that's True when type checking, but False here.\n",
            "TYPE_CHECKING = False\n",
            "\n",
            "\n",
            "class IO(Generic[AnyStr]):\n",
            "    \"\"\"Generic base class for TextIO and BinaryIO.\n",
            "\n",
            "    This is an abstract, generic version of the return of open().\n",
            "\n",
            "    NOTE: This does not distinguish between the different possible\n",
            "    classes (text vs. binary, read vs. write vs. read/write,\n",
            "    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n",
            "    below capture the distinctions between text vs. binary, which is\n",
            "    pervasive in the interface; however we currently do not offer a\n",
            "    way to track the other distinctions in the type system.\n",
            "    \"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractproperty\n",
            "    def mode(self) -> str:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def name(self) -> str:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def close(self) -> None:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def closed(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def fileno(self) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def flush(self) -> None:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def isatty(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def read(self, n: int = -1) -> AnyStr:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def readable(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def readline(self, limit: int = -1) -> AnyStr:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def readlines(self, hint: int = -1) -> List[AnyStr]:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def seek(self, offset: int, whence: int = 0) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def seekable(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def tell(self) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def truncate(self, size: int = None) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def writable(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def write(self, s: AnyStr) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def writelines(self, lines: List[AnyStr]) -> None:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def __enter__(self) -> 'IO[AnyStr]':\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def __exit__(self, type, value, traceback) -> None:\n",
            "        pass\n",
            "\n",
            "\n",
            "class BinaryIO(IO[bytes]):\n",
            "    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractmethod\n",
            "    def write(self, s: Union[bytes, bytearray]) -> int:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def __enter__(self) -> 'BinaryIO':\n",
            "        pass\n",
            "\n",
            "\n",
            "class TextIO(IO[str]):\n",
            "    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n",
            "\n",
            "    __slots__ = ()\n",
            "\n",
            "    @abstractproperty\n",
            "    def buffer(self) -> BinaryIO:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def encoding(self) -> str:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def errors(self) -> Optional[str]:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def line_buffering(self) -> bool:\n",
            "        pass\n",
            "\n",
            "    @abstractproperty\n",
            "    def newlines(self) -> Any:\n",
            "        pass\n",
            "\n",
            "    @abstractmethod\n",
            "    def __enter__(self) -> 'TextIO':\n",
            "        pass\n",
            "\n",
            "\n",
            "class io:\n",
            "    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n",
            "\n",
            "    __all__ = ['IO', 'TextIO', 'BinaryIO']\n",
            "    IO = IO\n",
            "    TextIO = TextIO\n",
            "    BinaryIO = BinaryIO\n",
            "\n",
            "\n",
            "io.__name__ = __name__ + '.io'\n",
            "sys.modules[io.__name__] = io\n",
            "\n",
            "\n",
            "Pattern = _TypeAlias('Pattern', AnyStr, type(stdlib_re.compile('')),\n",
            "                     lambda p: p.pattern)\n",
            "Match = _TypeAlias('Match', AnyStr, type(stdlib_re.match('', '')),\n",
            "                   lambda m: m.re.pattern)\n",
            "\n",
            "\n",
            "class re:\n",
            "    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n",
            "\n",
            "    __all__ = ['Pattern', 'Match']\n",
            "    Pattern = Pattern\n",
            "    Match = Match\n",
            "\n",
            "\n",
            "re.__name__ = __name__ + '.re'\n",
            "sys.modules[re.__name__] = re\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "'''\n",
            "Copyright 2017 John Torakis\n",
            "\n",
            "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "you may not use this file except in compliance with the License.\n",
            "You may obtain a copy of the License at\n",
            "\n",
            " http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "Unless required by applicable law or agreed to in writing, software\n",
            "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "See the License for the specific language governing permissions and\n",
            "limitations under the License.\n",
            "'''\n",
            "\n",
            "import imp\n",
            "import sys\n",
            "import logging\n",
            "\n",
            "from contextlib import contextmanager\n",
            "try:\n",
            "    from urllib2 import urlopen\n",
            "except:\n",
            "    from urllib.request import urlopen\n",
            "\n",
            "__author__ = 'John Torakis - operatorequals'\n",
            "__version__ = '0.5.16'\n",
            "__github__ = 'https://github.com/operatorequals/httpimport'\n",
            "\n",
            "log_FORMAT = \"%(message)s\"\n",
            "logging.basicConfig(format=log_FORMAT)\n",
            "\n",
            "'''\n",
            "To enable debug logging set:\n",
            "\n",
            ">>> import logging; logging.getLogger('httpimport').setLevel(logging.DEBUG)\n",
            "\n",
            "in your script.\n",
            "'''\n",
            "logger = logging.getLogger(__name__)\n",
            "logger.setLevel(logging.WARN)\n",
            "# logger.setLevel(logging.DEBUG)\n",
            "\n",
            "NON_SOURCE = False\n",
            "INSECURE = False\n",
            "RELOAD = False\n",
            "\n",
            "class HttpImporter(object):\n",
            "    \"\"\"\n",
            "The class that implements the Importer API. Contains the \"find_module\" and \"load_module\" methods.\n",
            "The 'modules' parameter is a list, with the names of the modules/packages that can be imported from the given URL.\n",
            "The 'base_url' parameter is a string containing the URL where the repository/directory is served through HTTP/S\n",
            "\n",
            "It is better to not use this class directly, but through its wrappers ('remote_repo', 'github_repo', etc) that automatically load and unload this class' objects to the 'sys.meta_path' list.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, modules, base_url):\n",
            "        self.module_names = modules\n",
            "        self.base_url = base_url + '/'\n",
            "        self.non_source = NON_SOURCE\n",
            "\n",
            "        if not INSECURE and not self.__isHTTPS(base_url) :\n",
            "            logger.warning(\"[-] '%s.INSECURE' is not set! Aborting...\" % (__name__))\n",
            "            raise Exception(\"Plain HTTP URL provided with '%s.INSECURE' not set\" % __name__)\n",
            "\n",
            "        if not self.__isHTTPS(base_url):\n",
            "            logger.warning(\"[!] Using non HTTPS URLs ('%s') can be a security hazard!\" % self.base_url)\n",
            "\n",
            "\n",
            "    def find_module(self, fullname, path=None):\n",
            "        logger.debug(\"FINDER=================\")\n",
            "        logger.debug(\"[!] Searching %s\" % fullname)\n",
            "        logger.debug(\"[!] Path is %s\" % path)\n",
            "        logger.info(\"[@] Checking if in declared remote module names >\")\n",
            "        if fullname.split('.')[0] not in self.module_names:\n",
            "            logger.info(\"[-] Not found!\")\n",
            "            return None\n",
            "\n",
            "        logger.info(\"[@] Checking if built-in >\")\n",
            "        try:\n",
            "            loader = imp.find_module(fullname, path)\n",
            "            if loader:\n",
            "                logger.info(\"[-] Found locally!\")\n",
            "                return None\n",
            "        except ImportError:\n",
            "            pass\n",
            "        logger.info(\"[@] Checking if it is name repetition >\")\n",
            "        if fullname.split('.').count(fullname.split('.')[-1]) > 1:\n",
            "            logger.info(\"[-] Found locally!\")\n",
            "            return None\n",
            "\n",
            "        logger.info(\"[*]Module/Package '%s' can be loaded!\" % fullname)\n",
            "        return self\n",
            "\n",
            "\n",
            "    def load_module(self, name):\n",
            "        imp.acquire_lock()\n",
            "        logger.debug(\"LOADER=================\")\n",
            "        logger.debug(\"[+] Loading %s\" % name)\n",
            "        if name in sys.modules and not RELOAD:\n",
            "            logger.info('[+] Module \"%s\" already loaded!' % name)\n",
            "            imp.release_lock()\n",
            "            return sys.modules[name]\n",
            "\n",
            "        if name.split('.')[-1] in sys.modules and not RELOAD:\n",
            "            logger.info('[+] Module \"%s\" loaded as a top level module!' % name)\n",
            "            imp.release_lock()\n",
            "            return sys.modules[name.split('.')[-1]]\n",
            "\n",
            "        module_url = self.base_url + '%s.py' % name.replace('.', '/')\n",
            "        package_url = self.base_url + '%s/__init__.py' % name.replace('.', '/')\n",
            "        zip_url = self.base_url + '%s.zip' % name.replace('.', '/')\n",
            "        final_url = None\n",
            "        final_src = None\n",
            "\n",
            "        try:\n",
            "            logger.debug(\"[+] Trying to import as package from: '%s'\" % package_url)\n",
            "            package_src = None\n",
            "            if self.non_source :    # Try the .pyc file\n",
            "                package_src = self.__fetch_compiled(package_url)\n",
            "            if package_src == None :\n",
            "                package_src = urlopen(package_url).read()\n",
            "            final_src = package_src\n",
            "            final_url = package_url\n",
            "        except IOError as e:\n",
            "            package_src = None\n",
            "            logger.info(\"[-] '%s' is not a package:\" % name)\n",
            "\n",
            "        if final_src == None:\n",
            "            try:\n",
            "                logger.debug(\"[+] Trying to import as module from: '%s'\" % module_url)\n",
            "                module_src = None\n",
            "                if self.non_source :    # Try the .pyc file\n",
            "                    module_src = self.__fetch_compiled(module_url)\n",
            "                if module_src == None : # .pyc file not found, falling back to .py\n",
            "                    module_src = urlopen(module_url).read()\n",
            "                final_src = module_src\n",
            "                final_url = module_url\n",
            "            except IOError as e:\n",
            "                module_src = None\n",
            "                logger.info(\"[-] '%s' is not a module:\" % name)\n",
            "                logger.warning(\"[!] '%s' not found in HTTP repository. Moving to next Finder.\" % name)\n",
            "                imp.release_lock()\n",
            "                return None\n",
            "\n",
            "        logger.debug(\"[+] Importing '%s'\" % name)\n",
            "        mod = imp.new_module(name)\n",
            "        mod.__loader__ = self\n",
            "        mod.__file__ = final_url\n",
            "        if not package_src:\n",
            "            mod.__package__ = name\n",
            "        else:\n",
            "            mod.__package__ = name.split('.')[0]\n",
            "\n",
            "        mod.__path__ = ['/'.join(mod.__file__.split('/')[:-1]) + '/']\n",
            "        logger.debug(\"[+] Ready to execute '%s' code\" % name)\n",
            "        sys.modules[name] = mod\n",
            "        exec(final_src, mod.__dict__)\n",
            "        logger.info(\"[+] '%s' imported succesfully!\" % name)\n",
            "        imp.release_lock()\n",
            "        return mod\n",
            "\n",
            "    def __fetch_compiled(self, url) :\n",
            "        import marshal\n",
            "        module_src = None\n",
            "        try :\n",
            "            module_compiled = urlopen(url + 'c').read()  # from blah.py --> blah.pyc\n",
            "            try :\n",
            "                module_src = marshal.loads(module_compiled[8:]) # Strip the .pyc file header of Python up to 3.3\n",
            "                return module_src\n",
            "            except ValueError :\n",
            "                pass\n",
            "            try :\n",
            "                module_src = marshal.loads(module_compiled[12:])# Strip the .pyc file header of Python 3.3 and onwards (changed .pyc spec)\n",
            "                return module_src\n",
            "            except ValueError :\n",
            "                pass\n",
            "        except IOError as e:\n",
            "            logger.debug(\"[-] No compiled version ('.pyc') for '%s' module found!\" % url.split('/')[-1])\n",
            "        return module_src\n",
            "\n",
            "\n",
            "    def __isHTTPS(self, url) :\n",
            "        return self.base_url.startswith('https') \n",
            "\n",
            "\n",
            "@contextmanager\n",
            "# Default 'python -m SimpleHTTPServer' URL\n",
            "def remote_repo(modules, base_url='http://localhost:8000/'):\n",
            "    '''\n",
            "Context Manager that provides remote import functionality through a URL.\n",
            "The parameters are the same as the HttpImporter class contructor.\n",
            "    '''\n",
            "    importer = add_remote_repo(modules, base_url)\n",
            "    yield\n",
            "    remove_remote_repo(base_url)\n",
            "\n",
            "\n",
            "# Default 'python -m SimpleHTTPServer' URL\n",
            "def add_remote_repo(modules, base_url='http://localhost:8000/'):\n",
            "    '''\n",
            "Function that creates and adds to the 'sys.meta_path' an HttpImporter object.\n",
            "The parameters are the same as the HttpImporter class contructor.\n",
            "    '''\n",
            "    importer = HttpImporter(modules, base_url)\n",
            "    sys.meta_path.insert(0, importer)\n",
            "    return importer\n",
            "\n",
            "\n",
            "def remove_remote_repo(base_url):\n",
            "    '''\n",
            "Function that removes from the 'sys.meta_path' an HttpImporter object given its HTTP/S URL.\n",
            "    '''\n",
            "    for importer in sys.meta_path:\n",
            "        try:\n",
            "            if importer.base_url.startswith(base_url):  # an extra '/' is always added\n",
            "                sys.meta_path.remove(importer)\n",
            "                return True\n",
            "        except AttributeError as e:\n",
            "            pass\n",
            "    return False\n",
            "\n",
            "\n",
            "def __create_github_url(username, repo, branch='master'):\n",
            "    '''\n",
            "Creates the HTTPS URL that points to the raw contents of a github repository.\n",
            "    '''\n",
            "    github_raw_url = 'https://raw.githubusercontent.com/{user}/{repo}/{branch}/'\n",
            "    return github_raw_url.format(user=username, repo=repo, branch=branch)\n",
            "\n",
            "\n",
            "def __create_bitbucket_url(username, repo, branch='master'):\n",
            "    '''\n",
            "Creates the HTTPS URL that points to the raw contents of a github repository.\n",
            "    '''\n",
            "    bitbucket_raw_url = 'https://bitbucket.org/{user}/{repo}/raw/{branch}/'\n",
            "    return bitbucket_raw_url.format(user=username, repo=repo, branch=branch)\n",
            "\n",
            "\n",
            "def _add_git_repo(url_builder, username=None, repo=None, module=None, branch=None, commit=None):\n",
            "    '''\n",
            "Function that creates and adds to the 'sys.meta_path' an HttpImporter object equipped with a URL of a Online Git server.\n",
            "The 'url_builder' parameter is a function that accepts the username, repo and branch/commit, and creates a HTTP/S URL of a Git server. Compatible functions are '__create_github_url', '__create_bitbucket_url'.\n",
            "The 'username' parameter defines the Github username which is the repository's owner.\n",
            "The 'repo' parameter defines the name of the repo that contains the modules/packages to be imported.\n",
            "The 'module' parameter is optional and is a list containing the modules/packages that are available in the chosen Github repository.\n",
            "If it is not provided, it defaults to the repositories name, as it is common that the a Python repository at \"github.com/someuser/somereponame\" contains a module/package of \"somereponame\".\n",
            "The 'branch' and 'commit' parameters cannot be both populated at the same call. They specify the branch (last commit) or specific commit, that should be served.\n",
            "    '''\n",
            "    if username == None or repo == None:\n",
            "        raise Error(\"'username' and 'repo' parameters cannot be None\")\n",
            "    if commit and branch:\n",
            "        raise Error(\"'branch' and 'commit' parameters cannot be both set!\")\n",
            "\n",
            "    if commit:\n",
            "        branch = commit\n",
            "    if not branch:\n",
            "        branch = 'master'\n",
            "    if not module:\n",
            "        module = repo\n",
            "    if type(module) == str:\n",
            "        module = [module]\n",
            "    url = url_builder(username, repo, branch)\n",
            "    return add_remote_repo(module, url)\n",
            "\n",
            "\n",
            "@contextmanager\n",
            "def github_repo(username=None, repo=None, module=None, branch=None, commit=None):\n",
            "    '''\n",
            "Context Manager that provides import functionality from Github repositories through HTTPS.\n",
            "The parameters are the same as the '_add_git_repo' function. No 'url_builder' function is needed.\n",
            "    '''\n",
            "    importer = _add_git_repo(__create_github_url,\n",
            "        username, repo, module=module, branch=branch, commit=commit)\n",
            "    yield\n",
            "    remove_remote_repo(importer.base_url)\n",
            "\n",
            "\n",
            "\n",
            "@contextmanager\n",
            "def bitbucket_repo(username=None, repo=None, module=None, branch=None, commit=None):\n",
            "    '''\n",
            "Context Manager that provides import functionality from BitBucket repositories through HTTPS.\n",
            "The parameters are the same as the '_add_git_repo' function. No 'url_builder' function is needed.\n",
            "    '''\n",
            "    importer = _add_git_repo(__create_bitbucket_url,\n",
            "        username, repo, module=module, branch=branch, commit=commit)\n",
            "    yield\n",
            "    remove_remote_repo(importer.base_url)\n",
            "\n",
            "\n",
            "def load(module_name, url = 'http://localhost:8000/'):\n",
            "    '''\n",
            "Loads a module on demand and returns it as a module object. Does NOT load it to the Namespace.\n",
            "Example:\n",
            "\n",
            ">>> mod = httpimport.load('covertutils','http://localhost:8000/')\n",
            ">>> mod\n",
            "<module 'covertutils' from 'http://localhost:8000//covertutils/__init__.py'>\n",
            ">>> \n",
            "    '''\n",
            "    importer = HttpImporter([module_name], url)\n",
            "    loader = importer.find_module(module_name)\n",
            "    if loader != None :\n",
            "        module = loader.load_module(module_name)\n",
            "        if module :\n",
            "            return module\n",
            "    raise ImportError(\"Module '%s' cannot be imported from URL: '%s'\" % (module_name, url) )\n",
            "\n",
            "\n",
            "\n",
            "__all__ = [\n",
            "    'HttpImporter',\n",
            "\n",
            "    'add_remote_repo',\n",
            "    'remove_remote_repo',\n",
            "\n",
            "    'remote_repo',\n",
            "    'github_repo',\n",
            "    'bitbucket_repo',\n",
            "\n",
            "]\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# coding: utf-8\n",
            "\n",
            "from __future__ import division\n",
            "\n",
            "import io\n",
            "import sys\n",
            "import posixpath\n",
            "import zipfile\n",
            "import functools\n",
            "import itertools\n",
            "\n",
            "import more_itertools\n",
            "\n",
            "__metaclass__ = type\n",
            "\n",
            "\n",
            "def _parents(path):\n",
            "    \"\"\"\n",
            "    Given a path with elements separated by\n",
            "    posixpath.sep, generate all parents of that path.\n",
            "\n",
            "    >>> list(_parents('b/d'))\n",
            "    ['b']\n",
            "    >>> list(_parents('/b/d/'))\n",
            "    ['/b']\n",
            "    >>> list(_parents('b/d/f/'))\n",
            "    ['b/d', 'b']\n",
            "    >>> list(_parents('b'))\n",
            "    []\n",
            "    >>> list(_parents(''))\n",
            "    []\n",
            "    \"\"\"\n",
            "    return itertools.islice(_ancestry(path), 1, None)\n",
            "\n",
            "\n",
            "def _ancestry(path):\n",
            "    \"\"\"\n",
            "    Given a path with elements separated by\n",
            "    posixpath.sep, generate all elements of that path\n",
            "\n",
            "    >>> list(_ancestry('b/d'))\n",
            "    ['b/d', 'b']\n",
            "    >>> list(_ancestry('/b/d/'))\n",
            "    ['/b/d', '/b']\n",
            "    >>> list(_ancestry('b/d/f/'))\n",
            "    ['b/d/f', 'b/d', 'b']\n",
            "    >>> list(_ancestry('b'))\n",
            "    ['b']\n",
            "    >>> list(_ancestry(''))\n",
            "    []\n",
            "    \"\"\"\n",
            "    path = path.rstrip(posixpath.sep)\n",
            "    while path and path != posixpath.sep:\n",
            "        yield path\n",
            "        path, tail = posixpath.split(path)\n",
            "\n",
            "\n",
            "class Path:\n",
            "    \"\"\"\n",
            "    A pathlib-compatible interface for zip files.\n",
            "\n",
            "    Consider a zip file with this structure::\n",
            "\n",
            "        .\n",
            "         a.txt\n",
            "         b\n",
            "             c.txt\n",
            "             d\n",
            "                 e.txt\n",
            "\n",
            "    >>> data = io.BytesIO()\n",
            "    >>> zf = zipfile.ZipFile(data, 'w')\n",
            "    >>> zf.writestr('a.txt', 'content of a')\n",
            "    >>> zf.writestr('b/c.txt', 'content of c')\n",
            "    >>> zf.writestr('b/d/e.txt', 'content of e')\n",
            "    >>> zf.filename = 'abcde.zip'\n",
            "\n",
            "    Path accepts the zipfile object itself or a filename\n",
            "\n",
            "    >>> root = Path(zf)\n",
            "\n",
            "    From there, several path operations are available.\n",
            "\n",
            "    Directory iteration (including the zip file itself):\n",
            "\n",
            "    >>> a, b = root.iterdir()\n",
            "    >>> a\n",
            "    Path('abcde.zip', 'a.txt')\n",
            "    >>> b\n",
            "    Path('abcde.zip', 'b/')\n",
            "\n",
            "    name property:\n",
            "\n",
            "    >>> b.name\n",
            "    'b'\n",
            "\n",
            "    join with divide operator:\n",
            "\n",
            "    >>> c = b / 'c.txt'\n",
            "    >>> c\n",
            "    Path('abcde.zip', 'b/c.txt')\n",
            "    >>> c.name\n",
            "    'c.txt'\n",
            "\n",
            "    Read text:\n",
            "\n",
            "    >>> c.read_text()\n",
            "    'content of c'\n",
            "\n",
            "    existence:\n",
            "\n",
            "    >>> c.exists()\n",
            "    True\n",
            "    >>> (b / 'missing.txt').exists()\n",
            "    False\n",
            "\n",
            "    Coercion to string:\n",
            "\n",
            "    >>> str(c)\n",
            "    'abcde.zip/b/c.txt'\n",
            "    \"\"\"\n",
            "\n",
            "    __repr = \"{self.__class__.__name__}({self.root.filename!r}, {self.at!r})\"\n",
            "\n",
            "    def __init__(self, root, at=\"\"):\n",
            "        self.root = (\n",
            "            root\n",
            "            if isinstance(root, zipfile.ZipFile)\n",
            "            else zipfile.ZipFile(self._pathlib_compat(root))\n",
            "        )\n",
            "        self.at = at\n",
            "\n",
            "    @staticmethod\n",
            "    def _pathlib_compat(path):\n",
            "        \"\"\"\n",
            "        For path-like objects, convert to a filename for compatibility\n",
            "        on Python 3.6.1 and earlier.\n",
            "        \"\"\"\n",
            "        try:\n",
            "            return path.__fspath__()\n",
            "        except AttributeError:\n",
            "            return str(path)\n",
            "\n",
            "    @property\n",
            "    def open(self):\n",
            "        return functools.partial(self.root.open, self.at)\n",
            "\n",
            "    @property\n",
            "    def name(self):\n",
            "        return posixpath.basename(self.at.rstrip(\"/\"))\n",
            "\n",
            "    def read_text(self, *args, **kwargs):\n",
            "        with self.open() as strm:\n",
            "            return io.TextIOWrapper(strm, *args, **kwargs).read()\n",
            "\n",
            "    def read_bytes(self):\n",
            "        with self.open() as strm:\n",
            "            return strm.read()\n",
            "\n",
            "    def _is_child(self, path):\n",
            "        return posixpath.dirname(path.at.rstrip(\"/\")) == self.at.rstrip(\"/\")\n",
            "\n",
            "    def _next(self, at):\n",
            "        return Path(self.root, at)\n",
            "\n",
            "    def is_dir(self):\n",
            "        return not self.at or self.at.endswith(\"/\")\n",
            "\n",
            "    def is_file(self):\n",
            "        return not self.is_dir()\n",
            "\n",
            "    def exists(self):\n",
            "        return self.at in self._names()\n",
            "\n",
            "    def iterdir(self):\n",
            "        if not self.is_dir():\n",
            "            raise ValueError(\"Can't listdir a file\")\n",
            "        subs = map(self._next, self._names())\n",
            "        return filter(self._is_child, subs)\n",
            "\n",
            "    def __str__(self):\n",
            "        return posixpath.join(self.root.filename, self.at)\n",
            "\n",
            "    def __repr__(self):\n",
            "        return self.__repr.format(self=self)\n",
            "\n",
            "    def joinpath(self, add):\n",
            "        add = self._pathlib_compat(add)\n",
            "        next = posixpath.join(self.at, add)\n",
            "        next_dir = posixpath.join(self.at, add, \"\")\n",
            "        names = self._names()\n",
            "        return self._next(next_dir if next not in names and next_dir in names else next)\n",
            "\n",
            "    __truediv__ = joinpath\n",
            "\n",
            "    @staticmethod\n",
            "    def _implied_dirs(names):\n",
            "        return more_itertools.unique_everseen(\n",
            "            parent + \"/\"\n",
            "            for name in names\n",
            "            for parent in _parents(name)\n",
            "            if parent + \"/\" not in names\n",
            "        )\n",
            "\n",
            "    @classmethod\n",
            "    def _add_implied_dirs(cls, names):\n",
            "        return names + list(cls._implied_dirs(names))\n",
            "\n",
            "    @property\n",
            "    def parent(self):\n",
            "        parent_at = posixpath.dirname(self.at.rstrip('/'))\n",
            "        if parent_at:\n",
            "            parent_at += '/'\n",
            "        return self._next(parent_at)\n",
            "\n",
            "    def _names(self):\n",
            "        return self._add_implied_dirs(self.root.namelist())\n",
            "\n",
            "    if sys.version_info < (3,):\n",
            "        __div__ = __truediv__\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Discover and load entry points from installed packages.\"\"\"\n",
            "# Copyright (c) Thomas Kluyver and contributors\n",
            "# Distributed under the terms of the MIT license; see LICENSE file.\n",
            "\n",
            "from contextlib import contextmanager\n",
            "import glob\n",
            "from importlib import import_module\n",
            "import io\n",
            "import itertools\n",
            "import os.path as osp\n",
            "import re\n",
            "import sys\n",
            "import warnings\n",
            "import zipfile\n",
            "\n",
            "if sys.version_info[0] >= 3:\n",
            "    import configparser\n",
            "else:\n",
            "    from backports import configparser\n",
            "\n",
            "entry_point_pattern = re.compile(r\"\"\"\n",
            "(?P<modulename>\\w+(\\.\\w+)*)\n",
            "(:(?P<objectname>\\w+(\\.\\w+)*))?\n",
            "\\s*\n",
            "(\\[(?P<extras>.+)\\])?\n",
            "$\n",
            "\"\"\", re.VERBOSE)\n",
            "\n",
            "file_in_zip_pattern = re.compile(r\"\"\"\n",
            "(?P<dist_version>[^/\\\\]+)\\.(dist|egg)-info\n",
            "[/\\\\]entry_points.txt$\n",
            "\"\"\", re.VERBOSE)\n",
            "\n",
            "__version__ = '0.3'\n",
            "\n",
            "class BadEntryPoint(Exception):\n",
            "    \"\"\"Raised when an entry point can't be parsed.\n",
            "    \"\"\"\n",
            "    def __init__(self, epstr):\n",
            "        self.epstr = epstr\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"Couldn't parse entry point spec: %r\" % self.epstr\n",
            "\n",
            "    @staticmethod\n",
            "    @contextmanager\n",
            "    def err_to_warnings():\n",
            "        try:\n",
            "            yield\n",
            "        except BadEntryPoint as e:\n",
            "            warnings.warn(str(e))\n",
            "\n",
            "class NoSuchEntryPoint(Exception):\n",
            "    \"\"\"Raised by :func:`get_single` when no matching entry point is found.\"\"\"\n",
            "    def __init__(self, group, name):\n",
            "        self.group = group\n",
            "        self.name = name\n",
            "\n",
            "    def __str__(self):\n",
            "        return \"No {!r} entry point found in group {!r}\".format(self.name, self.group)\n",
            "\n",
            "\n",
            "class CaseSensitiveConfigParser(configparser.ConfigParser):\n",
            "    optionxform = staticmethod(str)\n",
            "\n",
            "\n",
            "class EntryPoint(object):\n",
            "    def __init__(self, name, module_name, object_name, extras=None, distro=None):\n",
            "        self.name = name\n",
            "        self.module_name = module_name\n",
            "        self.object_name = object_name\n",
            "        self.extras = extras\n",
            "        self.distro = distro\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"EntryPoint(%r, %r, %r, %r)\" % \\\n",
            "            (self.name, self.module_name, self.object_name, self.distro)\n",
            "\n",
            "    def load(self):\n",
            "        \"\"\"Load the object to which this entry point refers.\n",
            "        \"\"\"\n",
            "        mod = import_module(self.module_name)\n",
            "        obj = mod\n",
            "        if self.object_name:\n",
            "            for attr in self.object_name.split('.'):\n",
            "                obj = getattr(obj, attr)\n",
            "        return obj\n",
            "\n",
            "    @classmethod\n",
            "    def from_string(cls, epstr, name, distro=None):\n",
            "        \"\"\"Parse an entry point from the syntax in entry_points.txt\n",
            "\n",
            "        :param str epstr: The entry point string (not including 'name =')\n",
            "        :param str name: The name of this entry point\n",
            "        :param Distribution distro: The distribution in which the entry point was found\n",
            "        :rtype: EntryPoint\n",
            "        :raises BadEntryPoint: if *epstr* can't be parsed as an entry point.\n",
            "        \"\"\"\n",
            "        m = entry_point_pattern.match(epstr)\n",
            "        if m:\n",
            "            mod, obj, extras = m.group('modulename', 'objectname', 'extras')\n",
            "            if extras is not None:\n",
            "                extras = re.split(r',\\s*', extras)\n",
            "            return cls(name, mod, obj, extras, distro)\n",
            "        else:\n",
            "            raise BadEntryPoint(epstr)\n",
            "\n",
            "class Distribution(object):\n",
            "    def __init__(self, name, version):\n",
            "        self.name = name\n",
            "        self.version = version\n",
            "\n",
            "    def __repr__(self):\n",
            "        return \"Distribution(%r, %r)\" % (self.name, self.version)\n",
            "\n",
            "\n",
            "def iter_files_distros(path=None, repeated_distro='first'):\n",
            "    if path is None:\n",
            "        path = sys.path\n",
            "\n",
            "    # Distributions found earlier in path will shadow those with the same name\n",
            "    # found later. If these distributions used different module names, it may\n",
            "    # actually be possible to import both, but in most cases this shadowing\n",
            "    # will be correct.\n",
            "    distro_names_seen = set()\n",
            "\n",
            "    for folder in path:\n",
            "        if folder.rstrip('/\\\\').endswith('.egg'):\n",
            "            # Gah, eggs\n",
            "            egg_name = osp.basename(folder)\n",
            "            if '-' in egg_name:\n",
            "                distro = Distribution(*egg_name.split('-')[:2])\n",
            "\n",
            "                if (repeated_distro == 'first') \\\n",
            "                        and (distro.name in distro_names_seen):\n",
            "                    continue\n",
            "                distro_names_seen.add(distro.name)\n",
            "            else:\n",
            "                distro = None\n",
            "\n",
            "            if osp.isdir(folder):\n",
            "                ep_path = osp.join(folder, 'EGG-INFO', 'entry_points.txt')\n",
            "                if osp.isfile(ep_path):\n",
            "                    cp = CaseSensitiveConfigParser(delimiters=('=',))\n",
            "                    cp.read([ep_path])\n",
            "                    yield cp, distro\n",
            "\n",
            "            elif zipfile.is_zipfile(folder):\n",
            "                z = zipfile.ZipFile(folder)\n",
            "                try:\n",
            "                    info = z.getinfo('EGG-INFO/entry_points.txt')\n",
            "                except KeyError:\n",
            "                    continue\n",
            "                cp = CaseSensitiveConfigParser(delimiters=('=',))\n",
            "                with z.open(info) as f:\n",
            "                    fu = io.TextIOWrapper(f)\n",
            "                    cp.read_file(fu,\n",
            "                        source=osp.join(folder, 'EGG-INFO', 'entry_points.txt'))\n",
            "                yield cp, distro\n",
            "\n",
            "        # zip imports, not egg\n",
            "        elif zipfile.is_zipfile(folder):\n",
            "            with zipfile.ZipFile(folder) as zf:\n",
            "                for info in zf.infolist():\n",
            "                    m = file_in_zip_pattern.match(info.filename)\n",
            "                    if not m:\n",
            "                        continue\n",
            "\n",
            "                    distro_name_version = m.group('dist_version')\n",
            "                    if '-' in distro_name_version:\n",
            "                        distro = Distribution(*distro_name_version.split('-', 1))\n",
            "\n",
            "                        if (repeated_distro == 'first') \\\n",
            "                                and (distro.name in distro_names_seen):\n",
            "                            continue\n",
            "                        distro_names_seen.add(distro.name)\n",
            "                    else:\n",
            "                        distro = None\n",
            "\n",
            "                    cp = CaseSensitiveConfigParser(delimiters=('=',))\n",
            "                    with zf.open(info) as f:\n",
            "                        fu = io.TextIOWrapper(f)\n",
            "                        cp.read_file(fu, source=osp.join(folder, info.filename))\n",
            "                    yield cp, distro\n",
            "\n",
            "        # Regular file imports (not egg, not zip file)\n",
            "        for path in itertools.chain(\n",
            "            glob.iglob(osp.join(folder, '*.dist-info', 'entry_points.txt')),\n",
            "            glob.iglob(osp.join(folder, '*.egg-info', 'entry_points.txt'))\n",
            "        ):\n",
            "            distro_name_version = osp.splitext(osp.basename(osp.dirname(path)))[0]\n",
            "            if '-' in distro_name_version:\n",
            "                distro = Distribution(*distro_name_version.split('-', 1))\n",
            "\n",
            "                if (repeated_distro == 'first') \\\n",
            "                        and (distro.name in distro_names_seen):\n",
            "                    continue\n",
            "                distro_names_seen.add(distro.name)\n",
            "            else:\n",
            "                distro = None\n",
            "            cp = CaseSensitiveConfigParser(delimiters=('=',))\n",
            "            cp.read([path])\n",
            "            yield cp, distro\n",
            "\n",
            "def get_single(group, name, path=None):\n",
            "    \"\"\"Find a single entry point.\n",
            "\n",
            "    Returns an :class:`EntryPoint` object, or raises :exc:`NoSuchEntryPoint`\n",
            "    if no match is found.\n",
            "    \"\"\"\n",
            "    for config, distro in iter_files_distros(path=path):\n",
            "        if (group in config) and (name in config[group]):\n",
            "            epstr = config[group][name]\n",
            "            with BadEntryPoint.err_to_warnings():\n",
            "                return EntryPoint.from_string(epstr, name, distro)\n",
            "\n",
            "    raise NoSuchEntryPoint(group, name)\n",
            "\n",
            "def get_group_named(group, path=None):\n",
            "    \"\"\"Find a group of entry points with unique names.\n",
            "\n",
            "    Returns a dictionary of names to :class:`EntryPoint` objects.\n",
            "    \"\"\"\n",
            "    result = {}\n",
            "    for ep in get_group_all(group, path=path):\n",
            "        if ep.name not in result:\n",
            "            result[ep.name] = ep\n",
            "    return result\n",
            "\n",
            "def get_group_all(group, path=None):\n",
            "    \"\"\"Find all entry points in a group.\n",
            "\n",
            "    Returns a list of :class:`EntryPoint` objects.\n",
            "    \"\"\"\n",
            "    result = []\n",
            "    for config, distro in iter_files_distros(path=path):\n",
            "        if group in config:\n",
            "            for name, epstr in config[group].items():\n",
            "                with BadEntryPoint.err_to_warnings():\n",
            "                    result.append(EntryPoint.from_string(epstr, name, distro))\n",
            "\n",
            "    return result\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    import pprint\n",
            "    pprint.pprint(get_group_all('console_scripts'))\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "#!/usr/bin/env python\n",
            "\"\"\"\n",
            "SocksiPy + urllib2 handler\n",
            "\n",
            "version: 0.3\n",
            "author: e<e@tr0ll.in>\n",
            "\n",
            "This module provides a Handler which you can use with urllib2 to allow it to tunnel your connection through a socks.sockssocket socket, with out monkey patching the original socket...\n",
            "\"\"\"\n",
            "import ssl\n",
            "\n",
            "try:\n",
            "    import urllib2\n",
            "    import httplib\n",
            "except ImportError: # Python 3\n",
            "    import urllib.request as urllib2\n",
            "    import http.client as httplib\n",
            "\n",
            "import socks # $ pip install PySocks\n",
            "\n",
            "def merge_dict(a, b):\n",
            "    d = a.copy()\n",
            "    d.update(b)\n",
            "    return d\n",
            "\n",
            "class SocksiPyConnection(httplib.HTTPConnection):\n",
            "    def __init__(self, proxytype, proxyaddr, proxyport=None, rdns=True, username=None, password=None, *args, **kwargs):\n",
            "        self.proxyargs = (proxytype, proxyaddr, proxyport, rdns, username, password)\n",
            "        httplib.HTTPConnection.__init__(self, *args, **kwargs)\n",
            "\n",
            "    def connect(self):\n",
            "        self.sock = socks.socksocket()\n",
            "        self.sock.setproxy(*self.proxyargs)\n",
            "        if type(self.timeout) in (int, float):\n",
            "            self.sock.settimeout(self.timeout)\n",
            "        self.sock.connect((self.host, self.port))\n",
            "\n",
            "class SocksiPyConnectionS(httplib.HTTPSConnection):\n",
            "    def __init__(self, proxytype, proxyaddr, proxyport=None, rdns=True, username=None, password=None, *args, **kwargs):\n",
            "        self.proxyargs = (proxytype, proxyaddr, proxyport, rdns, username, password)\n",
            "        httplib.HTTPSConnection.__init__(self, *args, **kwargs)\n",
            "\n",
            "    def connect(self):\n",
            "        sock = socks.socksocket()\n",
            "        sock.setproxy(*self.proxyargs)\n",
            "        if type(self.timeout) in (int, float):\n",
            "            sock.settimeout(self.timeout)\n",
            "        sock.connect((self.host, self.port))\n",
            "        self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file)\n",
            "\n",
            "class SocksiPyHandler(urllib2.HTTPHandler, urllib2.HTTPSHandler):\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        self.args = args\n",
            "        self.kw = kwargs\n",
            "        urllib2.HTTPHandler.__init__(self)\n",
            "\n",
            "    def http_open(self, req):\n",
            "        def build(host, port=None, timeout=0, **kwargs):\n",
            "            kw = merge_dict(self.kw, kwargs)\n",
            "            conn = SocksiPyConnection(*self.args, host=host, port=port, timeout=timeout, **kw)\n",
            "            return conn\n",
            "        return self.do_open(build, req)\n",
            "\n",
            "    def https_open(self, req):\n",
            "        def build(host, port=None, timeout=0, **kwargs):\n",
            "            kw = merge_dict(self.kw, kwargs)\n",
            "            conn = SocksiPyConnectionS(*self.args, host=host, port=port, timeout=timeout, **kw)\n",
            "            return conn\n",
            "        return self.do_open(build, req)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    import sys\n",
            "    try:\n",
            "        port = int(sys.argv[1])\n",
            "    except (ValueError, IndexError):\n",
            "        port = 9050\n",
            "    opener = urllib2.build_opener(SocksiPyHandler(socks.PROXY_TYPE_SOCKS5, \"localhost\", port))\n",
            "    print(\"HTTP: \" + opener.open(\"http://httpbin.org/ip\").read().decode())\n",
            "    print(\"HTTPS: \" + opener.open(\"https://httpbin.org/ip\").read().decode())\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "\"\"\"Run the EasyInstall command\"\"\"\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    from setuptools.command.easy_install import main\n",
            "    main()\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow\n",
            "Directories ['python', 'tools', 'examples', 'contrib', 'lite', 'core', '_api', '__pycache__', 'compiler', 'include']\n",
            "Files ['__init__.py', 'libtensorflow_framework.so.1']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Bring in all of the public TensorFlow interface into this module.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import as _absolute_import\n",
            "from __future__ import division as _division\n",
            "from __future__ import print_function as _print_function\n",
            "\n",
            "import distutils as _distutils\n",
            "import inspect as _inspect\n",
            "import os as _os\n",
            "import site as _site\n",
            "import sys as _sys\n",
            "\n",
            "# pylint: disable=g-bad-import-order\n",
            "from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
            "from tensorflow.python.tools import module_util as _module_util\n",
            "\n",
            "from tensorflow._api.v1 import app\n",
            "from tensorflow._api.v1 import audio\n",
            "from tensorflow._api.v1 import autograph\n",
            "from tensorflow._api.v1 import bitwise\n",
            "from tensorflow._api.v1 import compat\n",
            "from tensorflow._api.v1 import config\n",
            "from tensorflow._api.v1 import data\n",
            "from tensorflow._api.v1 import debugging\n",
            "from tensorflow._api.v1 import distribute\n",
            "from tensorflow._api.v1 import distributions\n",
            "from tensorflow._api.v1 import dtypes\n",
            "from tensorflow._api.v1 import errors\n",
            "from tensorflow._api.v1 import experimental\n",
            "from tensorflow._api.v1 import feature_column\n",
            "from tensorflow._api.v1 import gfile\n",
            "from tensorflow._api.v1 import graph_util\n",
            "from tensorflow._api.v1 import image\n",
            "from tensorflow._api.v1 import initializers\n",
            "from tensorflow._api.v1 import io\n",
            "from tensorflow._api.v1 import layers\n",
            "from tensorflow._api.v1 import linalg\n",
            "from tensorflow._api.v1 import lite\n",
            "from tensorflow._api.v1 import logging\n",
            "from tensorflow._api.v1 import lookup\n",
            "from tensorflow._api.v1 import losses\n",
            "from tensorflow._api.v1 import manip\n",
            "from tensorflow._api.v1 import math\n",
            "from tensorflow._api.v1 import metrics\n",
            "from tensorflow._api.v1 import nest\n",
            "from tensorflow._api.v1 import nn\n",
            "from tensorflow._api.v1 import profiler\n",
            "from tensorflow._api.v1 import python_io\n",
            "from tensorflow._api.v1 import quantization\n",
            "from tensorflow._api.v1 import queue\n",
            "from tensorflow._api.v1 import ragged\n",
            "from tensorflow._api.v1 import random\n",
            "from tensorflow._api.v1 import raw_ops\n",
            "from tensorflow._api.v1 import resource_loader\n",
            "from tensorflow._api.v1 import saved_model\n",
            "from tensorflow._api.v1 import sets\n",
            "from tensorflow._api.v1 import signal\n",
            "from tensorflow._api.v1 import sparse\n",
            "from tensorflow._api.v1 import spectral\n",
            "from tensorflow._api.v1 import strings\n",
            "from tensorflow._api.v1 import summary\n",
            "from tensorflow._api.v1 import sysconfig\n",
            "from tensorflow._api.v1 import test\n",
            "from tensorflow._api.v1 import tpu\n",
            "from tensorflow._api.v1 import train\n",
            "from tensorflow._api.v1 import user_ops\n",
            "from tensorflow._api.v1 import version\n",
            "from tensorflow._api.v1 import xla\n",
            "from tensorflow.lite.python.lite import _import_graph_def as import_graph_def\n",
            "from tensorflow.python import AggregationMethod\n",
            "from tensorflow.python import Assert\n",
            "from tensorflow.python import AttrValue\n",
            "from tensorflow.python import ConditionalAccumulator\n",
            "from tensorflow.python import ConditionalAccumulatorBase\n",
            "from tensorflow.python import ConfigProto\n",
            "from tensorflow.python import Constant as constant_initializer\n",
            "from tensorflow.python import CriticalSection\n",
            "from tensorflow.python import DType\n",
            "from tensorflow.python import DeviceSpec\n",
            "from tensorflow.python import Dimension\n",
            "from tensorflow.python import Event\n",
            "from tensorflow.python import FIFOQueue\n",
            "from tensorflow.python import FixedLenFeature\n",
            "from tensorflow.python import FixedLenSequenceFeature\n",
            "from tensorflow.python import FixedLengthRecordReader\n",
            "from tensorflow.python import GPUOptions\n",
            "from tensorflow.python import GlorotNormal as glorot_normal_initializer\n",
            "from tensorflow.python import GlorotUniform as glorot_uniform_initializer\n",
            "from tensorflow.python import GradientTape\n",
            "from tensorflow.python import Graph\n",
            "from tensorflow.python import GraphDef\n",
            "from tensorflow.python import GraphKeys\n",
            "from tensorflow.python import GraphOptions\n",
            "from tensorflow.python import HistogramProto\n",
            "from tensorflow.python import IdentityReader\n",
            "from tensorflow.python import IndexedSlices\n",
            "from tensorflow.python import InteractiveSession\n",
            "from tensorflow.python import LMDBReader\n",
            "from tensorflow.python import LogMessage\n",
            "from tensorflow.python import MetaGraphDef\n",
            "from tensorflow.python import NameAttrList\n",
            "from tensorflow.python import NoGradient\n",
            "from tensorflow.python import NoGradient as NotDifferentiable\n",
            "from tensorflow.python import NoGradient as no_gradient\n",
            "from tensorflow.python import NodeDef\n",
            "from tensorflow.python import Ones as ones_initializer\n",
            "from tensorflow.python import OpError\n",
            "from tensorflow.python import Operation\n",
            "from tensorflow.python import OptimizerOptions\n",
            "from tensorflow.python import Orthogonal as orthogonal_initializer\n",
            "from tensorflow.python import PaddingFIFOQueue\n",
            "from tensorflow.python import Print\n",
            "from tensorflow.python import PriorityQueue\n",
            "from tensorflow.python import QueueBase\n",
            "from tensorflow.python import RandomNormal as random_normal_initializer\n",
            "from tensorflow.python import RandomShuffleQueue\n",
            "from tensorflow.python import ReaderBase\n",
            "from tensorflow.python import RegisterGradient\n",
            "from tensorflow.python import RunMetadata\n",
            "from tensorflow.python import RunOptions\n",
            "from tensorflow.python import Session\n",
            "from tensorflow.python import SessionLog\n",
            "from tensorflow.python import SparseConditionalAccumulator\n",
            "from tensorflow.python import SparseFeature\n",
            "from tensorflow.python import SparseTensor\n",
            "from tensorflow.python import SparseTensorValue\n",
            "from tensorflow.python import Summary\n",
            "from tensorflow.python import SummaryMetadata\n",
            "from tensorflow.python import TFRecordReader\n",
            "from tensorflow.python import Tensor\n",
            "from tensorflow.python import TensorArray\n",
            "from tensorflow.python import TensorInfo\n",
            "from tensorflow.python import TensorShape\n",
            "from tensorflow.python import TextLineReader\n",
            "from tensorflow.python import UnconnectedGradients\n",
            "from tensorflow.python import UniformUnitScaling as uniform_unit_scaling_initializer\n",
            "from tensorflow.python import VarLenFeature\n",
            "from tensorflow.python import VariableAggregation\n",
            "from tensorflow.python import VariableScope\n",
            "from tensorflow.python import VariableSynchronization\n",
            "from tensorflow.python import VariableV1 as Variable\n",
            "from tensorflow.python import VarianceScaling as variance_scaling_initializer\n",
            "from tensorflow.python import WholeFileReader\n",
            "from tensorflow.python import Zeros as zeros_initializer\n",
            "from tensorflow.python import abs\n",
            "from tensorflow.python import accumulate_n\n",
            "from tensorflow.python import acos\n",
            "from tensorflow.python import acosh\n",
            "from tensorflow.python import add\n",
            "from tensorflow.python import add_check_numerics_ops\n",
            "from tensorflow.python import add_n\n",
            "from tensorflow.python import add_to_collection\n",
            "from tensorflow.python import add_to_collections\n",
            "from tensorflow.python import all_variables\n",
            "from tensorflow.python import angle\n",
            "from tensorflow.python import arg_max\n",
            "from tensorflow.python import arg_min\n",
            "from tensorflow.python import argmax\n",
            "from tensorflow.python import argmin\n",
            "from tensorflow.python import argsort\n",
            "from tensorflow.python import as_dtype\n",
            "from tensorflow.python import as_string\n",
            "from tensorflow.python import asin\n",
            "from tensorflow.python import asinh\n",
            "from tensorflow.python import assert_equal\n",
            "from tensorflow.python import assert_greater\n",
            "from tensorflow.python import assert_greater_equal\n",
            "from tensorflow.python import assert_integer\n",
            "from tensorflow.python import assert_less\n",
            "from tensorflow.python import assert_less_equal\n",
            "from tensorflow.python import assert_near\n",
            "from tensorflow.python import assert_negative\n",
            "from tensorflow.python import assert_non_negative\n",
            "from tensorflow.python import assert_non_positive\n",
            "from tensorflow.python import assert_none_equal\n",
            "from tensorflow.python import assert_positive\n",
            "from tensorflow.python import assert_proper_iterable\n",
            "from tensorflow.python import assert_rank\n",
            "from tensorflow.python import assert_rank_at_least\n",
            "from tensorflow.python import assert_rank_in\n",
            "from tensorflow.python import assert_same_float_dtype\n",
            "from tensorflow.python import assert_scalar\n",
            "from tensorflow.python import assert_type\n",
            "from tensorflow.python import assert_variables_initialized\n",
            "from tensorflow.python import assign\n",
            "from tensorflow.python import assign_add\n",
            "from tensorflow.python import assign_sub\n",
            "from tensorflow.python import atan\n",
            "from tensorflow.python import atan2\n",
            "from tensorflow.python import atanh\n",
            "from tensorflow.python import batch_function as nondifferentiable_batch_function\n",
            "from tensorflow.python import batch_gather\n",
            "from tensorflow.python import batch_to_space\n",
            "from tensorflow.python import batch_to_space_nd\n",
            "from tensorflow.python import betainc\n",
            "from tensorflow.python import bincount_v1 as bincount\n",
            "from tensorflow.python import bitcast\n",
            "from tensorflow.python import boolean_mask\n",
            "from tensorflow.python import broadcast_dynamic_shape\n",
            "from tensorflow.python import broadcast_static_shape\n",
            "from tensorflow.python import broadcast_to\n",
            "from tensorflow.python import case\n",
            "from tensorflow.python import cast\n",
            "from tensorflow.python import ceil\n",
            "from tensorflow.python import check_numerics\n",
            "from tensorflow.python import cholesky\n",
            "from tensorflow.python import cholesky_solve\n",
            "from tensorflow.python import clip_by_average_norm\n",
            "from tensorflow.python import clip_by_global_norm\n",
            "from tensorflow.python import clip_by_norm\n",
            "from tensorflow.python import clip_by_value\n",
            "from tensorflow.python import complex\n",
            "from tensorflow.python import concat\n",
            "from tensorflow.python import cond\n",
            "from tensorflow.python import conj\n",
            "from tensorflow.python import container\n",
            "from tensorflow.python import control_dependencies\n",
            "from tensorflow.python import convert_to_tensor\n",
            "from tensorflow.python import convert_to_tensor_or_indexed_slices\n",
            "from tensorflow.python import convert_to_tensor_or_sparse_tensor\n",
            "from tensorflow.python import cos\n",
            "from tensorflow.python import cosh\n",
            "from tensorflow.python import count_nonzero\n",
            "from tensorflow.python import count_up_to\n",
            "from tensorflow.python import create_partitioned_variables\n",
            "from tensorflow.python import cross\n",
            "from tensorflow.python import cumprod\n",
            "from tensorflow.python import cumsum\n",
            "from tensorflow.python import custom_gradient\n",
            "from tensorflow.python import decode_base64\n",
            "from tensorflow.python import decode_compressed\n",
            "from tensorflow.python import decode_csv\n",
            "from tensorflow.python import decode_json_example\n",
            "from tensorflow.python import decode_raw_v1 as decode_raw\n",
            "from tensorflow.python import delete_session_tensor\n",
            "from tensorflow.python import depth_to_space\n",
            "from tensorflow.python import dequantize\n",
            "from tensorflow.python import deserialize_many_sparse\n",
            "from tensorflow.python import device\n",
            "from tensorflow.python import diag\n",
            "from tensorflow.python import diag_part\n",
            "from tensorflow.python import digamma\n",
            "from tensorflow.python import div\n",
            "from tensorflow.python import div_no_nan\n",
            "from tensorflow.python import divide\n",
            "from tensorflow.python import dynamic_partition\n",
            "from tensorflow.python import dynamic_stitch\n",
            "from tensorflow.python import edit_distance\n",
            "from tensorflow.python import einsum\n",
            "from tensorflow.python import enable_eager_execution\n",
            "from tensorflow.python import encode_base64\n",
            "from tensorflow.python import equal\n",
            "from tensorflow.python import erf\n",
            "from tensorflow.python import erfc\n",
            "from tensorflow.python import executing_eagerly\n",
            "from tensorflow.python import exp\n",
            "from tensorflow.python import expand_dims\n",
            "from tensorflow.python import expm1\n",
            "from tensorflow.python import extract_image_patches\n",
            "from tensorflow.python import extract_volume_patches\n",
            "from tensorflow.python import eye\n",
            "from tensorflow.python import fake_quant_with_min_max_args\n",
            "from tensorflow.python import fake_quant_with_min_max_args_gradient\n",
            "from tensorflow.python import fake_quant_with_min_max_vars\n",
            "from tensorflow.python import fake_quant_with_min_max_vars_gradient\n",
            "from tensorflow.python import fake_quant_with_min_max_vars_per_channel\n",
            "from tensorflow.python import fake_quant_with_min_max_vars_per_channel_gradient\n",
            "from tensorflow.python import fill\n",
            "from tensorflow.python import fingerprint\n",
            "from tensorflow.python import fixed_size_partitioner\n",
            "from tensorflow.python import floor\n",
            "from tensorflow.python import floor_div\n",
            "from tensorflow.python import floor_mod as floormod\n",
            "from tensorflow.python import floor_mod as mod\n",
            "from tensorflow.python import floordiv\n",
            "from tensorflow.python import foldl\n",
            "from tensorflow.python import foldr\n",
            "from tensorflow.python import function\n",
            "from tensorflow.python import gather\n",
            "from tensorflow.python import gather_nd\n",
            "from tensorflow.python import get_collection\n",
            "from tensorflow.python import get_collection_ref\n",
            "from tensorflow.python import get_default_graph\n",
            "from tensorflow.python import get_default_session\n",
            "from tensorflow.python import get_local_variable\n",
            "from tensorflow.python import get_seed\n",
            "from tensorflow.python import get_session_handle\n",
            "from tensorflow.python import get_session_tensor\n",
            "from tensorflow.python import get_variable\n",
            "from tensorflow.python import get_variable_scope\n",
            "from tensorflow.python import global_norm\n",
            "from tensorflow.python import global_variables\n",
            "from tensorflow.python import global_variables_initializer\n",
            "from tensorflow.python import gradients\n",
            "from tensorflow.python import greater\n",
            "from tensorflow.python import greater_equal\n",
            "from tensorflow.python import group\n",
            "from tensorflow.python import guarantee_const\n",
            "from tensorflow.python import hessians\n",
            "from tensorflow.python import histogram_fixed_width\n",
            "from tensorflow.python import histogram_fixed_width_bins\n",
            "from tensorflow.python import identity\n",
            "from tensorflow.python import identity_n\n",
            "from tensorflow.python import igamma\n",
            "from tensorflow.python import igammac\n",
            "from tensorflow.python import imag\n",
            "from tensorflow.python import initialize_all_tables\n",
            "from tensorflow.python import initialize_all_variables\n",
            "from tensorflow.python import initialize_local_variables\n",
            "from tensorflow.python import initialize_variables\n",
            "from tensorflow.python import invert_permutation\n",
            "from tensorflow.python import is_finite\n",
            "from tensorflow.python import is_inf\n",
            "from tensorflow.python import is_nan\n",
            "from tensorflow.python import is_non_decreasing\n",
            "from tensorflow.python import is_numeric_tensor\n",
            "from tensorflow.python import is_strictly_increasing\n",
            "from tensorflow.python import is_variable_initialized\n",
            "from tensorflow.python import lbeta\n",
            "from tensorflow.python import less\n",
            "from tensorflow.python import less_equal\n",
            "from tensorflow.python import lgamma\n",
            "from tensorflow.python import lin_space\n",
            "from tensorflow.python import lin_space as linspace\n",
            "from tensorflow.python import load_file_system_library\n",
            "from tensorflow.python import load_library\n",
            "from tensorflow.python import load_op_library\n",
            "from tensorflow.python import local_variables\n",
            "from tensorflow.python import local_variables_initializer\n",
            "from tensorflow.python import log\n",
            "from tensorflow.python import log1p\n",
            "from tensorflow.python import log_sigmoid\n",
            "from tensorflow.python import logical_and\n",
            "from tensorflow.python import logical_not\n",
            "from tensorflow.python import logical_or\n",
            "from tensorflow.python import logical_xor\n",
            "from tensorflow.python import make_ndarray\n",
            "from tensorflow.python import make_template\n",
            "from tensorflow.python import make_tensor_proto\n",
            "from tensorflow.python import matching_files\n",
            "from tensorflow.python import matmul\n",
            "from tensorflow.python import matrix_band_part\n",
            "from tensorflow.python import matrix_determinant\n",
            "from tensorflow.python import matrix_diag\n",
            "from tensorflow.python import matrix_diag_part\n",
            "from tensorflow.python import matrix_inverse\n",
            "from tensorflow.python import matrix_set_diag\n",
            "from tensorflow.python import matrix_solve\n",
            "from tensorflow.python import matrix_solve_ls\n",
            "from tensorflow.python import matrix_square_root\n",
            "from tensorflow.python import matrix_transpose\n",
            "from tensorflow.python import matrix_triangular_solve\n",
            "from tensorflow.python import maximum\n",
            "from tensorflow.python import meshgrid\n",
            "from tensorflow.python import min_max_variable_partitioner\n",
            "from tensorflow.python import minimum\n",
            "from tensorflow.python import model_variables\n",
            "from tensorflow.python import moving_average_variables\n",
            "from tensorflow.python import multinomial\n",
            "from tensorflow.python import multiply\n",
            "from tensorflow.python import name_scope\n",
            "from tensorflow.python import neg as negative\n",
            "from tensorflow.python import no_op\n",
            "from tensorflow.python import no_regularizer\n",
            "from tensorflow.python import norm\n",
            "from tensorflow.python import not_equal\n",
            "from tensorflow.python import one_hot\n",
            "from tensorflow.python import ones\n",
            "from tensorflow.python import ones_like\n",
            "from tensorflow.python import op_scope\n",
            "from tensorflow.python import pad\n",
            "from tensorflow.python import parallel_stack\n",
            "from tensorflow.python import parse_example\n",
            "from tensorflow.python import parse_single_example\n",
            "from tensorflow.python import parse_single_sequence_example\n",
            "from tensorflow.python import parse_tensor\n",
            "from tensorflow.python import placeholder\n",
            "from tensorflow.python import placeholder_with_default\n",
            "from tensorflow.python import polygamma\n",
            "from tensorflow.python import pow\n",
            "from tensorflow.python import py_func\n",
            "from tensorflow.python import qr\n",
            "from tensorflow.python import quantize\n",
            "from tensorflow.python import quantize_v2\n",
            "from tensorflow.python import quantized_concat\n",
            "from tensorflow.python import random_crop\n",
            "from tensorflow.python import random_gamma\n",
            "from tensorflow.python import random_normal\n",
            "from tensorflow.python import random_poisson\n",
            "from tensorflow.python import random_shuffle\n",
            "from tensorflow.python import random_uniform\n",
            "from tensorflow.python import random_uniform_initializer\n",
            "from tensorflow.python import range\n",
            "from tensorflow.python import rank\n",
            "from tensorflow.python import read_file\n",
            "from tensorflow.python import real\n",
            "from tensorflow.python import real_div as realdiv\n",
            "from tensorflow.python import reciprocal\n",
            "from tensorflow.python import reduce_all_v1 as reduce_all\n",
            "from tensorflow.python import reduce_any_v1 as reduce_any\n",
            "from tensorflow.python import reduce_join\n",
            "from tensorflow.python import reduce_logsumexp_v1 as reduce_logsumexp\n",
            "from tensorflow.python import reduce_max_v1 as reduce_max\n",
            "from tensorflow.python import reduce_mean_v1 as reduce_mean\n",
            "from tensorflow.python import reduce_min_v1 as reduce_min\n",
            "from tensorflow.python import reduce_prod_v1 as reduce_prod\n",
            "from tensorflow.python import reduce_sum_v1 as reduce_sum\n",
            "from tensorflow.python import regex_replace\n",
            "from tensorflow.python import register_tensor_conversion_function\n",
            "from tensorflow.python import report_uninitialized_variables\n",
            "from tensorflow.python import required_space_to_batch_paddings\n",
            "from tensorflow.python import reset_default_graph\n",
            "from tensorflow.python import reshape\n",
            "from tensorflow.python import reverse\n",
            "from tensorflow.python import reverse as reverse_v2\n",
            "from tensorflow.python import reverse_sequence\n",
            "from tensorflow.python import rint\n",
            "from tensorflow.python import roll\n",
            "from tensorflow.python import round\n",
            "from tensorflow.python import rsqrt\n",
            "from tensorflow.python import saturate_cast\n",
            "from tensorflow.python import scalar_mul\n",
            "from tensorflow.python import scan\n",
            "from tensorflow.python import scatter_add\n",
            "from tensorflow.python import scatter_div\n",
            "from tensorflow.python import scatter_max\n",
            "from tensorflow.python import scatter_min\n",
            "from tensorflow.python import scatter_mul\n",
            "from tensorflow.python import scatter_nd\n",
            "from tensorflow.python import scatter_nd_add\n",
            "from tensorflow.python import scatter_nd_sub\n",
            "from tensorflow.python import scatter_nd_update\n",
            "from tensorflow.python import scatter_sub\n",
            "from tensorflow.python import scatter_update\n",
            "from tensorflow.python import searchsorted\n",
            "from tensorflow.python import segment_max\n",
            "from tensorflow.python import segment_mean\n",
            "from tensorflow.python import segment_min\n",
            "from tensorflow.python import segment_prod\n",
            "from tensorflow.python import segment_sum\n",
            "from tensorflow.python import self_adjoint_eig\n",
            "from tensorflow.python import self_adjoint_eigvals\n",
            "from tensorflow.python import sequence_mask\n",
            "from tensorflow.python import serialize_many_sparse\n",
            "from tensorflow.python import serialize_sparse\n",
            "from tensorflow.python import serialize_tensor\n",
            "from tensorflow.python import set_random_seed\n",
            "from tensorflow.python import setdiff1d\n",
            "from tensorflow.python import shape\n",
            "from tensorflow.python import shape_n\n",
            "from tensorflow.python import sigmoid\n",
            "from tensorflow.python import sign\n",
            "from tensorflow.python import sin\n",
            "from tensorflow.python import sinh\n",
            "from tensorflow.python import size\n",
            "from tensorflow.python import slice\n",
            "from tensorflow.python import sort\n",
            "from tensorflow.python import space_to_batch\n",
            "from tensorflow.python import space_to_batch_nd\n",
            "from tensorflow.python import space_to_depth\n",
            "from tensorflow.python import sparse_add\n",
            "from tensorflow.python import sparse_concat\n",
            "from tensorflow.python import sparse_fill_empty_rows\n",
            "from tensorflow.python import sparse_mask\n",
            "from tensorflow.python import sparse_mat_mul as sparse_matmul\n",
            "from tensorflow.python import sparse_maximum\n",
            "from tensorflow.python import sparse_merge\n",
            "from tensorflow.python import sparse_minimum\n",
            "from tensorflow.python import sparse_placeholder\n",
            "from tensorflow.python import sparse_reduce_max\n",
            "from tensorflow.python import sparse_reduce_max_sparse\n",
            "from tensorflow.python import sparse_reduce_sum\n",
            "from tensorflow.python import sparse_reduce_sum_sparse\n",
            "from tensorflow.python import sparse_reorder\n",
            "from tensorflow.python import sparse_reset_shape\n",
            "from tensorflow.python import sparse_reshape\n",
            "from tensorflow.python import sparse_retain\n",
            "from tensorflow.python import sparse_segment_mean\n",
            "from tensorflow.python import sparse_segment_sqrt_n\n",
            "from tensorflow.python import sparse_segment_sum\n",
            "from tensorflow.python import sparse_slice\n",
            "from tensorflow.python import sparse_softmax\n",
            "from tensorflow.python import sparse_split\n",
            "from tensorflow.python import sparse_tensor_dense_matmul\n",
            "from tensorflow.python import sparse_tensor_to_dense\n",
            "from tensorflow.python import sparse_to_dense\n",
            "from tensorflow.python import sparse_to_indicator\n",
            "from tensorflow.python import sparse_transpose\n",
            "from tensorflow.python import split\n",
            "from tensorflow.python import sqrt\n",
            "from tensorflow.python import square\n",
            "from tensorflow.python import squared_difference\n",
            "from tensorflow.python import squeeze\n",
            "from tensorflow.python import stack\n",
            "from tensorflow.python import stop_gradient\n",
            "from tensorflow.python import strided_slice\n",
            "from tensorflow.python import string_join\n",
            "from tensorflow.python import string_strip\n",
            "from tensorflow.python import string_to_hash_bucket_fast\n",
            "from tensorflow.python import string_to_hash_bucket_strong\n",
            "from tensorflow.python import string_to_hash_bucket_v1 as string_to_hash_bucket\n",
            "from tensorflow.python import string_to_number_v1 as string_to_number\n",
            "from tensorflow.python import substr_deprecated as substr\n",
            "from tensorflow.python import subtract\n",
            "from tensorflow.python import svd\n",
            "from tensorflow.python import tables_initializer\n",
            "from tensorflow.python import tan\n",
            "from tensorflow.python import tanh\n",
            "from tensorflow.python import tensor_scatter_add\n",
            "from tensorflow.python import tensor_scatter_add as tensor_scatter_nd_add\n",
            "from tensorflow.python import tensor_scatter_sub\n",
            "from tensorflow.python import tensor_scatter_sub as tensor_scatter_nd_sub\n",
            "from tensorflow.python import tensor_scatter_update\n",
            "from tensorflow.python import tensor_scatter_update as tensor_scatter_nd_update\n",
            "from tensorflow.python import tensordot\n",
            "from tensorflow.python import tile\n",
            "from tensorflow.python import timestamp\n",
            "from tensorflow.python import to_bfloat16\n",
            "from tensorflow.python import to_complex128\n",
            "from tensorflow.python import to_complex64\n",
            "from tensorflow.python import to_double\n",
            "from tensorflow.python import to_float\n",
            "from tensorflow.python import to_int32\n",
            "from tensorflow.python import to_int64\n",
            "from tensorflow.python import trace\n",
            "from tensorflow.python import trainable_variables\n",
            "from tensorflow.python import transpose\n",
            "from tensorflow.python import truediv\n",
            "from tensorflow.python import truncate_div as truncatediv\n",
            "from tensorflow.python import truncate_mod as truncatemod\n",
            "from tensorflow.python import truncated_normal\n",
            "from tensorflow.python import truncated_normal_initializer\n",
            "from tensorflow.python import tuple\n",
            "from tensorflow.python import unique\n",
            "from tensorflow.python import unique_with_counts\n",
            "from tensorflow.python import unravel_index\n",
            "from tensorflow.python import unsorted_segment_max\n",
            "from tensorflow.python import unsorted_segment_mean\n",
            "from tensorflow.python import unsorted_segment_min\n",
            "from tensorflow.python import unsorted_segment_prod\n",
            "from tensorflow.python import unsorted_segment_sqrt_n\n",
            "from tensorflow.python import unsorted_segment_sum\n",
            "from tensorflow.python import unstack\n",
            "from tensorflow.python import variable_axis_size_partitioner\n",
            "from tensorflow.python import variable_op_scope\n",
            "from tensorflow.python import variable_scope\n",
            "from tensorflow.python import variables_initializer\n",
            "from tensorflow.python import vectorized_map\n",
            "from tensorflow.python import verify_tensor_all_finite\n",
            "from tensorflow.python import where\n",
            "from tensorflow.python import where_v2\n",
            "from tensorflow.python import while_loop\n",
            "from tensorflow.python import write_file\n",
            "from tensorflow.python import zeros\n",
            "from tensorflow.python import zeros_like\n",
            "from tensorflow.python import zeta\n",
            "from tensorflow.python.compat.v2_compat import disable_v2_behavior\n",
            "from tensorflow.python.compat.v2_compat import enable_v2_behavior\n",
            "from tensorflow.python.eager.wrap_function import wrap_function\n",
            "from tensorflow.python.framework.constant_op import constant_v1 as constant\n",
            "from tensorflow.python.framework.dtypes import QUANTIZED_DTYPES\n",
            "from tensorflow.python.framework.dtypes import bfloat16\n",
            "from tensorflow.python.framework.dtypes import bool\n",
            "from tensorflow.python.framework.dtypes import complex128\n",
            "from tensorflow.python.framework.dtypes import complex64\n",
            "from tensorflow.python.framework.dtypes import double\n",
            "from tensorflow.python.framework.dtypes import float16\n",
            "from tensorflow.python.framework.dtypes import float32\n",
            "from tensorflow.python.framework.dtypes import float64\n",
            "from tensorflow.python.framework.dtypes import half\n",
            "from tensorflow.python.framework.dtypes import int16\n",
            "from tensorflow.python.framework.dtypes import int32\n",
            "from tensorflow.python.framework.dtypes import int64\n",
            "from tensorflow.python.framework.dtypes import int8\n",
            "from tensorflow.python.framework.dtypes import qint16\n",
            "from tensorflow.python.framework.dtypes import qint32\n",
            "from tensorflow.python.framework.dtypes import qint8\n",
            "from tensorflow.python.framework.dtypes import quint16\n",
            "from tensorflow.python.framework.dtypes import quint8\n",
            "from tensorflow.python.framework.dtypes import resource\n",
            "from tensorflow.python.framework.dtypes import string\n",
            "from tensorflow.python.framework.dtypes import uint16\n",
            "from tensorflow.python.framework.dtypes import uint32\n",
            "from tensorflow.python.framework.dtypes import uint64\n",
            "from tensorflow.python.framework.dtypes import uint8\n",
            "from tensorflow.python.framework.dtypes import variant\n",
            "from tensorflow.python.framework.ops import _colocate_with as colocate_with\n",
            "from tensorflow.python.framework.ops import disable_eager_execution\n",
            "from tensorflow.python.framework.ops import init_scope\n",
            "from tensorflow.python.framework.tensor_shape import dimension_at_index\n",
            "from tensorflow.python.framework.tensor_shape import dimension_value\n",
            "from tensorflow.python.framework.tensor_shape import disable_v2_tensorshape\n",
            "from tensorflow.python.framework.tensor_shape import enable_v2_tensorshape\n",
            "from tensorflow.python.framework.tensor_spec import TensorSpec\n",
            "from tensorflow.python.framework.tensor_util import constant_value as get_static_value\n",
            "from tensorflow.python.framework.tensor_util import is_tensor\n",
            "from tensorflow.python.framework.versions import COMPILER_VERSION\n",
            "from tensorflow.python.framework.versions import COMPILER_VERSION as __compiler_version__\n",
            "from tensorflow.python.framework.versions import CXX11_ABI_FLAG\n",
            "from tensorflow.python.framework.versions import CXX11_ABI_FLAG as __cxx11_abi_flag__\n",
            "from tensorflow.python.framework.versions import GIT_VERSION\n",
            "from tensorflow.python.framework.versions import GIT_VERSION as __git_version__\n",
            "from tensorflow.python.framework.versions import GRAPH_DEF_VERSION\n",
            "from tensorflow.python.framework.versions import GRAPH_DEF_VERSION_MIN_CONSUMER\n",
            "from tensorflow.python.framework.versions import GRAPH_DEF_VERSION_MIN_PRODUCER\n",
            "from tensorflow.python.framework.versions import MONOLITHIC_BUILD\n",
            "from tensorflow.python.framework.versions import MONOLITHIC_BUILD as __monolithic_build__\n",
            "from tensorflow.python.framework.versions import VERSION\n",
            "from tensorflow.python.framework.versions import VERSION as __version__\n",
            "from tensorflow.python.module.module import Module\n",
            "from tensorflow.python.ops.array_ops import newaxis\n",
            "from tensorflow.python.ops.check_ops import ensure_shape\n",
            "from tensorflow.python.ops.confusion_matrix import confusion_matrix_v1 as confusion_matrix\n",
            "from tensorflow.python.ops.control_flow_grad import switch_case\n",
            "from tensorflow.python.ops.gen_spectral_ops import fft\n",
            "from tensorflow.python.ops.gen_spectral_ops import fft2d\n",
            "from tensorflow.python.ops.gen_spectral_ops import fft3d\n",
            "from tensorflow.python.ops.gen_spectral_ops import ifft\n",
            "from tensorflow.python.ops.gen_spectral_ops import ifft2d\n",
            "from tensorflow.python.ops.gen_spectral_ops import ifft3d\n",
            "from tensorflow.python.ops.logging_ops import print_v2 as print\n",
            "from tensorflow.python.ops.map_fn import map_fn\n",
            "from tensorflow.python.ops.ragged.ragged_squeeze_op import RaggedTensor\n",
            "from tensorflow.python.ops.ragged.ragged_string_ops import string_split\n",
            "from tensorflow.python.ops.script_ops import eager_py_func as py_function\n",
            "from tensorflow.python.ops.script_ops import numpy_function\n",
            "from tensorflow.python.ops.state_ops import batch_scatter_update\n",
            "from tensorflow.python.ops.variable_scope import AUTO_REUSE\n",
            "from tensorflow.python.ops.variable_scope import disable_resource_variables\n",
            "from tensorflow.python.ops.variable_scope import enable_resource_variables\n",
            "from tensorflow.python.ops.variable_scope import resource_variables_enabled\n",
            "from tensorflow.python.ops.variable_scope import variable_creator_scope_v1 as variable_creator_scope\n",
            "from tensorflow.python.platform.tf_logging import get_logger\n",
            "_names_with_underscore = ['__version__', '__git_version__', '__compiler_version__', '__cxx11_abi_flag__', '__monolithic_build__']\n",
            "__all__ = [_s for _s in dir() if not _s.startswith('_')]\n",
            "__all__.extend([_s for _s in _names_with_underscore])\n",
            "\n",
            "\n",
            "# Make sure directory containing top level submodules is in\n",
            "# the __path__ so that \"from tensorflow.foo import bar\" works.\n",
            "# We're using bitwise, but there's nothing special about that.\n",
            "_API_MODULE = bitwise  # pylint: disable=undefined-variable\n",
            "_current_module = _sys.modules[__name__]\n",
            "_tf_api_dir = _os.path.dirname(_os.path.dirname(_API_MODULE.__file__))\n",
            "if not hasattr(_current_module, '__path__'):\n",
            "  __path__ = [_tf_api_dir]\n",
            "elif _tf_api_dir not in __path__:\n",
            "  __path__.append(_tf_api_dir)\n",
            "\n",
            "# Hook external TensorFlow modules.\n",
            "try:\n",
            "  from tensorflow_estimator.python.estimator.api._v1 import estimator\n",
            "  _current_module.__path__ = (\n",
            "      [_module_util.get_parent_dir(estimator)] + _current_module.__path__)\n",
            "except ImportError:\n",
            "  pass\n",
            "\n",
            "try:\n",
            "  from tensorflow.python.keras.api._v1 import keras\n",
            "  _current_module.__path__ = (\n",
            "      [_module_util.get_parent_dir(keras)] + _current_module.__path__)\n",
            "except ImportError:\n",
            "  pass\n",
            "\n",
            "\n",
            "from tensorflow.python.util.lazy_loader import LazyLoader  # pylint: disable=g-import-not-at-top\n",
            "_CONTRIB_WARNING = \"\"\"\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\"\"\"\n",
            "contrib = LazyLoader('contrib', globals(), 'tensorflow.contrib',\n",
            "                     _CONTRIB_WARNING)\n",
            "del LazyLoader\n",
            "# The templated code that replaces the placeholder above sometimes\n",
            "# sets the __all__ variable. If it does, we have to be sure to add\n",
            "# \"contrib\".\n",
            "if '__all__' in vars():\n",
            "  vars()['__all__'].append('contrib')\n",
            "\n",
            "from tensorflow.python.platform import flags  # pylint: disable=g-import-not-at-top\n",
            "# The 'app' module will be imported as part of the placeholder section above.\n",
            "app.flags = flags  # pylint: disable=undefined-variable\n",
            "\n",
            "# Load all plugin libraries from site-packages/tensorflow-plugins if we are\n",
            "# running under pip.\n",
            "# TODO(gunan): Enable setting an environment variable to define arbitrary plugin\n",
            "# directories.\n",
            "# TODO(gunan): Find a better location for this code snippet.\n",
            "from tensorflow.python.framework import load_library as _ll\n",
            "from tensorflow.python.lib.io import file_io as _fi\n",
            "\n",
            "# Get sitepackages directories for the python installation.\n",
            "_site_packages_dirs = []\n",
            "_site_packages_dirs += [_site.USER_SITE]\n",
            "_site_packages_dirs += [_p for _p in _sys.path if 'site-packages' in _p]\n",
            "if 'getsitepackages' in dir(_site):\n",
            "  _site_packages_dirs += _site.getsitepackages()\n",
            "\n",
            "if 'sysconfig' in dir(_distutils):\n",
            "  _site_packages_dirs += [_distutils.sysconfig.get_python_lib()]\n",
            "\n",
            "_site_packages_dirs = list(set(_site_packages_dirs))\n",
            "\n",
            "# Find the location of this exact file.\n",
            "_current_file_location = _inspect.getfile(_inspect.currentframe())\n",
            "\n",
            "def _running_from_pip_package():\n",
            "  return any(\n",
            "      _current_file_location.startswith(dir_) for dir_ in _site_packages_dirs)\n",
            "\n",
            "if _running_from_pip_package():\n",
            "  for s in _site_packages_dirs:\n",
            "    # TODO(gunan): Add sanity checks to loaded modules here.\n",
            "    plugin_dir = _os.path.join(s, 'tensorflow-plugins')\n",
            "    if _fi.file_exists(plugin_dir):\n",
            "      _ll.load_library(plugin_dir)\n",
            "\n",
            "# These symbols appear because we import the python package which\n",
            "# in turn imports from tensorflow.core and tensorflow.python. They\n",
            "# must come from this module. So python adds these symbols for the\n",
            "# resolution to succeed.\n",
            "# pylint: disable=undefined-variable\n",
            "try:\n",
            "  del python\n",
            "  if '__all__' in vars():\n",
            "    vars()['__all__'].remove('python')\n",
            "  del core\n",
            "  if '__all__' in vars():\n",
            "    vars()['__all__'].remove('core')\n",
            "except NameError:\n",
            "  # Don't fail if these modules are not available.\n",
            "  # For e.g. this file will be originally placed under tensorflow/_api/v1 which\n",
            "  # does not have 'python', 'core' directories. Then, it will be copied\n",
            "  # to tensorflow/ which does have these two directories.\n",
            "  pass\n",
            "# Similarly for compiler. Do it separately to make sure we do this even if the\n",
            "# others don't exist.\n",
            "try:\n",
            "  del compiler\n",
            "  if '__all__' in vars():\n",
            "    vars()['__all__'].remove('compiler')\n",
            "except NameError:\n",
            "  pass\n",
            "# pylint: enable=undefined-variable\n",
            "\n",
            "import sys as _sys\n",
            "from tensorflow.python.util import deprecation_wrapper as _deprecation_wrapper\n",
            "\n",
            "if not isinstance(_sys.modules[__name__], _deprecation_wrapper.DeprecationWrapper):\n",
            "  _sys.modules[__name__] = _deprecation_wrapper.DeprecationWrapper(\n",
            "      _sys.modules[__name__], \"\")\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python\n",
            "Directories ['keras', 'debug', 'profiler', 'compat', 'autograph', 'training', 'ops', 'client', 'tools', 'util', 'eager', 'lib', 'platform', 'tpu', 'layers', 'module', 'saved_model', 'feature_column', 'framework', 'estimator', '__pycache__', 'compiler', 'distribute', 'summary', 'kernel_tests', 'grappler', 'data', 'user_ops']\n",
            "Files ['__init__.py', '_pywrap_tensorflow_internal.so', 'tf2.py', 'pywrap_tensorflow.py', 'pywrap_tensorflow_internal.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Import core names of TensorFlow.\n",
            "\n",
            "Programs that want to build TensorFlow Ops and Graphs without having to import\n",
            "the constructors and utilities individually can import this file:\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import tensorflow as tf\n",
            "\"\"\"\n",
            "\n",
            "import ctypes\n",
            "import importlib\n",
            "import sys\n",
            "import traceback\n",
            "\n",
            "# TODO(drpng): write up instructions for editing this file in a doc and point to\n",
            "# the doc instead.\n",
            "# If you want to edit this file to expose modules in public tensorflow API, you\n",
            "# need to follow these steps:\n",
            "# 1. Consult with tensorflow team and get approval for adding a new API to the\n",
            "#    public interface.\n",
            "# 2. Document the module in the gen_docs_combined.py.\n",
            "# 3. Import the module in the main tensorflow namespace by adding an import\n",
            "#    statement in this file.\n",
            "# 4. Sanitize the entry point by making sure that your module does not expose\n",
            "#    transitively imported modules used for implementation, such as os, sys.\n",
            "\n",
            "# go/tf-wildcard-import\n",
            "# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "from tensorflow.python import pywrap_tensorflow\n",
            "\n",
            "# Protocol buffers\n",
            "from tensorflow.core.framework.graph_pb2 import *\n",
            "from tensorflow.core.framework.node_def_pb2 import *\n",
            "from tensorflow.core.framework.summary_pb2 import *\n",
            "from tensorflow.core.framework.attr_value_pb2 import *\n",
            "from tensorflow.core.protobuf.meta_graph_pb2 import TensorInfo\n",
            "from tensorflow.core.protobuf.meta_graph_pb2 import MetaGraphDef\n",
            "from tensorflow.core.protobuf.config_pb2 import *\n",
            "from tensorflow.core.protobuf.tensorflow_server_pb2 import *\n",
            "from tensorflow.core.util.event_pb2 import *\n",
            "\n",
            "# Framework\n",
            "from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\n",
            "from tensorflow.python.framework.versions import *\n",
            "from tensorflow.python.framework import config\n",
            "from tensorflow.python.framework import errors\n",
            "from tensorflow.python.framework import graph_util\n",
            "\n",
            "# Session\n",
            "from tensorflow.python.client.client_lib import *\n",
            "\n",
            "# Ops\n",
            "from tensorflow.python.ops.standard_ops import *\n",
            "\n",
            "# Namespaces\n",
            "from tensorflow.python.ops import initializers_ns as initializers\n",
            "\n",
            "# pylint: enable=wildcard-import\n",
            "\n",
            "# Bring in subpackages.\n",
            "from tensorflow.python import data\n",
            "from tensorflow.python import distribute\n",
            "from tensorflow.python import keras\n",
            "from tensorflow.python.feature_column import feature_column_lib as feature_column\n",
            "from tensorflow.python.layers import layers\n",
            "from tensorflow.python.module import module\n",
            "from tensorflow.python.ops import bitwise_ops as bitwise\n",
            "from tensorflow.python.ops import gradient_checker_v2\n",
            "from tensorflow.python.ops import image_ops as image\n",
            "from tensorflow.python.ops import manip_ops as manip\n",
            "from tensorflow.python.ops import metrics\n",
            "from tensorflow.python.ops import nn\n",
            "from tensorflow.python.ops import ragged\n",
            "from tensorflow.python.ops import sets\n",
            "from tensorflow.python.ops import stateful_random_ops\n",
            "from tensorflow.python.ops.distributions import distributions\n",
            "from tensorflow.python.ops.linalg import linalg\n",
            "from tensorflow.python.ops.losses import losses\n",
            "from tensorflow.python.ops.signal import signal\n",
            "from tensorflow.python.profiler import profiler\n",
            "from tensorflow.python.saved_model import saved_model\n",
            "from tensorflow.python.summary import summary\n",
            "from tensorflow.python.tpu import bfloat16 as _\n",
            "from tensorflow.python.tpu import tpu as _\n",
            "from tensorflow.python.tpu import tpu_optimizer as _\n",
            "from tensorflow.python.user_ops import user_ops\n",
            "from tensorflow.python.util import compat\n",
            "\n",
            "# Import audio ops to make sure the ops are registered.\n",
            "from tensorflow.python.ops import gen_audio_ops as _\n",
            "\n",
            "# Import boosted trees ops to make sure the ops are registered (but unused).\n",
            "from tensorflow.python.ops import gen_boosted_trees_ops as _gen_boosted_trees_ops\n",
            "\n",
            "# Import cudnn rnn ops to make sure their ops are registered.\n",
            "from tensorflow.python.ops import gen_cudnn_rnn_ops as _\n",
            "\n",
            "\n",
            "# Import the names from python/training.py as train.Name.\n",
            "from tensorflow.python.training import training as train\n",
            "\n",
            "# Sub-package for performing i/o directly instead of via ops in a graph.\n",
            "from tensorflow.python.lib.io import python_io\n",
            "\n",
            "# Make some application and test modules available.\n",
            "from tensorflow.python.platform import app\n",
            "from tensorflow.python.platform import flags\n",
            "from tensorflow.python.platform import gfile\n",
            "from tensorflow.python.platform import tf_logging as logging\n",
            "from tensorflow.python.platform import resource_loader\n",
            "from tensorflow.python.platform import sysconfig\n",
            "from tensorflow.python.platform import test\n",
            "\n",
            "from tensorflow.python.compat import v2_compat\n",
            "\n",
            "from tensorflow.python.util.all_util import make_all\n",
            "from tensorflow.python.util.tf_export import tf_export\n",
            "\n",
            "# Eager execution\n",
            "from tensorflow.python.eager.context import executing_eagerly\n",
            "from tensorflow.python.eager.remote import connect_to_remote_host\n",
            "from tensorflow.python.eager.def_function import function\n",
            "from tensorflow.python.framework.ops import enable_eager_execution\n",
            "\n",
            "# Necessary for the symbols in this module to be taken into account by\n",
            "# the namespace management system (API decorators).\n",
            "from tensorflow.python.ops import rnn\n",
            "from tensorflow.python.ops import rnn_cell\n",
            "\n",
            "# XLA JIT compiler APIs.\n",
            "from tensorflow.python.compiler.xla import jit\n",
            "from tensorflow.python.compiler.xla import xla\n",
            "\n",
            "# Required due to `rnn` and `rnn_cell` not being imported in `nn` directly\n",
            "# (due to a circular dependency issue: rnn depends on layers).\n",
            "nn.dynamic_rnn = rnn.dynamic_rnn\n",
            "nn.static_rnn = rnn.static_rnn\n",
            "nn.raw_rnn = rnn.raw_rnn\n",
            "nn.bidirectional_dynamic_rnn = rnn.bidirectional_dynamic_rnn\n",
            "nn.static_state_saving_rnn = rnn.static_state_saving_rnn\n",
            "nn.rnn_cell = rnn_cell\n",
            "\n",
            "# Export protos\n",
            "# pylint: disable=undefined-variable\n",
            "tf_export(v1=['AttrValue'])(AttrValue)\n",
            "tf_export(v1=['ConfigProto'])(ConfigProto)\n",
            "tf_export(v1=['Event', 'summary.Event'])(Event)\n",
            "tf_export(v1=['GPUOptions'])(GPUOptions)\n",
            "tf_export(v1=['GraphDef'])(GraphDef)\n",
            "tf_export(v1=['GraphOptions'])(GraphOptions)\n",
            "tf_export(v1=['HistogramProto'])(HistogramProto)\n",
            "tf_export(v1=['LogMessage'])(LogMessage)\n",
            "tf_export(v1=['MetaGraphDef'])(MetaGraphDef)\n",
            "tf_export(v1=['NameAttrList'])(NameAttrList)\n",
            "tf_export(v1=['NodeDef'])(NodeDef)\n",
            "tf_export(v1=['OptimizerOptions'])(OptimizerOptions)\n",
            "tf_export(v1=['RunMetadata'])(RunMetadata)\n",
            "tf_export(v1=['RunOptions'])(RunOptions)\n",
            "tf_export(v1=['SessionLog', 'summary.SessionLog'])(SessionLog)\n",
            "tf_export(v1=['Summary', 'summary.Summary'])(Summary)\n",
            "tf_export(v1=['summary.SummaryDescription'])(SummaryDescription)\n",
            "tf_export(v1=['SummaryMetadata'])(SummaryMetadata)\n",
            "tf_export(v1=['summary.TaggedRunMetadata'])(TaggedRunMetadata)\n",
            "tf_export(v1=['TensorInfo'])(TensorInfo)\n",
            "# pylint: enable=undefined-variable\n",
            "\n",
            "# Special dunders that we choose to export:\n",
            "_exported_dunders = set([\n",
            "    '__version__',\n",
            "    '__git_version__',\n",
            "    '__compiler_version__',\n",
            "    '__cxx11_abi_flag__',\n",
            "    '__monolithic_build__',\n",
            "])\n",
            "\n",
            "# Expose symbols minus dunders, unless they are whitelisted above.\n",
            "# This is necessary to export our dunders.\n",
            "__all__ = [s for s in dir() if s in _exported_dunders or not s.startswith('_')]\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Tools to help with the TensorFlow 2.0 transition.\n",
            "\n",
            "This module is meant for TensorFlow internal implementation, not for users of\n",
            "the TensorFlow library. For that see tf.compat instead.\n",
            "\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import os\n",
            "\n",
            "\n",
            "_force_enable = False\n",
            "\n",
            "\n",
            "def enable():\n",
            "  \"\"\"Enables v2 behaviors.\"\"\"\n",
            "  global _force_enable\n",
            "  _force_enable = True\n",
            "\n",
            "\n",
            "def disable():\n",
            "  \"\"\"Disables v2 behaviors (TF2_BEHAVIOR env variable is still respected).\"\"\"\n",
            "  global _force_enable\n",
            "  _force_enable = False\n",
            "\n",
            "\n",
            "def enabled():\n",
            "  \"\"\"Returns True iff TensorFlow 2.0 behavior should be enabled.\"\"\"\n",
            "  return _force_enable or os.getenv(\"TF2_BEHAVIOR\", \"0\") != \"0\"\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# =============================================================================\n",
            "\"\"\"A wrapper for TensorFlow SWIG-generated bindings.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import ctypes\n",
            "import sys\n",
            "import traceback\n",
            "\n",
            "from tensorflow.python.platform import self_check\n",
            "\n",
            "\n",
            "# Perform pre-load sanity checks in order to produce a more actionable error\n",
            "# than we get from an error during SWIG import.\n",
            "self_check.preload_check()\n",
            "\n",
            "# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n",
            "\n",
            "try:\n",
            "  # This import is expected to fail if there is an explicit shared object\n",
            "  # dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\n",
            "  from tensorflow.python import pywrap_dlopen_global_flags\n",
            "  _use_dlopen_global_flags = True\n",
            "except ImportError:\n",
            "  _use_dlopen_global_flags = False\n",
            "\n",
            "# On UNIX-based platforms, pywrap_tensorflow is a SWIG-generated\n",
            "# python library that dynamically loads _pywrap_tensorflow.so.\n",
            "_can_set_rtld_local = (hasattr(sys, 'getdlopenflags')\n",
            "                       and hasattr(sys, 'setdlopenflags'))\n",
            "if _can_set_rtld_local:\n",
            "  _default_dlopen_flags = sys.getdlopenflags()\n",
            "\n",
            "try:\n",
            "  if _use_dlopen_global_flags:\n",
            "    pywrap_dlopen_global_flags.set_dlopen_flags()\n",
            "  elif _can_set_rtld_local:\n",
            "    # Ensure RTLD_LOCAL behavior for platforms where it isn't the default\n",
            "    # (macOS). On Linux RTLD_LOCAL is 0, so this does nothing (and would not\n",
            "    # override an RTLD_GLOBAL in _default_dlopen_flags).\n",
            "    sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_LOCAL)\n",
            "\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import *\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import __version__\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import __git_version__\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import __compiler_version__\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import __cxx11_abi_flag__\n",
            "  from tensorflow.python.pywrap_tensorflow_internal import __monolithic_build__\n",
            "\n",
            "  if _use_dlopen_global_flags:\n",
            "    pywrap_dlopen_global_flags.reset_dlopen_flags()\n",
            "  elif _can_set_rtld_local:\n",
            "    sys.setdlopenflags(_default_dlopen_flags)\n",
            "except ImportError:\n",
            "  msg = \"\"\"%s\\n\\nFailed to load the native TensorFlow runtime.\\n\n",
            "See https://www.tensorflow.org/install/errors\\n\n",
            "for some common reasons and solutions.  Include the entire stack trace\n",
            "above this error message when asking for help.\"\"\" % traceback.format_exc()\n",
            "  raise ImportError(msg)\n",
            "\n",
            "# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# This file was automatically generated by SWIG (http://www.swig.org).\n",
            "# Version 3.0.8\n",
            "#\n",
            "# Do not make changes to this file unless you know what you are doing--modify\n",
            "# the SWIG interface file instead.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "from sys import version_info\n",
            "if version_info >= (2, 6, 0):\n",
            "    def swig_import_helper():\n",
            "        from os.path import dirname\n",
            "        import imp\n",
            "        fp = None\n",
            "        try:\n",
            "            fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\n",
            "        except ImportError:\n",
            "            import _pywrap_tensorflow_internal\n",
            "            return _pywrap_tensorflow_internal\n",
            "        if fp is not None:\n",
            "            try:\n",
            "                _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
            "            finally:\n",
            "                fp.close()\n",
            "            return _mod\n",
            "    _pywrap_tensorflow_internal = swig_import_helper()\n",
            "    del swig_import_helper\n",
            "else:\n",
            "    import _pywrap_tensorflow_internal\n",
            "del version_info\n",
            "try:\n",
            "    _swig_property = property\n",
            "except NameError:\n",
            "    pass  # Python < 2.2 doesn't have 'property'.\n",
            "\n",
            "\n",
            "def _swig_setattr_nondynamic(self, class_type, name, value, static=1):\n",
            "    if (name == \"thisown\"):\n",
            "        return self.this.own(value)\n",
            "    if (name == \"this\"):\n",
            "        if type(value).__name__ == 'SwigPyObject':\n",
            "            self.__dict__[name] = value\n",
            "            return\n",
            "    method = class_type.__swig_setmethods__.get(name, None)\n",
            "    if method:\n",
            "        return method(self, value)\n",
            "    if (not static):\n",
            "        if _newclass:\n",
            "            object.__setattr__(self, name, value)\n",
            "        else:\n",
            "            self.__dict__[name] = value\n",
            "    else:\n",
            "        raise AttributeError(\"You cannot add attributes to %s\" % self)\n",
            "\n",
            "\n",
            "def _swig_setattr(self, class_type, name, value):\n",
            "    return _swig_setattr_nondynamic(self, class_type, name, value, 0)\n",
            "\n",
            "\n",
            "def _swig_getattr_nondynamic(self, class_type, name, static=1):\n",
            "    if (name == \"thisown\"):\n",
            "        return self.this.own()\n",
            "    method = class_type.__swig_getmethods__.get(name, None)\n",
            "    if method:\n",
            "        return method(self)\n",
            "    if (not static):\n",
            "        return object.__getattr__(self, name)\n",
            "    else:\n",
            "        raise AttributeError(name)\n",
            "\n",
            "def _swig_getattr(self, class_type, name):\n",
            "    return _swig_getattr_nondynamic(self, class_type, name, 0)\n",
            "\n",
            "\n",
            "def _swig_repr(self):\n",
            "    try:\n",
            "        strthis = \"proxy of \" + self.this.__repr__()\n",
            "    except Exception:\n",
            "        strthis = \"\"\n",
            "    return \"<%s.%s; %s >\" % (self.__class__.__module__, self.__class__.__name__, strthis,)\n",
            "\n",
            "try:\n",
            "    _object = object\n",
            "    _newclass = 1\n",
            "except AttributeError:\n",
            "    class _object:\n",
            "        pass\n",
            "    _newclass = 0\n",
            "\n",
            "\n",
            "\n",
            "def TF_ListPhysicalDevices():\n",
            "    return _pywrap_tensorflow_internal.TF_ListPhysicalDevices()\n",
            "TF_ListPhysicalDevices = _pywrap_tensorflow_internal.TF_ListPhysicalDevices\n",
            "\n",
            "def TFE_NewContextOptions():\n",
            "    return _pywrap_tensorflow_internal.TFE_NewContextOptions()\n",
            "TFE_NewContextOptions = _pywrap_tensorflow_internal.TFE_NewContextOptions\n",
            "\n",
            "def TFE_ContextOptionsSetConfig(options, proto):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig(options, proto)\n",
            "TFE_ContextOptionsSetConfig = _pywrap_tensorflow_internal.TFE_ContextOptionsSetConfig\n",
            "\n",
            "_pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TFE_DEVICE_PLACEMENT_EXPLICIT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_EXPLICIT\n",
            "\n",
            "_pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN_swigconstant(_pywrap_tensorflow_internal)\n",
            "TFE_DEVICE_PLACEMENT_WARN = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_WARN\n",
            "\n",
            "_pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TFE_DEVICE_PLACEMENT_SILENT = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT\n",
            "\n",
            "_pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32_swigconstant(_pywrap_tensorflow_internal)\n",
            "TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 = _pywrap_tensorflow_internal.TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32\n",
            "\n",
            "def TFE_ContextOptionsSetAsync(arg1, enable):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetAsync(arg1, enable)\n",
            "TFE_ContextOptionsSetAsync = _pywrap_tensorflow_internal.TFE_ContextOptionsSetAsync\n",
            "\n",
            "def TFE_ContextOptionsSetDevicePlacementPolicy(arg1, arg2):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextOptionsSetDevicePlacementPolicy(arg1, arg2)\n",
            "TFE_ContextOptionsSetDevicePlacementPolicy = _pywrap_tensorflow_internal.TFE_ContextOptionsSetDevicePlacementPolicy\n",
            "\n",
            "def TFE_DeleteContextOptions(arg1):\n",
            "    return _pywrap_tensorflow_internal.TFE_DeleteContextOptions(arg1)\n",
            "TFE_DeleteContextOptions = _pywrap_tensorflow_internal.TFE_DeleteContextOptions\n",
            "\n",
            "def TFE_NewContext(opts):\n",
            "    return _pywrap_tensorflow_internal.TFE_NewContext(opts)\n",
            "TFE_NewContext = _pywrap_tensorflow_internal.TFE_NewContext\n",
            "\n",
            "def TFE_DeleteContext(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_DeleteContext(ctx)\n",
            "TFE_DeleteContext = _pywrap_tensorflow_internal.TFE_DeleteContext\n",
            "\n",
            "def TFE_ContextListDevices(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextListDevices(ctx)\n",
            "TFE_ContextListDevices = _pywrap_tensorflow_internal.TFE_ContextListDevices\n",
            "\n",
            "def TFE_ContextClearCaches(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextClearCaches(ctx)\n",
            "TFE_ContextClearCaches = _pywrap_tensorflow_internal.TFE_ContextClearCaches\n",
            "\n",
            "def TFE_ContextSetThreadLocalDevicePlacementPolicy(arg1, arg2):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextSetThreadLocalDevicePlacementPolicy(arg1, arg2)\n",
            "TFE_ContextSetThreadLocalDevicePlacementPolicy = _pywrap_tensorflow_internal.TFE_ContextSetThreadLocalDevicePlacementPolicy\n",
            "\n",
            "def TFE_ContextGetDevicePlacementPolicy(arg1):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextGetDevicePlacementPolicy(arg1)\n",
            "TFE_ContextGetDevicePlacementPolicy = _pywrap_tensorflow_internal.TFE_ContextGetDevicePlacementPolicy\n",
            "\n",
            "def TFE_ContextSetAsyncForThread(arg1, enable):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextSetAsyncForThread(arg1, enable)\n",
            "TFE_ContextSetAsyncForThread = _pywrap_tensorflow_internal.TFE_ContextSetAsyncForThread\n",
            "\n",
            "def TFE_ContextSetServerDef(ctx, keep_alive_secs, proto):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextSetServerDef(ctx, keep_alive_secs, proto)\n",
            "TFE_ContextSetServerDef = _pywrap_tensorflow_internal.TFE_ContextSetServerDef\n",
            "\n",
            "def TFE_ContextAsyncWait(arg1):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextAsyncWait(arg1)\n",
            "TFE_ContextAsyncWait = _pywrap_tensorflow_internal.TFE_ContextAsyncWait\n",
            "\n",
            "def TFE_ContextAsyncClearError(arg1):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextAsyncClearError(arg1)\n",
            "TFE_ContextAsyncClearError = _pywrap_tensorflow_internal.TFE_ContextAsyncClearError\n",
            "\n",
            "def TFE_OpNameGetAttrType(ctx, op_or_function_name, attr_name):\n",
            "    return _pywrap_tensorflow_internal.TFE_OpNameGetAttrType(ctx, op_or_function_name, attr_name)\n",
            "TFE_OpNameGetAttrType = _pywrap_tensorflow_internal.TFE_OpNameGetAttrType\n",
            "\n",
            "def TFE_ContextAddFunctionDef(ctx, serialized_function_def, size):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextAddFunctionDef(ctx, serialized_function_def, size)\n",
            "TFE_ContextAddFunctionDef = _pywrap_tensorflow_internal.TFE_ContextAddFunctionDef\n",
            "\n",
            "def TFE_ContextAddFunction(ctx, function):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextAddFunction(ctx, function)\n",
            "TFE_ContextAddFunction = _pywrap_tensorflow_internal.TFE_ContextAddFunction\n",
            "\n",
            "def TFE_ContextRemoveFunction(ctx, name):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextRemoveFunction(ctx, name)\n",
            "TFE_ContextRemoveFunction = _pywrap_tensorflow_internal.TFE_ContextRemoveFunction\n",
            "\n",
            "def TFE_ContextHasFunction(ctx, name):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextHasFunction(ctx, name)\n",
            "TFE_ContextHasFunction = _pywrap_tensorflow_internal.TFE_ContextHasFunction\n",
            "\n",
            "def TFE_ContextEnableRunMetadata(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextEnableRunMetadata(ctx)\n",
            "TFE_ContextEnableRunMetadata = _pywrap_tensorflow_internal.TFE_ContextEnableRunMetadata\n",
            "\n",
            "def TFE_ContextDisableRunMetadata(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextDisableRunMetadata(ctx)\n",
            "TFE_ContextDisableRunMetadata = _pywrap_tensorflow_internal.TFE_ContextDisableRunMetadata\n",
            "\n",
            "def TFE_ContextExportRunMetadata(ctx, buf):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextExportRunMetadata(ctx, buf)\n",
            "TFE_ContextExportRunMetadata = _pywrap_tensorflow_internal.TFE_ContextExportRunMetadata\n",
            "\n",
            "def TFE_ContextStartStep(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextStartStep(ctx)\n",
            "TFE_ContextStartStep = _pywrap_tensorflow_internal.TFE_ContextStartStep\n",
            "\n",
            "def TFE_ContextEndStep(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextEndStep(ctx)\n",
            "TFE_ContextEndStep = _pywrap_tensorflow_internal.TFE_ContextEndStep\n",
            "\n",
            "def TFE_Py_Execute(ctx, device_name, op_name, inputs, attrs, outputs):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_Execute(ctx, device_name, op_name, inputs, attrs, outputs)\n",
            "TFE_Py_Execute = _pywrap_tensorflow_internal.TFE_Py_Execute\n",
            "\n",
            "def TFE_Py_RegisterExceptionClass(e):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RegisterExceptionClass(e)\n",
            "TFE_Py_RegisterExceptionClass = _pywrap_tensorflow_internal.TFE_Py_RegisterExceptionClass\n",
            "\n",
            "def TFE_Py_RegisterResourceVariableType(e):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RegisterResourceVariableType(e)\n",
            "TFE_Py_RegisterResourceVariableType = _pywrap_tensorflow_internal.TFE_Py_RegisterResourceVariableType\n",
            "\n",
            "def TFE_Py_RegisterVSpace(e):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RegisterVSpace(e)\n",
            "TFE_Py_RegisterVSpace = _pywrap_tensorflow_internal.TFE_Py_RegisterVSpace\n",
            "\n",
            "def TFE_Py_RegisterFallbackExceptionClass(e):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RegisterFallbackExceptionClass(e)\n",
            "TFE_Py_RegisterFallbackExceptionClass = _pywrap_tensorflow_internal.TFE_Py_RegisterFallbackExceptionClass\n",
            "\n",
            "def TFE_Py_RegisterGradientFunction(e):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RegisterGradientFunction(e)\n",
            "TFE_Py_RegisterGradientFunction = _pywrap_tensorflow_internal.TFE_Py_RegisterGradientFunction\n",
            "\n",
            "def TFE_Py_UID():\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_UID()\n",
            "TFE_Py_UID = _pywrap_tensorflow_internal.TFE_Py_UID\n",
            "\n",
            "def TFE_Py_InitEagerTensor(base_class):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_InitEagerTensor(base_class)\n",
            "TFE_Py_InitEagerTensor = _pywrap_tensorflow_internal.TFE_Py_InitEagerTensor\n",
            "\n",
            "def TFE_Py_SetEagerTensorProfiler(profiler):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_SetEagerTensorProfiler(profiler)\n",
            "TFE_Py_SetEagerTensorProfiler = _pywrap_tensorflow_internal.TFE_Py_SetEagerTensorProfiler\n",
            "\n",
            "def TFE_Py_TapeSetNew(persistent, watch_accessed_variables):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetNew(persistent, watch_accessed_variables)\n",
            "TFE_Py_TapeSetNew = _pywrap_tensorflow_internal.TFE_Py_TapeSetNew\n",
            "\n",
            "def TFE_Py_TapeSetRemove(tape):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetRemove(tape)\n",
            "TFE_Py_TapeSetRemove = _pywrap_tensorflow_internal.TFE_Py_TapeSetRemove\n",
            "\n",
            "def TFE_Py_TapeSetAdd(tape):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetAdd(tape)\n",
            "TFE_Py_TapeSetAdd = _pywrap_tensorflow_internal.TFE_Py_TapeSetAdd\n",
            "\n",
            "def TFE_Py_TapeSetIsEmpty():\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetIsEmpty()\n",
            "TFE_Py_TapeSetIsEmpty = _pywrap_tensorflow_internal.TFE_Py_TapeSetIsEmpty\n",
            "\n",
            "def TFE_Py_TapeSetShouldRecord(tensors):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetShouldRecord(tensors)\n",
            "TFE_Py_TapeSetShouldRecord = _pywrap_tensorflow_internal.TFE_Py_TapeSetShouldRecord\n",
            "\n",
            "def TFE_Py_TapeWatch(tape, tensor):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeWatch(tape, tensor)\n",
            "TFE_Py_TapeWatch = _pywrap_tensorflow_internal.TFE_Py_TapeWatch\n",
            "\n",
            "def TFE_Py_TapeSetDeleteTrace(tensor_id):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetDeleteTrace(tensor_id)\n",
            "TFE_Py_TapeSetDeleteTrace = _pywrap_tensorflow_internal.TFE_Py_TapeSetDeleteTrace\n",
            "\n",
            "def TFE_Py_TapeSetStopOnThread():\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetStopOnThread()\n",
            "TFE_Py_TapeSetStopOnThread = _pywrap_tensorflow_internal.TFE_Py_TapeSetStopOnThread\n",
            "\n",
            "def TFE_Py_TapeSetRestartOnThread():\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetRestartOnThread()\n",
            "TFE_Py_TapeSetRestartOnThread = _pywrap_tensorflow_internal.TFE_Py_TapeSetRestartOnThread\n",
            "\n",
            "def TFE_Py_TapeSetRecordOperation(op_type, output_tensors, input_tensor_ids, backward_function):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeSetRecordOperation(op_type, output_tensors, input_tensor_ids, backward_function)\n",
            "TFE_Py_TapeSetRecordOperation = _pywrap_tensorflow_internal.TFE_Py_TapeSetRecordOperation\n",
            "\n",
            "def TFE_Py_TapeVariableAccessed(variable):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeVariableAccessed(variable)\n",
            "TFE_Py_TapeVariableAccessed = _pywrap_tensorflow_internal.TFE_Py_TapeVariableAccessed\n",
            "\n",
            "def TFE_Py_TapeWatchVariable(tape, variable):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeWatchVariable(tape, variable)\n",
            "TFE_Py_TapeWatchVariable = _pywrap_tensorflow_internal.TFE_Py_TapeWatchVariable\n",
            "\n",
            "def TFE_Py_TapeGradient(tape, target, sources, output_gradients, sources_raw, unconnected_gradients):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeGradient(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n",
            "TFE_Py_TapeGradient = _pywrap_tensorflow_internal.TFE_Py_TapeGradient\n",
            "\n",
            "def TFE_Py_RecordGradient(op_name, inputs, attrs, results, name):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_RecordGradient(op_name, inputs, attrs, results, name)\n",
            "TFE_Py_RecordGradient = _pywrap_tensorflow_internal.TFE_Py_RecordGradient\n",
            "\n",
            "def TFE_Py_TapeWatchedVariables(tape):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TapeWatchedVariables(tape)\n",
            "TFE_Py_TapeWatchedVariables = _pywrap_tensorflow_internal.TFE_Py_TapeWatchedVariables\n",
            "\n",
            "def TFE_Py_TensorShapeSlice(tensors, slice_dim):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TensorShapeSlice(tensors, slice_dim)\n",
            "TFE_Py_TensorShapeSlice = _pywrap_tensorflow_internal.TFE_Py_TensorShapeSlice\n",
            "\n",
            "def TFE_Py_TensorShapeOnDevice(tensor):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_TensorShapeOnDevice(tensor)\n",
            "TFE_Py_TensorShapeOnDevice = _pywrap_tensorflow_internal.TFE_Py_TensorShapeOnDevice\n",
            "\n",
            "def TFE_Py_EncodeArg(arg1, include_tensor_ranks_only):\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_EncodeArg(arg1, include_tensor_ranks_only)\n",
            "TFE_Py_EncodeArg = _pywrap_tensorflow_internal.TFE_Py_EncodeArg\n",
            "\n",
            "def TFE_Py_EnableInteractivePythonLogging():\n",
            "    return _pywrap_tensorflow_internal.TFE_Py_EnableInteractivePythonLogging()\n",
            "TFE_Py_EnableInteractivePythonLogging = _pywrap_tensorflow_internal.TFE_Py_EnableInteractivePythonLogging\n",
            "\n",
            "def TF_SetXlaEnableLazyCompilation(enable):\n",
            "    return _pywrap_tensorflow_internal.TF_SetXlaEnableLazyCompilation(enable)\n",
            "TF_SetXlaEnableLazyCompilation = _pywrap_tensorflow_internal.TF_SetXlaEnableLazyCompilation\n",
            "\n",
            "def TF_SetXLaAutoJitMode(mode):\n",
            "    return _pywrap_tensorflow_internal.TF_SetXLaAutoJitMode(mode)\n",
            "TF_SetXLaAutoJitMode = _pywrap_tensorflow_internal.TF_SetXLaAutoJitMode\n",
            "\n",
            "def TF_SetXlaMinClusterSize(size):\n",
            "    return _pywrap_tensorflow_internal.TF_SetXlaMinClusterSize(size)\n",
            "TF_SetXlaMinClusterSize = _pywrap_tensorflow_internal.TF_SetXlaMinClusterSize\n",
            "\n",
            "def TF_PickUnusedPortOrDie():\n",
            "    return _pywrap_tensorflow_internal.TF_PickUnusedPortOrDie()\n",
            "TF_PickUnusedPortOrDie = _pywrap_tensorflow_internal.TF_PickUnusedPortOrDie\n",
            "\n",
            "def TFE_EnableCollectiveOps(ctx, proto):\n",
            "    return _pywrap_tensorflow_internal.TFE_EnableCollectiveOps(ctx, proto)\n",
            "TFE_EnableCollectiveOps = _pywrap_tensorflow_internal.TFE_EnableCollectiveOps\n",
            "\n",
            "def TFE_NewProfiler(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_NewProfiler(ctx)\n",
            "TFE_NewProfiler = _pywrap_tensorflow_internal.TFE_NewProfiler\n",
            "\n",
            "def TFE_ProfilerIsOk(profiler):\n",
            "    return _pywrap_tensorflow_internal.TFE_ProfilerIsOk(profiler)\n",
            "TFE_ProfilerIsOk = _pywrap_tensorflow_internal.TFE_ProfilerIsOk\n",
            "\n",
            "def TFE_DeleteProfiler(profiler):\n",
            "    return _pywrap_tensorflow_internal.TFE_DeleteProfiler(profiler)\n",
            "TFE_DeleteProfiler = _pywrap_tensorflow_internal.TFE_DeleteProfiler\n",
            "\n",
            "def TFE_ProfilerSerializeToString(ctx, profiler, buf):\n",
            "    return _pywrap_tensorflow_internal.TFE_ProfilerSerializeToString(ctx, profiler, buf)\n",
            "TFE_ProfilerSerializeToString = _pywrap_tensorflow_internal.TFE_ProfilerSerializeToString\n",
            "\n",
            "def TFE_NewProfilerContext():\n",
            "    return _pywrap_tensorflow_internal.TFE_NewProfilerContext()\n",
            "TFE_NewProfilerContext = _pywrap_tensorflow_internal.TFE_NewProfilerContext\n",
            "\n",
            "def TFE_ProfilerContextSetEagerContext(profiler_context, eager_context):\n",
            "    return _pywrap_tensorflow_internal.TFE_ProfilerContextSetEagerContext(profiler_context, eager_context)\n",
            "TFE_ProfilerContextSetEagerContext = _pywrap_tensorflow_internal.TFE_ProfilerContextSetEagerContext\n",
            "\n",
            "def TFE_DeleteProfilerContext(profiler_context):\n",
            "    return _pywrap_tensorflow_internal.TFE_DeleteProfilerContext(profiler_context)\n",
            "TFE_DeleteProfilerContext = _pywrap_tensorflow_internal.TFE_DeleteProfilerContext\n",
            "\n",
            "def TFE_StartProfilerServer(context, port):\n",
            "    return _pywrap_tensorflow_internal.TFE_StartProfilerServer(context, port)\n",
            "TFE_StartProfilerServer = _pywrap_tensorflow_internal.TFE_StartProfilerServer\n",
            "\n",
            "def TFE_ContextEnableGraphCollection(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextEnableGraphCollection(ctx)\n",
            "TFE_ContextEnableGraphCollection = _pywrap_tensorflow_internal.TFE_ContextEnableGraphCollection\n",
            "\n",
            "def TFE_ContextDisableGraphCollection(ctx):\n",
            "    return _pywrap_tensorflow_internal.TFE_ContextDisableGraphCollection(ctx)\n",
            "TFE_ContextDisableGraphCollection = _pywrap_tensorflow_internal.TFE_ContextDisableGraphCollection\n",
            "\n",
            "def TFE_ProfilerClientStartTracing(service_addr, logdir, worker_list, include_dataset_ops, duration_ms, num_tracing_attempts):\n",
            "    return _pywrap_tensorflow_internal.TFE_ProfilerClientStartTracing(service_addr, logdir, worker_list, include_dataset_ops, duration_ms, num_tracing_attempts)\n",
            "TFE_ProfilerClientStartTracing = _pywrap_tensorflow_internal.TFE_ProfilerClientStartTracing\n",
            "\n",
            "def TFE_MonitoringCounterCellIncrementBy(cell, value):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringCounterCellIncrementBy(cell, value)\n",
            "TFE_MonitoringCounterCellIncrementBy = _pywrap_tensorflow_internal.TFE_MonitoringCounterCellIncrementBy\n",
            "\n",
            "def TFE_MonitoringCounterCellValue(cell):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringCounterCellValue(cell)\n",
            "TFE_MonitoringCounterCellValue = _pywrap_tensorflow_internal.TFE_MonitoringCounterCellValue\n",
            "\n",
            "def TFE_MonitoringNewCounter0(name, description):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewCounter0(name, description)\n",
            "TFE_MonitoringNewCounter0 = _pywrap_tensorflow_internal.TFE_MonitoringNewCounter0\n",
            "\n",
            "def TFE_MonitoringDeleteCounter0(counter):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter0(counter)\n",
            "TFE_MonitoringDeleteCounter0 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter0\n",
            "\n",
            "def TFE_MonitoringGetCellCounter0(counter):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter0(counter)\n",
            "TFE_MonitoringGetCellCounter0 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter0\n",
            "\n",
            "def TFE_MonitoringNewCounter1(name, description, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewCounter1(name, description, label1)\n",
            "TFE_MonitoringNewCounter1 = _pywrap_tensorflow_internal.TFE_MonitoringNewCounter1\n",
            "\n",
            "def TFE_MonitoringDeleteCounter1(counter):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter1(counter)\n",
            "TFE_MonitoringDeleteCounter1 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter1\n",
            "\n",
            "def TFE_MonitoringGetCellCounter1(counter, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter1(counter, label1)\n",
            "TFE_MonitoringGetCellCounter1 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter1\n",
            "\n",
            "def TFE_MonitoringNewCounter2(name, description, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewCounter2(name, description, label1, label2)\n",
            "TFE_MonitoringNewCounter2 = _pywrap_tensorflow_internal.TFE_MonitoringNewCounter2\n",
            "\n",
            "def TFE_MonitoringDeleteCounter2(counter):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter2(counter)\n",
            "TFE_MonitoringDeleteCounter2 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteCounter2\n",
            "\n",
            "def TFE_MonitoringGetCellCounter2(counter, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter2(counter, label1, label2)\n",
            "TFE_MonitoringGetCellCounter2 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellCounter2\n",
            "\n",
            "def TFE_MonitoringIntGaugeCellSet(cell, value):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringIntGaugeCellSet(cell, value)\n",
            "TFE_MonitoringIntGaugeCellSet = _pywrap_tensorflow_internal.TFE_MonitoringIntGaugeCellSet\n",
            "\n",
            "def TFE_MonitoringIntGaugeCellValue(cell):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringIntGaugeCellValue(cell)\n",
            "TFE_MonitoringIntGaugeCellValue = _pywrap_tensorflow_internal.TFE_MonitoringIntGaugeCellValue\n",
            "\n",
            "def TFE_MonitoringNewIntGauge0(name, description):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge0(name, description)\n",
            "TFE_MonitoringNewIntGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge0\n",
            "\n",
            "def TFE_MonitoringDeleteIntGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge0(gauge)\n",
            "TFE_MonitoringDeleteIntGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge0\n",
            "\n",
            "def TFE_MonitoringGetCellIntGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge0(gauge)\n",
            "TFE_MonitoringGetCellIntGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge0\n",
            "\n",
            "def TFE_MonitoringNewIntGauge1(name, description, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge1(name, description, label1)\n",
            "TFE_MonitoringNewIntGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge1\n",
            "\n",
            "def TFE_MonitoringDeleteIntGauge1(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge1(gauge)\n",
            "TFE_MonitoringDeleteIntGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge1\n",
            "\n",
            "def TFE_MonitoringGetCellIntGauge1(gauge, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge1(gauge, label1)\n",
            "TFE_MonitoringGetCellIntGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge1\n",
            "\n",
            "def TFE_MonitoringNewIntGauge2(name, description, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge2(name, description, label1, label2)\n",
            "TFE_MonitoringNewIntGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringNewIntGauge2\n",
            "\n",
            "def TFE_MonitoringDeleteIntGauge2(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge2(gauge)\n",
            "TFE_MonitoringDeleteIntGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteIntGauge2\n",
            "\n",
            "def TFE_MonitoringGetCellIntGauge2(gauge, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge2(gauge, label1, label2)\n",
            "TFE_MonitoringGetCellIntGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellIntGauge2\n",
            "\n",
            "def TFE_MonitoringStringGaugeCellSet(cell, value):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringStringGaugeCellSet(cell, value)\n",
            "TFE_MonitoringStringGaugeCellSet = _pywrap_tensorflow_internal.TFE_MonitoringStringGaugeCellSet\n",
            "\n",
            "def TFE_MonitoringStringGaugeCellValue(cell, buf):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringStringGaugeCellValue(cell, buf)\n",
            "TFE_MonitoringStringGaugeCellValue = _pywrap_tensorflow_internal.TFE_MonitoringStringGaugeCellValue\n",
            "\n",
            "def TFE_MonitoringNewStringGauge0(name, description):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge0(name, description)\n",
            "TFE_MonitoringNewStringGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge0\n",
            "\n",
            "def TFE_MonitoringDeleteStringGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge0(gauge)\n",
            "TFE_MonitoringDeleteStringGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge0\n",
            "\n",
            "def TFE_MonitoringGetCellStringGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge0(gauge)\n",
            "TFE_MonitoringGetCellStringGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge0\n",
            "\n",
            "def TFE_MonitoringNewStringGauge1(name, description, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge1(name, description, label1)\n",
            "TFE_MonitoringNewStringGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge1\n",
            "\n",
            "def TFE_MonitoringDeleteStringGauge1(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge1(gauge)\n",
            "TFE_MonitoringDeleteStringGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge1\n",
            "\n",
            "def TFE_MonitoringGetCellStringGauge1(gauge, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge1(gauge, label1)\n",
            "TFE_MonitoringGetCellStringGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge1\n",
            "\n",
            "def TFE_MonitoringNewStringGauge2(name, description, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge2(name, description, label1, label2)\n",
            "TFE_MonitoringNewStringGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringNewStringGauge2\n",
            "\n",
            "def TFE_MonitoringDeleteStringGauge2(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge2(gauge)\n",
            "TFE_MonitoringDeleteStringGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteStringGauge2\n",
            "\n",
            "def TFE_MonitoringGetCellStringGauge2(gauge, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge2(gauge, label1, label2)\n",
            "TFE_MonitoringGetCellStringGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellStringGauge2\n",
            "\n",
            "def TFE_MonitoringBoolGaugeCellSet(cell, value):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringBoolGaugeCellSet(cell, value)\n",
            "TFE_MonitoringBoolGaugeCellSet = _pywrap_tensorflow_internal.TFE_MonitoringBoolGaugeCellSet\n",
            "\n",
            "def TFE_MonitoringBoolGaugeCellValue(cell):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringBoolGaugeCellValue(cell)\n",
            "TFE_MonitoringBoolGaugeCellValue = _pywrap_tensorflow_internal.TFE_MonitoringBoolGaugeCellValue\n",
            "\n",
            "def TFE_MonitoringNewBoolGauge0(name, description):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge0(name, description)\n",
            "TFE_MonitoringNewBoolGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge0\n",
            "\n",
            "def TFE_MonitoringDeleteBoolGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge0(gauge)\n",
            "TFE_MonitoringDeleteBoolGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge0\n",
            "\n",
            "def TFE_MonitoringGetCellBoolGauge0(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge0(gauge)\n",
            "TFE_MonitoringGetCellBoolGauge0 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge0\n",
            "\n",
            "def TFE_MonitoringNewBoolGauge1(name, description, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge1(name, description, label1)\n",
            "TFE_MonitoringNewBoolGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge1\n",
            "\n",
            "def TFE_MonitoringDeleteBoolGauge1(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge1(gauge)\n",
            "TFE_MonitoringDeleteBoolGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge1\n",
            "\n",
            "def TFE_MonitoringGetCellBoolGauge1(gauge, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge1(gauge, label1)\n",
            "TFE_MonitoringGetCellBoolGauge1 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge1\n",
            "\n",
            "def TFE_MonitoringNewBoolGauge2(name, description, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge2(name, description, label1, label2)\n",
            "TFE_MonitoringNewBoolGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringNewBoolGauge2\n",
            "\n",
            "def TFE_MonitoringDeleteBoolGauge2(gauge):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge2(gauge)\n",
            "TFE_MonitoringDeleteBoolGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteBoolGauge2\n",
            "\n",
            "def TFE_MonitoringGetCellBoolGauge2(gauge, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge2(gauge, label1, label2)\n",
            "TFE_MonitoringGetCellBoolGauge2 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellBoolGauge2\n",
            "\n",
            "def TFE_MonitoringSamplerCellAdd(cell, value):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringSamplerCellAdd(cell, value)\n",
            "TFE_MonitoringSamplerCellAdd = _pywrap_tensorflow_internal.TFE_MonitoringSamplerCellAdd\n",
            "\n",
            "def TFE_MonitoringSamplerCellValue(cell, buf):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringSamplerCellValue(cell, buf)\n",
            "TFE_MonitoringSamplerCellValue = _pywrap_tensorflow_internal.TFE_MonitoringSamplerCellValue\n",
            "\n",
            "def TFE_MonitoringNewExponentialBuckets(scale, growth_factor, bucket_count):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewExponentialBuckets(scale, growth_factor, bucket_count)\n",
            "TFE_MonitoringNewExponentialBuckets = _pywrap_tensorflow_internal.TFE_MonitoringNewExponentialBuckets\n",
            "\n",
            "def TFE_MonitoringDeleteBuckets(buckets):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteBuckets(buckets)\n",
            "TFE_MonitoringDeleteBuckets = _pywrap_tensorflow_internal.TFE_MonitoringDeleteBuckets\n",
            "\n",
            "def TFE_MonitoringNewSampler0(name, buckets, description):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewSampler0(name, buckets, description)\n",
            "TFE_MonitoringNewSampler0 = _pywrap_tensorflow_internal.TFE_MonitoringNewSampler0\n",
            "\n",
            "def TFE_MonitoringDeleteSampler0(sampler):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler0(sampler)\n",
            "TFE_MonitoringDeleteSampler0 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler0\n",
            "\n",
            "def TFE_MonitoringGetCellSampler0(sampler):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler0(sampler)\n",
            "TFE_MonitoringGetCellSampler0 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler0\n",
            "\n",
            "def TFE_MonitoringNewSampler1(name, buckets, description, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewSampler1(name, buckets, description, label1)\n",
            "TFE_MonitoringNewSampler1 = _pywrap_tensorflow_internal.TFE_MonitoringNewSampler1\n",
            "\n",
            "def TFE_MonitoringDeleteSampler1(sampler):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler1(sampler)\n",
            "TFE_MonitoringDeleteSampler1 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler1\n",
            "\n",
            "def TFE_MonitoringGetCellSampler1(sampler, label1):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler1(sampler, label1)\n",
            "TFE_MonitoringGetCellSampler1 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler1\n",
            "\n",
            "def TFE_MonitoringNewSampler2(name, buckets, description, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringNewSampler2(name, buckets, description, label1, label2)\n",
            "TFE_MonitoringNewSampler2 = _pywrap_tensorflow_internal.TFE_MonitoringNewSampler2\n",
            "\n",
            "def TFE_MonitoringDeleteSampler2(sampler):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler2(sampler)\n",
            "TFE_MonitoringDeleteSampler2 = _pywrap_tensorflow_internal.TFE_MonitoringDeleteSampler2\n",
            "\n",
            "def TFE_MonitoringGetCellSampler2(sampler, label1, label2):\n",
            "    return _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler2(sampler, label1, label2)\n",
            "TFE_MonitoringGetCellSampler2 = _pywrap_tensorflow_internal.TFE_MonitoringGetCellSampler2\n",
            "\n",
            "def IsGoogleCudaEnabled():\n",
            "    return _pywrap_tensorflow_internal.IsGoogleCudaEnabled()\n",
            "IsGoogleCudaEnabled = _pywrap_tensorflow_internal.IsGoogleCudaEnabled\n",
            "\n",
            "def IsBuiltWithROCm():\n",
            "    return _pywrap_tensorflow_internal.IsBuiltWithROCm()\n",
            "IsBuiltWithROCm = _pywrap_tensorflow_internal.IsBuiltWithROCm\n",
            "\n",
            "def GpuSupportsHalfMatMulAndConv():\n",
            "    return _pywrap_tensorflow_internal.GpuSupportsHalfMatMulAndConv()\n",
            "GpuSupportsHalfMatMulAndConv = _pywrap_tensorflow_internal.GpuSupportsHalfMatMulAndConv\n",
            "\n",
            "def IsMklEnabled():\n",
            "    return _pywrap_tensorflow_internal.IsMklEnabled()\n",
            "IsMklEnabled = _pywrap_tensorflow_internal.IsMklEnabled\n",
            "\n",
            "def CheckpointReader_GetTensor(reader, name):\n",
            "    return _pywrap_tensorflow_internal.CheckpointReader_GetTensor(reader, name)\n",
            "CheckpointReader_GetTensor = _pywrap_tensorflow_internal.CheckpointReader_GetTensor\n",
            "\n",
            "def NewCheckpointReader(filepattern):\n",
            "  from tensorflow.python.util import compat\n",
            "  return CheckpointReader(compat.as_bytes(filepattern))\n",
            "\n",
            "NewCheckpointReader._tf_api_names_v1 = ['train.NewCheckpointReader']\n",
            "\n",
            "class CheckpointReader(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, CheckpointReader, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, CheckpointReader, name)\n",
            "    __repr__ = _swig_repr\n",
            "\n",
            "    def __init__(self, filename):\n",
            "        this = _pywrap_tensorflow_internal.new_CheckpointReader(filename)\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "\n",
            "    def _HasTensor(self, name):\n",
            "        return _pywrap_tensorflow_internal.CheckpointReader__HasTensor(self, name)\n",
            "\n",
            "    def debug_string(self):\n",
            "        return _pywrap_tensorflow_internal.CheckpointReader_debug_string(self)\n",
            "\n",
            "    def get_variable_to_shape_map(self):\n",
            "        return _pywrap_tensorflow_internal.CheckpointReader_get_variable_to_shape_map(self)\n",
            "\n",
            "    def _GetVariableToDataTypeMap(self):\n",
            "        return _pywrap_tensorflow_internal.CheckpointReader__GetVariableToDataTypeMap(self)\n",
            "\n",
            "    def get_variable_to_dtype_map(self):\n",
            "      from tensorflow.python.framework import dtypes\n",
            "      return {name: dtypes.DType(type_enum)\n",
            "              for name, type_enum in self._GetVariableToDataTypeMap().items()}\n",
            "\n",
            "    def has_tensor(self, tensor_str):\n",
            "      from tensorflow.python.util import compat\n",
            "      return self._HasTensor(compat.as_bytes(tensor_str))\n",
            "\n",
            "    def get_tensor(self, tensor_str):\n",
            "      from tensorflow.python.util import compat\n",
            "\n",
            "      return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))\n",
            "\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_CheckpointReader\n",
            "    __del__ = lambda self: None\n",
            "CheckpointReader_swigregister = _pywrap_tensorflow_internal.CheckpointReader_swigregister\n",
            "CheckpointReader_swigregister(CheckpointReader)\n",
            "TFE_Py_FastPathExecute = _pywrap_tensorflow_internal.TFE_Py_FastPathExecute\n",
            "\n",
            "\n",
            "def NewStatSummarizer(unused):\n",
            "    return _pywrap_tensorflow_internal.NewStatSummarizer(unused)\n",
            "NewStatSummarizer = _pywrap_tensorflow_internal.NewStatSummarizer\n",
            "\n",
            "def DeleteStatSummarizer(ss):\n",
            "    return _pywrap_tensorflow_internal.DeleteStatSummarizer(ss)\n",
            "DeleteStatSummarizer = _pywrap_tensorflow_internal.DeleteStatSummarizer\n",
            "class StatSummarizer(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, StatSummarizer, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, StatSummarizer, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_StatSummarizer\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def ProcessStepStats(self, step_stats):\n",
            "        return _pywrap_tensorflow_internal.StatSummarizer_ProcessStepStats(self, step_stats)\n",
            "\n",
            "    def GetOutputString(self):\n",
            "        return _pywrap_tensorflow_internal.StatSummarizer_GetOutputString(self)\n",
            "\n",
            "    def PrintStepStats(self):\n",
            "        return _pywrap_tensorflow_internal.StatSummarizer_PrintStepStats(self)\n",
            "\n",
            "    def ProcessStepStatsStr(self, step_stats_str):\n",
            "        return _pywrap_tensorflow_internal.StatSummarizer_ProcessStepStatsStr(self, step_stats_str)\n",
            "\n",
            "    def __init__(self, *args):\n",
            "        this = _pywrap_tensorflow_internal.new_StatSummarizer(*args)\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "StatSummarizer_swigregister = _pywrap_tensorflow_internal.StatSummarizer_swigregister\n",
            "StatSummarizer_swigregister(StatSummarizer)\n",
            "\n",
            "\n",
            "def NewProfiler(graph, op_log):\n",
            "    return _pywrap_tensorflow_internal.NewProfiler(graph, op_log)\n",
            "NewProfiler = _pywrap_tensorflow_internal.NewProfiler\n",
            "\n",
            "def DeleteProfiler():\n",
            "    return _pywrap_tensorflow_internal.DeleteProfiler()\n",
            "DeleteProfiler = _pywrap_tensorflow_internal.DeleteProfiler\n",
            "\n",
            "def AddStep(step, graph, run_meta, op_log):\n",
            "    return _pywrap_tensorflow_internal.AddStep(step, graph, run_meta, op_log)\n",
            "AddStep = _pywrap_tensorflow_internal.AddStep\n",
            "\n",
            "def WriteProfile(filename):\n",
            "    return _pywrap_tensorflow_internal.WriteProfile(filename)\n",
            "WriteProfile = _pywrap_tensorflow_internal.WriteProfile\n",
            "\n",
            "def ProfilerFromFile(filename):\n",
            "    return _pywrap_tensorflow_internal.ProfilerFromFile(filename)\n",
            "ProfilerFromFile = _pywrap_tensorflow_internal.ProfilerFromFile\n",
            "\n",
            "def SerializeToString():\n",
            "    return _pywrap_tensorflow_internal.SerializeToString()\n",
            "SerializeToString = _pywrap_tensorflow_internal.SerializeToString\n",
            "\n",
            "def Profile(command, options):\n",
            "    return _pywrap_tensorflow_internal.Profile(command, options)\n",
            "Profile = _pywrap_tensorflow_internal.Profile\n",
            "\n",
            "def PrintModelAnalysis(graph, run_meta, op_log, command, options):\n",
            "    return _pywrap_tensorflow_internal.PrintModelAnalysis(graph, run_meta, op_log, command, options)\n",
            "PrintModelAnalysis = _pywrap_tensorflow_internal.PrintModelAnalysis\n",
            "\n",
            "def InitializePyTrampoline(trampoline):\n",
            "    return _pywrap_tensorflow_internal.InitializePyTrampoline(trampoline)\n",
            "InitializePyTrampoline = _pywrap_tensorflow_internal.InitializePyTrampoline\n",
            "class PyExceptionRegistry(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, PyExceptionRegistry, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, PyExceptionRegistry, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_getmethods__[\"Init\"] = lambda x: _pywrap_tensorflow_internal.PyExceptionRegistry_Init\n",
            "    if _newclass:\n",
            "        Init = staticmethod(_pywrap_tensorflow_internal.PyExceptionRegistry_Init)\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_PyExceptionRegistry\n",
            "    __del__ = lambda self: None\n",
            "PyExceptionRegistry_swigregister = _pywrap_tensorflow_internal.PyExceptionRegistry_swigregister\n",
            "PyExceptionRegistry_swigregister(PyExceptionRegistry)\n",
            "\n",
            "def PyExceptionRegistry_Init(code_to_exc_type_map):\n",
            "    return _pywrap_tensorflow_internal.PyExceptionRegistry_Init(code_to_exc_type_map)\n",
            "PyExceptionRegistry_Init = _pywrap_tensorflow_internal.PyExceptionRegistry_Init\n",
            "\n",
            "class PyRecordReader(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, PyRecordReader, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, PyRecordReader, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_getmethods__[\"New\"] = lambda x: _pywrap_tensorflow_internal.PyRecordReader_New\n",
            "    if _newclass:\n",
            "        New = staticmethod(_pywrap_tensorflow_internal.PyRecordReader_New)\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_PyRecordReader\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def GetNext(self):\n",
            "        return _pywrap_tensorflow_internal.PyRecordReader_GetNext(self)\n",
            "\n",
            "    def record(self):\n",
            "        return _pywrap_tensorflow_internal.PyRecordReader_record(self)\n",
            "\n",
            "    def offset(self):\n",
            "        return _pywrap_tensorflow_internal.PyRecordReader_offset(self)\n",
            "\n",
            "    def Close(self):\n",
            "        return _pywrap_tensorflow_internal.PyRecordReader_Close(self)\n",
            "PyRecordReader_swigregister = _pywrap_tensorflow_internal.PyRecordReader_swigregister\n",
            "PyRecordReader_swigregister(PyRecordReader)\n",
            "\n",
            "def PyRecordReader_New(filename, start_offset, compression_type_string, out_status):\n",
            "    return _pywrap_tensorflow_internal.PyRecordReader_New(filename, start_offset, compression_type_string, out_status)\n",
            "PyRecordReader_New = _pywrap_tensorflow_internal.PyRecordReader_New\n",
            "\n",
            "class RecordWriterOptions(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, RecordWriterOptions, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, RecordWriterOptions, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_getmethods__[\"CreateRecordWriterOptions\"] = lambda x: _pywrap_tensorflow_internal.RecordWriterOptions_CreateRecordWriterOptions\n",
            "    if _newclass:\n",
            "        CreateRecordWriterOptions = staticmethod(_pywrap_tensorflow_internal.RecordWriterOptions_CreateRecordWriterOptions)\n",
            "    __swig_setmethods__[\"zlib_options\"] = _pywrap_tensorflow_internal.RecordWriterOptions_zlib_options_set\n",
            "    __swig_getmethods__[\"zlib_options\"] = _pywrap_tensorflow_internal.RecordWriterOptions_zlib_options_get\n",
            "    if _newclass:\n",
            "        zlib_options = _swig_property(_pywrap_tensorflow_internal.RecordWriterOptions_zlib_options_get, _pywrap_tensorflow_internal.RecordWriterOptions_zlib_options_set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_RecordWriterOptions()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_RecordWriterOptions\n",
            "    __del__ = lambda self: None\n",
            "RecordWriterOptions_swigregister = _pywrap_tensorflow_internal.RecordWriterOptions_swigregister\n",
            "RecordWriterOptions_swigregister(RecordWriterOptions)\n",
            "\n",
            "def RecordWriterOptions_CreateRecordWriterOptions(compression_type):\n",
            "    return _pywrap_tensorflow_internal.RecordWriterOptions_CreateRecordWriterOptions(compression_type)\n",
            "RecordWriterOptions_CreateRecordWriterOptions = _pywrap_tensorflow_internal.RecordWriterOptions_CreateRecordWriterOptions\n",
            "\n",
            "class ZlibCompressionOptions(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, ZlibCompressionOptions, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, ZlibCompressionOptions, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"flush_mode\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_flush_mode_set\n",
            "    __swig_getmethods__[\"flush_mode\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_flush_mode_get\n",
            "    if _newclass:\n",
            "        flush_mode = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_flush_mode_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_flush_mode_set)\n",
            "    __swig_setmethods__[\"input_buffer_size\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_input_buffer_size_set\n",
            "    __swig_getmethods__[\"input_buffer_size\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_input_buffer_size_get\n",
            "    if _newclass:\n",
            "        input_buffer_size = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_input_buffer_size_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_input_buffer_size_set)\n",
            "    __swig_setmethods__[\"output_buffer_size\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_output_buffer_size_set\n",
            "    __swig_getmethods__[\"output_buffer_size\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_output_buffer_size_get\n",
            "    if _newclass:\n",
            "        output_buffer_size = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_output_buffer_size_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_output_buffer_size_set)\n",
            "    __swig_setmethods__[\"window_bits\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_window_bits_set\n",
            "    __swig_getmethods__[\"window_bits\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_window_bits_get\n",
            "    if _newclass:\n",
            "        window_bits = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_window_bits_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_window_bits_set)\n",
            "    __swig_setmethods__[\"compression_level\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_level_set\n",
            "    __swig_getmethods__[\"compression_level\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_level_get\n",
            "    if _newclass:\n",
            "        compression_level = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_compression_level_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_level_set)\n",
            "    __swig_setmethods__[\"compression_method\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_method_set\n",
            "    __swig_getmethods__[\"compression_method\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_method_get\n",
            "    if _newclass:\n",
            "        compression_method = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_compression_method_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_method_set)\n",
            "    __swig_setmethods__[\"mem_level\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_mem_level_set\n",
            "    __swig_getmethods__[\"mem_level\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_mem_level_get\n",
            "    if _newclass:\n",
            "        mem_level = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_mem_level_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_mem_level_set)\n",
            "    __swig_setmethods__[\"compression_strategy\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_strategy_set\n",
            "    __swig_getmethods__[\"compression_strategy\"] = _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_strategy_get\n",
            "    if _newclass:\n",
            "        compression_strategy = _swig_property(_pywrap_tensorflow_internal.ZlibCompressionOptions_compression_strategy_get, _pywrap_tensorflow_internal.ZlibCompressionOptions_compression_strategy_set)\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_ZlibCompressionOptions\n",
            "    __del__ = lambda self: None\n",
            "ZlibCompressionOptions_swigregister = _pywrap_tensorflow_internal.ZlibCompressionOptions_swigregister\n",
            "ZlibCompressionOptions_swigregister(ZlibCompressionOptions)\n",
            "\n",
            "class PyRecordWriter(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, PyRecordWriter, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, PyRecordWriter, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_getmethods__[\"New\"] = lambda x: _pywrap_tensorflow_internal.PyRecordWriter_New\n",
            "    if _newclass:\n",
            "        New = staticmethod(_pywrap_tensorflow_internal.PyRecordWriter_New)\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_PyRecordWriter\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def WriteRecord(self, record, out_status):\n",
            "        return _pywrap_tensorflow_internal.PyRecordWriter_WriteRecord(self, record, out_status)\n",
            "\n",
            "    def Flush(self, out_status):\n",
            "        return _pywrap_tensorflow_internal.PyRecordWriter_Flush(self, out_status)\n",
            "\n",
            "    def Close(self, out_status):\n",
            "        return _pywrap_tensorflow_internal.PyRecordWriter_Close(self, out_status)\n",
            "PyRecordWriter_swigregister = _pywrap_tensorflow_internal.PyRecordWriter_swigregister\n",
            "PyRecordWriter_swigregister(PyRecordWriter)\n",
            "\n",
            "def PyRecordWriter_New(filename, compression_options, out_status):\n",
            "    return _pywrap_tensorflow_internal.PyRecordWriter_New(filename, compression_options, out_status)\n",
            "PyRecordWriter_New = _pywrap_tensorflow_internal.PyRecordWriter_New\n",
            "\n",
            "class Status(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, Status, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, Status, name)\n",
            "    __repr__ = _swig_repr\n",
            "\n",
            "    def __init__(self, *args):\n",
            "        this = _pywrap_tensorflow_internal.new_Status(*args)\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_getmethods__[\"OK\"] = lambda x: _pywrap_tensorflow_internal.Status_OK\n",
            "    if _newclass:\n",
            "        OK = staticmethod(_pywrap_tensorflow_internal.Status_OK)\n",
            "\n",
            "    def ok(self):\n",
            "        return _pywrap_tensorflow_internal.Status_ok(self)\n",
            "\n",
            "    def code(self):\n",
            "        return _pywrap_tensorflow_internal.Status_code(self)\n",
            "\n",
            "    def error_message(self):\n",
            "        return _pywrap_tensorflow_internal.Status_error_message(self)\n",
            "\n",
            "    def __eq__(self, x):\n",
            "        return _pywrap_tensorflow_internal.Status___eq__(self, x)\n",
            "\n",
            "    def __ne__(self, x):\n",
            "        return _pywrap_tensorflow_internal.Status___ne__(self, x)\n",
            "\n",
            "    def Update(self, new_status):\n",
            "        return _pywrap_tensorflow_internal.Status_Update(self, new_status)\n",
            "\n",
            "    def ToString(self):\n",
            "        return _pywrap_tensorflow_internal.Status_ToString(self)\n",
            "\n",
            "    def IgnoreError(self):\n",
            "        return _pywrap_tensorflow_internal.Status_IgnoreError(self)\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_Status\n",
            "    __del__ = lambda self: None\n",
            "Status_swigregister = _pywrap_tensorflow_internal.Status_swigregister\n",
            "Status_swigregister(Status)\n",
            "\n",
            "def Status_OK():\n",
            "    return _pywrap_tensorflow_internal.Status_OK()\n",
            "Status_OK = _pywrap_tensorflow_internal.Status_OK\n",
            "\n",
            "class StatusGroup(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, StatusGroup, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, StatusGroup, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_getmethods__[\"MakeDerived\"] = lambda x: _pywrap_tensorflow_internal.StatusGroup_MakeDerived\n",
            "    if _newclass:\n",
            "        MakeDerived = staticmethod(_pywrap_tensorflow_internal.StatusGroup_MakeDerived)\n",
            "    __swig_getmethods__[\"IsDerived\"] = lambda x: _pywrap_tensorflow_internal.StatusGroup_IsDerived\n",
            "    if _newclass:\n",
            "        IsDerived = staticmethod(_pywrap_tensorflow_internal.StatusGroup_IsDerived)\n",
            "\n",
            "    def as_summary_status(self):\n",
            "        return _pywrap_tensorflow_internal.StatusGroup_as_summary_status(self)\n",
            "\n",
            "    def as_concatenated_status(self):\n",
            "        return _pywrap_tensorflow_internal.StatusGroup_as_concatenated_status(self)\n",
            "\n",
            "    def ok(self):\n",
            "        return _pywrap_tensorflow_internal.StatusGroup_ok(self)\n",
            "\n",
            "    def Update(self, status):\n",
            "        return _pywrap_tensorflow_internal.StatusGroup_Update(self, status)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_StatusGroup()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_StatusGroup\n",
            "    __del__ = lambda self: None\n",
            "StatusGroup_swigregister = _pywrap_tensorflow_internal.StatusGroup_swigregister\n",
            "StatusGroup_swigregister(StatusGroup)\n",
            "\n",
            "def StatusGroup_MakeDerived(s):\n",
            "    return _pywrap_tensorflow_internal.StatusGroup_MakeDerived(s)\n",
            "StatusGroup_MakeDerived = _pywrap_tensorflow_internal.StatusGroup_MakeDerived\n",
            "\n",
            "def StatusGroup_IsDerived(s):\n",
            "    return _pywrap_tensorflow_internal.StatusGroup_IsDerived(s)\n",
            "StatusGroup_IsDerived = _pywrap_tensorflow_internal.StatusGroup_IsDerived\n",
            "\n",
            "\n",
            "def __lshift__(os, x):\n",
            "    return _pywrap_tensorflow_internal.__lshift__(os, x)\n",
            "__lshift__ = _pywrap_tensorflow_internal.__lshift__\n",
            "\n",
            "def TfCheckOpHelperOutOfLine(v, msg):\n",
            "    return _pywrap_tensorflow_internal.TfCheckOpHelperOutOfLine(v, msg)\n",
            "TfCheckOpHelperOutOfLine = _pywrap_tensorflow_internal.TfCheckOpHelperOutOfLine\n",
            "\n",
            "def TfCheckOpHelper(v, msg):\n",
            "    return _pywrap_tensorflow_internal.TfCheckOpHelper(v, msg)\n",
            "TfCheckOpHelper = _pywrap_tensorflow_internal.TfCheckOpHelper\n",
            "class EventsWriter(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, EventsWriter, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, EventsWriter, name)\n",
            "    __repr__ = _swig_repr\n",
            "\n",
            "    def __init__(self, file_prefix):\n",
            "        this = _pywrap_tensorflow_internal.new_EventsWriter(file_prefix)\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_EventsWriter\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def InitWithSuffix(self, suffix):\n",
            "        return _pywrap_tensorflow_internal.EventsWriter_InitWithSuffix(self, suffix)\n",
            "\n",
            "    def FileName(self):\n",
            "        return _pywrap_tensorflow_internal.EventsWriter_FileName(self)\n",
            "\n",
            "    def _WriteSerializedEvent(self, event_str):\n",
            "        return _pywrap_tensorflow_internal.EventsWriter__WriteSerializedEvent(self, event_str)\n",
            "\n",
            "    def Flush(self):\n",
            "        return _pywrap_tensorflow_internal.EventsWriter_Flush(self)\n",
            "\n",
            "    def Close(self):\n",
            "        return _pywrap_tensorflow_internal.EventsWriter_Close(self)\n",
            "\n",
            "    def WriteEvent(self, event):\n",
            "      from tensorflow.core.util.event_pb2 import Event\n",
            "      if not isinstance(event, Event):\n",
            "        raise TypeError(\"Expected an event_pb2.Event proto, \"\n",
            "                        \" but got %s\" % type(event))\n",
            "      return self._WriteSerializedEvent(event.SerializeToString())\n",
            "\n",
            "EventsWriter_swigregister = _pywrap_tensorflow_internal.EventsWriter_swigregister\n",
            "EventsWriter_swigregister(EventsWriter)\n",
            "\n",
            "\n",
            "_pywrap_tensorflow_internal.__version___swigconstant(_pywrap_tensorflow_internal)\n",
            "__version__ = _pywrap_tensorflow_internal.__version__\n",
            "\n",
            "_pywrap_tensorflow_internal.GRAPH_DEF_VERSION_swigconstant(_pywrap_tensorflow_internal)\n",
            "GRAPH_DEF_VERSION = _pywrap_tensorflow_internal.GRAPH_DEF_VERSION\n",
            "\n",
            "_pywrap_tensorflow_internal.GRAPH_DEF_VERSION_MIN_CONSUMER_swigconstant(_pywrap_tensorflow_internal)\n",
            "GRAPH_DEF_VERSION_MIN_CONSUMER = _pywrap_tensorflow_internal.GRAPH_DEF_VERSION_MIN_CONSUMER\n",
            "\n",
            "_pywrap_tensorflow_internal.GRAPH_DEF_VERSION_MIN_PRODUCER_swigconstant(_pywrap_tensorflow_internal)\n",
            "GRAPH_DEF_VERSION_MIN_PRODUCER = _pywrap_tensorflow_internal.GRAPH_DEF_VERSION_MIN_PRODUCER\n",
            "\n",
            "_pywrap_tensorflow_internal.__git_version___swigconstant(_pywrap_tensorflow_internal)\n",
            "__git_version__ = _pywrap_tensorflow_internal.__git_version__\n",
            "\n",
            "_pywrap_tensorflow_internal.__compiler_version___swigconstant(_pywrap_tensorflow_internal)\n",
            "__compiler_version__ = _pywrap_tensorflow_internal.__compiler_version__\n",
            "\n",
            "_pywrap_tensorflow_internal.__cxx11_abi_flag___swigconstant(_pywrap_tensorflow_internal)\n",
            "__cxx11_abi_flag__ = _pywrap_tensorflow_internal.__cxx11_abi_flag__\n",
            "\n",
            "_pywrap_tensorflow_internal.__monolithic_build___swigconstant(_pywrap_tensorflow_internal)\n",
            "__monolithic_build__ = _pywrap_tensorflow_internal.__monolithic_build__\n",
            "\n",
            "_pywrap_tensorflow_internal.TENSOR_HANDLE_KEY_swigconstant(_pywrap_tensorflow_internal)\n",
            "TENSOR_HANDLE_KEY = _pywrap_tensorflow_internal.TENSOR_HANDLE_KEY\n",
            "\n",
            "def TF_Version():\n",
            "    return _pywrap_tensorflow_internal.TF_Version()\n",
            "TF_Version = _pywrap_tensorflow_internal.TF_Version\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_FLOAT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_FLOAT = _pywrap_tensorflow_internal.TF_FLOAT\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_DOUBLE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_DOUBLE = _pywrap_tensorflow_internal.TF_DOUBLE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INT32_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INT32 = _pywrap_tensorflow_internal.TF_INT32\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UINT8_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UINT8 = _pywrap_tensorflow_internal.TF_UINT8\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INT16_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INT16 = _pywrap_tensorflow_internal.TF_INT16\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INT8_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INT8 = _pywrap_tensorflow_internal.TF_INT8\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_STRING_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_STRING = _pywrap_tensorflow_internal.TF_STRING\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_COMPLEX64_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_COMPLEX64 = _pywrap_tensorflow_internal.TF_COMPLEX64\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_COMPLEX_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_COMPLEX = _pywrap_tensorflow_internal.TF_COMPLEX\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INT64_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INT64 = _pywrap_tensorflow_internal.TF_INT64\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_BOOL_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_BOOL = _pywrap_tensorflow_internal.TF_BOOL\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_QINT8_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_QINT8 = _pywrap_tensorflow_internal.TF_QINT8\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_QUINT8_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_QUINT8 = _pywrap_tensorflow_internal.TF_QUINT8\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_QINT32_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_QINT32 = _pywrap_tensorflow_internal.TF_QINT32\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_BFLOAT16_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_BFLOAT16 = _pywrap_tensorflow_internal.TF_BFLOAT16\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_QINT16_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_QINT16 = _pywrap_tensorflow_internal.TF_QINT16\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_QUINT16_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_QUINT16 = _pywrap_tensorflow_internal.TF_QUINT16\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UINT16_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UINT16 = _pywrap_tensorflow_internal.TF_UINT16\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_COMPLEX128_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_COMPLEX128 = _pywrap_tensorflow_internal.TF_COMPLEX128\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_HALF_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_HALF = _pywrap_tensorflow_internal.TF_HALF\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_RESOURCE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_RESOURCE = _pywrap_tensorflow_internal.TF_RESOURCE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_VARIANT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_VARIANT = _pywrap_tensorflow_internal.TF_VARIANT\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UINT32_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UINT32 = _pywrap_tensorflow_internal.TF_UINT32\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UINT64_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UINT64 = _pywrap_tensorflow_internal.TF_UINT64\n",
            "\n",
            "def TF_DataTypeSize(dt):\n",
            "    return _pywrap_tensorflow_internal.TF_DataTypeSize(dt)\n",
            "TF_DataTypeSize = _pywrap_tensorflow_internal.TF_DataTypeSize\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_OK_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_OK = _pywrap_tensorflow_internal.TF_OK\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_CANCELLED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_CANCELLED = _pywrap_tensorflow_internal.TF_CANCELLED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UNKNOWN_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UNKNOWN = _pywrap_tensorflow_internal.TF_UNKNOWN\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INVALID_ARGUMENT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INVALID_ARGUMENT = _pywrap_tensorflow_internal.TF_INVALID_ARGUMENT\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_DEADLINE_EXCEEDED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_DEADLINE_EXCEEDED = _pywrap_tensorflow_internal.TF_DEADLINE_EXCEEDED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_NOT_FOUND_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_NOT_FOUND = _pywrap_tensorflow_internal.TF_NOT_FOUND\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ALREADY_EXISTS_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ALREADY_EXISTS = _pywrap_tensorflow_internal.TF_ALREADY_EXISTS\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_PERMISSION_DENIED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_PERMISSION_DENIED = _pywrap_tensorflow_internal.TF_PERMISSION_DENIED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UNAUTHENTICATED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UNAUTHENTICATED = _pywrap_tensorflow_internal.TF_UNAUTHENTICATED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_RESOURCE_EXHAUSTED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_RESOURCE_EXHAUSTED = _pywrap_tensorflow_internal.TF_RESOURCE_EXHAUSTED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_FAILED_PRECONDITION_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_FAILED_PRECONDITION = _pywrap_tensorflow_internal.TF_FAILED_PRECONDITION\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ABORTED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ABORTED = _pywrap_tensorflow_internal.TF_ABORTED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_OUT_OF_RANGE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_OUT_OF_RANGE = _pywrap_tensorflow_internal.TF_OUT_OF_RANGE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UNIMPLEMENTED_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UNIMPLEMENTED = _pywrap_tensorflow_internal.TF_UNIMPLEMENTED\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_INTERNAL_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_INTERNAL = _pywrap_tensorflow_internal.TF_INTERNAL\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_UNAVAILABLE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_UNAVAILABLE = _pywrap_tensorflow_internal.TF_UNAVAILABLE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_DATA_LOSS_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_DATA_LOSS = _pywrap_tensorflow_internal.TF_DATA_LOSS\n",
            "\n",
            "def TF_NewStatus():\n",
            "    return _pywrap_tensorflow_internal.TF_NewStatus()\n",
            "TF_NewStatus = _pywrap_tensorflow_internal.TF_NewStatus\n",
            "\n",
            "def TF_DeleteStatus(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteStatus(arg1)\n",
            "TF_DeleteStatus = _pywrap_tensorflow_internal.TF_DeleteStatus\n",
            "\n",
            "def TF_SetStatus(s, code, msg):\n",
            "    return _pywrap_tensorflow_internal.TF_SetStatus(s, code, msg)\n",
            "TF_SetStatus = _pywrap_tensorflow_internal.TF_SetStatus\n",
            "\n",
            "def TF_GetCode(s):\n",
            "    return _pywrap_tensorflow_internal.TF_GetCode(s)\n",
            "TF_GetCode = _pywrap_tensorflow_internal.TF_GetCode\n",
            "\n",
            "def TF_Message(s):\n",
            "    return _pywrap_tensorflow_internal.TF_Message(s)\n",
            "TF_Message = _pywrap_tensorflow_internal.TF_Message\n",
            "class TF_Buffer(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, TF_Buffer, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, TF_Buffer, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"data\"] = _pywrap_tensorflow_internal.TF_Buffer_data_set\n",
            "    __swig_getmethods__[\"data\"] = _pywrap_tensorflow_internal.TF_Buffer_data_get\n",
            "    if _newclass:\n",
            "        data = _swig_property(_pywrap_tensorflow_internal.TF_Buffer_data_get, _pywrap_tensorflow_internal.TF_Buffer_data_set)\n",
            "    __swig_setmethods__[\"length\"] = _pywrap_tensorflow_internal.TF_Buffer_length_set\n",
            "    __swig_getmethods__[\"length\"] = _pywrap_tensorflow_internal.TF_Buffer_length_get\n",
            "    if _newclass:\n",
            "        length = _swig_property(_pywrap_tensorflow_internal.TF_Buffer_length_get, _pywrap_tensorflow_internal.TF_Buffer_length_set)\n",
            "    __swig_setmethods__[\"data_deallocator\"] = _pywrap_tensorflow_internal.TF_Buffer_data_deallocator_set\n",
            "    __swig_getmethods__[\"data_deallocator\"] = _pywrap_tensorflow_internal.TF_Buffer_data_deallocator_get\n",
            "    if _newclass:\n",
            "        data_deallocator = _swig_property(_pywrap_tensorflow_internal.TF_Buffer_data_deallocator_get, _pywrap_tensorflow_internal.TF_Buffer_data_deallocator_set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_TF_Buffer()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_TF_Buffer\n",
            "    __del__ = lambda self: None\n",
            "TF_Buffer_swigregister = _pywrap_tensorflow_internal.TF_Buffer_swigregister\n",
            "TF_Buffer_swigregister(TF_Buffer)\n",
            "\n",
            "\n",
            "def TF_NewBufferFromString(proto):\n",
            "    return _pywrap_tensorflow_internal.TF_NewBufferFromString(proto)\n",
            "TF_NewBufferFromString = _pywrap_tensorflow_internal.TF_NewBufferFromString\n",
            "\n",
            "def TF_NewBuffer():\n",
            "    return _pywrap_tensorflow_internal.TF_NewBuffer()\n",
            "TF_NewBuffer = _pywrap_tensorflow_internal.TF_NewBuffer\n",
            "\n",
            "def TF_DeleteBuffer(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteBuffer(arg1)\n",
            "TF_DeleteBuffer = _pywrap_tensorflow_internal.TF_DeleteBuffer\n",
            "\n",
            "def TF_GetBuffer(buffer):\n",
            "    return _pywrap_tensorflow_internal.TF_GetBuffer(buffer)\n",
            "TF_GetBuffer = _pywrap_tensorflow_internal.TF_GetBuffer\n",
            "\n",
            "def TF_NewTensor(arg1, dims, num_dims, data, len, deallocator, deallocator_arg):\n",
            "    return _pywrap_tensorflow_internal.TF_NewTensor(arg1, dims, num_dims, data, len, deallocator, deallocator_arg)\n",
            "TF_NewTensor = _pywrap_tensorflow_internal.TF_NewTensor\n",
            "\n",
            "def TF_AllocateTensor(arg1, dims, num_dims, len):\n",
            "    return _pywrap_tensorflow_internal.TF_AllocateTensor(arg1, dims, num_dims, len)\n",
            "TF_AllocateTensor = _pywrap_tensorflow_internal.TF_AllocateTensor\n",
            "\n",
            "def TF_TensorMaybeMove(tensor):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorMaybeMove(tensor)\n",
            "TF_TensorMaybeMove = _pywrap_tensorflow_internal.TF_TensorMaybeMove\n",
            "\n",
            "def TF_DeleteTensor(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteTensor(arg1)\n",
            "TF_DeleteTensor = _pywrap_tensorflow_internal.TF_DeleteTensor\n",
            "\n",
            "def TF_TensorType(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorType(arg1)\n",
            "TF_TensorType = _pywrap_tensorflow_internal.TF_TensorType\n",
            "\n",
            "def TF_NumDims(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_NumDims(arg1)\n",
            "TF_NumDims = _pywrap_tensorflow_internal.TF_NumDims\n",
            "\n",
            "def TF_Dim(tensor, dim_index):\n",
            "    return _pywrap_tensorflow_internal.TF_Dim(tensor, dim_index)\n",
            "TF_Dim = _pywrap_tensorflow_internal.TF_Dim\n",
            "\n",
            "def TF_TensorByteSize(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorByteSize(arg1)\n",
            "TF_TensorByteSize = _pywrap_tensorflow_internal.TF_TensorByteSize\n",
            "\n",
            "def TF_TensorData(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorData(arg1)\n",
            "TF_TensorData = _pywrap_tensorflow_internal.TF_TensorData\n",
            "\n",
            "def TF_TensorElementCount(tensor):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorElementCount(tensor)\n",
            "TF_TensorElementCount = _pywrap_tensorflow_internal.TF_TensorElementCount\n",
            "\n",
            "def TF_TensorBitcastFrom(arg1, type, to, new_dims, num_new_dims):\n",
            "    return _pywrap_tensorflow_internal.TF_TensorBitcastFrom(arg1, type, to, new_dims, num_new_dims)\n",
            "TF_TensorBitcastFrom = _pywrap_tensorflow_internal.TF_TensorBitcastFrom\n",
            "\n",
            "def TF_StringEncode(src, src_len, dst, dst_len):\n",
            "    return _pywrap_tensorflow_internal.TF_StringEncode(src, src_len, dst, dst_len)\n",
            "TF_StringEncode = _pywrap_tensorflow_internal.TF_StringEncode\n",
            "\n",
            "def TF_StringDecode(src, src_len, dst, dst_len):\n",
            "    return _pywrap_tensorflow_internal.TF_StringDecode(src, src_len, dst, dst_len)\n",
            "TF_StringDecode = _pywrap_tensorflow_internal.TF_StringDecode\n",
            "\n",
            "def TF_StringEncodedSize(len):\n",
            "    return _pywrap_tensorflow_internal.TF_StringEncodedSize(len)\n",
            "TF_StringEncodedSize = _pywrap_tensorflow_internal.TF_StringEncodedSize\n",
            "\n",
            "def _TF_NewSessionOptions():\n",
            "    return _pywrap_tensorflow_internal._TF_NewSessionOptions()\n",
            "_TF_NewSessionOptions = _pywrap_tensorflow_internal._TF_NewSessionOptions\n",
            "\n",
            "def _TF_SetTarget(options, target):\n",
            "    return _pywrap_tensorflow_internal._TF_SetTarget(options, target)\n",
            "_TF_SetTarget = _pywrap_tensorflow_internal._TF_SetTarget\n",
            "\n",
            "def _TF_SetConfig(options, proto):\n",
            "    return _pywrap_tensorflow_internal._TF_SetConfig(options, proto)\n",
            "_TF_SetConfig = _pywrap_tensorflow_internal._TF_SetConfig\n",
            "\n",
            "def TF_DeleteSessionOptions(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteSessionOptions(arg1)\n",
            "TF_DeleteSessionOptions = _pywrap_tensorflow_internal.TF_DeleteSessionOptions\n",
            "\n",
            "def TF_NewGraph():\n",
            "    return _pywrap_tensorflow_internal.TF_NewGraph()\n",
            "TF_NewGraph = _pywrap_tensorflow_internal.TF_NewGraph\n",
            "\n",
            "def TF_DeleteGraph(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteGraph(arg1)\n",
            "TF_DeleteGraph = _pywrap_tensorflow_internal.TF_DeleteGraph\n",
            "class TF_Input(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, TF_Input, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, TF_Input, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"oper\"] = _pywrap_tensorflow_internal.TF_Input_oper_set\n",
            "    __swig_getmethods__[\"oper\"] = _pywrap_tensorflow_internal.TF_Input_oper_get\n",
            "    if _newclass:\n",
            "        oper = _swig_property(_pywrap_tensorflow_internal.TF_Input_oper_get, _pywrap_tensorflow_internal.TF_Input_oper_set)\n",
            "    __swig_setmethods__[\"index\"] = _pywrap_tensorflow_internal.TF_Input_index_set\n",
            "    __swig_getmethods__[\"index\"] = _pywrap_tensorflow_internal.TF_Input_index_get\n",
            "    if _newclass:\n",
            "        index = _swig_property(_pywrap_tensorflow_internal.TF_Input_index_get, _pywrap_tensorflow_internal.TF_Input_index_set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_TF_Input()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_TF_Input\n",
            "    __del__ = lambda self: None\n",
            "TF_Input_swigregister = _pywrap_tensorflow_internal.TF_Input_swigregister\n",
            "TF_Input_swigregister(TF_Input)\n",
            "\n",
            "class TF_Output(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, TF_Output, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, TF_Output, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"oper\"] = _pywrap_tensorflow_internal.TF_Output_oper_set\n",
            "    __swig_getmethods__[\"oper\"] = _pywrap_tensorflow_internal.TF_Output_oper_get\n",
            "    if _newclass:\n",
            "        oper = _swig_property(_pywrap_tensorflow_internal.TF_Output_oper_get, _pywrap_tensorflow_internal.TF_Output_oper_set)\n",
            "    __swig_setmethods__[\"index\"] = _pywrap_tensorflow_internal.TF_Output_index_set\n",
            "    __swig_getmethods__[\"index\"] = _pywrap_tensorflow_internal.TF_Output_index_get\n",
            "    if _newclass:\n",
            "        index = _swig_property(_pywrap_tensorflow_internal.TF_Output_index_get, _pywrap_tensorflow_internal.TF_Output_index_set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_TF_Output()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_TF_Output\n",
            "    __del__ = lambda self: None\n",
            "TF_Output_swigregister = _pywrap_tensorflow_internal.TF_Output_swigregister\n",
            "TF_Output_swigregister(TF_Output)\n",
            "\n",
            "\n",
            "def TF_GraphSetTensorShape(graph, output, dims, num_dims):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphSetTensorShape(graph, output, dims, num_dims)\n",
            "TF_GraphSetTensorShape = _pywrap_tensorflow_internal.TF_GraphSetTensorShape\n",
            "\n",
            "def TF_GraphGetTensorNumDims(graph, output):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphGetTensorNumDims(graph, output)\n",
            "TF_GraphGetTensorNumDims = _pywrap_tensorflow_internal.TF_GraphGetTensorNumDims\n",
            "\n",
            "def TF_GraphGetTensorShape(graph, output, dims, num_dims):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphGetTensorShape(graph, output, dims, num_dims)\n",
            "TF_GraphGetTensorShape = _pywrap_tensorflow_internal.TF_GraphGetTensorShape\n",
            "\n",
            "def TF_NewOperation(graph, op_type, oper_name):\n",
            "    return _pywrap_tensorflow_internal.TF_NewOperation(graph, op_type, oper_name)\n",
            "TF_NewOperation = _pywrap_tensorflow_internal.TF_NewOperation\n",
            "\n",
            "def TF_SetDevice(desc, device):\n",
            "    return _pywrap_tensorflow_internal.TF_SetDevice(desc, device)\n",
            "TF_SetDevice = _pywrap_tensorflow_internal.TF_SetDevice\n",
            "\n",
            "def TF_AddInput(desc, input):\n",
            "    return _pywrap_tensorflow_internal.TF_AddInput(desc, input)\n",
            "TF_AddInput = _pywrap_tensorflow_internal.TF_AddInput\n",
            "\n",
            "def TF_AddInputList(desc, inputs):\n",
            "    return _pywrap_tensorflow_internal.TF_AddInputList(desc, inputs)\n",
            "TF_AddInputList = _pywrap_tensorflow_internal.TF_AddInputList\n",
            "\n",
            "def TF_AddControlInput(desc, input):\n",
            "    return _pywrap_tensorflow_internal.TF_AddControlInput(desc, input)\n",
            "TF_AddControlInput = _pywrap_tensorflow_internal.TF_AddControlInput\n",
            "\n",
            "def TF_ColocateWith(desc, op):\n",
            "    return _pywrap_tensorflow_internal.TF_ColocateWith(desc, op)\n",
            "TF_ColocateWith = _pywrap_tensorflow_internal.TF_ColocateWith\n",
            "\n",
            "def TF_SetAttrString(desc, attr_name, value, length):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrString(desc, attr_name, value, length)\n",
            "TF_SetAttrString = _pywrap_tensorflow_internal.TF_SetAttrString\n",
            "\n",
            "def TF_SetAttrStringList(desc, attr_name, values, lengths, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrStringList(desc, attr_name, values, lengths, num_values)\n",
            "TF_SetAttrStringList = _pywrap_tensorflow_internal.TF_SetAttrStringList\n",
            "\n",
            "def TF_SetAttrInt(desc, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrInt(desc, attr_name, value)\n",
            "TF_SetAttrInt = _pywrap_tensorflow_internal.TF_SetAttrInt\n",
            "\n",
            "def TF_SetAttrIntList(desc, attr_name, values, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrIntList(desc, attr_name, values, num_values)\n",
            "TF_SetAttrIntList = _pywrap_tensorflow_internal.TF_SetAttrIntList\n",
            "\n",
            "def TF_SetAttrFloat(desc, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrFloat(desc, attr_name, value)\n",
            "TF_SetAttrFloat = _pywrap_tensorflow_internal.TF_SetAttrFloat\n",
            "\n",
            "def TF_SetAttrFloatList(desc, attr_name, values, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrFloatList(desc, attr_name, values, num_values)\n",
            "TF_SetAttrFloatList = _pywrap_tensorflow_internal.TF_SetAttrFloatList\n",
            "\n",
            "def TF_SetAttrBool(desc, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrBool(desc, attr_name, value)\n",
            "TF_SetAttrBool = _pywrap_tensorflow_internal.TF_SetAttrBool\n",
            "\n",
            "def TF_SetAttrBoolList(desc, attr_name, values, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrBoolList(desc, attr_name, values, num_values)\n",
            "TF_SetAttrBoolList = _pywrap_tensorflow_internal.TF_SetAttrBoolList\n",
            "\n",
            "def TF_SetAttrType(desc, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrType(desc, attr_name, value)\n",
            "TF_SetAttrType = _pywrap_tensorflow_internal.TF_SetAttrType\n",
            "\n",
            "def TF_SetAttrTypeList(desc, attr_name, values, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrTypeList(desc, attr_name, values, num_values)\n",
            "TF_SetAttrTypeList = _pywrap_tensorflow_internal.TF_SetAttrTypeList\n",
            "\n",
            "def TF_SetAttrPlaceholder(desc, attr_name, placeholder):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrPlaceholder(desc, attr_name, placeholder)\n",
            "TF_SetAttrPlaceholder = _pywrap_tensorflow_internal.TF_SetAttrPlaceholder\n",
            "\n",
            "def TF_SetAttrFuncName(desc, attr_name, value, length):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrFuncName(desc, attr_name, value, length)\n",
            "TF_SetAttrFuncName = _pywrap_tensorflow_internal.TF_SetAttrFuncName\n",
            "\n",
            "def TF_SetAttrShape(desc, attr_name, dims, num_dims):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrShape(desc, attr_name, dims, num_dims)\n",
            "TF_SetAttrShape = _pywrap_tensorflow_internal.TF_SetAttrShape\n",
            "\n",
            "def TF_SetAttrShapeList(desc, attr_name, dims, num_dims, num_shapes):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrShapeList(desc, attr_name, dims, num_dims, num_shapes)\n",
            "TF_SetAttrShapeList = _pywrap_tensorflow_internal.TF_SetAttrShapeList\n",
            "\n",
            "def TF_SetAttrTensorShapeProto(desc, attr_name, proto):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrTensorShapeProto(desc, attr_name, proto)\n",
            "TF_SetAttrTensorShapeProto = _pywrap_tensorflow_internal.TF_SetAttrTensorShapeProto\n",
            "\n",
            "def TF_SetAttrTensorShapeProtoList(desc, attr_name, protos, proto_lens, num_shapes):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrTensorShapeProtoList(desc, attr_name, protos, proto_lens, num_shapes)\n",
            "TF_SetAttrTensorShapeProtoList = _pywrap_tensorflow_internal.TF_SetAttrTensorShapeProtoList\n",
            "\n",
            "def TF_SetAttrTensor(desc, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrTensor(desc, attr_name, value)\n",
            "TF_SetAttrTensor = _pywrap_tensorflow_internal.TF_SetAttrTensor\n",
            "\n",
            "def TF_SetAttrTensorList(desc, attr_name, values, num_values):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrTensorList(desc, attr_name, values, num_values)\n",
            "TF_SetAttrTensorList = _pywrap_tensorflow_internal.TF_SetAttrTensorList\n",
            "\n",
            "def TF_SetAttrValueProto(desc, attr_name, proto):\n",
            "    return _pywrap_tensorflow_internal.TF_SetAttrValueProto(desc, attr_name, proto)\n",
            "TF_SetAttrValueProto = _pywrap_tensorflow_internal.TF_SetAttrValueProto\n",
            "\n",
            "def TF_FinishOperation(desc):\n",
            "    return _pywrap_tensorflow_internal.TF_FinishOperation(desc)\n",
            "TF_FinishOperation = _pywrap_tensorflow_internal.TF_FinishOperation\n",
            "\n",
            "def TF_OperationName(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationName(oper)\n",
            "TF_OperationName = _pywrap_tensorflow_internal.TF_OperationName\n",
            "\n",
            "def TF_OperationOpType(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationOpType(oper)\n",
            "TF_OperationOpType = _pywrap_tensorflow_internal.TF_OperationOpType\n",
            "\n",
            "def TF_OperationDevice(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationDevice(oper)\n",
            "TF_OperationDevice = _pywrap_tensorflow_internal.TF_OperationDevice\n",
            "\n",
            "def TF_OperationNumOutputs(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationNumOutputs(oper)\n",
            "TF_OperationNumOutputs = _pywrap_tensorflow_internal.TF_OperationNumOutputs\n",
            "\n",
            "def TF_OperationOutputType(oper_out):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationOutputType(oper_out)\n",
            "TF_OperationOutputType = _pywrap_tensorflow_internal.TF_OperationOutputType\n",
            "\n",
            "def TF_OperationOutputListLength(oper, arg_name):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationOutputListLength(oper, arg_name)\n",
            "TF_OperationOutputListLength = _pywrap_tensorflow_internal.TF_OperationOutputListLength\n",
            "\n",
            "def TF_OperationNumInputs(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationNumInputs(oper)\n",
            "TF_OperationNumInputs = _pywrap_tensorflow_internal.TF_OperationNumInputs\n",
            "\n",
            "def TF_OperationInputType(oper_in):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationInputType(oper_in)\n",
            "TF_OperationInputType = _pywrap_tensorflow_internal.TF_OperationInputType\n",
            "\n",
            "def TF_OperationInputListLength(oper, arg_name):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationInputListLength(oper, arg_name)\n",
            "TF_OperationInputListLength = _pywrap_tensorflow_internal.TF_OperationInputListLength\n",
            "\n",
            "def TF_OperationInput(oper_in):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationInput(oper_in)\n",
            "TF_OperationInput = _pywrap_tensorflow_internal.TF_OperationInput\n",
            "\n",
            "def TF_OperationOutputNumConsumers(oper_out):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationOutputNumConsumers(oper_out)\n",
            "TF_OperationOutputNumConsumers = _pywrap_tensorflow_internal.TF_OperationOutputNumConsumers\n",
            "\n",
            "def TF_OperationNumControlInputs(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationNumControlInputs(oper)\n",
            "TF_OperationNumControlInputs = _pywrap_tensorflow_internal.TF_OperationNumControlInputs\n",
            "\n",
            "def TF_OperationNumControlOutputs(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationNumControlOutputs(oper)\n",
            "TF_OperationNumControlOutputs = _pywrap_tensorflow_internal.TF_OperationNumControlOutputs\n",
            "class TF_AttrMetadata(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, TF_AttrMetadata, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, TF_AttrMetadata, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"is_list\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_is_list_set\n",
            "    __swig_getmethods__[\"is_list\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_is_list_get\n",
            "    if _newclass:\n",
            "        is_list = _swig_property(_pywrap_tensorflow_internal.TF_AttrMetadata_is_list_get, _pywrap_tensorflow_internal.TF_AttrMetadata_is_list_set)\n",
            "    __swig_setmethods__[\"list_size\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_list_size_set\n",
            "    __swig_getmethods__[\"list_size\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_list_size_get\n",
            "    if _newclass:\n",
            "        list_size = _swig_property(_pywrap_tensorflow_internal.TF_AttrMetadata_list_size_get, _pywrap_tensorflow_internal.TF_AttrMetadata_list_size_set)\n",
            "    __swig_setmethods__[\"type\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_type_set\n",
            "    __swig_getmethods__[\"type\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_type_get\n",
            "    if _newclass:\n",
            "        type = _swig_property(_pywrap_tensorflow_internal.TF_AttrMetadata_type_get, _pywrap_tensorflow_internal.TF_AttrMetadata_type_set)\n",
            "    __swig_setmethods__[\"total_size\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_total_size_set\n",
            "    __swig_getmethods__[\"total_size\"] = _pywrap_tensorflow_internal.TF_AttrMetadata_total_size_get\n",
            "    if _newclass:\n",
            "        total_size = _swig_property(_pywrap_tensorflow_internal.TF_AttrMetadata_total_size_get, _pywrap_tensorflow_internal.TF_AttrMetadata_total_size_set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_TF_AttrMetadata()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_TF_AttrMetadata\n",
            "    __del__ = lambda self: None\n",
            "TF_AttrMetadata_swigregister = _pywrap_tensorflow_internal.TF_AttrMetadata_swigregister\n",
            "TF_AttrMetadata_swigregister(TF_AttrMetadata)\n",
            "\n",
            "\n",
            "def TF_OperationGetAttrMetadata(oper, attr_name):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrMetadata(oper, attr_name)\n",
            "TF_OperationGetAttrMetadata = _pywrap_tensorflow_internal.TF_OperationGetAttrMetadata\n",
            "\n",
            "def TF_OperationGetAttrString(oper, attr_name, value, max_length):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrString(oper, attr_name, value, max_length)\n",
            "TF_OperationGetAttrString = _pywrap_tensorflow_internal.TF_OperationGetAttrString\n",
            "\n",
            "def TF_OperationGetAttrStringList(oper, attr_name, values, lengths, max_values, storage, storage_size):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrStringList(oper, attr_name, values, lengths, max_values, storage, storage_size)\n",
            "TF_OperationGetAttrStringList = _pywrap_tensorflow_internal.TF_OperationGetAttrStringList\n",
            "\n",
            "def TF_OperationGetAttrInt(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrInt(oper, attr_name, value)\n",
            "TF_OperationGetAttrInt = _pywrap_tensorflow_internal.TF_OperationGetAttrInt\n",
            "\n",
            "def TF_OperationGetAttrIntList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrIntList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrIntList = _pywrap_tensorflow_internal.TF_OperationGetAttrIntList\n",
            "\n",
            "def TF_OperationGetAttrFloat(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrFloat(oper, attr_name, value)\n",
            "TF_OperationGetAttrFloat = _pywrap_tensorflow_internal.TF_OperationGetAttrFloat\n",
            "\n",
            "def TF_OperationGetAttrFloatList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrFloatList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrFloatList = _pywrap_tensorflow_internal.TF_OperationGetAttrFloatList\n",
            "\n",
            "def TF_OperationGetAttrBool(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrBool(oper, attr_name, value)\n",
            "TF_OperationGetAttrBool = _pywrap_tensorflow_internal.TF_OperationGetAttrBool\n",
            "\n",
            "def TF_OperationGetAttrBoolList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrBoolList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrBoolList = _pywrap_tensorflow_internal.TF_OperationGetAttrBoolList\n",
            "\n",
            "def TF_OperationGetAttrType(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrType(oper, attr_name, value)\n",
            "TF_OperationGetAttrType = _pywrap_tensorflow_internal.TF_OperationGetAttrType\n",
            "\n",
            "def TF_OperationGetAttrTypeList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrTypeList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrTypeList = _pywrap_tensorflow_internal.TF_OperationGetAttrTypeList\n",
            "\n",
            "def TF_OperationGetAttrShape(oper, attr_name, value, num_dims):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrShape(oper, attr_name, value, num_dims)\n",
            "TF_OperationGetAttrShape = _pywrap_tensorflow_internal.TF_OperationGetAttrShape\n",
            "\n",
            "def TF_OperationGetAttrShapeList(oper, attr_name, dims, num_dims, num_shapes, storage, storage_size):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrShapeList(oper, attr_name, dims, num_dims, num_shapes, storage, storage_size)\n",
            "TF_OperationGetAttrShapeList = _pywrap_tensorflow_internal.TF_OperationGetAttrShapeList\n",
            "\n",
            "def TF_OperationGetAttrTensorShapeProto(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrTensorShapeProto(oper, attr_name, value)\n",
            "TF_OperationGetAttrTensorShapeProto = _pywrap_tensorflow_internal.TF_OperationGetAttrTensorShapeProto\n",
            "\n",
            "def TF_OperationGetAttrTensorShapeProtoList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrTensorShapeProtoList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrTensorShapeProtoList = _pywrap_tensorflow_internal.TF_OperationGetAttrTensorShapeProtoList\n",
            "\n",
            "def TF_OperationGetAttrTensor(oper, attr_name, value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrTensor(oper, attr_name, value)\n",
            "TF_OperationGetAttrTensor = _pywrap_tensorflow_internal.TF_OperationGetAttrTensor\n",
            "\n",
            "def TF_OperationGetAttrTensorList(oper, attr_name, values, max_values):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrTensorList(oper, attr_name, values, max_values)\n",
            "TF_OperationGetAttrTensorList = _pywrap_tensorflow_internal.TF_OperationGetAttrTensorList\n",
            "\n",
            "def TF_OperationGetAttrValueProto(oper, attr_name, output_attr_value):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetAttrValueProto(oper, attr_name, output_attr_value)\n",
            "TF_OperationGetAttrValueProto = _pywrap_tensorflow_internal.TF_OperationGetAttrValueProto\n",
            "\n",
            "def TF_GraphOperationByName(graph, oper_name):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphOperationByName(graph, oper_name)\n",
            "TF_GraphOperationByName = _pywrap_tensorflow_internal.TF_GraphOperationByName\n",
            "\n",
            "def TF_GraphNextOperation(graph, pos):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphNextOperation(graph, pos)\n",
            "TF_GraphNextOperation = _pywrap_tensorflow_internal.TF_GraphNextOperation\n",
            "\n",
            "def TF_GraphToGraphDef(graph, output_graph_def):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphToGraphDef(graph, output_graph_def)\n",
            "TF_GraphToGraphDef = _pywrap_tensorflow_internal.TF_GraphToGraphDef\n",
            "\n",
            "def TF_GraphGetOpDef(graph, op_name, output_op_def):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphGetOpDef(graph, op_name, output_op_def)\n",
            "TF_GraphGetOpDef = _pywrap_tensorflow_internal.TF_GraphGetOpDef\n",
            "\n",
            "def TF_GraphVersions(graph, output_version_def):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphVersions(graph, output_version_def)\n",
            "TF_GraphVersions = _pywrap_tensorflow_internal.TF_GraphVersions\n",
            "\n",
            "def TF_NewImportGraphDefOptions():\n",
            "    return _pywrap_tensorflow_internal.TF_NewImportGraphDefOptions()\n",
            "TF_NewImportGraphDefOptions = _pywrap_tensorflow_internal.TF_NewImportGraphDefOptions\n",
            "\n",
            "def TF_DeleteImportGraphDefOptions(opts):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteImportGraphDefOptions(opts)\n",
            "TF_DeleteImportGraphDefOptions = _pywrap_tensorflow_internal.TF_DeleteImportGraphDefOptions\n",
            "\n",
            "def TF_ImportGraphDefOptionsSetPrefix(opts, prefix):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetPrefix(opts, prefix)\n",
            "TF_ImportGraphDefOptionsSetPrefix = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetPrefix\n",
            "\n",
            "def TF_ImportGraphDefOptionsSetDefaultDevice(opts, device):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetDefaultDevice(opts, device)\n",
            "TF_ImportGraphDefOptionsSetDefaultDevice = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetDefaultDevice\n",
            "\n",
            "def TF_ImportGraphDefOptionsSetUniquifyNames(opts, uniquify_names):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetUniquifyNames(opts, uniquify_names)\n",
            "TF_ImportGraphDefOptionsSetUniquifyNames = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetUniquifyNames\n",
            "\n",
            "def TF_ImportGraphDefOptionsSetUniquifyPrefix(opts, uniquify_prefix):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetUniquifyPrefix(opts, uniquify_prefix)\n",
            "TF_ImportGraphDefOptionsSetUniquifyPrefix = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsSetUniquifyPrefix\n",
            "\n",
            "def TF_ImportGraphDefOptionsAddInputMapping(opts, src_name, src_index, dst):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddInputMapping(opts, src_name, src_index, dst)\n",
            "TF_ImportGraphDefOptionsAddInputMapping = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddInputMapping\n",
            "\n",
            "def TF_ImportGraphDefOptionsRemapControlDependency(opts, src_name, dst):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsRemapControlDependency(opts, src_name, dst)\n",
            "TF_ImportGraphDefOptionsRemapControlDependency = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsRemapControlDependency\n",
            "\n",
            "def TF_ImportGraphDefOptionsAddControlDependency(opts, oper):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddControlDependency(opts, oper)\n",
            "TF_ImportGraphDefOptionsAddControlDependency = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddControlDependency\n",
            "\n",
            "def TF_ImportGraphDefOptionsAddReturnOutput(opts, oper_name, index):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddReturnOutput(opts, oper_name, index)\n",
            "TF_ImportGraphDefOptionsAddReturnOutput = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddReturnOutput\n",
            "\n",
            "def TF_ImportGraphDefOptionsNumReturnOutputs(opts):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsNumReturnOutputs(opts)\n",
            "TF_ImportGraphDefOptionsNumReturnOutputs = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsNumReturnOutputs\n",
            "\n",
            "def TF_ImportGraphDefOptionsAddReturnOperation(opts, oper_name):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddReturnOperation(opts, oper_name)\n",
            "TF_ImportGraphDefOptionsAddReturnOperation = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsAddReturnOperation\n",
            "\n",
            "def TF_ImportGraphDefOptionsNumReturnOperations(opts):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsNumReturnOperations(opts)\n",
            "TF_ImportGraphDefOptionsNumReturnOperations = _pywrap_tensorflow_internal.TF_ImportGraphDefOptionsNumReturnOperations\n",
            "\n",
            "def TF_ImportGraphDefResultsReturnOutputs(results):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefResultsReturnOutputs(results)\n",
            "TF_ImportGraphDefResultsReturnOutputs = _pywrap_tensorflow_internal.TF_ImportGraphDefResultsReturnOutputs\n",
            "\n",
            "def TF_ImportGraphDefResultsReturnOperations(results):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefResultsReturnOperations(results)\n",
            "TF_ImportGraphDefResultsReturnOperations = _pywrap_tensorflow_internal.TF_ImportGraphDefResultsReturnOperations\n",
            "\n",
            "def TF_DeleteImportGraphDefResults(results):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteImportGraphDefResults(results)\n",
            "TF_DeleteImportGraphDefResults = _pywrap_tensorflow_internal.TF_DeleteImportGraphDefResults\n",
            "\n",
            "def TF_GraphImportGraphDefWithResults(graph, graph_def, options):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphImportGraphDefWithResults(graph, graph_def, options)\n",
            "TF_GraphImportGraphDefWithResults = _pywrap_tensorflow_internal.TF_GraphImportGraphDefWithResults\n",
            "\n",
            "def TF_GraphImportGraphDefWithReturnOutputs(graph, graph_def, options, return_outputs, num_return_outputs):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphImportGraphDefWithReturnOutputs(graph, graph_def, options, return_outputs, num_return_outputs)\n",
            "TF_GraphImportGraphDefWithReturnOutputs = _pywrap_tensorflow_internal.TF_GraphImportGraphDefWithReturnOutputs\n",
            "\n",
            "def TF_GraphImportGraphDef(graph, graph_def, options):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphImportGraphDef(graph, graph_def, options)\n",
            "TF_GraphImportGraphDef = _pywrap_tensorflow_internal.TF_GraphImportGraphDef\n",
            "\n",
            "def TF_GraphCopyFunction(g, func, grad):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphCopyFunction(g, func, grad)\n",
            "TF_GraphCopyFunction = _pywrap_tensorflow_internal.TF_GraphCopyFunction\n",
            "\n",
            "def TF_GraphNumFunctions(g):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphNumFunctions(g)\n",
            "TF_GraphNumFunctions = _pywrap_tensorflow_internal.TF_GraphNumFunctions\n",
            "\n",
            "def TF_GraphGetFunctions(g, funcs, max_func):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphGetFunctions(g, funcs, max_func)\n",
            "TF_GraphGetFunctions = _pywrap_tensorflow_internal.TF_GraphGetFunctions\n",
            "\n",
            "def TF_OperationToNodeDef(oper, output_node_def):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationToNodeDef(oper, output_node_def)\n",
            "TF_OperationToNodeDef = _pywrap_tensorflow_internal.TF_OperationToNodeDef\n",
            "\n",
            "def TF_AddGradients(g, y, ny, x, nx, dx, dy):\n",
            "    return _pywrap_tensorflow_internal.TF_AddGradients(g, y, ny, x, nx, dx, dy)\n",
            "TF_AddGradients = _pywrap_tensorflow_internal.TF_AddGradients\n",
            "\n",
            "def TF_AddGradientsWithPrefix(g, prefix, y, ny, x, nx, dx, dy):\n",
            "    return _pywrap_tensorflow_internal.TF_AddGradientsWithPrefix(g, prefix, y, ny, x, nx, dx, dy)\n",
            "TF_AddGradientsWithPrefix = _pywrap_tensorflow_internal.TF_AddGradientsWithPrefix\n",
            "\n",
            "def TF_GraphToFunction(fn_body, fn_name, append_hash_to_fn_name, num_opers, opers, ninputs, inputs, noutputs, outputs, output_names, opts, description):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphToFunction(fn_body, fn_name, append_hash_to_fn_name, num_opers, opers, ninputs, inputs, noutputs, outputs, output_names, opts, description)\n",
            "TF_GraphToFunction = _pywrap_tensorflow_internal.TF_GraphToFunction\n",
            "\n",
            "def TF_GraphToFunctionWithControlOutputs(fn_body, fn_name, append_hash_to_fn_name, num_opers, opers, ninputs, inputs, noutputs, outputs, output_names, ncontrol_outputs, control_outputs, control_output_names, opts, description):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphToFunctionWithControlOutputs(fn_body, fn_name, append_hash_to_fn_name, num_opers, opers, ninputs, inputs, noutputs, outputs, output_names, ncontrol_outputs, control_outputs, control_output_names, opts, description)\n",
            "TF_GraphToFunctionWithControlOutputs = _pywrap_tensorflow_internal.TF_GraphToFunctionWithControlOutputs\n",
            "\n",
            "def TF_FunctionName(func):\n",
            "    return _pywrap_tensorflow_internal.TF_FunctionName(func)\n",
            "TF_FunctionName = _pywrap_tensorflow_internal.TF_FunctionName\n",
            "\n",
            "def TF_FunctionToFunctionDef(func, output_func_def):\n",
            "    return _pywrap_tensorflow_internal.TF_FunctionToFunctionDef(func, output_func_def)\n",
            "TF_FunctionToFunctionDef = _pywrap_tensorflow_internal.TF_FunctionToFunctionDef\n",
            "\n",
            "def TF_FunctionImportFunctionDef(proto):\n",
            "    return _pywrap_tensorflow_internal.TF_FunctionImportFunctionDef(proto)\n",
            "TF_FunctionImportFunctionDef = _pywrap_tensorflow_internal.TF_FunctionImportFunctionDef\n",
            "\n",
            "def TF_FunctionSetAttrValueProto(func, attr_name, proto):\n",
            "    return _pywrap_tensorflow_internal.TF_FunctionSetAttrValueProto(func, attr_name, proto)\n",
            "TF_FunctionSetAttrValueProto = _pywrap_tensorflow_internal.TF_FunctionSetAttrValueProto\n",
            "\n",
            "def TF_FunctionGetAttrValueProto(func, attr_name, output_attr_value):\n",
            "    return _pywrap_tensorflow_internal.TF_FunctionGetAttrValueProto(func, attr_name, output_attr_value)\n",
            "TF_FunctionGetAttrValueProto = _pywrap_tensorflow_internal.TF_FunctionGetAttrValueProto\n",
            "\n",
            "def TF_DeleteFunction(func):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteFunction(func)\n",
            "TF_DeleteFunction = _pywrap_tensorflow_internal.TF_DeleteFunction\n",
            "\n",
            "def TF_TryEvaluateConstant(graph, output, result):\n",
            "    return _pywrap_tensorflow_internal.TF_TryEvaluateConstant(graph, output, result)\n",
            "TF_TryEvaluateConstant = _pywrap_tensorflow_internal.TF_TryEvaluateConstant\n",
            "\n",
            "def TF_NewSession(graph, opts):\n",
            "    return _pywrap_tensorflow_internal.TF_NewSession(graph, opts)\n",
            "TF_NewSession = _pywrap_tensorflow_internal.TF_NewSession\n",
            "\n",
            "def TF_LoadSessionFromSavedModel(session_options, run_options, export_dir, tags, tags_len, graph, meta_graph_def):\n",
            "    return _pywrap_tensorflow_internal.TF_LoadSessionFromSavedModel(session_options, run_options, export_dir, tags, tags_len, graph, meta_graph_def)\n",
            "TF_LoadSessionFromSavedModel = _pywrap_tensorflow_internal.TF_LoadSessionFromSavedModel\n",
            "\n",
            "def TF_CloseSession(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_CloseSession(arg1)\n",
            "TF_CloseSession = _pywrap_tensorflow_internal.TF_CloseSession\n",
            "\n",
            "def TF_DeleteSession(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteSession(arg1)\n",
            "TF_DeleteSession = _pywrap_tensorflow_internal.TF_DeleteSession\n",
            "\n",
            "def TF_DeletePRunHandle(handle):\n",
            "    return _pywrap_tensorflow_internal.TF_DeletePRunHandle(handle)\n",
            "TF_DeletePRunHandle = _pywrap_tensorflow_internal.TF_DeletePRunHandle\n",
            "\n",
            "def TF_NewDeprecatedSession(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_NewDeprecatedSession(arg1)\n",
            "TF_NewDeprecatedSession = _pywrap_tensorflow_internal.TF_NewDeprecatedSession\n",
            "\n",
            "def TF_CloseDeprecatedSession(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_CloseDeprecatedSession(arg1)\n",
            "TF_CloseDeprecatedSession = _pywrap_tensorflow_internal.TF_CloseDeprecatedSession\n",
            "\n",
            "def TF_DeleteDeprecatedSession(arg1):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteDeprecatedSession(arg1)\n",
            "TF_DeleteDeprecatedSession = _pywrap_tensorflow_internal.TF_DeleteDeprecatedSession\n",
            "\n",
            "def TF_Reset(opt, containers, ncontainers):\n",
            "    return _pywrap_tensorflow_internal.TF_Reset(opt, containers, ncontainers)\n",
            "TF_Reset = _pywrap_tensorflow_internal.TF_Reset\n",
            "\n",
            "def TF_ExtendGraph(arg1, proto, arg3):\n",
            "    return _pywrap_tensorflow_internal.TF_ExtendGraph(arg1, proto, arg3)\n",
            "TF_ExtendGraph = _pywrap_tensorflow_internal.TF_ExtendGraph\n",
            "\n",
            "def TF_SessionListDevices(session):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionListDevices(session)\n",
            "TF_SessionListDevices = _pywrap_tensorflow_internal.TF_SessionListDevices\n",
            "\n",
            "def TF_DeprecatedSessionListDevices(session):\n",
            "    return _pywrap_tensorflow_internal.TF_DeprecatedSessionListDevices(session)\n",
            "TF_DeprecatedSessionListDevices = _pywrap_tensorflow_internal.TF_DeprecatedSessionListDevices\n",
            "\n",
            "def TF_DeleteDeviceList(list):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteDeviceList(list)\n",
            "TF_DeleteDeviceList = _pywrap_tensorflow_internal.TF_DeleteDeviceList\n",
            "\n",
            "def TF_DeviceListCount(list):\n",
            "    return _pywrap_tensorflow_internal.TF_DeviceListCount(list)\n",
            "TF_DeviceListCount = _pywrap_tensorflow_internal.TF_DeviceListCount\n",
            "\n",
            "def TF_DeviceListName(list, index):\n",
            "    return _pywrap_tensorflow_internal.TF_DeviceListName(list, index)\n",
            "TF_DeviceListName = _pywrap_tensorflow_internal.TF_DeviceListName\n",
            "\n",
            "def TF_DeviceListType(list, index):\n",
            "    return _pywrap_tensorflow_internal.TF_DeviceListType(list, index)\n",
            "TF_DeviceListType = _pywrap_tensorflow_internal.TF_DeviceListType\n",
            "\n",
            "def TF_DeviceListMemoryBytes(list, index):\n",
            "    return _pywrap_tensorflow_internal.TF_DeviceListMemoryBytes(list, index)\n",
            "TF_DeviceListMemoryBytes = _pywrap_tensorflow_internal.TF_DeviceListMemoryBytes\n",
            "\n",
            "def TF_DeviceListIncarnation(list, index):\n",
            "    return _pywrap_tensorflow_internal.TF_DeviceListIncarnation(list, index)\n",
            "TF_DeviceListIncarnation = _pywrap_tensorflow_internal.TF_DeviceListIncarnation\n",
            "\n",
            "def TF_LoadLibrary(library_filename):\n",
            "    return _pywrap_tensorflow_internal.TF_LoadLibrary(library_filename)\n",
            "TF_LoadLibrary = _pywrap_tensorflow_internal.TF_LoadLibrary\n",
            "\n",
            "def TF_GetOpList(lib_handle):\n",
            "    return _pywrap_tensorflow_internal.TF_GetOpList(lib_handle)\n",
            "TF_GetOpList = _pywrap_tensorflow_internal.TF_GetOpList\n",
            "\n",
            "def TF_DeleteLibraryHandle(lib_handle):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteLibraryHandle(lib_handle)\n",
            "TF_DeleteLibraryHandle = _pywrap_tensorflow_internal.TF_DeleteLibraryHandle\n",
            "\n",
            "def TF_GetAllOpList():\n",
            "    return _pywrap_tensorflow_internal.TF_GetAllOpList()\n",
            "TF_GetAllOpList = _pywrap_tensorflow_internal.TF_GetAllOpList\n",
            "\n",
            "def TF_NewApiDefMap(op_list_buffer):\n",
            "    return _pywrap_tensorflow_internal.TF_NewApiDefMap(op_list_buffer)\n",
            "TF_NewApiDefMap = _pywrap_tensorflow_internal.TF_NewApiDefMap\n",
            "\n",
            "def TF_DeleteApiDefMap(apimap):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteApiDefMap(apimap)\n",
            "TF_DeleteApiDefMap = _pywrap_tensorflow_internal.TF_DeleteApiDefMap\n",
            "\n",
            "def TF_ApiDefMapPut(api_def_map, text, text_len):\n",
            "    return _pywrap_tensorflow_internal.TF_ApiDefMapPut(api_def_map, text, text_len)\n",
            "TF_ApiDefMapPut = _pywrap_tensorflow_internal.TF_ApiDefMapPut\n",
            "\n",
            "def TF_ApiDefMapGet(api_def_map, name, name_len):\n",
            "    return _pywrap_tensorflow_internal.TF_ApiDefMapGet(api_def_map, name, name_len)\n",
            "TF_ApiDefMapGet = _pywrap_tensorflow_internal.TF_ApiDefMapGet\n",
            "\n",
            "def TF_GetAllRegisteredKernels():\n",
            "    return _pywrap_tensorflow_internal.TF_GetAllRegisteredKernels()\n",
            "TF_GetAllRegisteredKernels = _pywrap_tensorflow_internal.TF_GetAllRegisteredKernels\n",
            "\n",
            "def TF_GetRegisteredKernelsForOp(name):\n",
            "    return _pywrap_tensorflow_internal.TF_GetRegisteredKernelsForOp(name)\n",
            "TF_GetRegisteredKernelsForOp = _pywrap_tensorflow_internal.TF_GetRegisteredKernelsForOp\n",
            "\n",
            "def TF_NewServer(proto):\n",
            "    return _pywrap_tensorflow_internal.TF_NewServer(proto)\n",
            "TF_NewServer = _pywrap_tensorflow_internal.TF_NewServer\n",
            "\n",
            "def TF_ServerStart(server):\n",
            "    return _pywrap_tensorflow_internal.TF_ServerStart(server)\n",
            "TF_ServerStart = _pywrap_tensorflow_internal.TF_ServerStart\n",
            "\n",
            "def TF_ServerStop(server):\n",
            "    return _pywrap_tensorflow_internal.TF_ServerStop(server)\n",
            "TF_ServerStop = _pywrap_tensorflow_internal.TF_ServerStop\n",
            "\n",
            "def TF_ServerJoin(server):\n",
            "    return _pywrap_tensorflow_internal.TF_ServerJoin(server)\n",
            "TF_ServerJoin = _pywrap_tensorflow_internal.TF_ServerJoin\n",
            "\n",
            "def TF_ServerTarget(server):\n",
            "    return _pywrap_tensorflow_internal.TF_ServerTarget(server)\n",
            "TF_ServerTarget = _pywrap_tensorflow_internal.TF_ServerTarget\n",
            "\n",
            "def TF_DeleteServer(server):\n",
            "    return _pywrap_tensorflow_internal.TF_DeleteServer(server)\n",
            "TF_DeleteServer = _pywrap_tensorflow_internal.TF_DeleteServer\n",
            "\n",
            "def TF_RegisterLogListener(listener):\n",
            "    return _pywrap_tensorflow_internal.TF_RegisterLogListener(listener)\n",
            "TF_RegisterLogListener = _pywrap_tensorflow_internal.TF_RegisterLogListener\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_STRING_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_STRING = _pywrap_tensorflow_internal.TF_ATTR_STRING\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_INT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_INT = _pywrap_tensorflow_internal.TF_ATTR_INT\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_FLOAT_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_FLOAT = _pywrap_tensorflow_internal.TF_ATTR_FLOAT\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_BOOL_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_BOOL = _pywrap_tensorflow_internal.TF_ATTR_BOOL\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_TYPE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_TYPE = _pywrap_tensorflow_internal.TF_ATTR_TYPE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_SHAPE_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_SHAPE = _pywrap_tensorflow_internal.TF_ATTR_SHAPE\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_TENSOR_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_TENSOR = _pywrap_tensorflow_internal.TF_ATTR_TENSOR\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_PLACEHOLDER_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_PLACEHOLDER = _pywrap_tensorflow_internal.TF_ATTR_PLACEHOLDER\n",
            "\n",
            "_pywrap_tensorflow_internal.TF_ATTR_FUNC_swigconstant(_pywrap_tensorflow_internal)\n",
            "TF_ATTR_FUNC = _pywrap_tensorflow_internal.TF_ATTR_FUNC\n",
            "\n",
            "def AddControlInput(graph, op, input):\n",
            "    return _pywrap_tensorflow_internal.AddControlInput(graph, op, input)\n",
            "AddControlInput = _pywrap_tensorflow_internal.AddControlInput\n",
            "\n",
            "def SetAttr(graph, op, attr_name, attr_value_proto):\n",
            "    return _pywrap_tensorflow_internal.SetAttr(graph, op, attr_name, attr_value_proto)\n",
            "SetAttr = _pywrap_tensorflow_internal.SetAttr\n",
            "\n",
            "def ClearAttr(graph, op, attr_name):\n",
            "    return _pywrap_tensorflow_internal.ClearAttr(graph, op, attr_name)\n",
            "ClearAttr = _pywrap_tensorflow_internal.ClearAttr\n",
            "\n",
            "def SetRequestedDevice(graph, op, device):\n",
            "    return _pywrap_tensorflow_internal.SetRequestedDevice(graph, op, device)\n",
            "SetRequestedDevice = _pywrap_tensorflow_internal.SetRequestedDevice\n",
            "\n",
            "def UpdateEdge(graph, new_src, dst):\n",
            "    return _pywrap_tensorflow_internal.UpdateEdge(graph, new_src, dst)\n",
            "UpdateEdge = _pywrap_tensorflow_internal.UpdateEdge\n",
            "\n",
            "def RemoveAllControlInputs(graph, op):\n",
            "    return _pywrap_tensorflow_internal.RemoveAllControlInputs(graph, op)\n",
            "RemoveAllControlInputs = _pywrap_tensorflow_internal.RemoveAllControlInputs\n",
            "\n",
            "def SetRequireShapeInferenceFns(graph, require):\n",
            "    return _pywrap_tensorflow_internal.SetRequireShapeInferenceFns(graph, require)\n",
            "SetRequireShapeInferenceFns = _pywrap_tensorflow_internal.SetRequireShapeInferenceFns\n",
            "\n",
            "def ExtendSession(session):\n",
            "    return _pywrap_tensorflow_internal.ExtendSession(session)\n",
            "ExtendSession = _pywrap_tensorflow_internal.ExtendSession\n",
            "\n",
            "def GetHandleShapeAndType(graph, output):\n",
            "    return _pywrap_tensorflow_internal.GetHandleShapeAndType(graph, output)\n",
            "GetHandleShapeAndType = _pywrap_tensorflow_internal.GetHandleShapeAndType\n",
            "\n",
            "def SetHandleShapeAndType(graph, output, proto):\n",
            "    return _pywrap_tensorflow_internal.SetHandleShapeAndType(graph, output, proto)\n",
            "SetHandleShapeAndType = _pywrap_tensorflow_internal.SetHandleShapeAndType\n",
            "\n",
            "def AddWhileInputHack(graph, new_src, dst):\n",
            "    return _pywrap_tensorflow_internal.AddWhileInputHack(graph, new_src, dst)\n",
            "AddWhileInputHack = _pywrap_tensorflow_internal.AddWhileInputHack\n",
            "\n",
            "def TF_NewSessionOptions(target=None, config=None):\n",
            "# NOTE: target and config are validated in the session constructor.\n",
            "  opts = _TF_NewSessionOptions()\n",
            "  if target is not None:\n",
            "    _TF_SetTarget(opts, target)\n",
            "  if config is not None:\n",
            "    from tensorflow.python.framework import errors\n",
            "    config_str = config.SerializeToString()\n",
            "    _TF_SetConfig(opts, config_str)\n",
            "  return opts\n",
            "\n",
            "\n",
            "def TF_Reset(target, containers=None, config=None):\n",
            "  from tensorflow.python.framework import errors\n",
            "  opts = TF_NewSessionOptions(target=target, config=config)\n",
            "  try:\n",
            "    TF_Reset_wrapper(opts, containers)\n",
            "  finally:\n",
            "    TF_DeleteSessionOptions(opts)\n",
            "\n",
            "\n",
            "def TF_NewSessionRef(graph, opts):\n",
            "    return _pywrap_tensorflow_internal.TF_NewSessionRef(graph, opts)\n",
            "TF_NewSessionRef = _pywrap_tensorflow_internal.TF_NewSessionRef\n",
            "\n",
            "def TF_Run(session, run_options, feed_dict, output_names, target_nodes, out_status, run_outputs):\n",
            "    return _pywrap_tensorflow_internal.TF_Run(session, run_options, feed_dict, output_names, target_nodes, out_status, run_outputs)\n",
            "TF_Run = _pywrap_tensorflow_internal.TF_Run\n",
            "\n",
            "def TF_DeprecatedSessionMakeCallable(session, callable_options):\n",
            "    return _pywrap_tensorflow_internal.TF_DeprecatedSessionMakeCallable(session, callable_options)\n",
            "TF_DeprecatedSessionMakeCallable = _pywrap_tensorflow_internal.TF_DeprecatedSessionMakeCallable\n",
            "\n",
            "def TF_SessionMakeCallable(session, callable_options):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionMakeCallable(session, callable_options)\n",
            "TF_SessionMakeCallable = _pywrap_tensorflow_internal.TF_SessionMakeCallable\n",
            "\n",
            "def TF_DeprecatedSessionRunCallable(session, handle, feed_values, run_metadata):\n",
            "    return _pywrap_tensorflow_internal.TF_DeprecatedSessionRunCallable(session, handle, feed_values, run_metadata)\n",
            "TF_DeprecatedSessionRunCallable = _pywrap_tensorflow_internal.TF_DeprecatedSessionRunCallable\n",
            "\n",
            "def TF_SessionRunCallable(session, handle, feed_values, run_metadata):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionRunCallable(session, handle, feed_values, run_metadata)\n",
            "TF_SessionRunCallable = _pywrap_tensorflow_internal.TF_SessionRunCallable\n",
            "\n",
            "def TF_DeprecatedSessionReleaseCallable(session, handle):\n",
            "    return _pywrap_tensorflow_internal.TF_DeprecatedSessionReleaseCallable(session, handle)\n",
            "TF_DeprecatedSessionReleaseCallable = _pywrap_tensorflow_internal.TF_DeprecatedSessionReleaseCallable\n",
            "\n",
            "def TF_SessionReleaseCallable(session, handle):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionReleaseCallable(session, handle)\n",
            "TF_SessionReleaseCallable = _pywrap_tensorflow_internal.TF_SessionReleaseCallable\n",
            "\n",
            "def TF_PRunSetup(session, input_names, output_names, target_nodes, out_status):\n",
            "    return _pywrap_tensorflow_internal.TF_PRunSetup(session, input_names, output_names, target_nodes, out_status)\n",
            "TF_PRunSetup = _pywrap_tensorflow_internal.TF_PRunSetup\n",
            "\n",
            "def TF_PRun(session, handle, feed_dict, output_names, out_status):\n",
            "    return _pywrap_tensorflow_internal.TF_PRun(session, handle, feed_dict, output_names, out_status)\n",
            "TF_PRun = _pywrap_tensorflow_internal.TF_PRun\n",
            "\n",
            "def TF_Reset_wrapper(opt, containers):\n",
            "    return _pywrap_tensorflow_internal.TF_Reset_wrapper(opt, containers)\n",
            "TF_Reset_wrapper = _pywrap_tensorflow_internal.TF_Reset_wrapper\n",
            "\n",
            "def EqualGraphDefWrapper(actual, expected):\n",
            "    return _pywrap_tensorflow_internal.EqualGraphDefWrapper(actual, expected)\n",
            "EqualGraphDefWrapper = _pywrap_tensorflow_internal.EqualGraphDefWrapper\n",
            "\n",
            "def EqualAttrValueWrapper(actual, expected):\n",
            "    return _pywrap_tensorflow_internal.EqualAttrValueWrapper(actual, expected)\n",
            "EqualAttrValueWrapper = _pywrap_tensorflow_internal.EqualAttrValueWrapper\n",
            "\n",
            "def TF_GraphGetTensorShapeHelper(graph, output):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphGetTensorShapeHelper(graph, output)\n",
            "TF_GraphGetTensorShapeHelper = _pywrap_tensorflow_internal.TF_GraphGetTensorShapeHelper\n",
            "\n",
            "def TF_SessionRun_wrapper(session, run_options, inputs, outputs, targets, run_metadata):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionRun_wrapper(session, run_options, inputs, outputs, targets, run_metadata)\n",
            "TF_SessionRun_wrapper = _pywrap_tensorflow_internal.TF_SessionRun_wrapper\n",
            "\n",
            "def TF_SessionPRunSetup_wrapper(session, inputs, outputs, targets):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionPRunSetup_wrapper(session, inputs, outputs, targets)\n",
            "TF_SessionPRunSetup_wrapper = _pywrap_tensorflow_internal.TF_SessionPRunSetup_wrapper\n",
            "\n",
            "def TF_SessionPRun_wrapper(session, handle, inputs, outputs):\n",
            "    return _pywrap_tensorflow_internal.TF_SessionPRun_wrapper(session, handle, inputs, outputs)\n",
            "TF_SessionPRun_wrapper = _pywrap_tensorflow_internal.TF_SessionPRun_wrapper\n",
            "\n",
            "def GetOperationInputs(oper):\n",
            "    return _pywrap_tensorflow_internal.GetOperationInputs(oper)\n",
            "GetOperationInputs = _pywrap_tensorflow_internal.GetOperationInputs\n",
            "\n",
            "def TF_OperationGetControlInputs_wrapper(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetControlInputs_wrapper(oper)\n",
            "TF_OperationGetControlInputs_wrapper = _pywrap_tensorflow_internal.TF_OperationGetControlInputs_wrapper\n",
            "\n",
            "def TF_OperationGetControlOutputs_wrapper(oper):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationGetControlOutputs_wrapper(oper)\n",
            "TF_OperationGetControlOutputs_wrapper = _pywrap_tensorflow_internal.TF_OperationGetControlOutputs_wrapper\n",
            "\n",
            "def TF_OperationOutputConsumers_wrapper(oper_out):\n",
            "    return _pywrap_tensorflow_internal.TF_OperationOutputConsumers_wrapper(oper_out)\n",
            "TF_OperationOutputConsumers_wrapper = _pywrap_tensorflow_internal.TF_OperationOutputConsumers_wrapper\n",
            "\n",
            "def TF_GraphToFunction_wrapper(fn_body, fn_name, append_hash_to_fn_name, opers, inputs, outputs, output_names, control_outputs, control_output_names, opts, description):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphToFunction_wrapper(fn_body, fn_name, append_hash_to_fn_name, opers, inputs, outputs, output_names, control_outputs, control_output_names, opts, description)\n",
            "TF_GraphToFunction_wrapper = _pywrap_tensorflow_internal.TF_GraphToFunction_wrapper\n",
            "\n",
            "def TF_GraphSetOutputHandleShapesAndTypes_wrapper(graph, output, shapes, ranks, types):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphSetOutputHandleShapesAndTypes_wrapper(graph, output, shapes, ranks, types)\n",
            "TF_GraphSetOutputHandleShapesAndTypes_wrapper = _pywrap_tensorflow_internal.TF_GraphSetOutputHandleShapesAndTypes_wrapper\n",
            "\n",
            "def TF_GraphSetTensorShape_wrapper(graph, output, dims, unknown_shape):\n",
            "    return _pywrap_tensorflow_internal.TF_GraphSetTensorShape_wrapper(graph, output, dims, unknown_shape)\n",
            "TF_GraphSetTensorShape_wrapper = _pywrap_tensorflow_internal.TF_GraphSetTensorShape_wrapper\n",
            "\n",
            "def TF_ImportGraphDefResultsMissingUnusedInputMappings_wrapper(results):\n",
            "    return _pywrap_tensorflow_internal.TF_ImportGraphDefResultsMissingUnusedInputMappings_wrapper(results)\n",
            "TF_ImportGraphDefResultsMissingUnusedInputMappings_wrapper = _pywrap_tensorflow_internal.TF_ImportGraphDefResultsMissingUnusedInputMappings_wrapper\n",
            "\n",
            "def TF_TryEvaluateConstant_wrapper(graph, output):\n",
            "    return _pywrap_tensorflow_internal.TF_TryEvaluateConstant_wrapper(graph, output)\n",
            "TF_TryEvaluateConstant_wrapper = _pywrap_tensorflow_internal.TF_TryEvaluateConstant_wrapper\n",
            "\n",
            "def ListDevices():\n",
            "    return _pywrap_tensorflow_internal.ListDevices()\n",
            "ListDevices = _pywrap_tensorflow_internal.ListDevices\n",
            "\n",
            "def ListDevicesWithSessionConfig(config):\n",
            "    return _pywrap_tensorflow_internal.ListDevicesWithSessionConfig(config)\n",
            "ListDevicesWithSessionConfig = _pywrap_tensorflow_internal.ListDevicesWithSessionConfig\n",
            "\n",
            "def list_devices(session_config=None):\n",
            "  from tensorflow.python.framework import errors\n",
            "\n",
            "  if session_config:\n",
            "    return ListDevicesWithSessionConfig(session_config.SerializeToString())\n",
            "  else:\n",
            "    return ListDevices()\n",
            "\n",
            "\n",
            "def TF_bfloat16_type():\n",
            "    return _pywrap_tensorflow_internal.TF_bfloat16_type()\n",
            "TF_bfloat16_type = _pywrap_tensorflow_internal.TF_bfloat16_type\n",
            "\n",
            "def FileExists(filename):\n",
            "    return _pywrap_tensorflow_internal.FileExists(filename)\n",
            "FileExists = _pywrap_tensorflow_internal.FileExists\n",
            "\n",
            "def DeleteFile(filename):\n",
            "    return _pywrap_tensorflow_internal.DeleteFile(filename)\n",
            "DeleteFile = _pywrap_tensorflow_internal.DeleteFile\n",
            "\n",
            "def ReadFileToString(filename):\n",
            "    return _pywrap_tensorflow_internal.ReadFileToString(filename)\n",
            "ReadFileToString = _pywrap_tensorflow_internal.ReadFileToString\n",
            "\n",
            "def WriteStringToFile(filename, file_content):\n",
            "    return _pywrap_tensorflow_internal.WriteStringToFile(filename, file_content)\n",
            "WriteStringToFile = _pywrap_tensorflow_internal.WriteStringToFile\n",
            "\n",
            "def GetChildren(dir):\n",
            "    return _pywrap_tensorflow_internal.GetChildren(dir)\n",
            "GetChildren = _pywrap_tensorflow_internal.GetChildren\n",
            "\n",
            "def GetMatchingFiles(filename):\n",
            "    return _pywrap_tensorflow_internal.GetMatchingFiles(filename)\n",
            "GetMatchingFiles = _pywrap_tensorflow_internal.GetMatchingFiles\n",
            "\n",
            "def CreateDir(dirname):\n",
            "    return _pywrap_tensorflow_internal.CreateDir(dirname)\n",
            "CreateDir = _pywrap_tensorflow_internal.CreateDir\n",
            "\n",
            "def RecursivelyCreateDir(dirname):\n",
            "    return _pywrap_tensorflow_internal.RecursivelyCreateDir(dirname)\n",
            "RecursivelyCreateDir = _pywrap_tensorflow_internal.RecursivelyCreateDir\n",
            "\n",
            "def CopyFile(oldpath, newpath, overwrite):\n",
            "    return _pywrap_tensorflow_internal.CopyFile(oldpath, newpath, overwrite)\n",
            "CopyFile = _pywrap_tensorflow_internal.CopyFile\n",
            "\n",
            "def RenameFile(oldname, newname, overwrite):\n",
            "    return _pywrap_tensorflow_internal.RenameFile(oldname, newname, overwrite)\n",
            "RenameFile = _pywrap_tensorflow_internal.RenameFile\n",
            "\n",
            "def DeleteRecursively(dirname):\n",
            "    return _pywrap_tensorflow_internal.DeleteRecursively(dirname)\n",
            "DeleteRecursively = _pywrap_tensorflow_internal.DeleteRecursively\n",
            "\n",
            "def IsDirectory(dirname, out_status):\n",
            "    return _pywrap_tensorflow_internal.IsDirectory(dirname, out_status)\n",
            "IsDirectory = _pywrap_tensorflow_internal.IsDirectory\n",
            "\n",
            "def Stat(filename, stats):\n",
            "    return _pywrap_tensorflow_internal.Stat(filename, stats)\n",
            "Stat = _pywrap_tensorflow_internal.Stat\n",
            "\n",
            "def CreateBufferedInputStream(filename, buffer_size):\n",
            "    return _pywrap_tensorflow_internal.CreateBufferedInputStream(filename, buffer_size)\n",
            "CreateBufferedInputStream = _pywrap_tensorflow_internal.CreateBufferedInputStream\n",
            "\n",
            "def CreateWritableFile(filename, mode):\n",
            "    return _pywrap_tensorflow_internal.CreateWritableFile(filename, mode)\n",
            "CreateWritableFile = _pywrap_tensorflow_internal.CreateWritableFile\n",
            "\n",
            "def AppendToFile(file_content, file):\n",
            "    return _pywrap_tensorflow_internal.AppendToFile(file_content, file)\n",
            "AppendToFile = _pywrap_tensorflow_internal.AppendToFile\n",
            "\n",
            "def TellFile(file):\n",
            "    return _pywrap_tensorflow_internal.TellFile(file)\n",
            "TellFile = _pywrap_tensorflow_internal.TellFile\n",
            "\n",
            "def ReadFromStream(stream, bytes):\n",
            "    return _pywrap_tensorflow_internal.ReadFromStream(stream, bytes)\n",
            "ReadFromStream = _pywrap_tensorflow_internal.ReadFromStream\n",
            "class WritableFile(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, WritableFile, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, WritableFile, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined - class is abstract\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_WritableFile\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def Close(self):\n",
            "        return _pywrap_tensorflow_internal.WritableFile_Close(self)\n",
            "\n",
            "    def Flush(self):\n",
            "        return _pywrap_tensorflow_internal.WritableFile_Flush(self)\n",
            "WritableFile_swigregister = _pywrap_tensorflow_internal.WritableFile_swigregister\n",
            "WritableFile_swigregister(WritableFile)\n",
            "\n",
            "class BufferedInputStream(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, BufferedInputStream, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, BufferedInputStream, name)\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        raise AttributeError(\"No constructor defined\")\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_BufferedInputStream\n",
            "    __del__ = lambda self: None\n",
            "\n",
            "    def Tell(self):\n",
            "        return _pywrap_tensorflow_internal.BufferedInputStream_Tell(self)\n",
            "\n",
            "    def Seek(self, position):\n",
            "        return _pywrap_tensorflow_internal.BufferedInputStream_Seek(self, position)\n",
            "\n",
            "    def ReadLineAsString(self):\n",
            "        return _pywrap_tensorflow_internal.BufferedInputStream_ReadLineAsString(self)\n",
            "BufferedInputStream_swigregister = _pywrap_tensorflow_internal.BufferedInputStream_swigregister\n",
            "BufferedInputStream_swigregister(BufferedInputStream)\n",
            "\n",
            "\n",
            "def Set_TF_Status_from_Status(tf_status, status):\n",
            "    return _pywrap_tensorflow_internal.Set_TF_Status_from_Status(tf_status, status)\n",
            "Set_TF_Status_from_Status = _pywrap_tensorflow_internal.Set_TF_Status_from_Status\n",
            "\n",
            "def StatusFromTF_Status(tf_status):\n",
            "    return _pywrap_tensorflow_internal.StatusFromTF_Status(tf_status)\n",
            "StatusFromTF_Status = _pywrap_tensorflow_internal.StatusFromTF_Status\n",
            "\n",
            "def IsAbsolutePath(path):\n",
            "    return _pywrap_tensorflow_internal.IsAbsolutePath(path)\n",
            "IsAbsolutePath = _pywrap_tensorflow_internal.IsAbsolutePath\n",
            "\n",
            "def Dirname(path):\n",
            "    return _pywrap_tensorflow_internal.Dirname(path)\n",
            "Dirname = _pywrap_tensorflow_internal.Dirname\n",
            "\n",
            "def Basename(path):\n",
            "    return _pywrap_tensorflow_internal.Basename(path)\n",
            "Basename = _pywrap_tensorflow_internal.Basename\n",
            "\n",
            "def Extension(path):\n",
            "    return _pywrap_tensorflow_internal.Extension(path)\n",
            "Extension = _pywrap_tensorflow_internal.Extension\n",
            "\n",
            "def CleanPath(path):\n",
            "    return _pywrap_tensorflow_internal.CleanPath(path)\n",
            "CleanPath = _pywrap_tensorflow_internal.CleanPath\n",
            "\n",
            "def ParseURI(uri, scheme, host, path):\n",
            "    return _pywrap_tensorflow_internal.ParseURI(uri, scheme, host, path)\n",
            "ParseURI = _pywrap_tensorflow_internal.ParseURI\n",
            "\n",
            "def CreateURI(scheme, host, path):\n",
            "    return _pywrap_tensorflow_internal.CreateURI(scheme, host, path)\n",
            "CreateURI = _pywrap_tensorflow_internal.CreateURI\n",
            "\n",
            "def GetTempFilename(extension):\n",
            "    return _pywrap_tensorflow_internal.GetTempFilename(extension)\n",
            "GetTempFilename = _pywrap_tensorflow_internal.GetTempFilename\n",
            "class FileStatistics(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, FileStatistics, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, FileStatistics, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"length\"] = _pywrap_tensorflow_internal.FileStatistics_length_set\n",
            "    __swig_getmethods__[\"length\"] = _pywrap_tensorflow_internal.FileStatistics_length_get\n",
            "    if _newclass:\n",
            "        length = _swig_property(_pywrap_tensorflow_internal.FileStatistics_length_get, _pywrap_tensorflow_internal.FileStatistics_length_set)\n",
            "    __swig_setmethods__[\"mtime_nsec\"] = _pywrap_tensorflow_internal.FileStatistics_mtime_nsec_set\n",
            "    __swig_getmethods__[\"mtime_nsec\"] = _pywrap_tensorflow_internal.FileStatistics_mtime_nsec_get\n",
            "    if _newclass:\n",
            "        mtime_nsec = _swig_property(_pywrap_tensorflow_internal.FileStatistics_mtime_nsec_get, _pywrap_tensorflow_internal.FileStatistics_mtime_nsec_set)\n",
            "    __swig_setmethods__[\"is_directory\"] = _pywrap_tensorflow_internal.FileStatistics_is_directory_set\n",
            "    __swig_getmethods__[\"is_directory\"] = _pywrap_tensorflow_internal.FileStatistics_is_directory_get\n",
            "    if _newclass:\n",
            "        is_directory = _swig_property(_pywrap_tensorflow_internal.FileStatistics_is_directory_get, _pywrap_tensorflow_internal.FileStatistics_is_directory_set)\n",
            "\n",
            "    def __init__(self, *args):\n",
            "        this = _pywrap_tensorflow_internal.new_FileStatistics(*args)\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_FileStatistics\n",
            "    __del__ = lambda self: None\n",
            "FileStatistics_swigregister = _pywrap_tensorflow_internal.FileStatistics_swigregister\n",
            "FileStatistics_swigregister(FileStatistics)\n",
            "\n",
            "\n",
            "def DoQuantizeTrainingOnGraphDefHelper(input_graph, num_bits):\n",
            "    return _pywrap_tensorflow_internal.DoQuantizeTrainingOnGraphDefHelper(input_graph, num_bits)\n",
            "DoQuantizeTrainingOnGraphDefHelper = _pywrap_tensorflow_internal.DoQuantizeTrainingOnGraphDefHelper\n",
            "\n",
            "from tensorflow.python.util import deprecation\n",
            "from tensorflow.python.util.tf_export import tf_export\n",
            "\n",
            "@deprecation.deprecated(\n",
            "    None,\n",
            "    \"GraphDef quantized training rewriter is deprecated in the long term\")\n",
            "@tf_export(v1=[\"train.do_quantize_training_on_graphdef\"])\n",
            "def do_quantize_training_on_graphdef(input_graph, num_bits):\n",
            "  \"\"\"A general quantization scheme is being developed in `tf.contrib.quantize`.\n",
            "\n",
            "  Consider using that instead, though since it is in the tf.contrib namespace,\n",
            "  it is not subject to backward compatibility guarantees.\n",
            "  \"\"\"\n",
            "  from tensorflow.core.framework.graph_pb2 import GraphDef\n",
            "  from tensorflow.python.framework import errors\n",
            "\n",
            "  graph = GraphDef()\n",
            "  result_graph_string = DoQuantizeTrainingOnGraphDefHelper(\n",
            "      input_graph.SerializeToString(), num_bits)\n",
            "\n",
            "  graph.ParseFromString(result_graph_string)\n",
            "  return graph\n",
            "\n",
            "do_quantize_training_on_graphdef._tf_api_names = [\n",
            "    'train.do_quantize_training_on_graphdef']\n",
            "do_quantize_training_on_graphdef._tf_api_names_v1 = [\n",
            "    'train.do_quantize_training_on_graphdef']\n",
            "\n",
            "\n",
            "def GetPythonWrappers(op_list_buf):\n",
            "    return _pywrap_tensorflow_internal.GetPythonWrappers(op_list_buf)\n",
            "GetPythonWrappers = _pywrap_tensorflow_internal.GetPythonWrappers\n",
            "\n",
            "def RunCppShapeInference(graph_def_version, serialized_node_def, input_serialized_shapes, input_constant_tensor_values, input_constant_tensor_as_shape_values):\n",
            "    return _pywrap_tensorflow_internal.RunCppShapeInference(graph_def_version, serialized_node_def, input_serialized_shapes, input_constant_tensor_values, input_constant_tensor_as_shape_values)\n",
            "RunCppShapeInference = _pywrap_tensorflow_internal.RunCppShapeInference\n",
            "\n",
            "def InstallStacktraceHandler():\n",
            "    return _pywrap_tensorflow_internal.InstallStacktraceHandler()\n",
            "InstallStacktraceHandler = _pywrap_tensorflow_internal.InstallStacktraceHandler\n",
            "\n",
            "def TryFindKernelClass(serialized_node_def):\n",
            "    return _pywrap_tensorflow_internal.TryFindKernelClass(serialized_node_def)\n",
            "TryFindKernelClass = _pywrap_tensorflow_internal.TryFindKernelClass\n",
            "\n",
            "def TransformGraphWithStringInputs(graph_def_string, inputs_string, outputs_string, transforms_string, out_status):\n",
            "    return _pywrap_tensorflow_internal.TransformGraphWithStringInputs(graph_def_string, inputs_string, outputs_string, transforms_string, out_status)\n",
            "TransformGraphWithStringInputs = _pywrap_tensorflow_internal.TransformGraphWithStringInputs\n",
            "\n",
            "def IsSequence(o):\n",
            "    \"\"\"\n",
            "    Returns true if its input is a collections.Sequence (except strings).\n",
            "\n",
            "    Args:\n",
            "      seq: an input sequence.\n",
            "\n",
            "    Returns:\n",
            "      True if the sequence is a not a string and is a collections.Sequence or a\n",
            "      dict.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsSequence(o)\n",
            "\n",
            "def IsSequenceOrComposite(o):\n",
            "    \"\"\"\n",
            "    Returns true if its input is a sequence or a `CompositeTensor`.\n",
            "\n",
            "    Args:\n",
            "      seq: an input sequence.\n",
            "\n",
            "    Returns:\n",
            "      True if the sequence is a not a string and is a collections.Sequence or a\n",
            "      dict or a CompositeTensor.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsSequenceOrComposite(o)\n",
            "\n",
            "def IsCompositeTensor(o):\n",
            "    \"\"\"\n",
            "    Returns true if its input is a `CompositeTensor`.\n",
            "\n",
            "    Args:\n",
            "      seq: an input sequence.\n",
            "\n",
            "    Returns:\n",
            "      True if the sequence is a CompositeTensor.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsCompositeTensor(o)\n",
            "\n",
            "def IsNamedtuple(o, strict):\n",
            "    return _pywrap_tensorflow_internal.IsNamedtuple(o, strict)\n",
            "IsNamedtuple = _pywrap_tensorflow_internal.IsNamedtuple\n",
            "\n",
            "def IsMapping(o):\n",
            "    \"\"\"\n",
            "    Returns True iff `instance` is a `collections.Mapping`.\n",
            "\n",
            "    Args:\n",
            "      instance: An instance of a Python object.\n",
            "\n",
            "    Returns:\n",
            "      True if `instance` is a `collections.Mapping`.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsMapping(o)\n",
            "\n",
            "def IsAttrs(o):\n",
            "    \"\"\"\n",
            "    Returns True iff `instance` is an instance of an `attr.s` decorated class.\n",
            "\n",
            "    Args:\n",
            "      instance: An instance of a Python object.\n",
            "\n",
            "    Returns:\n",
            "      True if `instance` is an instance of an `attr.s` decorated class.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsAttrs(o)\n",
            "\n",
            "def IsTensor(o):\n",
            "    return _pywrap_tensorflow_internal.IsTensor(o)\n",
            "IsTensor = _pywrap_tensorflow_internal.IsTensor\n",
            "\n",
            "def SameNamedtuples(o1, o2):\n",
            "    \"\"\"Returns True if the two namedtuples have the same name and fields.\"\"\"\n",
            "    return _pywrap_tensorflow_internal.SameNamedtuples(o1, o2)\n",
            "\n",
            "def AssertSameStructure(o1, o2, check_types, expand_composites):\n",
            "    return _pywrap_tensorflow_internal.AssertSameStructure(o1, o2, check_types, expand_composites)\n",
            "AssertSameStructure = _pywrap_tensorflow_internal.AssertSameStructure\n",
            "\n",
            "def Flatten(nested, expand_composites=False):\n",
            "    \"\"\"\n",
            "    Returns a flat list from a given nested structure.\n",
            "\n",
            "    If `nest` is not a sequence, tuple, or dict, then returns a single-element\n",
            "    list: `[nest]`.\n",
            "\n",
            "    In the case of dict instances, the sequence consists of the values, sorted by\n",
            "    key to ensure deterministic behavior. This is true also for `OrderedDict`\n",
            "    instances: their sequence order is ignored, the sorting order of keys is\n",
            "    used instead. The same convention is followed in `pack_sequence_as`. This\n",
            "    correctly repacks dicts and `OrderedDict`s after they have been flattened,\n",
            "    and also allows flattening an `OrderedDict` and then repacking it back using\n",
            "    a corresponding plain dict, or vice-versa.\n",
            "    Dictionaries with non-sortable keys cannot be flattened.\n",
            "\n",
            "    Users must not modify any collections used in `nest` while this function is\n",
            "    running.\n",
            "\n",
            "    Args:\n",
            "      nest: an arbitrarily nested structure or a scalar object. Note, numpy\n",
            "          arrays are considered scalars.\n",
            "      expand_composites: If true, then composite tensors such as `tf.SparseTensor`\n",
            "          and `tf.RaggedTensor` are expanded into their component tensors.\n",
            "\n",
            "    Returns:\n",
            "      A Python list, the flattened version of the input.\n",
            "\n",
            "    Raises:\n",
            "      TypeError: The nest is or contains a dict with non-sortable keys.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.Flatten(nested, expand_composites)\n",
            "\n",
            "def IsSequenceForData(o):\n",
            "    \"\"\"\n",
            "    Returns a true if `seq` is a Sequence or dict (except strings/lists).\n",
            "\n",
            "    NOTE(mrry): This differs from `tensorflow.python.util.nest.is_sequence()`,\n",
            "    which *does* treat a Python list as a sequence. For ergonomic\n",
            "    reasons, `tf.data` users would prefer to treat lists as\n",
            "    implicit `tf.Tensor` objects, and dicts as (nested) sequences.\n",
            "\n",
            "    Args:\n",
            "      seq: an input sequence.\n",
            "\n",
            "    Returns:\n",
            "      True if the sequence is a not a string or list and is a\n",
            "      collections.Sequence.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.IsSequenceForData(o)\n",
            "\n",
            "def FlattenForData(nested):\n",
            "    \"\"\"\n",
            "    Returns a flat sequence from a given nested structure.\n",
            "\n",
            "    If `nest` is not a sequence, this returns a single-element list: `[nest]`.\n",
            "\n",
            "    Args:\n",
            "      nest: an arbitrarily nested structure or a scalar object.\n",
            "        Note, numpy arrays are considered scalars.\n",
            "\n",
            "    Returns:\n",
            "      A Python list, the flattened version of the input.\n",
            "\n",
            "    \"\"\"\n",
            "    return _pywrap_tensorflow_internal.FlattenForData(nested)\n",
            "\n",
            "def AssertSameStructureForData(o1, o2, check_types):\n",
            "    return _pywrap_tensorflow_internal.AssertSameStructureForData(o1, o2, check_types)\n",
            "AssertSameStructureForData = _pywrap_tensorflow_internal.AssertSameStructureForData\n",
            "\n",
            "def RegisterType(type_name, type):\n",
            "    return _pywrap_tensorflow_internal.RegisterType(type_name, type)\n",
            "RegisterType = _pywrap_tensorflow_internal.RegisterType\n",
            "\n",
            "_pywrap_tensorflow_internal.SHARED_PTR_DISOWN_swigconstant(_pywrap_tensorflow_internal)\n",
            "SHARED_PTR_DISOWN = _pywrap_tensorflow_internal.SHARED_PTR_DISOWN\n",
            "class GItem(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, GItem, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, GItem, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"item_\"] = _pywrap_tensorflow_internal.GItem_item__set\n",
            "    __swig_getmethods__[\"item_\"] = _pywrap_tensorflow_internal.GItem_item__get\n",
            "    if _newclass:\n",
            "        item_ = _swig_property(_pywrap_tensorflow_internal.GItem_item__get, _pywrap_tensorflow_internal.GItem_item__set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_GItem()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_GItem\n",
            "    __del__ = lambda self: None\n",
            "GItem_swigregister = _pywrap_tensorflow_internal.GItem_swigregister\n",
            "GItem_swigregister(GItem)\n",
            "\n",
            "\n",
            "def TF_NewItem(meta_graph, ignore_colocation, ignore_user_placement):\n",
            "    return _pywrap_tensorflow_internal.TF_NewItem(meta_graph, ignore_colocation, ignore_user_placement)\n",
            "TF_NewItem = _pywrap_tensorflow_internal.TF_NewItem\n",
            "\n",
            "def TF_IdentifyImportantOps(item, sort_topologically):\n",
            "    return _pywrap_tensorflow_internal.TF_IdentifyImportantOps(item, sort_topologically)\n",
            "TF_IdentifyImportantOps = _pywrap_tensorflow_internal.TF_IdentifyImportantOps\n",
            "\n",
            "def TF_GetOpProperties(item):\n",
            "    return _pywrap_tensorflow_internal.TF_GetOpProperties(item)\n",
            "TF_GetOpProperties = _pywrap_tensorflow_internal.TF_GetOpProperties\n",
            "\n",
            "def TF_GetColocationGroups(item):\n",
            "    return _pywrap_tensorflow_internal.TF_GetColocationGroups(item)\n",
            "TF_GetColocationGroups = _pywrap_tensorflow_internal.TF_GetColocationGroups\n",
            "class GCluster(_object):\n",
            "    __swig_setmethods__ = {}\n",
            "    __setattr__ = lambda self, name, value: _swig_setattr(self, GCluster, name, value)\n",
            "    __swig_getmethods__ = {}\n",
            "    __getattr__ = lambda self, name: _swig_getattr(self, GCluster, name)\n",
            "    __repr__ = _swig_repr\n",
            "    __swig_setmethods__[\"cluster_\"] = _pywrap_tensorflow_internal.GCluster_cluster__set\n",
            "    __swig_getmethods__[\"cluster_\"] = _pywrap_tensorflow_internal.GCluster_cluster__get\n",
            "    if _newclass:\n",
            "        cluster_ = _swig_property(_pywrap_tensorflow_internal.GCluster_cluster__get, _pywrap_tensorflow_internal.GCluster_cluster__set)\n",
            "\n",
            "    def __init__(self):\n",
            "        this = _pywrap_tensorflow_internal.new_GCluster()\n",
            "        try:\n",
            "            self.this.append(this)\n",
            "        except Exception:\n",
            "            self.this = this\n",
            "    __swig_destroy__ = _pywrap_tensorflow_internal.delete_GCluster\n",
            "    __del__ = lambda self: None\n",
            "GCluster_swigregister = _pywrap_tensorflow_internal.GCluster_swigregister\n",
            "GCluster_swigregister(GCluster)\n",
            "\n",
            "\n",
            "def TF_NewCluster(allow_soft_placement, disable_detailed_stats):\n",
            "    return _pywrap_tensorflow_internal.TF_NewCluster(allow_soft_placement, disable_detailed_stats)\n",
            "TF_NewCluster = _pywrap_tensorflow_internal.TF_NewCluster\n",
            "\n",
            "def TF_NewVirtualCluster(named_devices):\n",
            "    return _pywrap_tensorflow_internal.TF_NewVirtualCluster(named_devices)\n",
            "TF_NewVirtualCluster = _pywrap_tensorflow_internal.TF_NewVirtualCluster\n",
            "\n",
            "def TF_ShutdownCluster(cluster):\n",
            "    return _pywrap_tensorflow_internal.TF_ShutdownCluster(cluster)\n",
            "TF_ShutdownCluster = _pywrap_tensorflow_internal.TF_ShutdownCluster\n",
            "\n",
            "def TF_ListDevices(cluster):\n",
            "    return _pywrap_tensorflow_internal.TF_ListDevices(cluster)\n",
            "TF_ListDevices = _pywrap_tensorflow_internal.TF_ListDevices\n",
            "\n",
            "def TF_ListAvailableOps():\n",
            "    return _pywrap_tensorflow_internal.TF_ListAvailableOps()\n",
            "TF_ListAvailableOps = _pywrap_tensorflow_internal.TF_ListAvailableOps\n",
            "\n",
            "def TF_GetSupportedDevices(cluster, item):\n",
            "    return _pywrap_tensorflow_internal.TF_GetSupportedDevices(cluster, item)\n",
            "TF_GetSupportedDevices = _pywrap_tensorflow_internal.TF_GetSupportedDevices\n",
            "\n",
            "def TF_EstimatePerformance(device):\n",
            "    return _pywrap_tensorflow_internal.TF_EstimatePerformance(device)\n",
            "TF_EstimatePerformance = _pywrap_tensorflow_internal.TF_EstimatePerformance\n",
            "\n",
            "def TF_MeasureCosts(item, cluster, generate_timeline):\n",
            "    return _pywrap_tensorflow_internal.TF_MeasureCosts(item, cluster, generate_timeline)\n",
            "TF_MeasureCosts = _pywrap_tensorflow_internal.TF_MeasureCosts\n",
            "\n",
            "def TF_DeterminePeakMemoryUsage(item, cluster):\n",
            "    return _pywrap_tensorflow_internal.TF_DeterminePeakMemoryUsage(item, cluster)\n",
            "TF_DeterminePeakMemoryUsage = _pywrap_tensorflow_internal.TF_DeterminePeakMemoryUsage\n",
            "\n",
            "def TF_OptimizeGraph(cluster, config_proto, metagraph, verbose, graph_id):\n",
            "    return _pywrap_tensorflow_internal.TF_OptimizeGraph(cluster, config_proto, metagraph, verbose, graph_id)\n",
            "TF_OptimizeGraph = _pywrap_tensorflow_internal.TF_OptimizeGraph\n",
            "\n",
            "def GenerateCostReport(metagraph, per_node_report, verbose, cluster):\n",
            "    return _pywrap_tensorflow_internal.GenerateCostReport(metagraph, per_node_report, verbose, cluster)\n",
            "GenerateCostReport = _pywrap_tensorflow_internal.GenerateCostReport\n",
            "\n",
            "def GraphAnalyzer(file_path, n):\n",
            "    return _pywrap_tensorflow_internal.GraphAnalyzer(file_path, n)\n",
            "GraphAnalyzer = _pywrap_tensorflow_internal.GraphAnalyzer\n",
            "\n",
            "def GenerateModelReport(metagraph, assume_valid_feeds, debug):\n",
            "    return _pywrap_tensorflow_internal.GenerateModelReport(metagraph, assume_valid_feeds, debug)\n",
            "GenerateModelReport = _pywrap_tensorflow_internal.GenerateModelReport\n",
            "# This file is compatible with both classic and new-style classes.\n",
            "\n",
            "\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras\n",
            "Directories ['preprocessing', 'utils', 'applications', 'saving', 'optimizer_v2', 'layers', 'engine', 'api', 'estimator', '__pycache__', 'distribute', 'mixed_precision', 'datasets', 'wrappers']\n",
            "Files ['callbacks.py', '__init__.py', 'metrics.py', 'activations.py', 'initializers.py', 'callbacks_v1.py', 'models.py', 'testing_utils.py', 'losses.py', 'regularizers.py', 'backend.py', 'keras_parameterized.py', 'backend_config.py', 'constraints.py', 'optimizers.py', 'ops.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Callbacks: utilities called at certain points during model training.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import collections\n",
            "import copy\n",
            "import csv\n",
            "import io\n",
            "import json\n",
            "import os\n",
            "import re\n",
            "import tempfile\n",
            "import time\n",
            "\n",
            "import numpy as np\n",
            "import six\n",
            "\n",
            "from tensorflow.python.data.ops import iterator_ops\n",
            "from tensorflow.python.distribute import distribute_coordinator_context as dc_context\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils.data_utils import Sequence\n",
            "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
            "from tensorflow.python.keras.utils.mode_keys import ModeKeys\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import summary_ops_v2\n",
            "from tensorflow.python.platform import tf_logging as logging\n",
            "from tensorflow.python.training import checkpoint_management\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "try:\n",
            "  import requests\n",
            "except ImportError:\n",
            "  requests = None\n",
            "\n",
            "# Constant for `tf.keras.Model` to store the epoch at which the most recently\n",
            "# saved checkpoint was saved. See `Model._get_updated_initial_epoch()`'s\n",
            "# docstring for more information.\n",
            "CKPT_SAVED_EPOCH = '_ckpt_saved_epoch'\n",
            "\n",
            "\n",
            "def configure_callbacks(callbacks,\n",
            "                        model,\n",
            "                        do_validation=False,\n",
            "                        batch_size=None,\n",
            "                        epochs=None,\n",
            "                        steps_per_epoch=None,\n",
            "                        samples=None,\n",
            "                        verbose=1,\n",
            "                        count_mode='steps',\n",
            "                        mode=ModeKeys.TRAIN):\n",
            "  \"\"\"Configures callbacks for use in various training loops.\n",
            "\n",
            "  Arguments:\n",
            "      callbacks: List of Callbacks.\n",
            "      model: Model being trained.\n",
            "      do_validation: Whether or not validation loop will be run.\n",
            "      batch_size: Number of samples per batch.\n",
            "      epochs: Number of epoch to train.\n",
            "      steps_per_epoch: Number of batches to run per training epoch.\n",
            "      samples: Number of training samples.\n",
            "      verbose: int, 0 or 1. Keras logging verbosity to pass to ProgbarLogger.\n",
            "      count_mode: One of 'steps' or 'samples'. Per-batch or per-sample count.\n",
            "      mode: String. One of ModeKeys.TRAIN, ModeKeys.TEST, or ModeKeys.PREDICT.\n",
            "        Which loop mode to configure callbacks for.\n",
            "\n",
            "  Returns:\n",
            "      Instance of CallbackList used to control all Callbacks.\n",
            "  \"\"\"\n",
            "  # Check if callbacks have already been configured.\n",
            "  if isinstance(callbacks, CallbackList):\n",
            "    return callbacks\n",
            "\n",
            "  if not callbacks:\n",
            "    callbacks = []\n",
            "\n",
            "  # Add additional callbacks during training.\n",
            "  if mode == ModeKeys.TRAIN:\n",
            "    model.history = History()\n",
            "    callbacks = [BaseLogger()] + (callbacks or []) + [model.history]\n",
            "    if verbose:\n",
            "      callbacks.append(ProgbarLogger(count_mode))\n",
            "  callback_list = CallbackList(callbacks)\n",
            "\n",
            "  # Set callback model\n",
            "  callback_model = model._get_callback_model()  # pylint: disable=protected-access\n",
            "  callback_list.set_model(callback_model)\n",
            "\n",
            "  set_callback_parameters(\n",
            "      callback_list,\n",
            "      model,\n",
            "      do_validation=do_validation,\n",
            "      batch_size=batch_size,\n",
            "      epochs=epochs,\n",
            "      steps_per_epoch=steps_per_epoch,\n",
            "      samples=samples,\n",
            "      verbose=verbose,\n",
            "      mode=mode)\n",
            "\n",
            "  callback_list.model.stop_training = False\n",
            "  # pylint: disable=protected-access\n",
            "  if callback_list.model._ckpt_saved_epoch is not None:\n",
            "    # The attribute `_ckpt_saved_epoch` is supposed to be None at the start of\n",
            "    # training (it should be made None at the end of successful multi-worker\n",
            "    # training), unless the user's `fit()` does not end successfully before\n",
            "    # making another `fit()` call.\n",
            "    raise ValueError(\n",
            "        '`tf.Keras.Model._ckpt_saved_epoch` attr should be None at '\n",
            "        'callback setup time. Please ensure `fit()` in multi-worker '\n",
            "        'training finishes successfully before starting a new one. If the '\n",
            "        'issue persists, try using only one `model.fit()` in multi-worker '\n",
            "        'training.')\n",
            "  return callback_list\n",
            "\n",
            "\n",
            "def set_callback_parameters(callback_list,\n",
            "                            model,\n",
            "                            do_validation=False,\n",
            "                            batch_size=None,\n",
            "                            epochs=None,\n",
            "                            steps_per_epoch=None,\n",
            "                            samples=None,\n",
            "                            verbose=1,\n",
            "                            mode=ModeKeys.TRAIN):\n",
            "  \"\"\"Sets callback parameters.\n",
            "\n",
            "  Arguments:\n",
            "      callback_list: CallbackList instance.\n",
            "      model: Model being trained.\n",
            "      do_validation: Whether or not validation loop will be run.\n",
            "      batch_size: Number of samples per batch.\n",
            "      epochs: Number of epoch to train.\n",
            "      steps_per_epoch: Number of batches to run per training epoch.\n",
            "      samples: Number of training samples.\n",
            "      verbose: int, 0 or 1. Keras logging verbosity to pass to ProgbarLogger.\n",
            "      mode: String. One of ModeKeys.TRAIN, ModeKeys.TEST, or ModeKeys.PREDICT.\n",
            "        Which loop mode to configure callbacks for.\n",
            "  \"\"\"\n",
            "  for cbk in callback_list:\n",
            "    if isinstance(cbk, (BaseLogger, ProgbarLogger)):\n",
            "      cbk.stateful_metrics = model.metrics_names[1:]  # Exclude `loss`\n",
            "\n",
            "  # Set callback parameters\n",
            "  callback_metrics = []\n",
            "  # When we have deferred build scenario with iterator input, we will compile\n",
            "  # when we standardize first batch of data.\n",
            "  if mode != ModeKeys.PREDICT and hasattr(model, 'metrics_names'):\n",
            "    callback_metrics = copy.copy(model.metrics_names)\n",
            "    if do_validation:\n",
            "      callback_metrics += ['val_' + n for n in model.metrics_names]\n",
            "  callback_params = {\n",
            "      'batch_size': batch_size,\n",
            "      'epochs': epochs,\n",
            "      'steps': steps_per_epoch,\n",
            "      'samples': samples,\n",
            "      'verbose': verbose,\n",
            "      'do_validation': do_validation,\n",
            "      'metrics': callback_metrics,\n",
            "  }\n",
            "  callback_list.set_params(callback_params)\n",
            "\n",
            "\n",
            "def _is_generator_like(data):\n",
            "  \"\"\"Checks if data is a generator, Sequence, or Iterator.\"\"\"\n",
            "  return (hasattr(data, 'next') or hasattr(data, '__next__') or isinstance(\n",
            "      data, (Sequence, iterator_ops.Iterator, iterator_ops.IteratorV2)))\n",
            "\n",
            "\n",
            "def make_logs(model, logs, outputs, mode, prefix=''):\n",
            "  \"\"\"Computes logs for sending to `on_batch_end` methods.\"\"\"\n",
            "  if mode in {ModeKeys.TRAIN, ModeKeys.TEST}:\n",
            "    if hasattr(model, 'metrics_names'):\n",
            "      for label, output in zip(model.metrics_names, outputs):\n",
            "        logs[prefix + label] = output\n",
            "  else:\n",
            "    logs['outputs'] = outputs\n",
            "  return logs\n",
            "\n",
            "\n",
            "class CallbackList(object):\n",
            "  \"\"\"Container abstracting a list of callbacks.\n",
            "\n",
            "  Arguments:\n",
            "      callbacks: List of `Callback` instances.\n",
            "      queue_length: Queue length for keeping\n",
            "          running statistics over callback execution time.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, callbacks=None, queue_length=10):\n",
            "    callbacks = callbacks or []\n",
            "    self.callbacks = [c for c in callbacks]\n",
            "    self.queue_length = queue_length\n",
            "    self.params = {}\n",
            "    self.model = None\n",
            "    self._reset_batch_timing()\n",
            "\n",
            "  def _reset_batch_timing(self):\n",
            "    self._delta_t_batch = 0.\n",
            "    self._delta_ts = collections.defaultdict(\n",
            "        lambda: collections.deque([], maxlen=self.queue_length))\n",
            "\n",
            "  def append(self, callback):\n",
            "    self.callbacks.append(callback)\n",
            "\n",
            "  def set_params(self, params):\n",
            "    self.params = params\n",
            "    for callback in self.callbacks:\n",
            "      callback.set_params(params)\n",
            "\n",
            "  def set_model(self, model):\n",
            "    self.model = model\n",
            "    for callback in self.callbacks:\n",
            "      callback.set_model(model)\n",
            "\n",
            "  def _call_batch_hook(self, mode, hook, batch, logs=None):\n",
            "    \"\"\"Helper function for all batch_{begin | end} methods.\"\"\"\n",
            "    if not self.callbacks:\n",
            "      return\n",
            "    hook_name = 'on_{mode}_batch_{hook}'.format(mode=mode, hook=hook)\n",
            "    if hook == 'begin':\n",
            "      self._t_enter_batch = time.time()\n",
            "    if hook == 'end':\n",
            "      # Batch is ending, calculate batch time.\n",
            "      self._delta_t_batch = time.time() - self._t_enter_batch\n",
            "\n",
            "    logs = logs or {}\n",
            "    t_before_callbacks = time.time()\n",
            "    for callback in self.callbacks:\n",
            "      batch_hook = getattr(callback, hook_name)\n",
            "      batch_hook(batch, logs)\n",
            "    self._delta_ts[hook_name].append(time.time() - t_before_callbacks)\n",
            "\n",
            "    delta_t_median = np.median(self._delta_ts[hook_name])\n",
            "    if (self._delta_t_batch > 0. and\n",
            "        delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
            "      logging.warning(\n",
            "          'Method (%s) is slow compared '\n",
            "          'to the batch update (%f). Check your callbacks.', hook_name,\n",
            "          delta_t_median)\n",
            "\n",
            "  def _call_begin_hook(self, mode):\n",
            "    \"\"\"Helper function for on_{train|test|predict}_begin methods.\"\"\"\n",
            "    if mode == ModeKeys.TRAIN:\n",
            "      self.on_train_begin()\n",
            "    elif mode == ModeKeys.TEST:\n",
            "      self.on_test_begin()\n",
            "    else:\n",
            "      self.on_predict_begin()\n",
            "\n",
            "  def _call_end_hook(self, mode):\n",
            "    \"\"\"Helper function for on_{train|test|predict}_end methods.\"\"\"\n",
            "    if mode == ModeKeys.TRAIN:\n",
            "      self.on_train_end()\n",
            "    elif mode == ModeKeys.TEST:\n",
            "      self.on_test_end()\n",
            "    else:\n",
            "      self.on_predict_end()\n",
            "\n",
            "  def on_batch_begin(self, batch, logs=None):\n",
            "    self._call_batch_hook(ModeKeys.TRAIN, 'begin', batch, logs=logs)\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    \"\"\"Calls the `on_epoch_begin` methods of its callbacks.\n",
            "\n",
            "    This function should only be called during TRAIN mode.\n",
            "\n",
            "    Arguments:\n",
            "        epoch: integer, index of epoch.\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    logs = logs or {}\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_epoch_begin(epoch, logs)\n",
            "    self._reset_batch_timing()\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    \"\"\"Calls the `on_epoch_end` methods of its callbacks.\n",
            "\n",
            "    This function should only be called during TRAIN mode.\n",
            "\n",
            "    Arguments:\n",
            "        epoch: integer, index of epoch.\n",
            "        logs: dict, metric results for this training epoch, and for the\n",
            "          validation epoch if validation is performed. Validation result keys\n",
            "          are prefixed with `val_`.\n",
            "    \"\"\"\n",
            "    logs = logs or {}\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_epoch_end(epoch, logs)\n",
            "\n",
            "  def on_train_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_train_batch_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.TRAIN, 'begin', batch, logs=logs)\n",
            "\n",
            "  def on_train_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_train_batch_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\n",
            "\n",
            "  def on_test_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_test_batch_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.TEST, 'begin', batch, logs=logs)\n",
            "\n",
            "  def on_test_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_test_batch_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.TEST, 'end', batch, logs=logs)\n",
            "\n",
            "  def on_predict_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_predict_batch_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.PREDICT, 'begin', batch, logs=logs)\n",
            "\n",
            "  def on_predict_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Calls the `on_predict_batch_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "    self._call_batch_hook(ModeKeys.PREDICT, 'end', batch, logs=logs)\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    \"\"\"Calls the `on_train_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_train_begin(logs)\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    \"\"\"Calls the `on_train_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_train_end(logs)\n",
            "\n",
            "  def on_test_begin(self, logs=None):\n",
            "    \"\"\"Calls the `on_test_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_test_begin(logs)\n",
            "\n",
            "  def on_test_end(self, logs=None):\n",
            "    \"\"\"Calls the `on_test_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_test_end(logs)\n",
            "\n",
            "  def on_predict_begin(self, logs=None):\n",
            "    \"\"\"Calls the 'on_predict_begin` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_predict_begin(logs)\n",
            "\n",
            "  def on_predict_end(self, logs=None):\n",
            "    \"\"\"Calls the `on_predict_end` methods of its callbacks.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "    for callback in self.callbacks:\n",
            "      callback.on_predict_end(logs)\n",
            "\n",
            "  def __iter__(self):\n",
            "    return iter(self.callbacks)\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.Callback')\n",
            "class Callback(object):\n",
            "  \"\"\"Abstract base class used to build new callbacks.\n",
            "\n",
            "  Attributes:\n",
            "      params: dict. Training parameters\n",
            "          (eg. verbosity, batch size, number of epochs...).\n",
            "      model: instance of `keras.models.Model`.\n",
            "          Reference of the model being trained.\n",
            "      validation_data: Deprecated. Do not use.\n",
            "\n",
            "  The `logs` dictionary that callback methods\n",
            "  take as argument will contain keys for quantities relevant to\n",
            "  the current batch or epoch.\n",
            "\n",
            "  Currently, the `.fit()` method of the `Model` class\n",
            "  will include the following quantities in the `logs` that\n",
            "  it passes to its callbacks:\n",
            "\n",
            "      on_epoch_end: logs include `acc` and `loss`, and\n",
            "          optionally include `val_loss`\n",
            "          (if validation is enabled in `fit`), and `val_acc`\n",
            "          (if validation and accuracy monitoring are enabled).\n",
            "      on_batch_begin: logs include `size`,\n",
            "          the number of samples in the current batch.\n",
            "      on_batch_end: logs include `loss`, and optionally `acc`\n",
            "          (if accuracy monitoring is enabled).\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self):\n",
            "    self.validation_data = None\n",
            "    self.model = None\n",
            "    # Whether this Callback should only run on the chief worker in a\n",
            "    # Multi-Worker setting.\n",
            "    # TODO(omalleyt): Make this attr public once solution is stable.\n",
            "    self._chief_worker_only = None\n",
            "\n",
            "  def set_params(self, params):\n",
            "    self.params = params\n",
            "\n",
            "  def set_model(self, model):\n",
            "    self.model = model\n",
            "\n",
            "  def on_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"A backwards compatibility alias for `on_train_batch_begin`.\"\"\"\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    \"\"\"A backwards compatibility alias for `on_train_batch_end`.\"\"\"\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    \"\"\"Called at the start of an epoch.\n",
            "\n",
            "    Subclasses should override for any actions to run. This function should only\n",
            "    be called during TRAIN mode.\n",
            "\n",
            "    Arguments:\n",
            "        epoch: integer, index of epoch.\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    \"\"\"Called at the end of an epoch.\n",
            "\n",
            "    Subclasses should override for any actions to run. This function should only\n",
            "    be called during TRAIN mode.\n",
            "\n",
            "    Arguments:\n",
            "        epoch: integer, index of epoch.\n",
            "        logs: dict, metric results for this training epoch, and for the\n",
            "          validation epoch if validation is performed. Validation result keys\n",
            "          are prefixed with `val_`.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_train_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Called at the beginning of a training batch in `fit` methods.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "    # For backwards compatibility.\n",
            "    self.on_batch_begin(batch, logs=logs)\n",
            "\n",
            "  def on_train_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Called at the end of a training batch in `fit` methods.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "    # For backwards compatibility.\n",
            "    self.on_batch_end(batch, logs=logs)\n",
            "\n",
            "  def on_test_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Called at the beginning of a batch in `evaluate` methods.\n",
            "\n",
            "    Also called at the beginning of a validation batch in the `fit`\n",
            "    methods, if validation data is provided.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_test_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Called at the end of a batch in `evaluate` methods.\n",
            "\n",
            "    Also called at the end of a validation batch in the `fit`\n",
            "    methods, if validation data is provided.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_predict_batch_begin(self, batch, logs=None):\n",
            "    \"\"\"Called at the beginning of a batch in `predict` methods.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Has keys `batch` and `size` representing the current batch\n",
            "          number and the size of the batch.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_predict_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Called at the end of a batch in `predict` methods.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        batch: integer, index of batch within the current epoch.\n",
            "        logs: dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    \"\"\"Called at the beginning of training.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    \"\"\"Called at the end of training.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_test_begin(self, logs=None):\n",
            "    \"\"\"Called at the beginning of evaluation or validation.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_test_end(self, logs=None):\n",
            "    \"\"\"Called at the end of evaluation or validation.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_predict_begin(self, logs=None):\n",
            "    \"\"\"Called at the beginning of prediction.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "  def on_predict_end(self, logs=None):\n",
            "    \"\"\"Called at the end of prediction.\n",
            "\n",
            "    Subclasses should override for any actions to run.\n",
            "\n",
            "    Arguments:\n",
            "        logs: dict. Currently no data is passed to this argument for this method\n",
            "          but that may change in the future.\n",
            "    \"\"\"\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.BaseLogger')\n",
            "class BaseLogger(Callback):\n",
            "  \"\"\"Callback that accumulates epoch averages of metrics.\n",
            "\n",
            "  This callback is automatically applied to every Keras model.\n",
            "\n",
            "  Arguments:\n",
            "      stateful_metrics: Iterable of string names of metrics that\n",
            "          should *not* be averaged over an epoch.\n",
            "          Metrics in this list will be logged as-is in `on_epoch_end`.\n",
            "          All others will be averaged in `on_epoch_end`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, stateful_metrics=None):\n",
            "    super(BaseLogger, self).__init__()\n",
            "    self.stateful_metrics = set(stateful_metrics or [])\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    self.seen = 0\n",
            "    self.totals = {}\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    logs = logs or {}\n",
            "    batch_size = logs.get('size', 0)\n",
            "    # In case of distribution strategy we can potentially run multiple steps\n",
            "    # at the same time, we should account for that in the `seen` calculation.\n",
            "    num_steps = logs.get('num_steps', 1)\n",
            "    self.seen += batch_size * num_steps\n",
            "\n",
            "    for k, v in logs.items():\n",
            "      if k in self.stateful_metrics:\n",
            "        self.totals[k] = v\n",
            "      else:\n",
            "        if k in self.totals:\n",
            "          self.totals[k] += v * batch_size\n",
            "        else:\n",
            "          self.totals[k] = v * batch_size\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    if logs is not None:\n",
            "      for k in self.params['metrics']:\n",
            "        if k in self.totals:\n",
            "          # Make value available to next callbacks.\n",
            "          if k in self.stateful_metrics:\n",
            "            logs[k] = self.totals[k]\n",
            "          else:\n",
            "            logs[k] = self.totals[k] / self.seen\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.TerminateOnNaN')\n",
            "class TerminateOnNaN(Callback):\n",
            "  \"\"\"Callback that terminates training when a NaN loss is encountered.\n",
            "  \"\"\"\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    logs = logs or {}\n",
            "    loss = logs.get('loss')\n",
            "    if loss is not None:\n",
            "      if np.isnan(loss) or np.isinf(loss):\n",
            "        print('Batch %d: Invalid loss, terminating training' % (batch))\n",
            "        self.model.stop_training = True\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.ProgbarLogger')\n",
            "class ProgbarLogger(Callback):\n",
            "  \"\"\"Callback that prints metrics to stdout.\n",
            "\n",
            "  Arguments:\n",
            "      count_mode: One of \"steps\" or \"samples\".\n",
            "          Whether the progress bar should\n",
            "          count samples seen or steps (batches) seen.\n",
            "      stateful_metrics: Iterable of string names of metrics that\n",
            "          should *not* be averaged over an epoch.\n",
            "          Metrics in this list will be logged as-is.\n",
            "          All others will be averaged over time (e.g. loss, etc).\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In case of invalid `count_mode`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, count_mode='samples', stateful_metrics=None):\n",
            "    super(ProgbarLogger, self).__init__()\n",
            "    if count_mode == 'samples':\n",
            "      self.use_steps = False\n",
            "    elif count_mode == 'steps':\n",
            "      self.use_steps = True\n",
            "    else:\n",
            "      raise ValueError('Unknown `count_mode`: ' + str(count_mode))\n",
            "    self.stateful_metrics = set(stateful_metrics or [])\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    self.verbose = self.params['verbose']\n",
            "    self.epochs = self.params['epochs']\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    self.seen = 0\n",
            "    if self.use_steps:\n",
            "      self.target = self.params['steps']\n",
            "    else:\n",
            "      self.target = self.params['samples']\n",
            "\n",
            "    if self.verbose:\n",
            "      if self.epochs > 1:\n",
            "        print('Epoch %d/%d' % (epoch + 1, self.epochs))\n",
            "    self.progbar = Progbar(\n",
            "        target=self.target,\n",
            "        verbose=self.verbose,\n",
            "        stateful_metrics=self.stateful_metrics,\n",
            "        unit_name='step' if self.use_steps else 'sample')\n",
            "\n",
            "  def on_batch_begin(self, batch, logs=None):\n",
            "    self.log_values = []\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    logs = logs or {}\n",
            "    batch_size = logs.get('size', 0)\n",
            "    # In case of distribution strategy we can potentially run multiple steps\n",
            "    # at the same time, we should account for that in the `seen` calculation.\n",
            "    num_steps = logs.get('num_steps', 1)\n",
            "    if self.use_steps:\n",
            "      self.seen += num_steps\n",
            "    else:\n",
            "      self.seen += batch_size * num_steps\n",
            "\n",
            "    for k in self.params['metrics']:\n",
            "      if k in logs:\n",
            "        self.log_values.append((k, logs[k]))\n",
            "\n",
            "    # Skip progbar update for the last batch;\n",
            "    # will be handled by on_epoch_end.\n",
            "    if self.verbose and (self.target is None or self.seen < self.target):\n",
            "      self.progbar.update(self.seen, self.log_values)\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    logs = logs or {}\n",
            "    for k in self.params['metrics']:\n",
            "      if k in logs:\n",
            "        self.log_values.append((k, logs[k]))\n",
            "    if self.verbose:\n",
            "      self.progbar.update(self.seen, self.log_values)\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.History')\n",
            "class History(Callback):\n",
            "  \"\"\"Callback that records events into a `History` object.\n",
            "\n",
            "  This callback is automatically applied to\n",
            "  every Keras model. The `History` object\n",
            "  gets returned by the `fit` method of models.\n",
            "  \"\"\"\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    self.epoch = []\n",
            "    self.history = {}\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    logs = logs or {}\n",
            "    self.epoch.append(epoch)\n",
            "    for k, v in logs.items():\n",
            "      self.history.setdefault(k, []).append(v)\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.ModelCheckpoint')\n",
            "class ModelCheckpoint(Callback):\n",
            "  \"\"\"Save the model after every epoch.\n",
            "\n",
            "  `filepath` can contain named formatting options,\n",
            "  which will be filled the value of `epoch` and\n",
            "  keys in `logs` (passed in `on_epoch_end`).\n",
            "\n",
            "  For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
            "  then the model checkpoints will be saved with the epoch number and\n",
            "  the validation loss in the filename.\n",
            "\n",
            "  Arguments:\n",
            "      filepath: string, path to save the model file.\n",
            "      monitor: quantity to monitor.\n",
            "      verbose: verbosity mode, 0 or 1.\n",
            "      save_best_only: if `save_best_only=True`, the latest best model according\n",
            "        to the quantity monitored will not be overwritten.\n",
            "      mode: one of {auto, min, max}. If `save_best_only=True`, the decision to\n",
            "        overwrite the current save file is made based on either the maximization\n",
            "        or the minimization of the monitored quantity. For `val_acc`, this\n",
            "        should be `max`, for `val_loss` this should be `min`, etc. In `auto`\n",
            "        mode, the direction is automatically inferred from the name of the\n",
            "        monitored quantity.\n",
            "      save_weights_only: if True, then only the model's weights will be saved\n",
            "        (`model.save_weights(filepath)`), else the full model is saved\n",
            "        (`model.save(filepath)`).\n",
            "      save_freq: `'epoch'` or integer. When using `'epoch'`, the callback saves\n",
            "        the model after each epoch. When using integer, the callback saves the\n",
            "        model at end of a batch at which this many samples have been seen since\n",
            "        last saving. Note that if the saving isn't aligned to epochs, the\n",
            "        monitored metric may potentially be less reliable (it could reflect as\n",
            "        little as 1 batch, since the metrics get reset every epoch). Defaults to\n",
            "        `'epoch'`\n",
            "      load_weights_on_restart: Whether the training should restore the model. If\n",
            "        True, the model will attempt to load the checkpoint file from `filepath`\n",
            "        at the start of `model.fit()`. This saves the need of manually calling\n",
            "        `model.load_weights()` before `model.fit(). In multi-worker distributed\n",
            "        training, this provides fault-tolerance and loads the model\n",
            "        automatically upon recovery of workers. The callback gives up loading if\n",
            "        the filepath does not exist, and raises ValueError if format does not\n",
            "        match. Defaults to False.\n",
            "      **kwargs: Additional arguments for backwards compatibility. Possible key\n",
            "        is `period`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               filepath,\n",
            "               monitor='val_loss',\n",
            "               verbose=0,\n",
            "               save_best_only=False,\n",
            "               save_weights_only=False,\n",
            "               mode='auto',\n",
            "               save_freq='epoch',\n",
            "               load_weights_on_restart=False,\n",
            "               **kwargs):\n",
            "    super(ModelCheckpoint, self).__init__()\n",
            "    self.monitor = monitor\n",
            "    self.verbose = verbose\n",
            "    self.filepath = filepath\n",
            "    self.save_best_only = save_best_only\n",
            "    self.save_weights_only = save_weights_only\n",
            "    self.save_freq = save_freq\n",
            "    self.load_weights_on_restart = load_weights_on_restart\n",
            "    self.epochs_since_last_save = 0\n",
            "    self._samples_seen_since_last_saving = 0\n",
            "\n",
            "    # Deprecated field `period` is for the number of epochs between which\n",
            "    # the model is saved.\n",
            "    if 'period' in kwargs:\n",
            "      self.period = kwargs['period']\n",
            "      logging.warning('`period` argument is deprecated. Please use `save_freq` '\n",
            "                      'to specify the frequency in number of samples seen.')\n",
            "    else:\n",
            "      self.period = 1\n",
            "\n",
            "    if mode not in ['auto', 'min', 'max']:\n",
            "      logging.warning('ModelCheckpoint mode %s is unknown, '\n",
            "                      'fallback to auto mode.', mode)\n",
            "      mode = 'auto'\n",
            "\n",
            "    if mode == 'min':\n",
            "      self.monitor_op = np.less\n",
            "      self.best = np.Inf\n",
            "    elif mode == 'max':\n",
            "      self.monitor_op = np.greater\n",
            "      self.best = -np.Inf\n",
            "    else:\n",
            "      if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
            "        self.monitor_op = np.greater\n",
            "        self.best = -np.Inf\n",
            "      else:\n",
            "        self.monitor_op = np.less\n",
            "        self.best = np.Inf\n",
            "\n",
            "    if self.save_freq != 'epoch' and not isinstance(self.save_freq, int):\n",
            "      raise ValueError('Unrecognized save_freq: {}'.format(self.save_freq))\n",
            "\n",
            "    # Only the chief worker writes model checkpoints, but all workers\n",
            "    # restore checkpoint at on_train_begin().\n",
            "    self._chief_worker_only = False\n",
            "\n",
            "  def set_model(self, model):\n",
            "    self.model = model\n",
            "    # Use name matching rather than `isinstance` to avoid circular dependencies.\n",
            "    if (not self.save_weights_only and\n",
            "        not model._is_graph_network and  # pylint: disable=protected-access\n",
            "        model.__class__.__name__ != 'Sequential'):\n",
            "      self.save_weights_only = True\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    # TODO(rchao): Replace dc_context reference with\n",
            "    # distributed_training_utils.should_current_worker_load_model() once\n",
            "    # distributed_training_utils.py no longer depends on callbacks.py.\n",
            "    if K.in_multi_worker_mode(\n",
            "    ) and not dc_context.get_current_worker_context().experimental_should_init:\n",
            "      # For multi-worker training, it should not restore a model in certain\n",
            "      # worker setting (e.g. non-chief worker in ParameterServerStrategy).\n",
            "      return\n",
            "\n",
            "    filepath_to_load = self._get_most_recently_modified_file_matching_pattern(\n",
            "        self.filepath)\n",
            "    if (self.load_weights_on_restart and filepath_to_load is not None and\n",
            "        os.path.exists(filepath_to_load)):\n",
            "      try:\n",
            "        # `filepath` may contain placeholders such as `{epoch:02d}`, and thus\n",
            "        # it attempts to load the most recently modified file with file name\n",
            "        # matching the pattern.\n",
            "        self.model.load_weights(filepath_to_load)\n",
            "      except (IOError, ValueError) as e:\n",
            "        raise ValueError('Error loading file from {}. Reason: {}'.format(\n",
            "            filepath_to_load, e))\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    logs = logs or {}\n",
            "    # pylint: disable=protected-access\n",
            "    if self.model._ckpt_saved_epoch is not None:\n",
            "      # Make `_ckpt_saved_epoch` attribute `None` at the end of training as it\n",
            "      # is only used during the training. Currently it is decided not to\n",
            "      # support fault tolerance across multiple `model.fit()` or `model.fit()`\n",
            "      # with other `model` methods.\n",
            "      epoch = self.model._ckpt_saved_epoch\n",
            "      self.model._ckpt_saved_epoch = None\n",
            "      # TODO(rchao): Support all `save_weights_only` and `save_best_only` cases.\n",
            "      # This will be done with the help of a decoupled training state file that\n",
            "      # contains both epoch and model weights.\n",
            "      if self.save_weights_only and not self.save_best_only:\n",
            "        file_handle, filepath = self._get_file_handle_and_path(epoch, logs)\n",
            "        self.model.save_weights(filepath, overwrite=True)\n",
            "        self._maybe_remove_file(file_handle, filepath)\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    logs = logs or {}\n",
            "    if isinstance(self.save_freq, int):\n",
            "      self._samples_seen_since_last_saving += logs.get('size', 1)\n",
            "      if self._samples_seen_since_last_saving >= self.save_freq:\n",
            "        self._save_model(epoch=self._current_epoch, logs=logs)\n",
            "        self._samples_seen_since_last_saving = 0\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    self._current_epoch = epoch\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    self.epochs_since_last_save += 1\n",
            "    if self.save_freq == 'epoch':\n",
            "      self._save_model(epoch=epoch, logs=logs)\n",
            "\n",
            "  def _save_model(self, epoch, logs):\n",
            "    \"\"\"Saves the model.\n",
            "\n",
            "    Arguments:\n",
            "        epoch: the epoch this iteration is in.\n",
            "        logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n",
            "    \"\"\"\n",
            "    logs = logs or {}\n",
            "\n",
            "    if isinstance(self.save_freq,\n",
            "                  int) or self.epochs_since_last_save >= self.period:\n",
            "      self.epochs_since_last_save = 0\n",
            "      file_handle, filepath = self._get_file_handle_and_path(epoch, logs)\n",
            "\n",
            "      if self.save_best_only:\n",
            "        current = logs.get(self.monitor)\n",
            "        if current is None:\n",
            "          logging.warning('Can save best model only with %s available, '\n",
            "                          'skipping.', self.monitor)\n",
            "        else:\n",
            "          if self.monitor_op(current, self.best):\n",
            "            if self.verbose > 0:\n",
            "              print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
            "                    ' saving model to %s' % (epoch + 1, self.monitor, self.best,\n",
            "                                             current, filepath))\n",
            "            self.best = current\n",
            "            if self.save_weights_only:\n",
            "              self.model.save_weights(filepath, overwrite=True)\n",
            "            else:\n",
            "              self.model.save(filepath, overwrite=True)\n",
            "          else:\n",
            "            if self.verbose > 0:\n",
            "              print('\\nEpoch %05d: %s did not improve from %0.5f' %\n",
            "                    (epoch + 1, self.monitor, self.best))\n",
            "      else:\n",
            "        if self.verbose > 0:\n",
            "          print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
            "        if self.save_weights_only:\n",
            "          if K.in_multi_worker_mode():\n",
            "            # TODO(rchao): Save to an additional training state file for FT,\n",
            "            # instead of adding an attr to weight file. With this we can support\n",
            "            # the cases of all combinations with `save_weights_only`,\n",
            "            # `save_best_only`, and `save_format` parameters.\n",
            "            # pylint: disable=protected-access\n",
            "            self.model._ckpt_saved_epoch = epoch\n",
            "          self.model.save_weights(filepath, overwrite=True)\n",
            "        else:\n",
            "          self.model.save(filepath, overwrite=True)\n",
            "\n",
            "      self._maybe_remove_file(file_handle, filepath)\n",
            "\n",
            "  def _get_file_handle_and_path(self, epoch, logs):\n",
            "    \"\"\"Returns the file handle and path.\"\"\"\n",
            "    # TODO(rchao): Replace dc_context reference with\n",
            "    # distributed_training_utils.should_current_worker_checkpoint() once\n",
            "    # distributed_training_utils.py no longer depends on callbacks.py.\n",
            "    if not K.in_multi_worker_mode() or dc_context.get_current_worker_context(\n",
            "    ).should_checkpoint:\n",
            "      return None, self.filepath.format(epoch=epoch + 1, **logs)\n",
            "    else:\n",
            "      # If this is multi-worker training, and this worker should not\n",
            "      # save checkpoint, we replace the filepath with a dummy filepath so\n",
            "      # it writes to a file that will be removed at the end of _save_model()\n",
            "      # call. This is because the SyncOnReadVariable needs to be synced across\n",
            "      # all the workers in order to be read, and all workers need to initiate\n",
            "      # that.\n",
            "      file_handle, temp_file_name = tempfile.mkstemp()\n",
            "      extension = os.path.splitext(self.filepath)[1]\n",
            "      return file_handle, temp_file_name + '.' + extension\n",
            "\n",
            "  def _maybe_remove_file(self, file_handle, filepath):\n",
            "    # Remove the file in multi-worker training where this worker should\n",
            "    # not checkpoint. It is a dummy file previously saved for sync distributed\n",
            "    # training.\n",
            "    if K.in_multi_worker_mode(\n",
            "    ) and not dc_context.get_current_worker_context().should_checkpoint:\n",
            "      os.close(file_handle)\n",
            "      os.remove(filepath)\n",
            "\n",
            "  def _get_most_recently_modified_file_matching_pattern(self, pattern):\n",
            "    \"\"\"Returns the most recently modified filepath matching pattern.\n",
            "\n",
            "    Pattern may contain python formatting placeholder. If\n",
            "    `tf.train.latest_checkpoint()` does not return None, use that; otherwise,\n",
            "    check for most recently modified one that matches the pattern.\n",
            "\n",
            "    In the rare case where there are more than one pattern-matching file having\n",
            "    the same modified time that is most recent among all, return the filepath\n",
            "    that is largest (by `>` operator, lexicographically using the numeric\n",
            "    equivalents). This provides a tie-breaker when multiple files are most\n",
            "    recent. Note that a larger `filepath` can sometimes indicate a later time of\n",
            "    modification (for instance, when epoch/batch is used as formatting option),\n",
            "    but not necessarily (when accuracy or loss is used). The tie-breaker is\n",
            "    put in the logic as best effort to return the most recent, and to avoid\n",
            "    undeterministic result.\n",
            "\n",
            "    Modified time of a file is obtained with `os.path.getmtime()`.\n",
            "\n",
            "    This utility function is best demonstrated via an example:\n",
            "\n",
            "    ```python\n",
            "    file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.h5'\n",
            "    test_dir = self.get_temp_dir()\n",
            "    path_pattern = os.path.join(test_dir, file_pattern)\n",
            "    file_paths = [\n",
            "        os.path.join(test_dir, file_name) for file_name in\n",
            "        ['f.batch03epoch02.h5', 'f.batch02epoch02.h5', 'f.batch01epoch01.h5']\n",
            "    ]\n",
            "    for file_path in file_paths:\n",
            "      # Write something to each of the files\n",
            "    self.assertEqual(\n",
            "        _get_most_recently_modified_file_matching_pattern(path_pattern),\n",
            "        file_paths[-1])\n",
            "    ```\n",
            "\n",
            "    Arguments:\n",
            "        pattern: The file pattern that may optionally contain python placeholder\n",
            "            such as `{epoch:02d}`.\n",
            "\n",
            "    Returns:\n",
            "        The most recently modified file's full filepath matching `pattern`. If\n",
            "        `pattern` does not contain any placeholder, this returns the filepath\n",
            "        that\n",
            "        exactly matches `pattern`. Returns `None` if no match is found.\n",
            "    \"\"\"\n",
            "    dir_name = os.path.dirname(pattern)\n",
            "    base_name = os.path.basename(pattern)\n",
            "    base_name_regex = '^' + re.sub(r'{.*}', r'.*', base_name) + '$'\n",
            "\n",
            "    # If tf.train.latest_checkpoint tells us there exists a latest checkpoint,\n",
            "    # use that as it is more robust than `os.path.getmtime()`.\n",
            "    latest_tf_checkpoint = checkpoint_management.latest_checkpoint(dir_name)\n",
            "    if latest_tf_checkpoint is not None and re.match(\n",
            "        base_name_regex, os.path.basename(latest_tf_checkpoint)):\n",
            "      return latest_tf_checkpoint\n",
            "\n",
            "    latest_mod_time = 0\n",
            "    file_path_with_latest_mod_time = None\n",
            "    n_file_with_latest_mod_time = 0\n",
            "    file_path_with_largest_file_name = None\n",
            "\n",
            "    if os.path.exists(dir_name):\n",
            "      for file_name in os.listdir(dir_name):\n",
            "        # Only consider if `file_name` matches the pattern.\n",
            "        if re.match(base_name_regex, file_name):\n",
            "          file_path = os.path.join(dir_name, file_name)\n",
            "          mod_time = os.path.getmtime(file_path)\n",
            "          if (file_path_with_largest_file_name is None or\n",
            "              file_path > file_path_with_largest_file_name):\n",
            "            file_path_with_largest_file_name = file_path\n",
            "          if mod_time > latest_mod_time:\n",
            "            latest_mod_time = mod_time\n",
            "            file_path_with_latest_mod_time = file_path\n",
            "            # In the case a file with later modified time is found, reset\n",
            "            # the counter for the number of files with latest modified time.\n",
            "            n_file_with_latest_mod_time = 1\n",
            "          elif mod_time == latest_mod_time:\n",
            "            # In the case a file has modified time tied with the most recent,\n",
            "            # increment the counter for the number of files with latest modified\n",
            "            # time by 1.\n",
            "            n_file_with_latest_mod_time += 1\n",
            "\n",
            "    if n_file_with_latest_mod_time == 1:\n",
            "      # Return the sole file that has most recent modified time.\n",
            "      return file_path_with_latest_mod_time\n",
            "    else:\n",
            "      # If there are more than one file having latest modified time, return\n",
            "      # the file path with the largest file name.\n",
            "      return file_path_with_largest_file_name\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.EarlyStopping')\n",
            "class EarlyStopping(Callback):\n",
            "  \"\"\"Stop training when a monitored quantity has stopped improving.\n",
            "\n",
            "  Arguments:\n",
            "      monitor: Quantity to be monitored.\n",
            "      min_delta: Minimum change in the monitored quantity\n",
            "          to qualify as an improvement, i.e. an absolute\n",
            "          change of less than min_delta, will count as no\n",
            "          improvement.\n",
            "      patience: Number of epochs with no improvement\n",
            "          after which training will be stopped.\n",
            "      verbose: verbosity mode.\n",
            "      mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
            "          training will stop when the quantity\n",
            "          monitored has stopped decreasing; in `max`\n",
            "          mode it will stop when the quantity\n",
            "          monitored has stopped increasing; in `auto`\n",
            "          mode, the direction is automatically inferred\n",
            "          from the name of the monitored quantity.\n",
            "      baseline: Baseline value for the monitored quantity.\n",
            "          Training will stop if the model doesn't show improvement over the\n",
            "          baseline.\n",
            "      restore_best_weights: Whether to restore model weights from\n",
            "          the epoch with the best value of the monitored quantity.\n",
            "          If False, the model weights obtained at the last step of\n",
            "          training are used.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
            "  # This callback will stop the training when there is no improvement in\n",
            "  # the validation loss for three consecutive epochs.\n",
            "  model.fit(data, labels, epochs=100, callbacks=[callback],\n",
            "      validation_data=(val_data, val_labels))\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               monitor='val_loss',\n",
            "               min_delta=0,\n",
            "               patience=0,\n",
            "               verbose=0,\n",
            "               mode='auto',\n",
            "               baseline=None,\n",
            "               restore_best_weights=False):\n",
            "    super(EarlyStopping, self).__init__()\n",
            "\n",
            "    self.monitor = monitor\n",
            "    self.patience = patience\n",
            "    self.verbose = verbose\n",
            "    self.baseline = baseline\n",
            "    self.min_delta = abs(min_delta)\n",
            "    self.wait = 0\n",
            "    self.stopped_epoch = 0\n",
            "    self.restore_best_weights = restore_best_weights\n",
            "    self.best_weights = None\n",
            "\n",
            "    if mode not in ['auto', 'min', 'max']:\n",
            "      logging.warning('EarlyStopping mode %s is unknown, '\n",
            "                      'fallback to auto mode.', mode)\n",
            "      mode = 'auto'\n",
            "\n",
            "    if mode == 'min':\n",
            "      self.monitor_op = np.less\n",
            "    elif mode == 'max':\n",
            "      self.monitor_op = np.greater\n",
            "    else:\n",
            "      if 'acc' in self.monitor:\n",
            "        self.monitor_op = np.greater\n",
            "      else:\n",
            "        self.monitor_op = np.less\n",
            "\n",
            "    if self.monitor_op == np.greater:\n",
            "      self.min_delta *= 1\n",
            "    else:\n",
            "      self.min_delta *= -1\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    # Allow instances to be re-used\n",
            "    self.wait = 0\n",
            "    self.stopped_epoch = 0\n",
            "    if self.baseline is not None:\n",
            "      self.best = self.baseline\n",
            "    else:\n",
            "      self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    current = self.get_monitor_value(logs)\n",
            "    if current is None:\n",
            "      return\n",
            "    if self.monitor_op(current - self.min_delta, self.best):\n",
            "      self.best = current\n",
            "      self.wait = 0\n",
            "      if self.restore_best_weights:\n",
            "        self.best_weights = self.model.get_weights()\n",
            "    else:\n",
            "      self.wait += 1\n",
            "      if self.wait >= self.patience:\n",
            "        self.stopped_epoch = epoch\n",
            "        self.model.stop_training = True\n",
            "        if self.restore_best_weights:\n",
            "          if self.verbose > 0:\n",
            "            print('Restoring model weights from the end of the best epoch.')\n",
            "          self.model.set_weights(self.best_weights)\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    if self.stopped_epoch > 0 and self.verbose > 0:\n",
            "      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n",
            "\n",
            "  def get_monitor_value(self, logs):\n",
            "    logs = logs or {}\n",
            "    monitor_value = logs.get(self.monitor)\n",
            "    if monitor_value is None:\n",
            "      logging.warning('Early stopping conditioned on metric `%s` '\n",
            "                      'which is not available. Available metrics are: %s',\n",
            "                      self.monitor, ','.join(list(logs.keys())))\n",
            "    return monitor_value\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.RemoteMonitor')\n",
            "class RemoteMonitor(Callback):\n",
            "  \"\"\"Callback used to stream events to a server.\n",
            "\n",
            "  Requires the `requests` library.\n",
            "  Events are sent to `root + '/publish/epoch/end/'` by default. Calls are\n",
            "  HTTP POST, with a `data` argument which is a\n",
            "  JSON-encoded dictionary of event data.\n",
            "  If send_as_json is set to True, the content type of the request will be\n",
            "  application/json. Otherwise the serialized JSON will be sent within a form.\n",
            "\n",
            "  Arguments:\n",
            "      root: String; root url of the target server.\n",
            "      path: String; path relative to `root` to which the events will be sent.\n",
            "      field: String; JSON field under which the data will be stored.\n",
            "          The field is used only if the payload is sent within a form\n",
            "          (i.e. send_as_json is set to False).\n",
            "      headers: Dictionary; optional custom HTTP headers.\n",
            "      send_as_json: Boolean; whether the request should be\n",
            "          sent as application/json.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               root='http://localhost:9000',\n",
            "               path='/publish/epoch/end/',\n",
            "               field='data',\n",
            "               headers=None,\n",
            "               send_as_json=False):\n",
            "    super(RemoteMonitor, self).__init__()\n",
            "\n",
            "    self.root = root\n",
            "    self.path = path\n",
            "    self.field = field\n",
            "    self.headers = headers\n",
            "    self.send_as_json = send_as_json\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    if requests is None:\n",
            "      raise ImportError('RemoteMonitor requires the `requests` library.')\n",
            "    logs = logs or {}\n",
            "    send = {}\n",
            "    send['epoch'] = epoch\n",
            "    for k, v in logs.items():\n",
            "      send[k] = v\n",
            "    try:\n",
            "      if self.send_as_json:\n",
            "        requests.post(self.root + self.path, json=send, headers=self.headers)\n",
            "      else:\n",
            "        requests.post(\n",
            "            self.root + self.path, {self.field: json.dumps(send)},\n",
            "            headers=self.headers)\n",
            "    except requests.exceptions.RequestException:\n",
            "      logging.warning('Warning: could not reach RemoteMonitor '\n",
            "                      'root server at ' + str(self.root))\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.LearningRateScheduler')\n",
            "class LearningRateScheduler(Callback):\n",
            "  \"\"\"Learning rate scheduler.\n",
            "\n",
            "  Arguments:\n",
            "      schedule: a function that takes an epoch index as input\n",
            "          (integer, indexed from 0) and returns a new\n",
            "          learning rate as output (float).\n",
            "      verbose: int. 0: quiet, 1: update messages.\n",
            "\n",
            "  ```python\n",
            "  # This function keeps the learning rate at 0.001 for the first ten epochs\n",
            "  # and decreases it exponentially after that.\n",
            "  def scheduler(epoch):\n",
            "    if epoch < 10:\n",
            "      return 0.001\n",
            "    else:\n",
            "      return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
            "\n",
            "  callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
            "  model.fit(data, labels, epochs=100, callbacks=[callback],\n",
            "            validation_data=(val_data, val_labels))\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, schedule, verbose=0):\n",
            "    super(LearningRateScheduler, self).__init__()\n",
            "    self.schedule = schedule\n",
            "    self.verbose = verbose\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    if not hasattr(self.model.optimizer, 'lr'):\n",
            "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
            "    try:  # new API\n",
            "      lr = float(K.get_value(self.model.optimizer.lr))\n",
            "      lr = self.schedule(epoch, lr)\n",
            "    except TypeError:  # Support for old API for backward compatibility\n",
            "      lr = self.schedule(epoch)\n",
            "    if not isinstance(lr, (float, np.float32, np.float64)):\n",
            "      raise ValueError('The output of the \"schedule\" function '\n",
            "                       'should be float.')\n",
            "    K.set_value(self.model.optimizer.lr, lr)\n",
            "    if self.verbose > 0:\n",
            "      print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n",
            "            'rate to %s.' % (epoch + 1, lr))\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    logs = logs or {}\n",
            "    logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.TensorBoard', v1=[])\n",
            "class TensorBoard(Callback):\n",
            "  # pylint: disable=line-too-long\n",
            "  \"\"\"Enable visualizations for TensorBoard.\n",
            "\n",
            "  TensorBoard is a visualization tool provided with TensorFlow.\n",
            "\n",
            "  This callback logs events for TensorBoard, including:\n",
            "  * Metrics summary plots\n",
            "  * Training graph visualization\n",
            "  * Activation histograms\n",
            "  * Sampled profiling\n",
            "\n",
            "  If you have installed TensorFlow with pip, you should be able\n",
            "  to launch TensorBoard from the command line:\n",
            "\n",
            "  ```sh\n",
            "  tensorboard --logdir=path_to_your_logs\n",
            "  ```\n",
            "\n",
            "  You can find more information about TensorBoard\n",
            "  [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
            "\n",
            "  Arguments:\n",
            "      log_dir: the path of the directory where to save the log files to be\n",
            "        parsed by TensorBoard.\n",
            "      histogram_freq: frequency (in epochs) at which to compute activation and\n",
            "        weight histograms for the layers of the model. If set to 0, histograms\n",
            "        won't be computed. Validation data (or split) must be specified for\n",
            "        histogram visualizations.\n",
            "      write_graph: whether to visualize the graph in TensorBoard. The log file\n",
            "        can become quite large when write_graph is set to True.\n",
            "      write_images: whether to write model weights to visualize as image in\n",
            "        TensorBoard.\n",
            "      update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
            "        writes the losses and metrics to TensorBoard after each batch. The same\n",
            "        applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
            "        callback will write the metrics and losses to TensorBoard every 1000\n",
            "        samples. Note that writing too frequently to TensorBoard can slow down\n",
            "        your training.\n",
            "      profile_batch: Profile the batch to sample compute characteristics. By\n",
            "        default, it will profile the second batch. Set profile_batch=0 to\n",
            "        disable profiling. Must run in TensorFlow eager mode.\n",
            "      embeddings_freq: frequency (in epochs) at which embedding layers will\n",
            "        be visualized. If set to 0, embeddings won't be visualized.\n",
            "      embeddings_metadata: a dictionary which maps layer name to a file name in\n",
            "        which metadata for this embedding layer is saved. See the\n",
            "        [details](\n",
            "          https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n",
            "        about metadata files format. In case if the same metadata file is\n",
            "        used for all embedding layers, string can be passed.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If histogram_freq is set and no validation data is provided.\n",
            "  \"\"\"\n",
            "\n",
            "  # pylint: enable=line-too-long\n",
            "\n",
            "  def __init__(self,\n",
            "               log_dir='logs',\n",
            "               histogram_freq=0,\n",
            "               write_graph=True,\n",
            "               write_images=False,\n",
            "               update_freq='epoch',\n",
            "               profile_batch=2,\n",
            "               embeddings_freq=0,\n",
            "               embeddings_metadata=None,\n",
            "               **kwargs):\n",
            "    super(TensorBoard, self).__init__()\n",
            "    self._validate_kwargs(kwargs)\n",
            "\n",
            "    self.log_dir = log_dir\n",
            "    self.histogram_freq = histogram_freq\n",
            "    self.write_graph = write_graph\n",
            "    self.write_images = write_images\n",
            "    if update_freq == 'batch':\n",
            "      self.update_freq = 1\n",
            "    else:\n",
            "      self.update_freq = update_freq\n",
            "    self.embeddings_freq = embeddings_freq\n",
            "    self.embeddings_metadata = embeddings_metadata\n",
            "\n",
            "    self._samples_seen = 0\n",
            "    self._samples_seen_at_last_write = 0\n",
            "    self._current_batch = 0\n",
            "    self._total_batches_seen = 0\n",
            "    self._total_val_batches_seen = 0\n",
            "\n",
            "    # A collection of file writers currently in use, to be closed when\n",
            "    # training ends for this callback. Writers are keyed by the\n",
            "    # directory name under the root logdir: e.g., \"train\" or\n",
            "    # \"validation\".\n",
            "    self._writers = {}\n",
            "    self._train_run_name = 'train'\n",
            "    self._validation_run_name = 'validation'\n",
            "\n",
            "    self._profile_batch = profile_batch\n",
            "    # True when a trace is running.\n",
            "    self._is_tracing = False\n",
            "\n",
            "    # TensorBoard should only write summaries on the chief when in a\n",
            "    # Multi-Worker setting.\n",
            "    self._chief_worker_only = True\n",
            "\n",
            "  def _validate_kwargs(self, kwargs):\n",
            "    \"\"\"Handle arguments were supported in V1.\"\"\"\n",
            "    if kwargs.get('write_grads', False):\n",
            "      logging.warning('`write_grads` will be ignored in TensorFlow 2.0 '\n",
            "                      'for the `TensorBoard` Callback.')\n",
            "    if kwargs.get('batch_size', False):\n",
            "      logging.warning('`batch_size` is no longer needed in the '\n",
            "                      '`TensorBoard` Callback and will be ignored '\n",
            "                      'in TensorFlow 2.0.')\n",
            "    if kwargs.get('embeddings_layer_names', False):\n",
            "      logging.warning('`embeddings_layer_names` is not supported in '\n",
            "                      'TensorFlow 2.0. Instead, all `Embedding` layers '\n",
            "                      'will be visualized.')\n",
            "    if kwargs.get('embeddings_data', False):\n",
            "      logging.warning('`embeddings_data` is not supported in TensorFlow '\n",
            "                      '2.0. Instead, all `Embedding` variables will be '\n",
            "                      'visualized.')\n",
            "\n",
            "    unrecognized_kwargs = set(kwargs.keys()) - {\n",
            "        'write_grads', 'embeddings_layer_names', 'embeddings_data', 'batch_size'\n",
            "    }\n",
            "\n",
            "    # Only allow kwargs that were supported in V1.\n",
            "    if unrecognized_kwargs:\n",
            "      raise ValueError('Unrecognized arguments in `TensorBoard` '\n",
            "                       'Callback: ' + str(unrecognized_kwargs))\n",
            "\n",
            "  def set_model(self, model):\n",
            "    \"\"\"Sets Keras model and writes graph if specified.\"\"\"\n",
            "    self.model = model\n",
            "    with context.eager_mode():\n",
            "      self._close_writers()\n",
            "      if self.write_graph:\n",
            "        with self._get_writer(self._train_run_name).as_default():\n",
            "          with summary_ops_v2.always_record_summaries():\n",
            "            if not model.run_eagerly:\n",
            "              summary_ops_v2.graph(K.get_graph(), step=0)\n",
            "\n",
            "            summary_writable = (\n",
            "                self.model._is_graph_network or  # pylint: disable=protected-access\n",
            "                self.model.__class__.__name__ == 'Sequential')  # pylint: disable=protected-access\n",
            "            if summary_writable:\n",
            "              summary_ops_v2.keras_model('keras', self.model, step=0)\n",
            "\n",
            "    if self.embeddings_freq:\n",
            "      self._configure_embeddings()\n",
            "\n",
            "  def _configure_embeddings(self):\n",
            "    \"\"\"Configure the Projector for embeddings.\"\"\"\n",
            "    # TODO(omalleyt): Add integration tests.\n",
            "    from tensorflow.python.keras.layers import embeddings\n",
            "    try:\n",
            "      from tensorboard.plugins import projector\n",
            "    except ImportError:\n",
            "      raise ImportError('Failed to import TensorBoard. Please make sure that '\n",
            "                        'TensorBoard integration is complete.\"')\n",
            "    config = projector.ProjectorConfig()\n",
            "    for layer in self.model.layers:\n",
            "      if isinstance(layer, embeddings.Embedding):\n",
            "        embedding = config.embeddings.add()\n",
            "        embedding.tensor_name = layer.embeddings.name\n",
            "\n",
            "        if self.embeddings_metadata is not None:\n",
            "          if isinstance(self.embeddings_metadata, str):\n",
            "            embedding.metadata_path = self.embeddings_metadata\n",
            "          else:\n",
            "            if layer.name in embedding.metadata_path:\n",
            "              embedding.metadata_path = self.embeddings_metadata.pop(layer.name)\n",
            "\n",
            "    if self.embeddings_metadata:\n",
            "      raise ValueError('Unrecognized `Embedding` layer names passed to '\n",
            "                       '`keras.callbacks.TensorBoard` `embeddings_metadata` '\n",
            "                       'argument: ' + str(self.embeddings_metadata.keys()))\n",
            "\n",
            "    class DummyWriter(object):\n",
            "      \"\"\"Dummy writer to conform to `Projector` API.\"\"\"\n",
            "\n",
            "      def __init__(self, logdir):\n",
            "        self.logdir = logdir\n",
            "\n",
            "      def get_logdir(self):\n",
            "        return self.logdir\n",
            "\n",
            "    writer = DummyWriter(self.log_dir)\n",
            "    projector.visualize_embeddings(writer, config)\n",
            "\n",
            "  def _close_writers(self):\n",
            "    \"\"\"Close all remaining open file writers owned by this callback.\n",
            "\n",
            "    If there are no such file writers, this is a no-op.\n",
            "    \"\"\"\n",
            "    with context.eager_mode():\n",
            "      for writer in six.itervalues(self._writers):\n",
            "        writer.close()\n",
            "      self._writers.clear()\n",
            "\n",
            "  def _get_writer(self, writer_name):\n",
            "    \"\"\"Get a summary writer for the given subdirectory under the logdir.\n",
            "\n",
            "    A writer will be created if it does not yet exist.\n",
            "\n",
            "    Arguments:\n",
            "      writer_name: The name of the directory for which to create or\n",
            "        retrieve a writer. Should be either `self._train_run_name` or\n",
            "        `self._validation_run_name`.\n",
            "\n",
            "    Returns:\n",
            "      A `SummaryWriter` object.\n",
            "    \"\"\"\n",
            "    if writer_name not in self._writers:\n",
            "      path = os.path.join(self.log_dir, writer_name)\n",
            "      writer = summary_ops_v2.create_file_writer_v2(path)\n",
            "      self._writers[writer_name] = writer\n",
            "    return self._writers[writer_name]\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    if self._profile_batch == 1:\n",
            "      summary_ops_v2.trace_on(graph=True, profiler=True)\n",
            "      self._is_tracing = True\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Writes scalar summaries for metrics on every training batch.\n",
            "\n",
            "    Performs profiling if current batch is in profiler_batches.\n",
            "\n",
            "    Arguments:\n",
            "      batch: Integer, index of batch within the current epoch.\n",
            "      logs: Dict. Metric results for this batch.\n",
            "    \"\"\"\n",
            "    # Don't output batch_size and batch number as TensorBoard summaries\n",
            "    logs = logs or {}\n",
            "    self._samples_seen += logs.get('size', 1)\n",
            "    samples_seen_since = self._samples_seen - self._samples_seen_at_last_write\n",
            "    if self.update_freq != 'epoch' and samples_seen_since >= self.update_freq:\n",
            "      self._log_metrics(logs, prefix='batch_', step=self._total_batches_seen)\n",
            "      self._samples_seen_at_last_write = self._samples_seen\n",
            "    self._total_batches_seen += 1\n",
            "    if self._is_tracing:\n",
            "      self._log_trace()\n",
            "    elif (not self._is_tracing and\n",
            "          self._total_batches_seen == self._profile_batch - 1):\n",
            "      self._enable_trace()\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    \"\"\"Runs metrics and histogram summaries at epoch end.\"\"\"\n",
            "    step = epoch if self.update_freq == 'epoch' else self._samples_seen\n",
            "    self._log_metrics(logs, prefix='epoch_', step=step)\n",
            "\n",
            "    if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
            "      self._log_weights(epoch)\n",
            "\n",
            "    if self.embeddings_freq and epoch % self.embeddings_freq == 0:\n",
            "      self._log_embeddings(epoch)\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    if self._is_tracing:\n",
            "      self._log_trace()\n",
            "    self._close_writers()\n",
            "\n",
            "  def _enable_trace(self):\n",
            "    if context.executing_eagerly():\n",
            "      summary_ops_v2.trace_on(graph=True, profiler=True)\n",
            "      self._is_tracing = True\n",
            "\n",
            "  def _log_trace(self):\n",
            "    if context.executing_eagerly():\n",
            "      with self._get_writer(self._train_run_name).as_default(), \\\n",
            "          summary_ops_v2.always_record_summaries():\n",
            "        # TODO(b/126388999): Remove step info in the summary name.\n",
            "        summary_ops_v2.trace_export(\n",
            "            name='batch_%d' % self._total_batches_seen,\n",
            "            step=self._total_batches_seen,\n",
            "            profiler_outdir=os.path.join(self.log_dir, 'train'))\n",
            "      self._is_tracing = False\n",
            "\n",
            "  def _log_metrics(self, logs, prefix, step):\n",
            "    \"\"\"Writes metrics out as custom scalar summaries.\n",
            "\n",
            "    Arguments:\n",
            "        logs: Dict. Keys are scalar summary names, values are NumPy scalars.\n",
            "        prefix: String. The prefix to apply to the scalar summary names.\n",
            "        step: Int. The global step to use for TensorBoard.\n",
            "    \"\"\"\n",
            "    if logs is None:\n",
            "      logs = {}\n",
            "\n",
            "    # Group metrics by the name of their associated file writer. Values\n",
            "    # are lists of metrics, as (name, scalar_value) pairs.\n",
            "    logs_by_writer = {\n",
            "        self._train_run_name: [],\n",
            "        self._validation_run_name: [],\n",
            "    }\n",
            "    validation_prefix = 'val_'\n",
            "    for (name, value) in logs.items():\n",
            "      if name in ('batch', 'size', 'num_steps'):\n",
            "        # Scrub non-metric items.\n",
            "        continue\n",
            "      if name.startswith(validation_prefix):\n",
            "        name = name[len(validation_prefix):]\n",
            "        writer_name = self._validation_run_name\n",
            "      else:\n",
            "        writer_name = self._train_run_name\n",
            "      name = prefix + name  # assign batch or epoch prefix\n",
            "      logs_by_writer[writer_name].append((name, value))\n",
            "\n",
            "    with context.eager_mode():\n",
            "      with summary_ops_v2.always_record_summaries():\n",
            "        for writer_name in logs_by_writer:\n",
            "          these_logs = logs_by_writer[writer_name]\n",
            "          if not these_logs:\n",
            "            # Don't create a \"validation\" events file if we don't\n",
            "            # actually have any validation data.\n",
            "            continue\n",
            "          writer = self._get_writer(writer_name)\n",
            "          with writer.as_default():\n",
            "            for (name, value) in these_logs:\n",
            "              summary_ops_v2.scalar(name, value, step=step)\n",
            "\n",
            "  def _log_weights(self, epoch):\n",
            "    \"\"\"Logs the weights of the Model to TensorBoard.\"\"\"\n",
            "    writer = self._get_writer(self._train_run_name)\n",
            "    with context.eager_mode(), \\\n",
            "          writer.as_default(), \\\n",
            "          summary_ops_v2.always_record_summaries():\n",
            "      for layer in self.model.layers:\n",
            "        for weight in layer.weights:\n",
            "          weight_name = weight.name.replace(':', '_')\n",
            "          with ops.init_scope():\n",
            "            weight = K.get_value(weight)\n",
            "          summary_ops_v2.histogram(weight_name, weight, step=epoch)\n",
            "          if self.write_images:\n",
            "            self._log_weight_as_image(weight, weight_name, epoch)\n",
            "      writer.flush()\n",
            "\n",
            "  def _log_weight_as_image(self, weight, weight_name, epoch):\n",
            "    \"\"\"Logs a weight as a TensorBoard image.\"\"\"\n",
            "    w_img = array_ops.squeeze(weight)\n",
            "    shape = K.int_shape(w_img)\n",
            "    if len(shape) == 1:  # Bias case\n",
            "      w_img = array_ops.reshape(w_img, [1, shape[0], 1, 1])\n",
            "    elif len(shape) == 2:  # Dense layer kernel case\n",
            "      if shape[0] > shape[1]:\n",
            "        w_img = array_ops.transpose(w_img)\n",
            "        shape = K.int_shape(w_img)\n",
            "      w_img = array_ops.reshape(w_img, [1, shape[0], shape[1], 1])\n",
            "    elif len(shape) == 3:  # ConvNet case\n",
            "      if K.image_data_format() == 'channels_last':\n",
            "        # Switch to channels_first to display every kernel as a separate\n",
            "        # image.\n",
            "        w_img = array_ops.transpose(w_img, perm=[2, 0, 1])\n",
            "        shape = K.int_shape(w_img)\n",
            "      w_img = array_ops.reshape(w_img, [shape[0], shape[1], shape[2], 1])\n",
            "\n",
            "    shape = K.int_shape(w_img)\n",
            "    # Not possible to handle 3D convnets etc.\n",
            "    if len(shape) == 4 and shape[-1] in [1, 3, 4]:\n",
            "      summary_ops_v2.image(weight_name, w_img, step=epoch)\n",
            "\n",
            "  def _log_embeddings(self, epoch):\n",
            "    embeddings_ckpt = os.path.join(self.log_dir, 'train',\n",
            "                                   'keras_embedding.ckpt-{}'.format(epoch))\n",
            "    self.model.save_weights(embeddings_ckpt)\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.ReduceLROnPlateau')\n",
            "class ReduceLROnPlateau(Callback):\n",
            "  \"\"\"Reduce learning rate when a metric has stopped improving.\n",
            "\n",
            "  Models often benefit from reducing the learning rate by a factor\n",
            "  of 2-10 once learning stagnates. This callback monitors a\n",
            "  quantity and if no improvement is seen for a 'patience' number\n",
            "  of epochs, the learning rate is reduced.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
            "                                patience=5, min_lr=0.001)\n",
            "  model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      monitor: quantity to be monitored.\n",
            "      factor: factor by which the learning rate will be reduced. new_lr = lr *\n",
            "        factor\n",
            "      patience: number of epochs with no improvement after which learning rate\n",
            "        will be reduced.\n",
            "      verbose: int. 0: quiet, 1: update messages.\n",
            "      mode: one of {auto, min, max}. In `min` mode, lr will be reduced when the\n",
            "        quantity monitored has stopped decreasing; in `max` mode it will be\n",
            "        reduced when the quantity monitored has stopped increasing; in `auto`\n",
            "        mode, the direction is automatically inferred from the name of the\n",
            "        monitored quantity.\n",
            "      min_delta: threshold for measuring the new optimum, to only focus on\n",
            "        significant changes.\n",
            "      cooldown: number of epochs to wait before resuming normal operation after\n",
            "        lr has been reduced.\n",
            "      min_lr: lower bound on the learning rate.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               monitor='val_loss',\n",
            "               factor=0.1,\n",
            "               patience=10,\n",
            "               verbose=0,\n",
            "               mode='auto',\n",
            "               min_delta=1e-4,\n",
            "               cooldown=0,\n",
            "               min_lr=0,\n",
            "               **kwargs):\n",
            "    super(ReduceLROnPlateau, self).__init__()\n",
            "\n",
            "    self.monitor = monitor\n",
            "    if factor >= 1.0:\n",
            "      raise ValueError('ReduceLROnPlateau ' 'does not support a factor >= 1.0.')\n",
            "    if 'epsilon' in kwargs:\n",
            "      min_delta = kwargs.pop('epsilon')\n",
            "      logging.warning('`epsilon` argument is deprecated and '\n",
            "                      'will be removed, use `min_delta` instead.')\n",
            "    self.factor = factor\n",
            "    self.min_lr = min_lr\n",
            "    self.min_delta = min_delta\n",
            "    self.patience = patience\n",
            "    self.verbose = verbose\n",
            "    self.cooldown = cooldown\n",
            "    self.cooldown_counter = 0  # Cooldown counter.\n",
            "    self.wait = 0\n",
            "    self.best = 0\n",
            "    self.mode = mode\n",
            "    self.monitor_op = None\n",
            "    self._reset()\n",
            "\n",
            "  def _reset(self):\n",
            "    \"\"\"Resets wait counter and cooldown counter.\n",
            "    \"\"\"\n",
            "    if self.mode not in ['auto', 'min', 'max']:\n",
            "      logging.warning('Learning Rate Plateau Reducing mode %s is unknown, '\n",
            "                      'fallback to auto mode.', self.mode)\n",
            "      self.mode = 'auto'\n",
            "    if (self.mode == 'min' or\n",
            "        (self.mode == 'auto' and 'acc' not in self.monitor)):\n",
            "      self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
            "      self.best = np.Inf\n",
            "    else:\n",
            "      self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
            "      self.best = -np.Inf\n",
            "    self.cooldown_counter = 0\n",
            "    self.wait = 0\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    self._reset()\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    logs = logs or {}\n",
            "    logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
            "    current = logs.get(self.monitor)\n",
            "    if current is None:\n",
            "      logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n",
            "                      'which is not available. Available metrics are: %s',\n",
            "                      self.monitor, ','.join(list(logs.keys())))\n",
            "\n",
            "    else:\n",
            "      if self.in_cooldown():\n",
            "        self.cooldown_counter -= 1\n",
            "        self.wait = 0\n",
            "\n",
            "      if self.monitor_op(current, self.best):\n",
            "        self.best = current\n",
            "        self.wait = 0\n",
            "      elif not self.in_cooldown():\n",
            "        self.wait += 1\n",
            "        if self.wait >= self.patience:\n",
            "          old_lr = float(K.get_value(self.model.optimizer.lr))\n",
            "          if old_lr > self.min_lr:\n",
            "            new_lr = old_lr * self.factor\n",
            "            new_lr = max(new_lr, self.min_lr)\n",
            "            K.set_value(self.model.optimizer.lr, new_lr)\n",
            "            if self.verbose > 0:\n",
            "              print('\\nEpoch %05d: ReduceLROnPlateau reducing learning '\n",
            "                    'rate to %s.' % (epoch + 1, new_lr))\n",
            "            self.cooldown_counter = self.cooldown\n",
            "            self.wait = 0\n",
            "\n",
            "  def in_cooldown(self):\n",
            "    return self.cooldown_counter > 0\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.CSVLogger')\n",
            "class CSVLogger(Callback):\n",
            "  \"\"\"Callback that streams epoch results to a csv file.\n",
            "\n",
            "  Supports all values that can be represented as a string,\n",
            "  including 1D iterables such as np.ndarray.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  csv_logger = CSVLogger('training.log')\n",
            "  model.fit(X_train, Y_train, callbacks=[csv_logger])\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      filename: filename of the csv file, e.g. 'run/log.csv'.\n",
            "      separator: string used to separate elements in the csv file.\n",
            "      append: True: append if file exists (useful for continuing\n",
            "          training). False: overwrite existing file,\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, filename, separator=',', append=False):\n",
            "    self.sep = separator\n",
            "    self.filename = filename\n",
            "    self.append = append\n",
            "    self.writer = None\n",
            "    self.keys = None\n",
            "    self.append_header = True\n",
            "    if six.PY2:\n",
            "      self.file_flags = 'b'\n",
            "      self._open_args = {}\n",
            "    else:\n",
            "      self.file_flags = ''\n",
            "      self._open_args = {'newline': '\\n'}\n",
            "    super(CSVLogger, self).__init__()\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    if self.append:\n",
            "      if os.path.exists(self.filename):\n",
            "        with open(self.filename, 'r' + self.file_flags) as f:\n",
            "          self.append_header = not bool(len(f.readline()))\n",
            "      mode = 'a'\n",
            "    else:\n",
            "      mode = 'w'\n",
            "    self.csv_file = io.open(self.filename,\n",
            "                            mode + self.file_flags,\n",
            "                            **self._open_args)\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    logs = logs or {}\n",
            "\n",
            "    def handle_value(k):\n",
            "      is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n",
            "      if isinstance(k, six.string_types):\n",
            "        return k\n",
            "      elif isinstance(k, collections.Iterable) and not is_zero_dim_ndarray:\n",
            "        return '\"[%s]\"' % (', '.join(map(str, k)))\n",
            "      else:\n",
            "        return k\n",
            "\n",
            "    if self.keys is None:\n",
            "      self.keys = sorted(logs.keys())\n",
            "\n",
            "    if self.model.stop_training:\n",
            "      # We set NA so that csv parsers do not fail for this last epoch.\n",
            "      logs = dict([(k, logs[k]) if k in logs else (k, 'NA') for k in self.keys])\n",
            "\n",
            "    if not self.writer:\n",
            "\n",
            "      class CustomDialect(csv.excel):\n",
            "        delimiter = self.sep\n",
            "\n",
            "      fieldnames = ['epoch'] + self.keys\n",
            "      if six.PY2:\n",
            "        fieldnames = [unicode(x) for x in fieldnames]\n",
            "\n",
            "      self.writer = csv.DictWriter(\n",
            "          self.csv_file,\n",
            "          fieldnames=fieldnames,\n",
            "          dialect=CustomDialect)\n",
            "      if self.append_header:\n",
            "        self.writer.writeheader()\n",
            "\n",
            "    row_dict = collections.OrderedDict({'epoch': epoch})\n",
            "    row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n",
            "    self.writer.writerow(row_dict)\n",
            "    self.csv_file.flush()\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    self.csv_file.close()\n",
            "    self.writer = None\n",
            "\n",
            "\n",
            "@keras_export('keras.callbacks.LambdaCallback')\n",
            "class LambdaCallback(Callback):\n",
            "  r\"\"\"Callback for creating simple, custom callbacks on-the-fly.\n",
            "\n",
            "  This callback is constructed with anonymous functions that will be called\n",
            "  at the appropriate time. Note that the callbacks expects positional\n",
            "  arguments, as:\n",
            "\n",
            "   - `on_epoch_begin` and `on_epoch_end` expect two positional arguments:\n",
            "      `epoch`, `logs`\n",
            "   - `on_batch_begin` and `on_batch_end` expect two positional arguments:\n",
            "      `batch`, `logs`\n",
            "   - `on_train_begin` and `on_train_end` expect one positional argument:\n",
            "      `logs`\n",
            "\n",
            "  Arguments:\n",
            "      on_epoch_begin: called at the beginning of every epoch.\n",
            "      on_epoch_end: called at the end of every epoch.\n",
            "      on_batch_begin: called at the beginning of every batch.\n",
            "      on_batch_end: called at the end of every batch.\n",
            "      on_train_begin: called at the beginning of model training.\n",
            "      on_train_end: called at the end of model training.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  # Print the batch number at the beginning of every batch.\n",
            "  batch_print_callback = LambdaCallback(\n",
            "      on_batch_begin=lambda batch,logs: print(batch))\n",
            "\n",
            "  # Stream the epoch loss to a file in JSON format. The file content\n",
            "  # is not well-formed JSON but rather has a JSON object per line.\n",
            "  import json\n",
            "  json_log = open('loss_log.json', mode='wt', buffering=1)\n",
            "  json_logging_callback = LambdaCallback(\n",
            "      on_epoch_end=lambda epoch, logs: json_log.write(\n",
            "          json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n",
            "      on_train_end=lambda logs: json_log.close()\n",
            "  )\n",
            "\n",
            "  # Terminate some processes after having finished model training.\n",
            "  processes = ...\n",
            "  cleanup_callback = LambdaCallback(\n",
            "      on_train_end=lambda logs: [\n",
            "          p.terminate() for p in processes if p.is_alive()])\n",
            "\n",
            "  model.fit(...,\n",
            "            callbacks=[batch_print_callback,\n",
            "                       json_logging_callback,\n",
            "                       cleanup_callback])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               on_epoch_begin=None,\n",
            "               on_epoch_end=None,\n",
            "               on_batch_begin=None,\n",
            "               on_batch_end=None,\n",
            "               on_train_begin=None,\n",
            "               on_train_end=None,\n",
            "               **kwargs):\n",
            "    super(LambdaCallback, self).__init__()\n",
            "    self.__dict__.update(kwargs)\n",
            "    if on_epoch_begin is not None:\n",
            "      self.on_epoch_begin = on_epoch_begin\n",
            "    else:\n",
            "      self.on_epoch_begin = lambda epoch, logs: None\n",
            "    if on_epoch_end is not None:\n",
            "      self.on_epoch_end = on_epoch_end\n",
            "    else:\n",
            "      self.on_epoch_end = lambda epoch, logs: None\n",
            "    if on_batch_begin is not None:\n",
            "      self.on_batch_begin = on_batch_begin\n",
            "    else:\n",
            "      self.on_batch_begin = lambda batch, logs: None\n",
            "    if on_batch_end is not None:\n",
            "      self.on_batch_end = on_batch_end\n",
            "    else:\n",
            "      self.on_batch_end = lambda batch, logs: None\n",
            "    if on_train_begin is not None:\n",
            "      self.on_train_begin = on_train_begin\n",
            "    else:\n",
            "      self.on_train_begin = lambda logs: None\n",
            "    if on_train_end is not None:\n",
            "      self.on_train_end = on_train_end\n",
            "    else:\n",
            "      self.on_train_end = lambda logs: None\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Implementation of the Keras API meant to be a high-level API for TensorFlow.\n",
            "\n",
            "Detailed documentation and user guides are available at\n",
            "[keras.io](https://keras.io).\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python import tf2\n",
            "\n",
            "from tensorflow.python.keras import activations\n",
            "from tensorflow.python.keras import applications\n",
            "from tensorflow.python.keras import backend\n",
            "from tensorflow.python.keras import callbacks\n",
            "from tensorflow.python.keras import callbacks_v1\n",
            "from tensorflow.python.keras import constraints\n",
            "from tensorflow.python.keras import datasets\n",
            "from tensorflow.python.keras import estimator\n",
            "from tensorflow.python.keras import initializers\n",
            "from tensorflow.python.keras import layers\n",
            "from tensorflow.python.keras import losses\n",
            "from tensorflow.python.keras import metrics\n",
            "from tensorflow.python.keras import models\n",
            "from tensorflow.python.keras import ops\n",
            "from tensorflow.python.keras import optimizers\n",
            "from tensorflow.python.keras import preprocessing\n",
            "from tensorflow.python.keras import regularizers\n",
            "from tensorflow.python.keras import utils\n",
            "from tensorflow.python.keras import wrappers\n",
            "from tensorflow.python.keras.layers import Input\n",
            "from tensorflow.python.keras.models import Model\n",
            "from tensorflow.python.keras.models import Sequential\n",
            "\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "if tf2.enabled():\n",
            "  __version__ = '2.3.0-tf'\n",
            "else:\n",
            "  __version__ = '2.2.4-tf'\n",
            "\n",
            "keras_export('keras.__version__').export_constant(__name__, '__version__')\n",
            "\n",
            "del absolute_import\n",
            "del division\n",
            "del print_function\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=unused-import\n",
            "\"\"\"Built-in metrics.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import abc\n",
            "import types\n",
            "import numpy as np\n",
            "import six\n",
            "\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.eager import def_function\n",
            "from tensorflow.python.framework import dtypes\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.framework import tensor_shape\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.engine.base_layer import Layer\n",
            "from tensorflow.python.keras.losses import binary_crossentropy\n",
            "from tensorflow.python.keras.losses import categorical_crossentropy\n",
            "from tensorflow.python.keras.losses import categorical_hinge\n",
            "from tensorflow.python.keras.losses import cosine_similarity\n",
            "from tensorflow.python.keras.losses import hinge\n",
            "from tensorflow.python.keras.losses import kullback_leibler_divergence\n",
            "from tensorflow.python.keras.losses import logcosh\n",
            "from tensorflow.python.keras.losses import mean_absolute_error\n",
            "from tensorflow.python.keras.losses import mean_absolute_percentage_error\n",
            "from tensorflow.python.keras.losses import mean_squared_error\n",
            "from tensorflow.python.keras.losses import mean_squared_logarithmic_error\n",
            "from tensorflow.python.keras.losses import poisson\n",
            "from tensorflow.python.keras.losses import sparse_categorical_crossentropy\n",
            "from tensorflow.python.keras.losses import squared_hinge\n",
            "from tensorflow.python.keras.utils import metrics_utils\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import to_list\n",
            "from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions\n",
            "from tensorflow.python.keras.utils.tf_utils import is_tensor_or_variable\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import confusion_matrix\n",
            "from tensorflow.python.ops import init_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import nn\n",
            "from tensorflow.python.ops import variables as tf_variables\n",
            "from tensorflow.python.ops import weights_broadcast_ops\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "from tensorflow.tools.docs import doc_controls\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Metric')\n",
            "@six.add_metaclass(abc.ABCMeta)\n",
            "class Metric(Layer):\n",
            "  \"\"\"Encapsulates metric logic and state.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = SomeMetric(...)\n",
            "  for input in ...:\n",
            "    m.update_state(input)\n",
            "  print('Final result: ', m.result().numpy())\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Sequential()\n",
            "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
            "  model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
            "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
            "\n",
            "  model.compile(optimizer=tf.compat.v1.train.RMSPropOptimizer(0.01),\n",
            "                loss=tf.keras.losses.categorical_crossentropy,\n",
            "                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
            "\n",
            "  data = np.random.random((1000, 32))\n",
            "  labels = np.random.random((1000, 10))\n",
            "\n",
            "  dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
            "  dataset = dataset.batch(32)\n",
            "  dataset = dataset.repeat()\n",
            "\n",
            "  model.fit(dataset, epochs=10, steps_per_epoch=30)\n",
            "  ```\n",
            "\n",
            "  To be implemented by subclasses:\n",
            "  * `__init__()`: All state variables should be created in this method by\n",
            "    calling `self.add_weight()` like: `self.var = self.add_weight(...)`\n",
            "  * `update_state()`: Has all updates to the state variables like:\n",
            "    self.var.assign_add(...).\n",
            "  * `result()`: Computes and returns a value for the metric\n",
            "    from the state variables.\n",
            "\n",
            "  Example subclass implementation:\n",
            "\n",
            "  ```\n",
            "  class BinaryTruePositives(tf.keras.metrics.Metric):\n",
            "\n",
            "    def __init__(self, name='binary_true_positives', **kwargs):\n",
            "      super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n",
            "      self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
            "\n",
            "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "      y_true = tf.cast(y_true, tf.bool)\n",
            "      y_pred = tf.cast(y_pred, tf.bool)\n",
            "\n",
            "      values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
            "      values = tf.cast(values, self.dtype)\n",
            "      if sample_weight is not None:\n",
            "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
            "        sample_weight = tf.broadcast_weights(sample_weight, values)\n",
            "        values = tf.multiply(values, sample_weight)\n",
            "      self.true_positives.assign_add(tf.reduce_sum(values))\n",
            "\n",
            "    def result(self):\n",
            "      return self.true_positives\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name=None, dtype=None, **kwargs):\n",
            "    super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\n",
            "    self.stateful = True  # All metric layers are stateful.\n",
            "    self.built = True\n",
            "    self._dtype = K.floatx() if dtype is None else dtypes.as_dtype(dtype).name\n",
            "\n",
            "  def __new__(cls, *args, **kwargs):\n",
            "    obj = super(Metric, cls).__new__(cls)\n",
            "\n",
            "    # TODO(psv): We are excluding wrapping `update_state` of built-in metrics\n",
            "    # with function here because of b/121302287. With this, built-in metrics\n",
            "    # will continue to work with TPUs and custom metrics will not, however\n",
            "    # users writing custom metrics need not worry about control dependencies\n",
            "    # and returning ops.\n",
            "    if cls.__module__ == Metric.__module__:\n",
            "      update_state_fn = obj.update_state\n",
            "    else:\n",
            "      update_state_fn = def_function.function(obj.update_state)\n",
            "\n",
            "    obj.update_state = types.MethodType(\n",
            "        metrics_utils.update_state_wrapper(update_state_fn), obj)\n",
            "    obj.result = types.MethodType(metrics_utils.result_wrapper(obj.result), obj)\n",
            "    return obj\n",
            "\n",
            "  def __call__(self, *args, **kwargs):\n",
            "    \"\"\"Accumulates statistics and then computes metric result value.\n",
            "\n",
            "    Args:\n",
            "      *args:\n",
            "      **kwargs: A mini-batch of inputs to the Metric,\n",
            "        passed on to `update_state()`.\n",
            "\n",
            "    Returns:\n",
            "      The metric value tensor.\n",
            "    \"\"\"\n",
            "    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\n",
            "    with ops.control_dependencies([update_op]):\n",
            "      result_t = self.result()  # pylint: disable=not-callable\n",
            "\n",
            "      # We are adding the metric object as metadata on the result tensor.\n",
            "      # This is required when we want to use a metric with `add_metric` API on\n",
            "      # a Model/Layer in graph mode. This metric instance will later be used\n",
            "      # to reset variable state after each epoch of training.\n",
            "      # Example:\n",
            "      #   model = Model()\n",
            "      #   mean = Mean()\n",
            "      #   model.add_metric(mean(values), name='mean')\n",
            "      result_t._metric_obj = self  # pylint: disable=protected-access\n",
            "      return result_t\n",
            "\n",
            "  @property\n",
            "  def dtype(self):\n",
            "    return self._dtype\n",
            "\n",
            "  def get_config(self):\n",
            "    \"\"\"Returns the serializable config of the metric.\"\"\"\n",
            "    return {'name': self.name, 'dtype': self.dtype}\n",
            "\n",
            "  def reset_states(self):\n",
            "    \"\"\"Resets all of the metric state variables.\n",
            "\n",
            "    This function is called between epochs/steps,\n",
            "    when a metric is evaluated during training.\n",
            "    \"\"\"\n",
            "    K.batch_set_value([(v, 0) for v in self.variables])\n",
            "\n",
            "  @abc.abstractmethod\n",
            "  def update_state(self, *args, **kwargs):\n",
            "    \"\"\"Accumulates statistics for the metric.\n",
            "\n",
            "    Note: This function is executed as a graph function in graph mode.\n",
            "    This means:\n",
            "      a) Operations on the same resource are executed in textual order.\n",
            "         This should make it easier to do things like add the updated\n",
            "         value of a variable to another, for example.\n",
            "      b) You don't need to worry about collecting the update ops to execute.\n",
            "         All update ops added to the graph by this function will be executed.\n",
            "      As a result, code should generally work the same way with graph or\n",
            "      eager execution.\n",
            "\n",
            "    Please use `tf.config.experimental_run_functions_eagerly(True)` to execute\n",
            "    this function eagerly for debugging or profiling.\n",
            "\n",
            "    Args:\n",
            "      *args:\n",
            "      **kwargs: A mini-batch of inputs to the Metric.\n",
            "    \"\"\"\n",
            "    NotImplementedError('Must be implemented in subclasses.')\n",
            "\n",
            "  @abc.abstractmethod\n",
            "  def result(self):\n",
            "    \"\"\"Computes and returns the metric value tensor.\n",
            "\n",
            "    Result computation is an idempotent operation that simply calculates the\n",
            "    metric value using the state variables.\n",
            "    \"\"\"\n",
            "    NotImplementedError('Must be implemented in subclasses.')\n",
            "\n",
            "  ### For use by subclasses ###\n",
            "  @doc_controls.for_subclass_implementers\n",
            "  def add_weight(self,\n",
            "                 name,\n",
            "                 shape=(),\n",
            "                 aggregation=tf_variables.VariableAggregation.SUM,\n",
            "                 synchronization=tf_variables.VariableSynchronization.ON_READ,\n",
            "                 initializer=None,\n",
            "                 dtype=None):\n",
            "    \"\"\"Adds state variable. Only for use by subclasses.\"\"\"\n",
            "    return super(Metric, self).add_weight(\n",
            "        name=name,\n",
            "        shape=shape,\n",
            "        dtype=self._dtype if dtype is None else dtype,\n",
            "        trainable=False,\n",
            "        initializer=initializer,\n",
            "        collections=[],\n",
            "        synchronization=synchronization,\n",
            "        aggregation=aggregation)\n",
            "\n",
            "  ### End: For use by subclasses ###\n",
            "\n",
            "\n",
            "class Reduce(Metric):\n",
            "  \"\"\"Encapsulates metrics that perform a reduce operation on the values.\"\"\"\n",
            "\n",
            "  def __init__(self, reduction, name, dtype=None):\n",
            "    \"\"\"Creates a `Reduce` instance.\n",
            "\n",
            "    Args:\n",
            "      reduction: a `tf.keras.metrics.Reduction` enum value.\n",
            "      name: string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(Reduce, self).__init__(name=name, dtype=dtype)\n",
            "    self.reduction = reduction\n",
            "    self.total = self.add_weight(\n",
            "        'total', initializer=init_ops.zeros_initializer)\n",
            "    if reduction in [metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n",
            "                     metrics_utils.Reduction.WEIGHTED_MEAN]:\n",
            "      self.count = self.add_weight(\n",
            "          'count', initializer=init_ops.zeros_initializer)\n",
            "\n",
            "  def update_state(self, values, sample_weight=None):\n",
            "    \"\"\"Accumulates statistics for computing the reduction metric.\n",
            "\n",
            "    For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE,\n",
            "    then the value of `result()` is 4. If the `sample_weight` is specified as\n",
            "    [1, 1, 0, 0] then value of `result()` would be 2.\n",
            "\n",
            "    Args:\n",
            "      values: Per-example value.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    values = math_ops.cast(values, self._dtype)\n",
            "    if sample_weight is not None:\n",
            "      sample_weight = math_ops.cast(sample_weight, self._dtype)\n",
            "      # Update dimensions of weights to match with values if possible.\n",
            "      values, _, sample_weight = squeeze_or_expand_dimensions(\n",
            "          values, None, sample_weight)\n",
            "      try:\n",
            "        # Broadcast weights if possible.\n",
            "        sample_weight = weights_broadcast_ops.broadcast_weights(\n",
            "            sample_weight, values)\n",
            "      except ValueError:\n",
            "        # Reduce values to same ndim as weight array\n",
            "        ndim = K.ndim(values)\n",
            "        weight_ndim = K.ndim(sample_weight)\n",
            "        if self.reduction == metrics_utils.Reduction.SUM:\n",
            "          values = math_ops.reduce_sum(\n",
            "              values, axis=list(range(weight_ndim, ndim)))\n",
            "        else:\n",
            "          values = math_ops.reduce_mean(\n",
            "              values, axis=list(range(weight_ndim, ndim)))\n",
            "      values = math_ops.multiply(values, sample_weight)\n",
            "\n",
            "    value_sum = math_ops.reduce_sum(values)\n",
            "    with ops.control_dependencies([value_sum]):\n",
            "      update_total_op = self.total.assign_add(value_sum)\n",
            "\n",
            "    # Exit early if the reduction doesn't have a denominator.\n",
            "    if self.reduction == metrics_utils.Reduction.SUM:\n",
            "      return update_total_op\n",
            "\n",
            "    # Update `count` for reductions that require a denominator.\n",
            "    if self.reduction == metrics_utils.Reduction.SUM_OVER_BATCH_SIZE:\n",
            "      num_values = math_ops.cast(array_ops.size(values), self._dtype)\n",
            "    elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN:\n",
            "      if sample_weight is None:\n",
            "        num_values = math_ops.cast(array_ops.size(values), self._dtype)\n",
            "      else:\n",
            "        num_values = math_ops.reduce_sum(sample_weight)\n",
            "    else:\n",
            "      raise NotImplementedError(\n",
            "          'reduction [%s] not implemented' % self.reduction)\n",
            "\n",
            "    with ops.control_dependencies([update_total_op]):\n",
            "      return self.count.assign_add(num_values)\n",
            "\n",
            "  def result(self):\n",
            "    if self.reduction == metrics_utils.Reduction.SUM:\n",
            "      return array_ops.identity(self.total)\n",
            "    elif self.reduction in [\n",
            "        metrics_utils.Reduction.WEIGHTED_MEAN,\n",
            "        metrics_utils.Reduction.SUM_OVER_BATCH_SIZE\n",
            "    ]:\n",
            "      return math_ops.div_no_nan(self.total, self.count)\n",
            "    else:\n",
            "      raise NotImplementedError(\n",
            "          'reduction [%s] not implemented' % self.reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Sum')\n",
            "class Sum(Reduce):\n",
            "  \"\"\"Computes the (weighted) sum of the given values.\n",
            "\n",
            "  For example, if values is [1, 3, 5, 7] then the sum is 16.\n",
            "  If the weights were specified as [1, 1, 0, 0] then the sum would be 4.\n",
            "\n",
            "  This metric creates one variable, `total`, that is used to compute the sum of\n",
            "  `values`. This is ultimately returned as `sum`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n",
            "  to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Sum()\n",
            "  m.update_state([1, 3, 5, 7])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 16.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))\n",
            "  model.compile('sgd', loss='mse')\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='sum', dtype=None):\n",
            "    \"\"\"Creates a `Sum` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n",
            "                              name=name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Mean')\n",
            "class Mean(Reduce):\n",
            "  \"\"\"Computes the (weighted) mean of the given values.\n",
            "\n",
            "  For example, if values is [1, 3, 5, 7] then the mean is 4.\n",
            "  If the weights were specified as [1, 1, 0, 0] then the mean would be 2.\n",
            "\n",
            "  This metric creates two variables, `total` and `count` that are used to\n",
            "  compute the average of `values`. This average is ultimately returned as `mean`\n",
            "  which is an idempotent operation that simply divides `total` by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Mean()\n",
            "  m.update_state([1, 3, 5, 7])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 4.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))\n",
            "  model.compile('sgd', loss='mse')\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean', dtype=None):\n",
            "    \"\"\"Creates a `Mean` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(Mean, self).__init__(\n",
            "        reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanRelativeError')\n",
            "class MeanRelativeError(Mean):\n",
            "  \"\"\"Computes the mean relative error by normalizing with the given values.\n",
            "\n",
            "  This metric creates two local variables, `total` and `count` that are used to\n",
            "  compute the mean relative absolute error. This average is weighted by\n",
            "  `sample_weight`, and it is ultimately returned as `mean_relative_error`:\n",
            "  an idempotent operation that simply divides `total` by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])\n",
            "  m.update_state([1, 3, 2, 3], [2, 4, 6, 8])\n",
            "\n",
            "  # metric = mean(|y_pred - y_true| / normalizer)\n",
            "  #        = mean([1, 1, 4, 5] / [1, 3, 2, 3]) = mean([1, 1/3, 2, 5/3])\n",
            "  #        = 5/4 = 1.25\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.25\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    loss='mse',\n",
            "    metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, normalizer, name=None, dtype=None):\n",
            "    \"\"\"Creates a `MeanRelativeError` instance.\n",
            "\n",
            "    Args:\n",
            "      normalizer: The normalizer values with same shape as predictions.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(MeanRelativeError, self).__init__(name=name, dtype=dtype)\n",
            "    normalizer = math_ops.cast(normalizer, self._dtype)\n",
            "    self.normalizer = normalizer\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates metric statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    y_true = math_ops.cast(y_true, self._dtype)\n",
            "    y_pred = math_ops.cast(y_pred, self._dtype)\n",
            "    y_pred, y_true, sample_weight = squeeze_or_expand_dimensions(\n",
            "        y_pred, y_true, sample_weight)\n",
            "\n",
            "    y_pred, self.normalizer = confusion_matrix.remove_squeezable_dimensions(\n",
            "        y_pred, self.normalizer)\n",
            "    y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
            "    relative_errors = math_ops.div_no_nan(\n",
            "        math_ops.abs(y_true - y_pred), self.normalizer)\n",
            "\n",
            "    return super(MeanRelativeError, self).update_state(\n",
            "        relative_errors, sample_weight=sample_weight)\n",
            "\n",
            "  def get_config(self):\n",
            "    n = self.normalizer\n",
            "    config = {'normalizer': K.eval(n) if is_tensor_or_variable(n) else n}\n",
            "    base_config = super(MeanRelativeError, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class MeanMetricWrapper(Mean):\n",
            "  \"\"\"Wraps a stateless metric function with the Mean metric.\"\"\"\n",
            "\n",
            "  def __init__(self, fn, name=None, dtype=None, **kwargs):\n",
            "    \"\"\"Creates a `MeanMetricWrapper` instance.\n",
            "\n",
            "    Args:\n",
            "      fn: The metric function to wrap, with signature\n",
            "        `fn(y_true, y_pred, **kwargs)`.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      **kwargs: The keyword arguments that are passed on to `fn`.\n",
            "    \"\"\"\n",
            "    super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n",
            "    self._fn = fn\n",
            "    self._fn_kwargs = kwargs\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates metric statistics.\n",
            "\n",
            "    `y_true` and `y_pred` should have the same shape.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be\n",
            "        a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n",
            "        and must be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    y_true = math_ops.cast(y_true, self._dtype)\n",
            "    y_pred = math_ops.cast(y_pred, self._dtype)\n",
            "    y_pred, y_true, sample_weight = squeeze_or_expand_dimensions(\n",
            "        y_pred, y_true, sample_weight)\n",
            "\n",
            "    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    return super(MeanMetricWrapper, self).update_state(\n",
            "        matches, sample_weight=sample_weight)\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {}\n",
            "    for k, v in six.iteritems(self._fn_kwargs):\n",
            "      config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n",
            "    base_config = super(MeanMetricWrapper, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Accuracy')\n",
            "class Accuracy(MeanMetricWrapper):\n",
            "  \"\"\"Calculates how often predictions matches labels.\n",
            "\n",
            "  For example, if `y_true` is [1, 2, 3, 4] and `y_pred` is [0, 2, 3, 4]\n",
            "  then the accuracy is 3/4 or .75.  If the weights were specified as\n",
            "  [1, 1, 0, 0] then the accuracy would be 1/2 or .5.\n",
            "\n",
            "  This metric creates two local variables, `total` and `count` that are used to\n",
            "  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n",
            "  ultimately returned as `binary accuracy`: an idempotent operation that simply\n",
            "  divides `total` by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Accuracy()\n",
            "  m.update_state([1, 2, 3, 4], [0, 2, 3, 4])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.Accuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='accuracy', dtype=None):\n",
            "    super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.BinaryAccuracy')\n",
            "class BinaryAccuracy(MeanMetricWrapper):\n",
            "  \"\"\"Calculates how often predictions matches labels.\n",
            "\n",
            "  For example, if `y_true` is [1, 1, 0, 0] and `y_pred` is [0.98, 1, 0, 0.6]\n",
            "  then the binary accuracy is 3/4 or .75.  If the weights were specified as\n",
            "  [1, 0, 0, 1] then the binary accuracy would be 1/2 or .5.\n",
            "\n",
            "  This metric creates two local variables, `total` and `count` that are used to\n",
            "  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n",
            "  ultimately returned as `binary accuracy`: an idempotent operation that simply\n",
            "  divides `total` by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.BinaryAccuracy()\n",
            "  m.update_state([1, 1, 0, 0], [0.98, 1, 0, 0.6])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n",
            "    \"\"\"Creates a `BinaryAccuracy` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      threshold: (Optional) Float representing the threshold for deciding\n",
            "      whether prediction values are 1 or 0.\n",
            "    \"\"\"\n",
            "    super(BinaryAccuracy, self).__init__(\n",
            "        binary_accuracy, name, dtype=dtype, threshold=threshold)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.CategoricalAccuracy')\n",
            "class CategoricalAccuracy(MeanMetricWrapper):\n",
            "  \"\"\"Calculates how often predictions matches labels.\n",
            "\n",
            "  For example, if `y_true` is [[0, 0, 1], [0, 1, 0]] and `y_pred` is\n",
            "  [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n",
            "  If the weights were specified as [0.7, 0.3] then the categorical accuracy\n",
            "  would be .3. You can provide logits of classes as `y_pred`, since argmax of\n",
            "  logits and probabilities are same.\n",
            "\n",
            "  This metric creates two local variables, `total` and `count` that are used to\n",
            "  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n",
            "  ultimately returned as `categorical accuracy`: an idempotent operation that\n",
            "  simply divides `total` by `count`.\n",
            "\n",
            "  `y_pred` and `y_true` should be passed in as vectors of probabilities, rather\n",
            "  than as labels. If necessary, use `tf.one_hot` to expand `y_true` as a vector.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.CategoricalAccuracy()\n",
            "  m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.5\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    loss='mse',\n",
            "    metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='categorical_accuracy', dtype=None):\n",
            "    \"\"\"Creates a `CategoricalAccuracy` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(CategoricalAccuracy, self).__init__(\n",
            "        categorical_accuracy, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SparseCategoricalAccuracy')\n",
            "class SparseCategoricalAccuracy(MeanMetricWrapper):\n",
            "  \"\"\"Calculates how often predictions matches integer labels.\n",
            "\n",
            "  For example, if `y_true` is [[2], [1]] and `y_pred` is\n",
            "  [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n",
            "  If the weights were specified as [0.7, 0.3] then the categorical accuracy\n",
            "  would be .3. You can provide logits of classes as `y_pred`, since argmax of\n",
            "  logits and probabilities are same.\n",
            "\n",
            "  This metric creates two local variables, `total` and `count` that are used to\n",
            "  compute the frequency with which `y_pred` matches `y_true`. This frequency is\n",
            "  ultimately returned as `sparse categorical accuracy`: an idempotent operation\n",
            "  that simply divides `total` by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SparseCategoricalAccuracy()\n",
            "  m.update_state([[2], [1]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.5\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "      'sgd',\n",
            "      loss='mse',\n",
            "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n",
            "    super(SparseCategoricalAccuracy, self).__init__(\n",
            "        sparse_categorical_accuracy, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.TopKCategoricalAccuracy')\n",
            "class TopKCategoricalAccuracy(MeanMetricWrapper):\n",
            "  \"\"\"Computes how often targets are in the top `K` predictions.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.TopKCategoricalAccuracy()\n",
            "  m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n",
            "    \"\"\"Creates a `TopKCategoricalAccuracy` instance.\n",
            "\n",
            "    Args:\n",
            "      k: (Optional) Number of top elements to look at for computing accuracy.\n",
            "        Defaults to 5.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(TopKCategoricalAccuracy, self).__init__(\n",
            "        top_k_categorical_accuracy, name, dtype=dtype, k=k)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SparseTopKCategoricalAccuracy')\n",
            "class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n",
            "  \"\"\"Computes how often integer targets are in the top `K` predictions.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SparseTopKCategoricalAccuracy()\n",
            "  m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n",
            "    \"\"\"Creates a `SparseTopKCategoricalAccuracy` instance.\n",
            "\n",
            "    Args:\n",
            "      k: (Optional) Number of top elements to look at for computing accuracy.\n",
            "        Defaults to 5.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(SparseTopKCategoricalAccuracy, self).__init__(\n",
            "        sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n",
            "\n",
            "\n",
            "class _ConfusionMatrixConditionCount(Metric):\n",
            "  \"\"\"Calculates the number of the given confusion matrix condition.\"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               confusion_matrix_cond,\n",
            "               thresholds=None,\n",
            "               name=None,\n",
            "               dtype=None):\n",
            "    \"\"\"Creates a `_ConfusionMatrixConditionCount` instance.\n",
            "\n",
            "    Args:\n",
            "      confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n",
            "      thresholds: (Optional) Defaults to 0.5. A float value or a python\n",
            "        list/tuple of float threshold values in [0, 1]. A threshold is compared\n",
            "        with prediction values to determine the truth value of predictions\n",
            "        (i.e., above the threshold is `true`, below is `false`). One metric\n",
            "        value is generated for each threshold value.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(_ConfusionMatrixConditionCount, self).__init__(name=name, dtype=dtype)\n",
            "    self._confusion_matrix_cond = confusion_matrix_cond\n",
            "    self.init_thresholds = thresholds\n",
            "    self.thresholds = metrics_utils.parse_init_thresholds(\n",
            "        thresholds, default_threshold=0.5)\n",
            "    self.accumulator = self.add_weight(\n",
            "        'accumulator',\n",
            "        shape=(len(self.thresholds),),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates the given confusion matrix condition statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    return metrics_utils.update_confusion_matrix_variables(\n",
            "        {self._confusion_matrix_cond: self.accumulator},\n",
            "        y_true,\n",
            "        y_pred,\n",
            "        thresholds=self.thresholds,\n",
            "        sample_weight=sample_weight)\n",
            "\n",
            "  def result(self):\n",
            "    if len(self.thresholds) == 1:\n",
            "      result = self.accumulator[0]\n",
            "    else:\n",
            "      result = self.accumulator\n",
            "    return ops.convert_to_tensor(result)\n",
            "\n",
            "  def reset_states(self):\n",
            "    num_thresholds = len(to_list(self.thresholds))\n",
            "    K.batch_set_value(\n",
            "        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {'thresholds': self.init_thresholds}\n",
            "    base_config = super(_ConfusionMatrixConditionCount, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.FalsePositives')\n",
            "class FalsePositives(_ConfusionMatrixConditionCount):\n",
            "  \"\"\"Calculates the number of false positives.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 0, 0] and `y_pred` is [0, 0, 1, 1]\n",
            "  then the false positives value is 2.  If the weights were specified as\n",
            "  [0, 0, 1, 0] then the false positives value would be 1.\n",
            "\n",
            "  If `sample_weight` is given, calculates the sum of the weights of\n",
            "  false positives. This metric creates one local variable, `accumulator`\n",
            "  that is used to keep track of the number of false positives.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.FalsePositives()\n",
            "  m.update_state([0, 1, 0, 0], [0, 0, 1, 1])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.FalsePositives()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, thresholds=None, name=None, dtype=None):\n",
            "    \"\"\"Creates a `FalsePositives` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) Defaults to 0.5. A float value or a python\n",
            "        list/tuple of float threshold values in [0, 1]. A threshold is compared\n",
            "        with prediction values to determine the truth value of predictions\n",
            "        (i.e., above the threshold is `true`, below is `false`). One metric\n",
            "        value is generated for each threshold value.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(FalsePositives, self).__init__(\n",
            "        confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n",
            "        thresholds=thresholds,\n",
            "        name=name,\n",
            "        dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.FalseNegatives')\n",
            "class FalseNegatives(_ConfusionMatrixConditionCount):\n",
            "  \"\"\"Calculates the number of false negatives.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [0, 1, 0, 0]\n",
            "  then the false negatives value is 2.  If the weights were specified as\n",
            "  [0, 0, 1, 0] then the false negatives value would be 1.\n",
            "\n",
            "  If `sample_weight` is given, calculates the sum of the weights of\n",
            "  false negatives. This metric creates one local variable, `accumulator`\n",
            "  that is used to keep track of the number of false negatives.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.FalseNegatives()\n",
            "  m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.FalseNegatives()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, thresholds=None, name=None, dtype=None):\n",
            "    \"\"\"Creates a `FalseNegatives` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) Defaults to 0.5. A float value or a python\n",
            "        list/tuple of float threshold values in [0, 1]. A threshold is compared\n",
            "        with prediction values to determine the truth value of predictions\n",
            "        (i.e., above the threshold is `true`, below is `false`). One metric\n",
            "        value is generated for each threshold value.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(FalseNegatives, self).__init__(\n",
            "        confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n",
            "        thresholds=thresholds,\n",
            "        name=name,\n",
            "        dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.TrueNegatives')\n",
            "class TrueNegatives(_ConfusionMatrixConditionCount):\n",
            "  \"\"\"Calculates the number of true negatives.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 0, 0] and `y_pred` is [1, 1, 0, 0]\n",
            "  then the true negatives value is 2.  If the weights were specified as\n",
            "  [0, 0, 1, 0] then the true negatives value would be 1.\n",
            "\n",
            "  If `sample_weight` is given, calculates the sum of the weights of\n",
            "  true negatives. This metric creates one local variable, `accumulator`\n",
            "  that is used to keep track of the number of true negatives.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.TrueNegatives()\n",
            "  m.update_state([0, 1, 0, 0], [1, 1, 0, 0])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.TrueNegatives()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, thresholds=None, name=None, dtype=None):\n",
            "    \"\"\"Creates a `TrueNegatives` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) Defaults to 0.5. A float value or a python\n",
            "        list/tuple of float threshold values in [0, 1]. A threshold is compared\n",
            "        with prediction values to determine the truth value of predictions\n",
            "        (i.e., above the threshold is `true`, below is `false`). One metric\n",
            "        value is generated for each threshold value.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(TrueNegatives, self).__init__(\n",
            "        confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n",
            "        thresholds=thresholds,\n",
            "        name=name,\n",
            "        dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.TruePositives')\n",
            "class TruePositives(_ConfusionMatrixConditionCount):\n",
            "  \"\"\"Calculates the number of true positives.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n",
            "  then the true positives value is 2.  If the weights were specified as\n",
            "  [0, 0, 1, 0] then the true positives value would be 1.\n",
            "\n",
            "  If `sample_weight` is given, calculates the sum of the weights of\n",
            "  true positives. This metric creates one local variable, `true_positives`\n",
            "  that is used to keep track of the number of true positives.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.TruePositives()\n",
            "  m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.TruePositives()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, thresholds=None, name=None, dtype=None):\n",
            "    \"\"\"Creates a `TruePositives` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) Defaults to 0.5. A float value or a python\n",
            "        list/tuple of float threshold values in [0, 1]. A threshold is compared\n",
            "        with prediction values to determine the truth value of predictions\n",
            "        (i.e., above the threshold is `true`, below is `false`). One metric\n",
            "        value is generated for each threshold value.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(TruePositives, self).__init__(\n",
            "        confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n",
            "        thresholds=thresholds,\n",
            "        name=name,\n",
            "        dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Precision')\n",
            "class Precision(Metric):\n",
            "  \"\"\"Computes the precision of the predictions with respect to the labels.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n",
            "  then the precision value is 2/(2+1) ie. 0.66. If the weights were specified as\n",
            "  [0, 0, 1, 0] then the precision value would be 1.\n",
            "\n",
            "  The metric creates two local variables, `true_positives` and `false_positives`\n",
            "  that are used to compute the precision. This value is ultimately returned as\n",
            "  `precision`, an idempotent operation that simply divides `true_positives`\n",
            "  by the sum of `true_positives` and `false_positives`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  If `top_k` is set, we'll calculate precision as how often on average a class\n",
            "  among the top-k classes with the highest predicted values of a batch entry is\n",
            "  correct and can be found in the label for that entry.\n",
            "\n",
            "  If `class_id` is specified, we calculate precision by considering only the\n",
            "  entries in the batch for which `class_id` is above the threshold and/or in the\n",
            "  top-k highest predictions, and computing the fraction of them for which\n",
            "  `class_id` is indeed a correct label.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Precision()\n",
            "  m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.66\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.Precision()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               thresholds=None,\n",
            "               top_k=None,\n",
            "               class_id=None,\n",
            "               name=None,\n",
            "               dtype=None):\n",
            "    \"\"\"Creates a `Precision` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) A float value or a python list/tuple of float\n",
            "        threshold values in [0, 1]. A threshold is compared with prediction\n",
            "        values to determine the truth value of predictions (i.e., above the\n",
            "        threshold is `true`, below is `false`). One metric value is generated\n",
            "        for each threshold value. If neither thresholds nor top_k are set, the\n",
            "        default is to calculate precision with `thresholds=0.5`.\n",
            "      top_k: (Optional) Unset by default. An int value specifying the top-k\n",
            "        predictions to consider when calculating precision.\n",
            "      class_id: (Optional) Integer class ID for which we want binary metrics.\n",
            "        This must be in the half-open interval `[0, num_classes)`, where\n",
            "        `num_classes` is the last dimension of predictions.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(Precision, self).__init__(name=name, dtype=dtype)\n",
            "    self.init_thresholds = thresholds\n",
            "    self.top_k = top_k\n",
            "    self.class_id = class_id\n",
            "\n",
            "    default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
            "    self.thresholds = metrics_utils.parse_init_thresholds(\n",
            "        thresholds, default_threshold=default_threshold)\n",
            "    self.true_positives = self.add_weight(\n",
            "        'true_positives',\n",
            "        shape=(len(self.thresholds),),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_positives = self.add_weight(\n",
            "        'false_positives',\n",
            "        shape=(len(self.thresholds),),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates true positive and false positive statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values, with the same dimensions as `y_pred`.\n",
            "        Will be cast to `bool`.\n",
            "      y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    return metrics_utils.update_confusion_matrix_variables(\n",
            "        {\n",
            "            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
            "            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives\n",
            "        },\n",
            "        y_true,\n",
            "        y_pred,\n",
            "        thresholds=self.thresholds,\n",
            "        top_k=self.top_k,\n",
            "        class_id=self.class_id,\n",
            "        sample_weight=sample_weight)\n",
            "\n",
            "  def result(self):\n",
            "    result = math_ops.div_no_nan(self.true_positives,\n",
            "                                 self.true_positives + self.false_positives)\n",
            "    return result[0] if len(self.thresholds) == 1 else result\n",
            "\n",
            "  def reset_states(self):\n",
            "    num_thresholds = len(to_list(self.thresholds))\n",
            "    K.batch_set_value(\n",
            "        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'thresholds': self.init_thresholds,\n",
            "        'top_k': self.top_k,\n",
            "        'class_id': self.class_id\n",
            "    }\n",
            "    base_config = super(Precision, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Recall')\n",
            "class Recall(Metric):\n",
            "  \"\"\"Computes the recall of the predictions with respect to the labels.\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 1, 1] and `y_pred` is [1, 0, 1, 1]\n",
            "  then the recall value is 2/(2+1) ie. 0.66. If the weights were specified as\n",
            "  [0, 0, 1, 0] then the recall value would be 1.\n",
            "\n",
            "  This metric creates two local variables, `true_positives` and\n",
            "  `false_negatives`, that are used to compute the recall. This value is\n",
            "  ultimately returned as `recall`, an idempotent operation that simply divides\n",
            "  `true_positives` by the sum of `true_positives` and `false_negatives`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  If `top_k` is set, recall will be computed as how often on average a class\n",
            "  among the labels of a batch entry is in the top-k predictions.\n",
            "\n",
            "  If `class_id` is specified, we calculate recall by considering only the\n",
            "  entries in the batch for which `class_id` is in the label, and computing the\n",
            "  fraction of them for which `class_id` is above the threshold and/or in the\n",
            "  top-k predictions.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Recall()\n",
            "  m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.66\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.Recall()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               thresholds=None,\n",
            "               top_k=None,\n",
            "               class_id=None,\n",
            "               name=None,\n",
            "               dtype=None):\n",
            "    \"\"\"Creates a `Recall` instance.\n",
            "\n",
            "    Args:\n",
            "      thresholds: (Optional) A float value or a python list/tuple of float\n",
            "        threshold values in [0, 1]. A threshold is compared with prediction\n",
            "        values to determine the truth value of predictions (i.e., above the\n",
            "        threshold is `true`, below is `false`). One metric value is generated\n",
            "        for each threshold value. If neither thresholds nor top_k are set, the\n",
            "        default is to calculate recall with `thresholds=0.5`.\n",
            "      top_k: (Optional) Unset by default. An int value specifying the top-k\n",
            "        predictions to consider when calculating recall.\n",
            "      class_id: (Optional) Integer class ID for which we want binary metrics.\n",
            "        This must be in the half-open interval `[0, num_classes)`, where\n",
            "        `num_classes` is the last dimension of predictions.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(Recall, self).__init__(name=name, dtype=dtype)\n",
            "    self.init_thresholds = thresholds\n",
            "    self.top_k = top_k\n",
            "    self.class_id = class_id\n",
            "\n",
            "    default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
            "    self.thresholds = metrics_utils.parse_init_thresholds(\n",
            "        thresholds, default_threshold=default_threshold)\n",
            "    self.true_positives = self.add_weight(\n",
            "        'true_positives',\n",
            "        shape=(len(self.thresholds),),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_negatives = self.add_weight(\n",
            "        'false_negatives',\n",
            "        shape=(len(self.thresholds),),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates true positive and false negative statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values, with the same dimensions as `y_pred`.\n",
            "        Will be cast to `bool`.\n",
            "      y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    return metrics_utils.update_confusion_matrix_variables(\n",
            "        {\n",
            "            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
            "            metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives\n",
            "        },\n",
            "        y_true,\n",
            "        y_pred,\n",
            "        thresholds=self.thresholds,\n",
            "        top_k=self.top_k,\n",
            "        class_id=self.class_id,\n",
            "        sample_weight=sample_weight)\n",
            "\n",
            "  def result(self):\n",
            "    result = math_ops.div_no_nan(self.true_positives,\n",
            "                                 self.true_positives + self.false_negatives)\n",
            "    return result[0] if len(self.thresholds) == 1 else result\n",
            "\n",
            "  def reset_states(self):\n",
            "    num_thresholds = len(to_list(self.thresholds))\n",
            "    K.batch_set_value(\n",
            "        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'thresholds': self.init_thresholds,\n",
            "        'top_k': self.top_k,\n",
            "        'class_id': self.class_id\n",
            "    }\n",
            "    base_config = super(Recall, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@six.add_metaclass(abc.ABCMeta)\n",
            "class SensitivitySpecificityBase(Metric):\n",
            "  \"\"\"Abstract base class for computing sensitivity and specificity.\n",
            "\n",
            "  For additional information about specificity and sensitivity, see the\n",
            "  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, value, num_thresholds=200, name=None, dtype=None):\n",
            "    super(SensitivitySpecificityBase, self).__init__(name=name, dtype=dtype)\n",
            "    if num_thresholds <= 0:\n",
            "      raise ValueError('`num_thresholds` must be > 0.')\n",
            "    self.value = value\n",
            "    self.true_positives = self.add_weight(\n",
            "        'true_positives',\n",
            "        shape=(num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.true_negatives = self.add_weight(\n",
            "        'true_negatives',\n",
            "        shape=(num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_positives = self.add_weight(\n",
            "        'false_positives',\n",
            "        shape=(num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_negatives = self.add_weight(\n",
            "        'false_negatives',\n",
            "        shape=(num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "\n",
            "    # Compute `num_thresholds` thresholds in [0, 1]\n",
            "    if num_thresholds == 1:\n",
            "      self.thresholds = [0.5]\n",
            "    else:\n",
            "      thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n",
            "                    for i in range(num_thresholds - 2)]\n",
            "      self.thresholds = [0.0] + thresholds + [1.0]\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates confusion matrix statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    return metrics_utils.update_confusion_matrix_variables(\n",
            "        {\n",
            "            metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
            "            metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,\n",
            "            metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n",
            "            metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,\n",
            "        },\n",
            "        y_true,\n",
            "        y_pred,\n",
            "        thresholds=self.thresholds,\n",
            "        sample_weight=sample_weight)\n",
            "\n",
            "  def reset_states(self):\n",
            "    num_thresholds = len(self.thresholds)\n",
            "    K.batch_set_value(\n",
            "        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SensitivityAtSpecificity')\n",
            "class SensitivityAtSpecificity(SensitivitySpecificityBase):\n",
            "  \"\"\"Computes the sensitivity at a given specificity.\n",
            "\n",
            "  `Sensitivity` measures the proportion of actual positives that are correctly\n",
            "  identified as such (tp / (tp + fn)).\n",
            "  `Specificity` measures the proportion of actual negatives that are correctly\n",
            "  identified as such (tn / (tn + fp)).\n",
            "\n",
            "  This metric creates four local variables, `true_positives`, `true_negatives`,\n",
            "  `false_positives` and `false_negatives` that are used to compute the\n",
            "  sensitivity at the given specificity. The threshold for the given specificity\n",
            "  value is computed and used to evaluate the corresponding sensitivity.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  For additional information about specificity and sensitivity, see the\n",
            "  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SensitivityAtSpecificity(0.4, num_thresholds=1)\n",
            "  m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.5\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "      'sgd',\n",
            "      loss='mse',\n",
            "      metrics=[tf.keras.metrics.SensitivityAtSpecificity()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, specificity, num_thresholds=200, name=None, dtype=None):\n",
            "    \"\"\"Creates a `SensitivityAtSpecificity` instance.\n",
            "\n",
            "    Args:\n",
            "      specificity: A scalar value in range `[0, 1]`.\n",
            "      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n",
            "        use for matching the given specificity.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    if specificity < 0 or specificity > 1:\n",
            "      raise ValueError('`specificity` must be in the range [0, 1].')\n",
            "    self.specificity = specificity\n",
            "    self.num_thresholds = num_thresholds\n",
            "    super(SensitivityAtSpecificity, self).__init__(\n",
            "        specificity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n",
            "\n",
            "  def result(self):\n",
            "    # Calculate specificities at all the thresholds.\n",
            "    specificities = math_ops.div_no_nan(\n",
            "        self.true_negatives, self.true_negatives + self.false_positives)\n",
            "\n",
            "    # Find the index of the threshold where the specificity is closest to the\n",
            "    # given specificity.\n",
            "    min_index = math_ops.argmin(\n",
            "        math_ops.abs(specificities - self.value), axis=0)\n",
            "    min_index = math_ops.cast(min_index, dtypes.int32)\n",
            "\n",
            "    # Compute sensitivity at that index.\n",
            "    return math_ops.div_no_nan(\n",
            "        self.true_positives[min_index],\n",
            "        self.true_positives[min_index] + self.false_negatives[min_index])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'num_thresholds': self.num_thresholds,\n",
            "        'specificity': self.specificity\n",
            "    }\n",
            "    base_config = super(SensitivityAtSpecificity, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SpecificityAtSensitivity')\n",
            "class SpecificityAtSensitivity(SensitivitySpecificityBase):\n",
            "  \"\"\"Computes the specificity at a given sensitivity.\n",
            "\n",
            "  `Sensitivity` measures the proportion of actual positives that are correctly\n",
            "  identified as such (tp / (tp + fn)).\n",
            "  `Specificity` measures the proportion of actual negatives that are correctly\n",
            "  identified as such (tn / (tn + fp)).\n",
            "\n",
            "  This metric creates four local variables, `true_positives`, `true_negatives`,\n",
            "  `false_positives` and `false_negatives` that are used to compute the\n",
            "  specificity at the given sensitivity. The threshold for the given sensitivity\n",
            "  value is computed and used to evaluate the corresponding specificity.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  For additional information about specificity and sensitivity, see the\n",
            "  following: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SpecificityAtSensitivity(0.8, num_thresholds=1)\n",
            "  m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "      'sgd',\n",
            "      loss='mse',\n",
            "      metrics=[tf.keras.metrics.SpecificityAtSensitivity()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, sensitivity, num_thresholds=200, name=None, dtype=None):\n",
            "    \"\"\"Creates a `SpecificityAtSensitivity` instance.\n",
            "\n",
            "    Args:\n",
            "      sensitivity: A scalar value in range `[0, 1]`.\n",
            "      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n",
            "        use for matching the given specificity.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    if sensitivity < 0 or sensitivity > 1:\n",
            "      raise ValueError('`sensitivity` must be in the range [0, 1].')\n",
            "    self.sensitivity = sensitivity\n",
            "    self.num_thresholds = num_thresholds\n",
            "    super(SpecificityAtSensitivity, self).__init__(\n",
            "        sensitivity, num_thresholds=num_thresholds, name=name, dtype=dtype)\n",
            "\n",
            "  def result(self):\n",
            "    # Calculate sensitivities at all the thresholds.\n",
            "    sensitivities = math_ops.div_no_nan(\n",
            "        self.true_positives, self.true_positives + self.false_negatives)\n",
            "\n",
            "    # Find the index of the threshold where the sensitivity is closest to the\n",
            "    # given specificity.\n",
            "    min_index = math_ops.argmin(\n",
            "        math_ops.abs(sensitivities - self.value), axis=0)\n",
            "    min_index = math_ops.cast(min_index, dtypes.int32)\n",
            "\n",
            "    # Compute specificity at that index.\n",
            "    return math_ops.div_no_nan(\n",
            "        self.true_negatives[min_index],\n",
            "        self.true_negatives[min_index] + self.false_positives[min_index])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'num_thresholds': self.num_thresholds,\n",
            "        'sensitivity': self.sensitivity\n",
            "    }\n",
            "    base_config = super(SpecificityAtSensitivity, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.AUC')\n",
            "class AUC(Metric):\n",
            "  \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n",
            "\n",
            "  This metric creates four local variables, `true_positives`, `true_negatives`,\n",
            "  `false_positives` and `false_negatives` that are used to compute the AUC.\n",
            "  To discretize the AUC curve, a linearly spaced set of thresholds is used to\n",
            "  compute pairs of recall and precision values. The area under the ROC-curve is\n",
            "  therefore computed using the height of the recall values by the false positive\n",
            "  rate, while the area under the PR-curve is the computed using the height of\n",
            "  the precision values by the recall.\n",
            "\n",
            "  This value is ultimately returned as `auc`, an idempotent operation that\n",
            "  computes the area under a discretized curve of precision versus recall values\n",
            "  (computed using the aforementioned variables). The `num_thresholds` variable\n",
            "  controls the degree of discretization with larger numbers of thresholds more\n",
            "  closely approximating the true AUC. The quality of the approximation may vary\n",
            "  dramatically depending on `num_thresholds`. The `thresholds` parameter can be\n",
            "  used to manually specify thresholds which split the predictions more evenly.\n",
            "\n",
            "  For best results, `predictions` should be distributed approximately uniformly\n",
            "  in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC\n",
            "  approximation may be poor if this is not the case. Setting `summation_method`\n",
            "  to 'minoring' or 'majoring' can help quantify the error in the approximation\n",
            "  by providing lower or upper bound estimate of the AUC.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.AUC(num_thresholds=3)\n",
            "  m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n",
            "\n",
            "  # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n",
            "  # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n",
            "  # recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n",
            "  # auc = ((((1+0.5)/2)*(1-0))+ (((0.5+0)/2)*(0-0))) = 0.75\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss='mse', metrics=[tf.keras.metrics.AUC()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               num_thresholds=200,\n",
            "               curve='ROC',\n",
            "               summation_method='interpolation',\n",
            "               name=None,\n",
            "               dtype=None,\n",
            "               thresholds=None):\n",
            "    \"\"\"Creates an `AUC` instance.\n",
            "\n",
            "    Args:\n",
            "      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n",
            "        use when discretizing the roc curve. Values must be > 1.\n",
            "      curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n",
            "        [default] or 'PR' for the Precision-Recall-curve.\n",
            "      summation_method: (Optional) Specifies the Riemann summation method used\n",
            "        (https://en.wikipedia.org/wiki/Riemann_sum): 'interpolation' [default],\n",
            "          applies mid-point summation scheme for `ROC`. For PR-AUC, interpolates\n",
            "          (true/false) positives but not the ratio that is precision (see Davis\n",
            "          & Goadrich 2006 for details); 'minoring' that applies left summation\n",
            "          for increasing intervals and right summation for decreasing intervals;\n",
            "          'majoring' that does the opposite.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      thresholds: (Optional) A list of floating point values to use as the\n",
            "        thresholds for discretizing the curve. If set, the `num_thresholds`\n",
            "        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n",
            "        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n",
            "        be automatically included with these to correctly handle predictions\n",
            "        equal to exactly 0 or 1.\n",
            "    \"\"\"\n",
            "    # Validate configurations.\n",
            "    if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n",
            "        metrics_utils.AUCCurve):\n",
            "      raise ValueError('Invalid curve: \"{}\". Valid options are: \"{}\"'.format(\n",
            "          curve, list(metrics_utils.AUCCurve)))\n",
            "    if isinstance(\n",
            "        summation_method,\n",
            "        metrics_utils.AUCSummationMethod) and summation_method not in list(\n",
            "            metrics_utils.AUCSummationMethod):\n",
            "      raise ValueError(\n",
            "          'Invalid summation method: \"{}\". Valid options are: \"{}\"'.format(\n",
            "              summation_method, list(metrics_utils.AUCSummationMethod)))\n",
            "\n",
            "    # Update properties.\n",
            "    if thresholds is not None:\n",
            "      # If specified, use the supplied thresholds.\n",
            "      self.num_thresholds = len(thresholds) + 2\n",
            "      thresholds = sorted(thresholds)\n",
            "    else:\n",
            "      if num_thresholds <= 1:\n",
            "        raise ValueError('`num_thresholds` must be > 1.')\n",
            "\n",
            "      # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n",
            "      # (0, 1).\n",
            "      self.num_thresholds = num_thresholds\n",
            "      thresholds = [(i + 1) * 1.0 / (num_thresholds - 1)\n",
            "                    for i in range(num_thresholds - 2)]\n",
            "\n",
            "    # Add an endpoint \"threshold\" below zero and above one for either\n",
            "    # threshold method to account for floating point imprecisions.\n",
            "    self.thresholds = [0.0 - K.epsilon()] + thresholds + [1.0 + K.epsilon()]\n",
            "\n",
            "    if isinstance(curve, metrics_utils.AUCCurve):\n",
            "      self.curve = curve\n",
            "    else:\n",
            "      self.curve = metrics_utils.AUCCurve.from_str(curve)\n",
            "    if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n",
            "      self.summation_method = summation_method\n",
            "    else:\n",
            "      self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n",
            "          summation_method)\n",
            "    super(AUC, self).__init__(name=name, dtype=dtype)\n",
            "\n",
            "    # Create metric variables\n",
            "    self.true_positives = self.add_weight(\n",
            "        'true_positives',\n",
            "        shape=(self.num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.true_negatives = self.add_weight(\n",
            "        'true_negatives',\n",
            "        shape=(self.num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_positives = self.add_weight(\n",
            "        'false_positives',\n",
            "        shape=(self.num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "    self.false_negatives = self.add_weight(\n",
            "        'false_negatives',\n",
            "        shape=(self.num_thresholds,),\n",
            "        initializer=init_ops.zeros_initializer)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates confusion matrix statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    return metrics_utils.update_confusion_matrix_variables({\n",
            "        metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,\n",
            "        metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,\n",
            "        metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,\n",
            "        metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,\n",
            "    }, y_true, y_pred, self.thresholds, sample_weight=sample_weight)\n",
            "\n",
            "  def interpolate_pr_auc(self):\n",
            "    \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n",
            "\n",
            "    https://www.biostat.wisc.edu/~page/rocpr.pdf\n",
            "\n",
            "    Note here we derive & use a closed formula not present in the paper\n",
            "    as follows:\n",
            "\n",
            "      Precision = TP / (TP + FP) = TP / P\n",
            "\n",
            "    Modeling all of TP (true positive), FP (false positive) and their sum\n",
            "    P = TP + FP (predicted positive) as varying linearly within each interval\n",
            "    [A, B] between successive thresholds, we get\n",
            "\n",
            "      Precision slope = dTP / dP\n",
            "                      = (TP_B - TP_A) / (P_B - P_A)\n",
            "                      = (TP - TP_A) / (P - P_A)\n",
            "      Precision = (TP_A + slope * (P - P_A)) / P\n",
            "\n",
            "    The area within the interval is (slope / total_pos_weight) times\n",
            "\n",
            "      int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n",
            "      int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n",
            "\n",
            "    where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n",
            "\n",
            "      int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n",
            "\n",
            "    Bringing back the factor (slope / total_pos_weight) we'd put aside, we get\n",
            "\n",
            "      slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n",
            "\n",
            "    where dTP == TP_B - TP_A.\n",
            "\n",
            "    Note that when P_A == 0 the above calculation simplifies into\n",
            "\n",
            "      int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)\n",
            "\n",
            "    which is really equivalent to imputing constant precision throughout the\n",
            "    first bucket having >0 true positives.\n",
            "\n",
            "    Returns:\n",
            "      pr_auc: an approximation of the area under the P-R curve.\n",
            "    \"\"\"\n",
            "    dtp = self.true_positives[:self.num_thresholds -\n",
            "                              1] - self.true_positives[1:]\n",
            "    p = self.true_positives + self.false_positives\n",
            "    dp = p[:self.num_thresholds - 1] - p[1:]\n",
            "\n",
            "    prec_slope = math_ops.div_no_nan(\n",
            "        dtp, math_ops.maximum(dp, 0), name='prec_slope')\n",
            "    intercept = self.true_positives[1:] - math_ops.multiply(prec_slope, p[1:])\n",
            "\n",
            "    safe_p_ratio = array_ops.where(\n",
            "        math_ops.logical_and(p[:self.num_thresholds - 1] > 0, p[1:] > 0),\n",
            "        math_ops.div_no_nan(\n",
            "            p[:self.num_thresholds - 1],\n",
            "            math_ops.maximum(p[1:], 0),\n",
            "            name='recall_relative_ratio'),\n",
            "        array_ops.ones_like(p[1:]))\n",
            "\n",
            "    return math_ops.reduce_sum(\n",
            "        math_ops.div_no_nan(\n",
            "            prec_slope * (dtp + intercept * math_ops.log(safe_p_ratio)),\n",
            "            math_ops.maximum(self.true_positives[1:] + self.false_negatives[1:],\n",
            "                             0),\n",
            "            name='pr_auc_increment'),\n",
            "        name='interpolate_pr_auc')\n",
            "\n",
            "  def result(self):\n",
            "    if (self.curve == metrics_utils.AUCCurve.PR and\n",
            "        self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION\n",
            "       ):\n",
            "      # This use case is different and is handled separately.\n",
            "      return self.interpolate_pr_auc()\n",
            "\n",
            "    # Set `x` and `y` values for the curves based on `curve` config.\n",
            "    recall = math_ops.div_no_nan(self.true_positives,\n",
            "                                 self.true_positives + self.false_negatives)\n",
            "    if self.curve == metrics_utils.AUCCurve.ROC:\n",
            "      fp_rate = math_ops.div_no_nan(self.false_positives,\n",
            "                                    self.false_positives + self.true_negatives)\n",
            "      x = fp_rate\n",
            "      y = recall\n",
            "    else:  # curve == 'PR'.\n",
            "      precision = math_ops.div_no_nan(\n",
            "          self.true_positives, self.true_positives + self.false_positives)\n",
            "      x = recall\n",
            "      y = precision\n",
            "\n",
            "    # Find the rectangle heights based on `summation_method`.\n",
            "    if self.summation_method == metrics_utils.AUCSummationMethod.INTERPOLATION:\n",
            "      # Note: the case ('PR', 'interpolation') has been handled above.\n",
            "      heights = (y[:self.num_thresholds - 1] + y[1:]) / 2.\n",
            "    elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n",
            "      heights = math_ops.minimum(y[:self.num_thresholds - 1], y[1:])\n",
            "    else:  # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n",
            "      heights = math_ops.maximum(y[:self.num_thresholds - 1], y[1:])\n",
            "\n",
            "    # Sum up the areas of all the rectangles.\n",
            "    return math_ops.reduce_sum(\n",
            "        math_ops.multiply(x[:self.num_thresholds - 1] - x[1:], heights),\n",
            "        name=self.name)\n",
            "\n",
            "  def reset_states(self):\n",
            "    K.batch_set_value(\n",
            "        [(v, np.zeros((self.num_thresholds,))) for v in self.variables])\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'num_thresholds': self.num_thresholds,\n",
            "        'curve': self.curve.value,\n",
            "        'summation_method': self.summation_method.value,\n",
            "        # We remove the endpoint thresholds as an inverse of how the thresholds\n",
            "        # were initialized. This ensures that a metric initialized from this\n",
            "        # config has the same thresholds.\n",
            "        'thresholds': self.thresholds[1:-1],\n",
            "    }\n",
            "    base_config = super(AUC, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.CosineSimilarity')\n",
            "class CosineSimilarity(MeanMetricWrapper):\n",
            "  \"\"\"Computes the cosine similarity between the labels and predictions.\n",
            "\n",
            "  cosine similarity = (a . b) / ||a|| ||b||\n",
            "  [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
            "\n",
            "  For example, if `y_true` is [0, 1, 1], and `y_pred` is [1, 0, 1], the cosine\n",
            "  similarity is 0.5.\n",
            "\n",
            "  This metric keeps the average cosine similarity between `predictions` and\n",
            "  `labels` over a stream of data.\n",
            "\n",
            "  Usage:\n",
            "  ```python\n",
            "  m = tf.keras.metrics.CosineSimilarity(axis=1)\n",
            "  m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n",
            "  # l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]\n",
            "  # l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]\n",
            "  # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n",
            "  # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n",
            "         = ((0. + 0.) +  (0.5 + 0.5)) / 2\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.5\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "      'sgd',\n",
            "      loss='mse',\n",
            "      metrics=[tf.keras.metrics.CosineSimilarity(axis=1)])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='cosine_similarity', dtype=None, axis=-1):\n",
            "    \"\"\"Creates a `CosineSimilarity` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      axis: (Optional) Defaults to -1. The dimension along which the cosine\n",
            "        similarity is computed.\n",
            "    \"\"\"\n",
            "    super(CosineSimilarity, self).__init__(\n",
            "        cosine_similarity, name, dtype=dtype, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanAbsoluteError')\n",
            "class MeanAbsoluteError(MeanMetricWrapper):\n",
            "  \"\"\"Computes the mean absolute error between the labels and predictions.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.], and `y_pred` is [1., 1., 1., 0.]\n",
            "  the mean absolute error is 3/4 (0.75).\n",
            "\n",
            "  Usage:\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanAbsoluteError()\n",
            "  m.update_state([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean_absolute_error', dtype=None):\n",
            "    super(MeanAbsoluteError, self).__init__(\n",
            "        mean_absolute_error, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanAbsolutePercentageError')\n",
            "class MeanAbsolutePercentageError(MeanMetricWrapper):\n",
            "  \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.], and `y_pred` is [1., 1., 1., 0.]\n",
            "  the mean absolute percentage error is 5e+08.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanAbsolutePercentageError()\n",
            "  m.update_state([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 5e+08\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean_absolute_percentage_error', dtype=None):\n",
            "    super(MeanAbsolutePercentageError, self).__init__(\n",
            "        mean_absolute_percentage_error, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanSquaredError')\n",
            "class MeanSquaredError(MeanMetricWrapper):\n",
            "  \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.], and `y_pred` is [1., 1., 1., 0.]\n",
            "  the mean squared error is 3/4 (0.75).\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanSquaredError()\n",
            "  m.update_state([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.MeanSquaredError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean_squared_error', dtype=None):\n",
            "    super(MeanSquaredError, self).__init__(\n",
            "        mean_squared_error, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanSquaredLogarithmicError')\n",
            "class MeanSquaredLogarithmicError(MeanMetricWrapper):\n",
            "  \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.], and `y_pred` is [1., 1., 1., 0.]\n",
            "  the mean squared logarithmic error is 0.36034.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanSquaredLogarithmicError()\n",
            "  m.update_state([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.36034\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean_squared_logarithmic_error', dtype=None):\n",
            "    super(MeanSquaredLogarithmicError, self).__init__(\n",
            "        mean_squared_logarithmic_error, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Hinge')\n",
            "class Hinge(MeanMetricWrapper):\n",
            "  \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n",
            "\n",
            "  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
            "  provided we will convert them to -1 or 1.\n",
            "\n",
            "  For example, if `y_true` is [-1., 1., 1.], and `y_pred` is [0.6, -0.7, -0.5]\n",
            "  the hinge metric value is 1.6.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Hinge()\n",
            "  m.update_state([-1., 1., 1.], [0.6, -0.7, -0.5])\n",
            "\n",
            "  # result = max(0, 1-y_true * y_pred) = [1.6 + 1.7 + 1.5] / 3\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.6\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.Hinge()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='hinge', dtype=None):\n",
            "    super(Hinge, self).__init__(hinge, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SquaredHinge')\n",
            "class SquaredHinge(MeanMetricWrapper):\n",
            "  \"\"\"Computes the squared hinge metric between `y_true` and `y_pred`.\n",
            "\n",
            "  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
            "  provided we will convert them to -1 or 1.\n",
            "\n",
            "  For example, if `y_true` is [-1., 1., 1.], and `y_pred` is [0.6, -0.7, -0.5]\n",
            "  the squared hinge metric value is 2.6.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SquaredHinge()\n",
            "  m.update_state([-1., 1., 1.], [0.6, -0.7, -0.5])\n",
            "\n",
            "  # result = max(0, 1-y_true * y_pred) = [1.6^2 + 1.7^2 + 1.5^2] / 3\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2.6\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.SquaredHinge()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='squared_hinge', dtype=None):\n",
            "    super(SquaredHinge, self).__init__(squared_hinge, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.CategoricalHinge')\n",
            "class CategoricalHinge(MeanMetricWrapper):\n",
            "  \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 1., 1.], and `y_pred` is [1., 0., 1.]\n",
            "  the categorical hinge metric value is 1.0.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.CategoricalHinge()\n",
            "  m.update_state([0., 1., 1.], [1., 0., 1.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.CategoricalHinge()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='categorical_hinge', dtype=None):\n",
            "    super(CategoricalHinge, self).__init__(categorical_hinge, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.RootMeanSquaredError')\n",
            "class RootMeanSquaredError(Mean):\n",
            "  \"\"\"Computes root mean squared error metric between `y_true` and `y_pred`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.RootMeanSquaredError()\n",
            "  m.update_state([2., 4., 6.], [1., 3., 2.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 2.449\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='root_mean_squared_error', dtype=None):\n",
            "    super(RootMeanSquaredError, self).__init__(name, dtype=dtype)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates root mean squared error statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    y_true = math_ops.cast(y_true, self._dtype)\n",
            "    y_pred = math_ops.cast(y_pred, self._dtype)\n",
            "    y_pred, y_true, sample_weight = squeeze_or_expand_dimensions(\n",
            "        y_pred, y_true, sample_weight)\n",
            "    error_sq = math_ops.squared_difference(y_pred, y_true)\n",
            "    return super(RootMeanSquaredError, self).update_state(\n",
            "        error_sq, sample_weight=sample_weight)\n",
            "\n",
            "  def result(self):\n",
            "    return math_ops.sqrt(math_ops.div_no_nan(self.total, self.count))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.LogCoshError')\n",
            "class LogCoshError(MeanMetricWrapper):\n",
            "  \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
            "\n",
            "  `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred - y_true)\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.LogCoshError()\n",
            "  m.update_state([0., 1., 1.], [1., 0., 1.])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.289\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.LogCoshError()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='logcosh', dtype=None):\n",
            "    super(LogCoshError, self).__init__(logcosh, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.Poisson')\n",
            "class Poisson(MeanMetricWrapper):\n",
            "  \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n",
            "\n",
            "  `metric = y_pred - y_true * log(y_pred)`\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.Poisson()\n",
            "  m.update_state([1, 9, 2], [4, 8, 12])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: -4.63\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.Poisson()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='poisson', dtype=None):\n",
            "    super(Poisson, self).__init__(poisson, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.KLDivergence')\n",
            "class KLDivergence(MeanMetricWrapper):\n",
            "  \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and `y_pred`.\n",
            "\n",
            "  `metric = y_true * log(y_true / y_pred)`\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.KLDivergence()\n",
            "  m.update_state([.4, .9, .2], [.5, .8, .12])\n",
            "  print('Final result: ', m.result().numpy())  # Final result: -0.043\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', metrics=[tf.keras.metrics.KLDivergence()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='kullback_leibler_divergence', dtype=None):\n",
            "    super(KLDivergence, self).__init__(\n",
            "        kullback_leibler_divergence, name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanIoU')\n",
            "class MeanIoU(Metric):\n",
            "  \"\"\"Computes the mean Intersection-Over-Union metric.\n",
            "\n",
            "  Mean Intersection-Over-Union is a common evaluation metric for semantic image\n",
            "  segmentation, which first computes the IOU for each semantic class and then\n",
            "  computes the average over classes. IOU is defined as follows:\n",
            "    IOU = true_positive / (true_positive + false_positive + false_negative).\n",
            "  The predictions are accumulated in a confusion matrix, weighted by\n",
            "  `sample_weight` and the metric is then calculated from it.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use `sample_weight` of 0 to mask values.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanIoU(num_classes=2)\n",
            "  m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n",
            "\n",
            "    # cm = [[1, 1],\n",
            "            [1, 1]]\n",
            "    # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n",
            "    # iou = true_positives / (sum_row + sum_col - true_positives))\n",
            "    # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 0.33\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    loss='mse',\n",
            "    metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, num_classes, name=None, dtype=None):\n",
            "    \"\"\"Creates a `MeanIoU` instance.\n",
            "\n",
            "    Args:\n",
            "      num_classes: The possible number of labels the prediction task can have.\n",
            "        This value must be provided, since a confusion matrix of dimension =\n",
            "        [num_classes, num_classes] will be allocated.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(MeanIoU, self).__init__(name=name, dtype=dtype)\n",
            "    self.num_classes = num_classes\n",
            "\n",
            "    # Variable to accumulate the predictions in the confusion matrix. Setting\n",
            "    # the type to be `float64` as required by confusion_matrix_ops.\n",
            "    self.total_cm = self.add_weight(\n",
            "        'total_confusion_matrix',\n",
            "        shape=(num_classes, num_classes),\n",
            "        initializer=init_ops.zeros_initializer,\n",
            "        dtype=dtypes.float64)\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Accumulates the confusion matrix statistics.\n",
            "\n",
            "    Args:\n",
            "      y_true: The ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
            "        `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
            "        be broadcastable to `y_true`.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "\n",
            "    y_true = math_ops.cast(y_true, self._dtype)\n",
            "    y_pred = math_ops.cast(y_pred, self._dtype)\n",
            "\n",
            "    # Flatten the input if its rank > 1.\n",
            "    if y_pred.shape.ndims > 1:\n",
            "      y_pred = array_ops.reshape(y_pred, [-1])\n",
            "\n",
            "    if y_true.shape.ndims > 1:\n",
            "      y_true = array_ops.reshape(y_true, [-1])\n",
            "\n",
            "    if sample_weight is not None and sample_weight.shape.ndims > 1:\n",
            "      sample_weight = array_ops.reshape(sample_weight, [-1])\n",
            "\n",
            "    # Accumulate the prediction to current confusion matrix.\n",
            "    current_cm = confusion_matrix.confusion_matrix(\n",
            "        y_true,\n",
            "        y_pred,\n",
            "        self.num_classes,\n",
            "        weights=sample_weight,\n",
            "        dtype=dtypes.float64)\n",
            "    return self.total_cm.assign_add(current_cm)\n",
            "\n",
            "  def result(self):\n",
            "    \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n",
            "    sum_over_row = math_ops.cast(\n",
            "        math_ops.reduce_sum(self.total_cm, axis=0), dtype=self._dtype)\n",
            "    sum_over_col = math_ops.cast(\n",
            "        math_ops.reduce_sum(self.total_cm, axis=1), dtype=self._dtype)\n",
            "    true_positives = math_ops.cast(\n",
            "        array_ops.diag_part(self.total_cm), dtype=self._dtype)\n",
            "\n",
            "    # sum_over_row + sum_over_col =\n",
            "    #     2 * true_positives + false_positives + false_negatives.\n",
            "    denominator = sum_over_row + sum_over_col - true_positives\n",
            "\n",
            "    # The mean is only computed over classes that appear in the\n",
            "    # label or prediction tensor. If the denominator is 0, we need to\n",
            "    # ignore the class.\n",
            "    num_valid_entries = math_ops.reduce_sum(\n",
            "        math_ops.cast(math_ops.not_equal(denominator, 0), dtype=self._dtype))\n",
            "\n",
            "    iou = math_ops.div_no_nan(true_positives, denominator)\n",
            "\n",
            "    return math_ops.div_no_nan(\n",
            "        math_ops.reduce_sum(iou, name='mean_iou'), num_valid_entries)\n",
            "\n",
            "  def reset_states(self):\n",
            "    K.set_value(self.total_cm, np.zeros((self.num_classes, self.num_classes)))\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {'num_classes': self.num_classes}\n",
            "    base_config = super(MeanIoU, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.MeanTensor')\n",
            "class MeanTensor(Metric):\n",
            "  \"\"\"Computes the element-wise (weighted) mean of the given tensors.\n",
            "\n",
            "  `MeanTensor` returns a tensor with the same shape of the input tensors. The\n",
            "  mean value is updated by keeping local variables `total` and `count`. The\n",
            "  `total` tracks the sum of the weighted values, and `count` stores the sum of\n",
            "  the weighted counts.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.MeanTensor()\n",
            "  m.update_state([0, 1, 2, 3])\n",
            "  m.update_state([4, 5, 6, 7])\n",
            "  print('Result: ', m.result().numpy())  # Result: [2, 3, 4, 5]\n",
            "  m.update_state([12, 10, 8, 6], sample_weights= [0, 0.2, 0.5, 1])\n",
            "  print('Result: ', m.result().numpy())  # Result: [2, 3.636, 4.8, 5.333]\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='mean_tensor', dtype=None):\n",
            "    \"\"\"Creates a `MeanTensor` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "    \"\"\"\n",
            "    super(MeanTensor, self).__init__(name=name, dtype=dtype)\n",
            "    self._shape = None\n",
            "    self._total = None\n",
            "    self._count = None\n",
            "    self._built = False\n",
            "\n",
            "  def _build(self, shape):\n",
            "    self._shape = tensor_shape.TensorShape(shape)\n",
            "    # Create new state variables\n",
            "    self._total = self.add_weight(\n",
            "        'total', shape=shape, initializer=init_ops.zeros_initializer)\n",
            "    self._count = self.add_weight(\n",
            "        'count', shape=shape, initializer=init_ops.zeros_initializer)\n",
            "    with ops.init_scope():\n",
            "      if not context.executing_eagerly():\n",
            "        K._initialize_variables(K._get_session())  # pylint: disable=protected-access\n",
            "    self._built = True\n",
            "\n",
            "  @property\n",
            "  def total(self):\n",
            "    return self._total if self._built else None\n",
            "\n",
            "  @property\n",
            "  def count(self):\n",
            "    return self._count if self._built else None\n",
            "\n",
            "  def update_state(self, values, sample_weight=None):\n",
            "    \"\"\"Accumulates statistics for computing the element-wise mean.\n",
            "\n",
            "    Args:\n",
            "      values: Per-example value.\n",
            "      sample_weight: Optional weighting of each example. Defaults to 1.\n",
            "\n",
            "    Returns:\n",
            "      Update op.\n",
            "    \"\"\"\n",
            "    values = math_ops.cast(values, self._dtype)\n",
            "    if not self._built:\n",
            "      self._build(values.shape)\n",
            "    elif values.shape != self._shape:\n",
            "      raise ValueError('MeanTensor input values must always have the same '\n",
            "                       'shape. Expected shape (set during the first call): {}. '\n",
            "                       'Got: {}'.format(self._shape, values.shape))\n",
            "\n",
            "    num_values = array_ops.ones_like(values)\n",
            "    if sample_weight is not None:\n",
            "      sample_weight = math_ops.cast(sample_weight, self._dtype)\n",
            "\n",
            "      # Update dimensions of weights to match with values if possible.\n",
            "      values, _, sample_weight = squeeze_or_expand_dimensions(\n",
            "          values, None, sample_weight)\n",
            "      try:\n",
            "        # Broadcast weights if possible.\n",
            "        sample_weight = weights_broadcast_ops.broadcast_weights(\n",
            "            sample_weight, values)\n",
            "      except ValueError:\n",
            "        # Reduce values to same ndim as weight array\n",
            "        ndim = K.ndim(values)\n",
            "        weight_ndim = K.ndim(sample_weight)\n",
            "        values = math_ops.reduce_mean(\n",
            "            values, axis=list(range(weight_ndim, ndim)))\n",
            "\n",
            "      num_values = math_ops.multiply(num_values, sample_weight)\n",
            "      values = math_ops.multiply(values, sample_weight)\n",
            "\n",
            "    update_total_op = self._total.assign_add(values)\n",
            "    with ops.control_dependencies([update_total_op]):\n",
            "      return self._count.assign_add(num_values)\n",
            "\n",
            "  def result(self):\n",
            "    if not self._built:\n",
            "      raise ValueError(\n",
            "          'MeanTensor does not have any result yet. Please call the MeanTensor '\n",
            "          'instance or use `.update_state(value)` before retrieving the result.'\n",
            "          )\n",
            "    return math_ops.div_no_nan(self.total, self.count)\n",
            "\n",
            "  def reset_states(self):\n",
            "    if self._built:\n",
            "      K.batch_set_value(\n",
            "          [(v, np.zeros(self._shape.as_list())) for v in self.variables])\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.BinaryCrossentropy')\n",
            "class BinaryCrossentropy(MeanMetricWrapper):\n",
            "  \"\"\"Computes the crossentropy metric between the labels and predictions.\n",
            "\n",
            "  This is the crossentropy metric class to be used when there are only two\n",
            "  label classes (0 and 1).\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.BinaryCrossentropy()\n",
            "  m.update_state([1., 0., 1., 0.], [1., 1., 1., 0.])\n",
            "\n",
            "  # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n",
            "  # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n",
            "  # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n",
            "\n",
            "  # Metric = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n",
            "  #        = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n",
            "  #           -log(Y_MAX + EPSILON), -log(1)]\n",
            "  #        = [(0 + 15.33) / 2, (0 + 0) / 2]\n",
            "  # Reduced metric = 7.665 / 2\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 3.833\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "      'sgd',\n",
            "      loss='mse',\n",
            "      metrics=[tf.keras.metrics.BinaryCrossentropy()])\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               name='binary_crossentropy',\n",
            "               dtype=None,\n",
            "               from_logits=False,\n",
            "               label_smoothing=0):\n",
            "    \"\"\"Creates a `BinaryCrossentropy` instance.\n",
            "\n",
            "    Args:\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      from_logits: (Optional )Whether output is expected to be a logits tensor.\n",
            "        By default, we consider that output encodes a probability distribution.\n",
            "      label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are\n",
            "        smoothed, meaning the confidence on label values are relaxed.\n",
            "        e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for\n",
            "        label `0` and `0.9` for label `1`\"\n",
            "    \"\"\"\n",
            "\n",
            "    super(BinaryCrossentropy, self).__init__(\n",
            "        binary_crossentropy,\n",
            "        name,\n",
            "        dtype=dtype,\n",
            "        from_logits=from_logits,\n",
            "        label_smoothing=label_smoothing)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.CategoricalCrossentropy')\n",
            "class CategoricalCrossentropy(MeanMetricWrapper):\n",
            "  \"\"\"Computes the crossentropy metric between the labels and predictions.\n",
            "\n",
            "  This is the crossentropy metric class to be used when there are multiple\n",
            "  label classes (2 or more). Here we assume that labels are given as a `one_hot`\n",
            "  representation. eg., When labels values are [2, 0, 1],\n",
            "   `y_true` = [[0, 0, 1], [1, 0, 0], [0, 1, 0]].\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.CategoricalCrossentropy()\n",
            "  m.update_state([[0, 1, 0], [0, 0, 1]],\n",
            "                 [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
            "\n",
            "  # EPSILON = 1e-7, y = y_true, y` = y_pred\n",
            "  # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n",
            "  # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n",
            "\n",
            "  # xent = -sum(y * log(y'), axis = -1)\n",
            "  #      = -((log 0.95), (log 0.1))\n",
            "  #      = [0.051, 2.302]\n",
            "  # Reduced xent = (0.051 + 2.302) / 2\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.176\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    loss='mse',\n",
            "    metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    name: (Optional) string name of the metric instance.\n",
            "    dtype: (Optional) data type of the metric result.\n",
            "    from_logits: (Optional ) Whether `y_pred` is expected to be a logits tensor.\n",
            "      By default, we assume that `y_pred` encodes a probability distribution.\n",
            "    label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n",
            "      meaning the confidence on label values are relaxed. e.g.\n",
            "      `label_smoothing=0.2` means that we will use a value of `0.1` for label\n",
            "      `0` and `0.9` for label `1`\"\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               name='categorical_crossentropy',\n",
            "               dtype=None,\n",
            "               from_logits=False,\n",
            "               label_smoothing=0):\n",
            "\n",
            "    super(CategoricalCrossentropy, self).__init__(\n",
            "        categorical_crossentropy,\n",
            "        name,\n",
            "        dtype=dtype,\n",
            "        from_logits=from_logits,\n",
            "        label_smoothing=label_smoothing)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.SparseCategoricalCrossentropy')\n",
            "class SparseCategoricalCrossentropy(MeanMetricWrapper):\n",
            "  \"\"\"Computes the crossentropy metric between the labels and predictions.\n",
            "\n",
            "  Use this crossentropy metric when there are two or more label classes.\n",
            "  We expect labels to be provided as integers. If you want to provide labels\n",
            "  using `one-hot` representation, please use `CategoricalCrossentropy` metric.\n",
            "  There should be `# classes` floating point values per feature for `y_pred`\n",
            "  and a single floating point value per feature for `y_true`.\n",
            "\n",
            "  In the snippet below, there is a single floating point value per example for\n",
            "  `y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
            "  The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
            "  `[batch_size, num_classes]`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  m = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
            "  m.update_state(\n",
            "    [1, 2],\n",
            "    [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
            "\n",
            "  # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n",
            "  # logits = log(y_pred)\n",
            "  # softmax = exp(logits) / sum(exp(logits), axis=-1)\n",
            "  # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n",
            "\n",
            "  # xent = -sum(y * log(softmax), 1)\n",
            "  # log(softmax) = [[-2.9957, -0.0513, -16.1181], [-2.3026, -0.2231, -2.3026]]\n",
            "  # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n",
            "\n",
            "  # xent = [0.0513, 2.3026]\n",
            "  # Reduced xent = (0.0513 + 2.3026) / 2\n",
            "\n",
            "  print('Final result: ', m.result().numpy())  # Final result: 1.176\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile(\n",
            "    'sgd',\n",
            "    loss='mse',\n",
            "    metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    name: (Optional) string name of the metric instance.\n",
            "    dtype: (Optional) data type of the metric result.\n",
            "    from_logits: (Optional ) Whether `y_pred` is expected to be a logits tensor.\n",
            "      By default, we assume that `y_pred` encodes a probability distribution.\n",
            "    axis: (Optional) Defaults to -1. The dimension along which the metric is\n",
            "      computed.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               name='sparse_categorical_crossentropy',\n",
            "               dtype=None,\n",
            "               from_logits=False,\n",
            "               axis=-1):\n",
            "\n",
            "    super(SparseCategoricalCrossentropy, self).__init__(\n",
            "        sparse_categorical_crossentropy,\n",
            "        name,\n",
            "        dtype=dtype,\n",
            "        from_logits=from_logits,\n",
            "        axis=axis)\n",
            "\n",
            "\n",
            "class SumOverBatchSize(Reduce):\n",
            "  \"\"\"Computes the weighted sum over batch size of the given values.\n",
            "\n",
            "  For example, if values is [1, 3, 5, 7] then the metric value is 4.\n",
            "  If the weights were specified as [1, 1, 0, 0] then the value would be 1.\n",
            "\n",
            "  This metric creates two variables, `total` and `count` that are used to\n",
            "  compute the average of `values`. This average is ultimately returned as sum\n",
            "  over batch size which is an idempotent operation that simply divides `total`\n",
            "  by `count`.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.  Use `sample_weight` of 0\n",
            "  to mask values.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, name='sum_over_batch_size', dtype=None):\n",
            "    super(SumOverBatchSize, self).__init__(\n",
            "        reduction=metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n",
            "        name=name,\n",
            "        dtype=dtype)\n",
            "\n",
            "\n",
            "class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n",
            "  \"\"\"Wraps a function with the `SumOverBatchSizeMetricWrapper` metric.\"\"\"\n",
            "\n",
            "  def __init__(self, fn, name=None, dtype=None, **kwargs):\n",
            "    \"\"\"Creates a `SumOverBatchSizeMetricWrapper` instance.\n",
            "\n",
            "    Args:\n",
            "      fn: The metric function to wrap, with signature `fn(y_true, y_pred,\n",
            "        **kwargs)`.\n",
            "      name: (Optional) string name of the metric instance.\n",
            "      dtype: (Optional) data type of the metric result.\n",
            "      **kwargs: The keyword arguments that are passed on to `fn`.\n",
            "    \"\"\"\n",
            "    super(SumOverBatchSizeMetricWrapper, self).__init__(name=name, dtype=dtype)\n",
            "    self._fn = fn\n",
            "    self._fn_kwargs = kwargs\n",
            "\n",
            "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
            "    y_true = math_ops.cast(y_true, self._dtype)\n",
            "    y_pred = math_ops.cast(y_pred, self._dtype)\n",
            "    y_pred, y_true, sample_weight = squeeze_or_expand_dimensions(\n",
            "        y_pred, y_true, sample_weight)\n",
            "\n",
            "    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    return super(SumOverBatchSizeMetricWrapper, self).update_state(\n",
            "        matches, sample_weight=sample_weight)\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {}\n",
            "    for k, v in six.iteritems(self._fn_kwargs):\n",
            "      config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n",
            "    base_config = super(SumOverBatchSizeMetricWrapper, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "def accuracy(y_true, y_pred):\n",
            "  y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
            "  if y_true.dtype != y_pred.dtype:\n",
            "    y_pred = math_ops.cast(y_pred, y_true.dtype)\n",
            "  return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.binary_accuracy')\n",
            "def binary_accuracy(y_true, y_pred, threshold=0.5):\n",
            "  threshold = math_ops.cast(threshold, y_pred.dtype)\n",
            "  y_pred = math_ops.cast(y_pred > threshold, y_pred.dtype)\n",
            "  return K.mean(math_ops.equal(y_true, y_pred), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.categorical_accuracy')\n",
            "def categorical_accuracy(y_true, y_pred):\n",
            "  return math_ops.cast(\n",
            "      math_ops.equal(\n",
            "          math_ops.argmax(y_true, axis=-1), math_ops.argmax(y_pred, axis=-1)),\n",
            "      K.floatx())\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.sparse_categorical_accuracy')\n",
            "def sparse_categorical_accuracy(y_true, y_pred):\n",
            "  y_pred_rank = ops.convert_to_tensor(y_pred).shape.ndims\n",
            "  y_true_rank = ops.convert_to_tensor(y_true).shape.ndims\n",
            "  # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
            "  if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n",
            "      K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
            "    y_true = array_ops.squeeze(y_true, [-1])\n",
            "  y_pred = math_ops.argmax(y_pred, axis=-1)\n",
            "\n",
            "  # If the predicted output and actual output types don't match, force cast them\n",
            "  # to match.\n",
            "  if K.dtype(y_pred) != K.dtype(y_true):\n",
            "    y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
            "\n",
            "  return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.top_k_categorical_accuracy')\n",
            "def top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
            "  return math_ops.cast(\n",
            "      nn.in_top_k(y_pred, math_ops.argmax(y_true, axis=-1), k), K.floatx())\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.sparse_top_k_categorical_accuracy')\n",
            "def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
            "  y_pred_rank = ops.convert_to_tensor(y_pred).shape.ndims\n",
            "  y_true_rank = ops.convert_to_tensor(y_true).shape.ndims\n",
            "  # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
            "  if (y_true_rank is not None) and (y_pred_rank is not None) and (len(\n",
            "      K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
            "    y_true = array_ops.squeeze(y_true, [-1])\n",
            "\n",
            "  return math_ops.cast(\n",
            "      nn.in_top_k(y_pred, math_ops.cast(y_true, 'int32'), k), K.floatx())\n",
            "\n",
            "# Aliases\n",
            "\n",
            "mse = MSE = mean_squared_error\n",
            "mae = MAE = mean_absolute_error\n",
            "mape = MAPE = mean_absolute_percentage_error\n",
            "msle = MSLE = mean_squared_logarithmic_error\n",
            "cosine_proximity = cosine_similarity\n",
            "\n",
            "\n",
            "def clone_metric(metric):\n",
            "  \"\"\"Returns a clone of the metric if stateful, otherwise returns it as is.\"\"\"\n",
            "  if isinstance(metric, Metric):\n",
            "    return metric.__class__.from_config(metric.get_config())\n",
            "  return metric\n",
            "\n",
            "\n",
            "def clone_metrics(metrics):\n",
            "  \"\"\"Clones the given metric list/dict.\"\"\"\n",
            "  if metrics is None:\n",
            "    return None\n",
            "  if isinstance(metrics, dict):\n",
            "    return {key: clone_metric(value) for key, value in metrics.items()}\n",
            "  return [clone_metric(metric) for metric in metrics]\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.serialize')\n",
            "def serialize(metric):\n",
            "  return serialize_keras_object(metric)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.deserialize')\n",
            "def deserialize(config, custom_objects=None):\n",
            "  return deserialize_keras_object(\n",
            "      config,\n",
            "      module_objects=globals(),\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='metric function')\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.get')\n",
            "def get(identifier):\n",
            "  if isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    return deserialize(str(identifier))\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret '\n",
            "                     'metric function identifier: %s' % identifier)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Built-in activation functions.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import nn\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "# b/123041942\n",
            "# In TF 2.x, if the `tf.nn.softmax` is used as an activation function in Keras\n",
            "# layers, it gets serialized as 'softmax_v2' instead of 'softmax' as the\n",
            "# internal method name is returned in serialization. This results in errors in\n",
            "# model exporting and loading as Keras can't find any activation function with\n",
            "# the name of `softmax_v2`.\n",
            "\n",
            "# This dict maps the activation function name from its v2 version to its\n",
            "# canonical name.\n",
            "_TF_ACTIVATIONS_V2 = {\n",
            "    'softmax_v2': 'softmax',\n",
            "}\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.softmax')\n",
            "def softmax(x, axis=-1):\n",
            "  \"\"\"The softmax activation function transforms the outputs so that all values are in\n",
            "\n",
            "  range (0, 1) and sum to 1. It is often used as the activation for the last\n",
            "  layer of a classification network because the result could be interpreted as\n",
            "  a probability distribution. The softmax of x is calculated by\n",
            "  exp(x)/tf.reduce_sum(exp(x)).\n",
            "\n",
            "  Arguments:\n",
            "      x : Input tensor.\n",
            "      axis: Integer, axis along which the softmax normalization is applied.\n",
            "\n",
            "  Returns:\n",
            "      Tensor, output of softmax transformation (all values are non-negative\n",
            "        and sum to 1).\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In case `dim(x) == 1`.\n",
            "  \"\"\"\n",
            "  ndim = K.ndim(x)\n",
            "  if ndim == 2:\n",
            "    return nn.softmax(x)\n",
            "  elif ndim > 2:\n",
            "    e = math_ops.exp(x - math_ops.reduce_max(x, axis=axis, keepdims=True))\n",
            "    s = math_ops.reduce_sum(e, axis=axis, keepdims=True)\n",
            "    return e / s\n",
            "  else:\n",
            "    raise ValueError('Cannot apply softmax to a tensor that is 1D. '\n",
            "                     'Received input: %s' % (x,))\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.elu')\n",
            "def elu(x, alpha=1.0):\n",
            "  \"\"\"Exponential linear unit.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "      alpha: A scalar, slope of negative section.\n",
            "\n",
            "  Returns:\n",
            "      The exponential linear activation: `x` if `x > 0` and\n",
            "        `alpha * (exp(x)-1)` if `x < 0`.\n",
            "\n",
            "  Reference:\n",
            "      - [Fast and Accurate Deep Network Learning by Exponential\n",
            "        Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)\n",
            "  \"\"\"\n",
            "  return K.elu(x, alpha)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.selu')\n",
            "def selu(x):\n",
            "  \"\"\"Scaled Exponential Linear Unit (SELU).\n",
            "\n",
            "  The Scaled Exponential Linear Unit (SELU) activation function is:\n",
            "  `scale * x` if `x > 0` and `scale * alpha * (exp(x) - 1)` if `x < 0`\n",
            "  where `alpha` and `scale` are pre-defined constants\n",
            "  (`alpha = 1.67326324`\n",
            "  and `scale = 1.05070098`).\n",
            "  The SELU activation function multiplies  `scale` > 1 with the\n",
            "  `[elu](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu)`\n",
            "  (Exponential Linear Unit (ELU)) to ensure a slope larger than one\n",
            "  for positive net inputs.\n",
            "\n",
            "  The values of `alpha` and `scale` are\n",
            "  chosen so that the mean and variance of the inputs are preserved\n",
            "  between two consecutive layers as long as the weights are initialized\n",
            "  correctly (see [`lecun_normal` initialization]\n",
            "  (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal))\n",
            "  and the number of inputs is \"large enough\"\n",
            "  (see references for more information).\n",
            "\n",
            "  ![](https://cdn-images-1.medium.com/max/1600/1*m0e8lZU_Zrkh4ESfQkY2Pw.png)\n",
            "  (Courtesy: Blog on Towards DataScience at\n",
            "  https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9)\n",
            "\n",
            "  Example Usage:\n",
            "  ```python3\n",
            "  n_classes = 10 #10-class problem\n",
            "  model = models.Sequential()\n",
            "  model.add(Dense(64, kernel_initializer='lecun_normal', activation='selu',\n",
            "  input_shape=(28, 28, 1))))\n",
            "  model.add(Dense(32, kernel_initializer='lecun_normal', activation='selu'))\n",
            "  model.add(Dense(16, kernel_initializer='lecun_normal', activation='selu'))\n",
            "  model.add(Dense(n_classes, activation='softmax'))\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable to compute the activation function for.\n",
            "\n",
            "  Returns:\n",
            "      The scaled exponential unit activation: `scale * elu(x, alpha)`.\n",
            "\n",
            "  # Note\n",
            "      - To be used together with the initialization \"[lecun_normal]\n",
            "      (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)\".\n",
            "      - To be used together with the dropout variant \"[AlphaDropout]\n",
            "      (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout)\".\n",
            "\n",
            "  References:\n",
            "      [Self-Normalizing Neural Networks (Klambauer et al, 2017)]\n",
            "      (https://arxiv.org/abs/1706.02515)\n",
            "  \"\"\"\n",
            "  alpha = 1.6732632423543772848170429916717\n",
            "  scale = 1.0507009873554804934193349852946\n",
            "  return scale * K.elu(x, alpha)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.softplus')\n",
            "def softplus(x):\n",
            "  \"\"\"Softplus activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The softplus activation: `log(exp(x) + 1)`.\n",
            "  \"\"\"\n",
            "  return nn.softplus(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.softsign')\n",
            "def softsign(x):\n",
            "  \"\"\"Softsign activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The softplus activation: `x / (abs(x) + 1)`.\n",
            "  \"\"\"\n",
            "  return nn.softsign(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.relu')\n",
            "def relu(x, alpha=0., max_value=None, threshold=0):\n",
            "  \"\"\"Rectified Linear Unit.\n",
            "\n",
            "  With default values, it returns element-wise `max(x, 0)`.\n",
            "\n",
            "  Otherwise, it follows:\n",
            "  `f(x) = max_value` for `x >= max_value`,\n",
            "  `f(x) = x` for `threshold <= x < max_value`,\n",
            "  `f(x) = alpha * (x - threshold)` otherwise.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      alpha: A scalar, slope of negative section (default=`0.`).\n",
            "      max_value: float. Saturation threshold.\n",
            "      threshold: float. Threshold value for thresholded activation.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return K.relu(x, alpha=alpha, max_value=max_value, threshold=threshold)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.tanh')\n",
            "def tanh(x):\n",
            "  \"\"\"Hyperbolic Tangent activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The tanh activation: `tanh(x) = sinh(x)/cosh(x) = ((exp(x) -\n",
            "      exp(-x))/(exp(x) + exp(-x)))`.\n",
            "  \"\"\"\n",
            "  return nn.tanh(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.sigmoid')\n",
            "def sigmoid(x):\n",
            "  \"\"\"Sigmoid.\n",
            "\n",
            "  Applies the sigmoid activation function. The sigmoid function is defined as\n",
            "  1 divided by (1 + exp(-x)). It's curve is like an \"S\" and is like a smoothed\n",
            "  version of the Heaviside (Unit Step Function) function. For small values\n",
            "  (<-5) the sigmoid returns a value close to zero and for larger values (>5)\n",
            "  the result of the function gets close to 1.\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  Sigmoid activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The sigmoid activation: `(1.0 / (1.0 + exp(-x)))`.\n",
            "  \"\"\"\n",
            "  return nn.sigmoid(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.exponential')\n",
            "def exponential(x):\n",
            "  \"\"\"Exponential activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The exponential activation: `exp(x)`.\n",
            "  \"\"\"\n",
            "  return math_ops.exp(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.hard_sigmoid')\n",
            "def hard_sigmoid(x):\n",
            "  \"\"\"Hard sigmoid activation function.\n",
            "\n",
            "  Faster to compute than sigmoid activation.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      Hard sigmoid activation:\n",
            "      - `0` if `x < -2.5`\n",
            "      - `1` if `x > 2.5`\n",
            "      - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.\n",
            "  \"\"\"\n",
            "  return K.hard_sigmoid(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.linear')\n",
            "def linear(x):\n",
            "  \"\"\"Linear activation function.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor.\n",
            "\n",
            "  Returns:\n",
            "      The linear activation: `x`.\n",
            "  \"\"\"\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.serialize')\n",
            "def serialize(activation):\n",
            "  if activation.__name__ in _TF_ACTIVATIONS_V2:\n",
            "    return _TF_ACTIVATIONS_V2[activation.__name__]\n",
            "  return activation.__name__\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.deserialize')\n",
            "def deserialize(name, custom_objects=None):\n",
            "  return deserialize_keras_object(\n",
            "      name,\n",
            "      module_objects=globals(),\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='activation function')\n",
            "\n",
            "\n",
            "@keras_export('keras.activations.get')\n",
            "def get(identifier):\n",
            "  if identifier is None:\n",
            "    return linear\n",
            "  if isinstance(identifier, six.string_types):\n",
            "    identifier = str(identifier)\n",
            "    return deserialize(identifier)\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret '\n",
            "                     'activation function identifier:', identifier)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras initializer serialization / deserialization.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python import tf2\n",
            "from tensorflow.python.framework import dtypes\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.ops import init_ops_v2\n",
            "\n",
            "# These imports are brought in so that keras.initializers.deserialize\n",
            "# has them available in module_objects.\n",
            "from tensorflow.python.ops.init_ops import Constant\n",
            "from tensorflow.python.ops.init_ops import GlorotNormal\n",
            "from tensorflow.python.ops.init_ops import GlorotUniform\n",
            "from tensorflow.python.ops.init_ops import he_normal  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import he_uniform  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import Identity\n",
            "from tensorflow.python.ops.init_ops import Initializer  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import lecun_normal  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import lecun_uniform  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import Ones\n",
            "from tensorflow.python.ops.init_ops import Orthogonal\n",
            "from tensorflow.python.ops.init_ops import RandomNormal as TFRandomNormal\n",
            "from tensorflow.python.ops.init_ops import RandomUniform as TFRandomUniform\n",
            "from tensorflow.python.ops.init_ops import TruncatedNormal as TFTruncatedNormal\n",
            "from tensorflow.python.ops.init_ops import VarianceScaling  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops.init_ops import Zeros\n",
            "# pylint: disable=unused-import, disable=line-too-long\n",
            "from tensorflow.python.ops.init_ops_v2 import Constant as ConstantV2\n",
            "from tensorflow.python.ops.init_ops_v2 import GlorotNormal as GlorotNormalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import GlorotUniform as GlorotUniformV2\n",
            "from tensorflow.python.ops.init_ops_v2 import he_normal as he_normalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import he_uniform as he_uniformV2\n",
            "from tensorflow.python.ops.init_ops_v2 import Identity as IdentityV2\n",
            "from tensorflow.python.ops.init_ops_v2 import Initializer as InitializerV2\n",
            "from tensorflow.python.ops.init_ops_v2 import lecun_normal as lecun_normalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import lecun_uniform  as lecun_uniformV2\n",
            "from tensorflow.python.ops.init_ops_v2 import Ones as OnesV2\n",
            "from tensorflow.python.ops.init_ops_v2 import Orthogonal as OrthogonalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import RandomNormal as RandomNormalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import RandomUniform as RandomUniformV2\n",
            "from tensorflow.python.ops.init_ops_v2 import TruncatedNormal as TruncatedNormalV2\n",
            "from tensorflow.python.ops.init_ops_v2 import VarianceScaling as VarianceScalingV2\n",
            "from tensorflow.python.ops.init_ops_v2 import Zeros as ZerosV2\n",
            "# pylint: enable=unused-import, enable=line-too-long\n",
            "\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.initializers.TruncatedNormal',\n",
            "                  'keras.initializers.truncated_normal'])\n",
            "class TruncatedNormal(TFTruncatedNormal):\n",
            "  \"\"\"Initializer that generates a truncated normal distribution.\n",
            "\n",
            "  These values are similar to values from a `random_normal_initializer`\n",
            "  except that values more than two standard deviations from the mean\n",
            "  are discarded and re-drawn. This is the recommended initializer for\n",
            "  neural network weights and filters.\n",
            "\n",
            "  Args:\n",
            "    mean: a python scalar or a scalar tensor. Mean of the random values to\n",
            "      generate. Defaults to 0.\n",
            "    stddev: a python scalar or a scalar tensor. Standard deviation of the random\n",
            "      values to generate. Defaults to 0.05.\n",
            "    seed: A Python integer. Used to create random seeds. See\n",
            "      `tf.compat.v1.set_random_seed` for behavior.\n",
            "    dtype: The data type. Only floating point types are supported.\n",
            "    \n",
            "  Returns:\n",
            "    A TruncatedNormal instance.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, mean=0.0, stddev=0.05, seed=None, dtype=dtypes.float32):\n",
            "    super(TruncatedNormal, self).__init__(\n",
            "        mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.initializers.RandomUniform',\n",
            "                  'keras.initializers.uniform',\n",
            "                  'keras.initializers.random_uniform'])\n",
            "class RandomUniform(TFRandomUniform):\n",
            "  \"\"\"Initializer that generates tensors with a uniform distribution.\n",
            "\n",
            "  Args:\n",
            "    minval: A python scalar or a scalar tensor. Lower bound of the range of\n",
            "      random values to generate. Defaults to -0.05.\n",
            "    maxval: A python scalar or a scalar tensor. Upper bound of the range of\n",
            "      random values to generate. Defaults to 0.05.\n",
            "    seed: A Python integer. Used to create random seeds. See\n",
            "      `tf.compat.v1.set_random_seed` for behavior.\n",
            "    dtype: The data type.\n",
            "    \n",
            "  Returns:\n",
            "    A RandomUniform instance.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, minval=-0.05, maxval=0.05, seed=None,\n",
            "               dtype=dtypes.float32):\n",
            "    super(RandomUniform, self).__init__(\n",
            "        minval=minval, maxval=maxval, seed=seed, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.initializers.RandomNormal',\n",
            "                  'keras.initializers.normal',\n",
            "                  'keras.initializers.random_normal'])\n",
            "class RandomNormal(TFRandomNormal):\n",
            "  \"\"\"Initializer that generates tensors with a normal distribution.\n",
            "\n",
            "  Args:\n",
            "    mean: a python scalar or a scalar tensor. Mean of the random values to\n",
            "      generate. Defaults to 0.\n",
            "    stddev: a python scalar or a scalar tensor. Standard deviation of the random\n",
            "      values to generate. Defaults to 0.05.\n",
            "    seed: A Python integer. Used to create random seeds. See\n",
            "      `tf.compat.v1.set_random_seed` for behavior.\n",
            "    dtype: The data type. Only floating point types are supported.\n",
            "\n",
            "  Returns:\n",
            "      RandomNormal instance.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, mean=0.0, stddev=0.05, seed=None, dtype=dtypes.float32):\n",
            "    super(RandomNormal, self).__init__(\n",
            "        mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n",
            "\n",
            "\n",
            "# Compatibility aliases\n",
            "\n",
            "# pylint: disable=invalid-name\n",
            "zero = zeros = Zeros\n",
            "one = ones = Ones\n",
            "constant = Constant\n",
            "uniform = random_uniform = RandomUniform\n",
            "normal = random_normal = RandomNormal\n",
            "truncated_normal = TruncatedNormal\n",
            "identity = Identity\n",
            "orthogonal = Orthogonal\n",
            "glorot_normal = GlorotNormal\n",
            "glorot_uniform = GlorotUniform\n",
            "\n",
            "\n",
            "# Utility functions\n",
            "\n",
            "\n",
            "@keras_export('keras.initializers.serialize')\n",
            "def serialize(initializer):\n",
            "  return serialize_keras_object(initializer)\n",
            "\n",
            "\n",
            "@keras_export('keras.initializers.deserialize')\n",
            "def deserialize(config, custom_objects=None):\n",
            "  \"\"\"Return an `Initializer` object from its config.\"\"\"\n",
            "  if tf2.enabled():\n",
            "    # Class names are the same for V1 and V2 but the V2 classes\n",
            "    # are aliased in this file so we need to grab them directly\n",
            "    # from `init_ops_v2`.\n",
            "    module_objects = {\n",
            "        obj_name: getattr(init_ops_v2, obj_name)\n",
            "        for obj_name in dir(init_ops_v2)\n",
            "    }\n",
            "  else:\n",
            "    module_objects = globals()\n",
            "  return deserialize_keras_object(\n",
            "      config,\n",
            "      module_objects=module_objects,\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='initializer')\n",
            "\n",
            "\n",
            "@keras_export('keras.initializers.get')\n",
            "def get(identifier):\n",
            "  if identifier is None:\n",
            "    return None\n",
            "  if isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    identifier = str(identifier)\n",
            "    # We have to special-case functions that return classes.\n",
            "    # TODO(omalleyt): Turn these into classes or class aliases.\n",
            "    special_cases = ['he_normal', 'he_uniform', 'lecun_normal', 'lecun_uniform']\n",
            "    if identifier in special_cases:\n",
            "      # Treat like a class.\n",
            "      return deserialize({'class_name': identifier, 'config': {}})\n",
            "    return deserialize(identifier)\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret initializer identifier: ' +\n",
            "                     str(identifier))\n",
            "\n",
            "\n",
            "# pylint: enable=invalid-name\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Callbacks: utilities called at certain points during model training.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import os\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.eager import profiler\n",
            "from tensorflow.python.framework import dtypes\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras import callbacks\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import state_ops\n",
            "from tensorflow.python.ops import summary_ops_v2\n",
            "from tensorflow.python.ops import variables\n",
            "from tensorflow.python.platform import tf_logging as logging\n",
            "from tensorflow.python.summary import summary as tf_summary\n",
            "from tensorflow.python.training import saver\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.callbacks.TensorBoard'])\n",
            "class TensorBoard(callbacks.Callback):\n",
            "  # pylint: disable=line-too-long\n",
            "  \"\"\"Enable visualizations for TensorBoard.\n",
            "\n",
            "  TensorBoard is a visualization tool provided with TensorFlow.\n",
            "\n",
            "  This callback logs events for TensorBoard, including:\n",
            "  * Metrics summary plots\n",
            "  * Training graph visualization\n",
            "  * Activation histograms\n",
            "  * Sampled profiling\n",
            "\n",
            "  If you have installed TensorFlow with pip, you should be able\n",
            "  to launch TensorBoard from the command line:\n",
            "\n",
            "  ```sh\n",
            "  tensorboard --logdir=path_to_your_logs\n",
            "  ```\n",
            "\n",
            "  You can find more information about TensorBoard\n",
            "  [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
            "\n",
            "  Arguments:\n",
            "      log_dir: the path of the directory where to save the log files to be\n",
            "        parsed by TensorBoard.\n",
            "      histogram_freq: frequency (in epochs) at which to compute activation and\n",
            "        weight histograms for the layers of the model. If set to 0, histograms\n",
            "        won't be computed. Validation data (or split) must be specified for\n",
            "        histogram visualizations.\n",
            "      write_graph: whether to visualize the graph in TensorBoard. The log file\n",
            "        can become quite large when write_graph is set to True.\n",
            "      write_grads: whether to visualize gradient histograms in TensorBoard.\n",
            "        `histogram_freq` must be greater than 0.\n",
            "      batch_size: size of batch of inputs to feed to the network for histograms\n",
            "        computation.\n",
            "      write_images: whether to write model weights to visualize as image in\n",
            "        TensorBoard.\n",
            "      embeddings_freq: frequency (in epochs) at which selected embedding layers\n",
            "        will be saved. If set to 0, embeddings won't be computed. Data to be\n",
            "        visualized in TensorBoard's Embedding tab must be passed as\n",
            "        `embeddings_data`.\n",
            "      embeddings_layer_names: a list of names of layers to keep eye on. If None\n",
            "        or empty list all the embedding layer will be watched.\n",
            "      embeddings_metadata: a dictionary which maps layer name to a file name in\n",
            "        which metadata for this embedding layer is saved. See the\n",
            "          [details](https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n",
            "            about metadata files format. In case if the same metadata file is\n",
            "            used for all embedding layers, string can be passed.\n",
            "      embeddings_data: data to be embedded at layers specified in\n",
            "        `embeddings_layer_names`. Numpy array (if the model has a single input)\n",
            "        or list of Numpy arrays (if the model has multiple inputs). Learn [more\n",
            "        about\n",
            "            embeddings](https://www.tensorflow.org/programmers_guide/embedding)\n",
            "      update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
            "        writes the losses and metrics to TensorBoard after each batch. The same\n",
            "        applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
            "        callback will write the metrics and losses to TensorBoard every 1000\n",
            "        samples. Note that writing too frequently to TensorBoard can slow down\n",
            "        your training.\n",
            "      profile_batch: Profile the batch to sample compute characteristics. By\n",
            "        default, it will profile the second batch. Set profile_batch=0 to\n",
            "        disable profiling.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If histogram_freq is set and no validation data is provided.\n",
            "\n",
            "  @compatibility(eager)\n",
            "  Using the `TensorBoard` callback will work when eager execution is enabled,\n",
            "  with the restriction that outputting histogram summaries of weights and\n",
            "  gradients is not supported. Consequently, `histogram_freq` will be ignored.\n",
            "  @end_compatibility\n",
            "  \"\"\"\n",
            "\n",
            "  # pylint: enable=line-too-long\n",
            "\n",
            "  def __init__(self,\n",
            "               log_dir='./logs',\n",
            "               histogram_freq=0,\n",
            "               batch_size=32,\n",
            "               write_graph=True,\n",
            "               write_grads=False,\n",
            "               write_images=False,\n",
            "               embeddings_freq=0,\n",
            "               embeddings_layer_names=None,\n",
            "               embeddings_metadata=None,\n",
            "               embeddings_data=None,\n",
            "               update_freq='epoch',\n",
            "               profile_batch=2):\n",
            "    super(TensorBoard, self).__init__()\n",
            "    self.log_dir = log_dir\n",
            "    self.histogram_freq = histogram_freq\n",
            "    if self.histogram_freq and context.executing_eagerly():\n",
            "      logging.warning(\n",
            "          UserWarning('Weight and gradient histograms not supported for eager'\n",
            "                      'execution, setting `histogram_freq` to `0`.'))\n",
            "      self.histogram_freq = 0\n",
            "    self.merged = None\n",
            "    self.write_graph = write_graph\n",
            "    self.write_grads = write_grads\n",
            "    self.write_images = write_images\n",
            "    self.batch_size = batch_size\n",
            "    self._current_batch = 0\n",
            "    self._total_batches_seen = 0\n",
            "    self._total_val_batches_seen = 0\n",
            "    self.embeddings_freq = embeddings_freq\n",
            "    self.embeddings_layer_names = embeddings_layer_names\n",
            "    self.embeddings_metadata = embeddings_metadata\n",
            "    self.embeddings_data = embeddings_data\n",
            "    if update_freq == 'batch':\n",
            "      self.update_freq = 1\n",
            "    else:\n",
            "      self.update_freq = update_freq\n",
            "    self._samples_seen = 0\n",
            "    self._samples_seen_at_last_write = 0\n",
            "    # TODO(fishx): Add a link to the full profiler tutorial.\n",
            "    self._profile_batch = profile_batch\n",
            "    # One profiler session is running if it is True.\n",
            "    self._is_profiling = False\n",
            "\n",
            "    # TensorBoard should only write summaries on the chief when in a\n",
            "    # Multi-Worker setting.\n",
            "    self._chief_worker_only = True\n",
            "\n",
            "  def _init_writer(self, model):\n",
            "    \"\"\"Sets file writer.\"\"\"\n",
            "    if context.executing_eagerly():\n",
            "      self.writer = summary_ops_v2.create_file_writer(self.log_dir)\n",
            "      if not model.run_eagerly and self.write_graph:\n",
            "        with self.writer.as_default():\n",
            "          summary_ops_v2.graph(K.get_graph(), step=0)\n",
            "    elif self.write_graph:\n",
            "      self.writer = tf_summary.FileWriter(self.log_dir, K.get_graph())\n",
            "    else:\n",
            "      self.writer = tf_summary.FileWriter(self.log_dir)\n",
            "\n",
            "  def _make_histogram_ops(self, model):\n",
            "    \"\"\"Defines histogram ops when histogram_freq > 0.\"\"\"\n",
            "    # only make histogram summary op if it hasn't already been made\n",
            "    if self.histogram_freq and self.merged is None:\n",
            "      for layer in self.model.layers:\n",
            "        for weight in layer.weights:\n",
            "          mapped_weight_name = weight.name.replace(':', '_')\n",
            "          tf_summary.histogram(mapped_weight_name, weight)\n",
            "          if self.write_images:\n",
            "            w_img = array_ops.squeeze(weight)\n",
            "            shape = K.int_shape(w_img)\n",
            "            if len(shape) == 2:  # dense layer kernel case\n",
            "              if shape[0] > shape[1]:\n",
            "                w_img = array_ops.transpose(w_img)\n",
            "                shape = K.int_shape(w_img)\n",
            "              w_img = array_ops.reshape(w_img, [1, shape[0], shape[1], 1])\n",
            "            elif len(shape) == 3:  # convnet case\n",
            "              if K.image_data_format() == 'channels_last':\n",
            "                # switch to channels_first to display\n",
            "                # every kernel as a separate image\n",
            "                w_img = array_ops.transpose(w_img, perm=[2, 0, 1])\n",
            "                shape = K.int_shape(w_img)\n",
            "              w_img = array_ops.reshape(w_img,\n",
            "                                        [shape[0], shape[1], shape[2], 1])\n",
            "            elif len(shape) == 1:  # bias case\n",
            "              w_img = array_ops.reshape(w_img, [1, shape[0], 1, 1])\n",
            "            else:\n",
            "              # not possible to handle 3D convnets etc.\n",
            "              continue\n",
            "\n",
            "            shape = K.int_shape(w_img)\n",
            "            assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n",
            "            tf_summary.image(mapped_weight_name, w_img)\n",
            "\n",
            "        if self.write_grads:\n",
            "          for weight in layer.trainable_weights:\n",
            "            mapped_weight_name = weight.name.replace(':', '_')\n",
            "            grads = model.optimizer.get_gradients(model.total_loss, weight)\n",
            "\n",
            "            def is_indexed_slices(grad):\n",
            "              return type(grad).__name__ == 'IndexedSlices'\n",
            "\n",
            "            grads = [\n",
            "                grad.values if is_indexed_slices(grad) else grad\n",
            "                for grad in grads\n",
            "            ]\n",
            "            tf_summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n",
            "\n",
            "        if hasattr(layer, 'output'):\n",
            "          if isinstance(layer.output, list):\n",
            "            for i, output in enumerate(layer.output):\n",
            "              tf_summary.histogram('{}_out_{}'.format(layer.name, i), output)\n",
            "          else:\n",
            "            tf_summary.histogram('{}_out'.format(layer.name), layer.output)\n",
            "\n",
            "  def set_model(self, model):\n",
            "    \"\"\"Sets Keras model and creates summary ops.\"\"\"\n",
            "\n",
            "    self.model = model\n",
            "    self._init_writer(model)\n",
            "    # histogram summaries only enabled in graph mode\n",
            "    if not context.executing_eagerly():\n",
            "      self._make_histogram_ops(model)\n",
            "      self.merged = tf_summary.merge_all()\n",
            "\n",
            "    # If both embedding_freq and embeddings_data are available, we will\n",
            "    # visualize embeddings.\n",
            "    if self.embeddings_freq and self.embeddings_data is not None:\n",
            "      # Avoid circular dependency.\n",
            "      from tensorflow.python.keras.engine import training_utils  # pylint: disable=g-import-not-at-top\n",
            "      self.embeddings_data = training_utils.standardize_input_data(\n",
            "          self.embeddings_data, model.input_names)\n",
            "\n",
            "      # If embedding_layer_names are not provided, get all of the embedding\n",
            "      # layers from the model.\n",
            "      embeddings_layer_names = self.embeddings_layer_names\n",
            "      if not embeddings_layer_names:\n",
            "        embeddings_layer_names = [\n",
            "            layer.name\n",
            "            for layer in self.model.layers\n",
            "            if type(layer).__name__ == 'Embedding'\n",
            "        ]\n",
            "\n",
            "      self.assign_embeddings = []\n",
            "      embeddings_vars = {}\n",
            "\n",
            "      self.batch_id = batch_id = array_ops.placeholder(dtypes.int32)\n",
            "      self.step = step = array_ops.placeholder(dtypes.int32)\n",
            "\n",
            "      for layer in self.model.layers:\n",
            "        if layer.name in embeddings_layer_names:\n",
            "          embedding_input = self.model.get_layer(layer.name).output\n",
            "          embedding_size = np.prod(embedding_input.shape[1:])\n",
            "          embedding_input = array_ops.reshape(embedding_input,\n",
            "                                              (step, int(embedding_size)))\n",
            "          shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n",
            "          embedding = variables.Variable(\n",
            "              array_ops.zeros(shape), name=layer.name + '_embedding')\n",
            "          embeddings_vars[layer.name] = embedding\n",
            "          batch = state_ops.assign(embedding[batch_id:batch_id + step],\n",
            "                                   embedding_input)\n",
            "          self.assign_embeddings.append(batch)\n",
            "\n",
            "      self.saver = saver.Saver(list(embeddings_vars.values()))\n",
            "\n",
            "      # Create embeddings_metadata dictionary\n",
            "      if isinstance(self.embeddings_metadata, str):\n",
            "        embeddings_metadata = {\n",
            "            layer_name: self.embeddings_metadata\n",
            "            for layer_name in embeddings_vars.keys()\n",
            "        }\n",
            "      else:\n",
            "        # If embedding_metadata is already a dictionary\n",
            "        embeddings_metadata = self.embeddings_metadata\n",
            "\n",
            "      try:\n",
            "        from tensorboard.plugins import projector\n",
            "      except ImportError:\n",
            "        raise ImportError('Failed to import TensorBoard. Please make sure that '\n",
            "                          'TensorBoard integration is complete.\"')\n",
            "\n",
            "      # TODO(psv): Add integration tests to test embedding visualization\n",
            "      # with TensorBoard callback. We are unable to write a unit test for this\n",
            "      # because TensorBoard dependency assumes TensorFlow package is installed.\n",
            "      config = projector.ProjectorConfig()\n",
            "      for layer_name, tensor in embeddings_vars.items():\n",
            "        embedding = config.embeddings.add()\n",
            "        embedding.tensor_name = tensor.name\n",
            "\n",
            "        if (embeddings_metadata is not None and\n",
            "            layer_name in embeddings_metadata):\n",
            "          embedding.metadata_path = embeddings_metadata[layer_name]\n",
            "\n",
            "      projector.visualize_embeddings(self.writer, config)\n",
            "\n",
            "  def _fetch_callback(self, summary):\n",
            "    self.writer.add_summary(summary, self._total_val_batches_seen)\n",
            "    self._total_val_batches_seen += 1\n",
            "\n",
            "  def _write_custom_summaries(self, step, logs=None):\n",
            "    \"\"\"Writes metrics out as custom scalar summaries.\n",
            "\n",
            "    Arguments:\n",
            "        step: the global step to use for TensorBoard.\n",
            "        logs: dict. Keys are scalar summary names, values are\n",
            "            NumPy scalars.\n",
            "\n",
            "    \"\"\"\n",
            "    logs = logs or {}\n",
            "    if context.executing_eagerly():\n",
            "      # use v2 summary ops\n",
            "      with self.writer.as_default(), summary_ops_v2.always_record_summaries():\n",
            "        for name, value in logs.items():\n",
            "          if isinstance(value, np.ndarray):\n",
            "            value = value.item()\n",
            "          summary_ops_v2.scalar(name, value, step=step)\n",
            "    else:\n",
            "      # use FileWriter from v1 summary\n",
            "      for name, value in logs.items():\n",
            "        if isinstance(value, np.ndarray):\n",
            "          value = value.item()\n",
            "        summary = tf_summary.Summary()\n",
            "        summary_value = summary.value.add()\n",
            "        summary_value.simple_value = value\n",
            "        summary_value.tag = name\n",
            "        self.writer.add_summary(summary, step)\n",
            "    self.writer.flush()\n",
            "\n",
            "  def on_batch_end(self, batch, logs=None):\n",
            "    \"\"\"Writes scalar summaries for metrics on every training batch.\n",
            "\n",
            "    Performs profiling if current batch is in profiler_batches.\n",
            "    \"\"\"\n",
            "    # Don't output batch_size and batch number as TensorBoard summaries\n",
            "    logs = logs or {}\n",
            "    self._samples_seen += logs.get('size', 1)\n",
            "    samples_seen_since = self._samples_seen - self._samples_seen_at_last_write\n",
            "    if self.update_freq != 'epoch' and samples_seen_since >= self.update_freq:\n",
            "      batch_logs = {('batch_' + k): v\n",
            "                    for k, v in logs.items()\n",
            "                    if k not in ['batch', 'size', 'num_steps']}\n",
            "      self._write_custom_summaries(self._total_batches_seen, batch_logs)\n",
            "      self._samples_seen_at_last_write = self._samples_seen\n",
            "    self._total_batches_seen += 1\n",
            "    if self._is_profiling:\n",
            "      profiler.save(self.log_dir, profiler.stop())\n",
            "      self._is_profiling = False\n",
            "    elif (not self._is_profiling and\n",
            "          self._total_batches_seen == self._profile_batch - 1):\n",
            "      profiler.start()\n",
            "      self._is_profiling = True\n",
            "\n",
            "  def on_train_begin(self, logs=None):\n",
            "    if self._profile_batch == 1:\n",
            "      profiler.start()\n",
            "      self._is_profiling = True\n",
            "\n",
            "  def on_epoch_begin(self, epoch, logs=None):\n",
            "    \"\"\"Add histogram op to Model eval_function callbacks, reset batch count.\"\"\"\n",
            "\n",
            "    # check if histogram summary should be run for this epoch\n",
            "    if self.histogram_freq and epoch % self.histogram_freq == 0:\n",
            "      self._epoch = epoch\n",
            "      # pylint: disable=protected-access\n",
            "      # add the histogram summary op if it should run this epoch\n",
            "      self.model._make_test_function()\n",
            "      if self.merged not in self.model.test_function.fetches:\n",
            "        self.model.test_function.fetches.append(self.merged)\n",
            "        self.model.test_function.fetch_callbacks[\n",
            "            self.merged] = self._fetch_callback\n",
            "      # pylint: enable=protected-access\n",
            "\n",
            "  def on_epoch_end(self, epoch, logs=None):\n",
            "    \"\"\"Checks if summary ops should run next epoch, logs scalar summaries.\"\"\"\n",
            "\n",
            "    # don't output batch_size and\n",
            "    # batch number as TensorBoard summaries\n",
            "    logs = {('epoch_' + k): v\n",
            "            for k, v in logs.items()\n",
            "            if k not in ['batch', 'size', 'num_steps']}\n",
            "    if self.update_freq == 'epoch':\n",
            "      step = epoch\n",
            "    else:\n",
            "      step = self._samples_seen\n",
            "    self._write_custom_summaries(step, logs)\n",
            "\n",
            "    # pop the histogram summary op after each epoch\n",
            "    if self.histogram_freq:\n",
            "      # pylint: disable=protected-access\n",
            "      if self.merged in self.model.test_function.fetches:\n",
            "        self.model.test_function.fetches.remove(self.merged)\n",
            "      if self.merged in self.model.test_function.fetch_callbacks:\n",
            "        self.model.test_function.fetch_callbacks.pop(self.merged)\n",
            "      # pylint: enable=protected-access\n",
            "\n",
            "    if self.embeddings_data is None and self.embeddings_freq:\n",
            "      raise ValueError('To visualize embeddings, embeddings_data must '\n",
            "                       'be provided.')\n",
            "\n",
            "    if self.embeddings_freq and self.embeddings_data is not None:\n",
            "      if epoch % self.embeddings_freq == 0:\n",
            "        # We need a second forward-pass here because we're passing\n",
            "        # the `embeddings_data` explicitly. This design allows to pass\n",
            "        # arbitrary data as `embeddings_data` and results from the fact\n",
            "        # that we need to know the size of the `tf.Variable`s which\n",
            "        # hold the embeddings in `set_model`. At this point, however,\n",
            "        # the `validation_data` is not yet set.\n",
            "\n",
            "        embeddings_data = self.embeddings_data\n",
            "        n_samples = embeddings_data[0].shape[0]\n",
            "        i = 0\n",
            "        sess = K.get_session()\n",
            "        while i < n_samples:\n",
            "          step = min(self.batch_size, n_samples - i)\n",
            "          batch = slice(i, i + step)\n",
            "\n",
            "          if isinstance(self.model.input, list):\n",
            "            feed_dict = {\n",
            "                model_input: embeddings_data[idx][batch]\n",
            "                for idx, model_input in enumerate(self.model.input)\n",
            "            }\n",
            "          else:\n",
            "            feed_dict = {self.model.input: embeddings_data[0][batch]}\n",
            "\n",
            "          feed_dict.update({self.batch_id: i, self.step: step})\n",
            "\n",
            "          if not isinstance(K.learning_phase(), int):\n",
            "            feed_dict[K.learning_phase()] = False\n",
            "\n",
            "          sess.run(self.assign_embeddings, feed_dict=feed_dict)\n",
            "          self.saver.save(sess,\n",
            "                          os.path.join(self.log_dir, 'keras_embedding.ckpt'),\n",
            "                          epoch)\n",
            "\n",
            "          i += self.batch_size\n",
            "\n",
            "  def on_train_end(self, logs=None):\n",
            "    if self._is_profiling:\n",
            "      profiler.save(self.log_dir, profiler.stop())\n",
            "      self._is_profiling = False\n",
            "    self.writer.close()\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "\"\"\"Code for model cloning, plus model-related API entries.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras import metrics as metrics_module\n",
            "from tensorflow.python.keras import optimizers\n",
            "from tensorflow.python.keras import saving\n",
            "from tensorflow.python.keras.engine import sequential\n",
            "from tensorflow.python.keras.engine import training\n",
            "from tensorflow.python.keras.engine.base_layer import Layer\n",
            "from tensorflow.python.keras.engine.input_layer import Input\n",
            "from tensorflow.python.keras.engine.input_layer import InputLayer\n",
            "from tensorflow.python.keras.engine.network import Network\n",
            "from tensorflow.python.keras.utils import generic_utils\n",
            "from tensorflow.python.keras.utils.generic_utils import CustomObjectScope\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "# API entries importable from `keras.models`:\n",
            "Model = training.Model  # pylint: disable=invalid-name\n",
            "Sequential = sequential.Sequential  # pylint: disable=invalid-name\n",
            "save_model = saving.save_model\n",
            "load_model = saving.load_model\n",
            "model_from_config = saving.model_from_config\n",
            "model_from_yaml = saving.model_from_yaml\n",
            "model_from_json = saving.model_from_json\n",
            "\n",
            "\n",
            "# Callable used to clone a layer with weights preserved.\n",
            "def share_weights(layer):\n",
            "  return layer\n",
            "\n",
            "\n",
            "def _clone_layer(layer):\n",
            "  return layer.__class__.from_config(layer.get_config())\n",
            "\n",
            "\n",
            "def _clone_functional_model(model, input_tensors=None, layer_fn=_clone_layer):\n",
            "  \"\"\"Clone a functional `Model` instance.\n",
            "\n",
            "  Model cloning is similar to calling a model on new inputs,\n",
            "  except that it creates new layers (and thus new weights) instead\n",
            "  of sharing the weights of the existing layers.\n",
            "\n",
            "  Input layers are always cloned.\n",
            "\n",
            "  Arguments:\n",
            "      model: Instance of `Model`.\n",
            "      input_tensors: optional list of input tensors\n",
            "          to build the model upon. If not provided,\n",
            "          placeholders will be created.\n",
            "      layer_fn: callable to be applied on non-input layers in the model. By\n",
            "          default it clones the layer. Another example is to preserve the layer\n",
            "          to share the weights. This is required when we create a per-replica\n",
            "          copy of the model with distribution strategy; we want the weights to\n",
            "          be shared but still feed inputs separately so we create new input\n",
            "          layers.\n",
            "\n",
            "  Returns:\n",
            "      An instance of `Model` reproducing the behavior\n",
            "      of the original model, on top of new inputs tensors,\n",
            "      using newly instantiated weights.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: in case of invalid `model` argument value or `layer_fn`\n",
            "      argument value.\n",
            "  \"\"\"\n",
            "  if not isinstance(model, Model):\n",
            "    raise ValueError('Expected `model` argument '\n",
            "                     'to be a `Model` instance, got ', model)\n",
            "  if isinstance(model, Sequential):\n",
            "    raise ValueError('Expected `model` argument '\n",
            "                     'to be a functional `Model` instance, '\n",
            "                     'got a `Sequential` instance instead:', model)\n",
            "  if not model._is_graph_network:\n",
            "    raise ValueError('Expected `model` argument '\n",
            "                     'to be a functional `Model` instance, '\n",
            "                     'but got a subclass model instead.')\n",
            "\n",
            "  layer_map = {}  # Cache for created layers.\n",
            "  tensor_map = {}  # Map {reference_tensor: corresponding_tensor}\n",
            "  if input_tensors is None:\n",
            "    # Create placeholders to build the model on top of.\n",
            "    input_tensors = []\n",
            "    for layer in model._input_layers:\n",
            "      input_tensor = Input(\n",
            "          batch_shape=layer._batch_input_shape,\n",
            "          dtype=layer.dtype,\n",
            "          sparse=layer.sparse,\n",
            "          name=layer.name)\n",
            "      input_tensors.append(input_tensor)\n",
            "      # Cache newly created input layer.\n",
            "      newly_created_input_layer = input_tensor._keras_history.layer\n",
            "      layer_map[layer] = newly_created_input_layer\n",
            "  else:\n",
            "    # Make sure that all input tensors come from a Keras layer.\n",
            "    # If tensor comes from an input layer: cache the input layer.\n",
            "    input_tensors = nest.flatten(input_tensors)\n",
            "    input_tensors_ = []\n",
            "    for i in range(len(input_tensors)):\n",
            "      input_tensor = input_tensors[i]\n",
            "      if not K.is_keras_tensor(input_tensor):\n",
            "        original_input_layer = model._input_layers[i]\n",
            "        name = original_input_layer.name\n",
            "        input_tensor = Input(tensor=input_tensor,\n",
            "                             name='input_wrapper_for_' + name)\n",
            "\n",
            "        input_tensors_.append(input_tensor)\n",
            "        # Cache newly created input layer.\n",
            "        newly_created_input_layer = input_tensor._keras_history.layer\n",
            "        layer_map[original_input_layer] = newly_created_input_layer\n",
            "      else:\n",
            "        input_tensors_.append(input_tensor)\n",
            "    input_tensors = input_tensors_\n",
            "\n",
            "  for x, y in zip(model.inputs, input_tensors):\n",
            "    tensor_map[x] = y\n",
            "\n",
            "  if not callable(layer_fn):\n",
            "    raise ValueError('Expected `layer_fn` argument to be a callable.')\n",
            "\n",
            "  new_nodes = set()\n",
            "\n",
            "  # Iterated over every node in the reference model, in depth order.\n",
            "  depth_keys = list(model._nodes_by_depth.keys())\n",
            "  depth_keys.sort(reverse=True)\n",
            "  for depth in depth_keys:\n",
            "    nodes = model._nodes_by_depth[depth]\n",
            "    for node in nodes:\n",
            "      # Recover the corresponding layer.\n",
            "      layer = node.outbound_layer\n",
            "\n",
            "      # Get or create layer.\n",
            "      if layer not in layer_map:\n",
            "        new_layer = layer_fn(layer)\n",
            "        layer_map[layer] = new_layer\n",
            "        layer = new_layer\n",
            "      else:\n",
            "        # Reuse previously cloned layer.\n",
            "        layer = layer_map[layer]\n",
            "        # Don't call InputLayer multiple times.\n",
            "        if isinstance(layer, InputLayer):\n",
            "          continue\n",
            "\n",
            "      # If all previous input tensors are available in tensor_map,\n",
            "      # then call node.inbound_layer on them.\n",
            "      if all(\n",
            "          tensor in tensor_map for tensor in nest.flatten(node.input_tensors)):\n",
            "        computed_tensors = nest.map_structure(lambda t: tensor_map[t],\n",
            "                                              node.input_tensors)\n",
            "        # Call layer.\n",
            "        kwargs = node.arguments or {}\n",
            "        output_tensors = layer(computed_tensors, **kwargs)\n",
            "\n",
            "        # Thread-safe way to keep track of what node was created.\n",
            "        first_output_tensor = nest.flatten(output_tensors)[0]\n",
            "        new_nodes.add(\n",
            "            layer._inbound_nodes[first_output_tensor._keras_history.node_index])\n",
            "\n",
            "        for x, y in zip(\n",
            "            nest.flatten(node.output_tensors), nest.flatten(output_tensors)):\n",
            "          tensor_map[x] = y\n",
            "\n",
            "  # Check that we did compute the model outputs,\n",
            "  # then instantiate a new model from inputs and outputs.\n",
            "  output_tensors = []\n",
            "  for x in model.outputs:\n",
            "    assert x in tensor_map, 'Could not compute output ' + str(x)\n",
            "    output_tensors.append(tensor_map[x])\n",
            "\n",
            "  input_tensors = nest.pack_sequence_as(model._nested_inputs, input_tensors)\n",
            "  output_tensors = nest.pack_sequence_as(model._nested_outputs, output_tensors)\n",
            "  model = Model(input_tensors, output_tensors, name=model.name)\n",
            "  # Layers not directly tied to outputs of the Model, such as loss layers\n",
            "  # created in `add_loss`.\n",
            "  ancillary_layers = [\n",
            "      layer for layer in layer_map.values() if layer not in model.layers\n",
            "  ]\n",
            "  if ancillary_layers:\n",
            "    nodes = set(\n",
            "        nest.flatten([layer._inbound_nodes for layer in ancillary_layers]))\n",
            "    relevant_nodes = list(nodes.intersection(new_nodes))\n",
            "    model._insert_layers(ancillary_layers, relevant_nodes=relevant_nodes)\n",
            "  return model\n",
            "\n",
            "\n",
            "def _clone_sequential_model(model, input_tensors=None, layer_fn=_clone_layer):\n",
            "  \"\"\"Clone a `Sequential` model instance.\n",
            "\n",
            "  Model cloning is similar to calling a model on new inputs,\n",
            "  except that it creates new layers (and thus new weights) instead\n",
            "  of sharing the weights of the existing layers.\n",
            "\n",
            "  Arguments:\n",
            "      model: Instance of `Sequential`.\n",
            "      input_tensors: optional list of input tensors\n",
            "          to build the model upon. If not provided,\n",
            "          placeholders will be created.\n",
            "      layer_fn: callable to be applied on non-input layers in the model. By\n",
            "          default it clones the layer. Another example is to preserve the layer\n",
            "          to share the weights. This is required when we create a per-replica\n",
            "          copy of the model with distribution strategy; we want the weights to\n",
            "          be shared but still feed inputs separately so we create new input\n",
            "          layers.\n",
            "\n",
            "  Returns:\n",
            "      An instance of `Sequential` reproducing the behavior\n",
            "      of the original model, on top of new inputs tensors,\n",
            "      using newly instantiated weights.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: in case of invalid `model` argument value or `layer_fn`\n",
            "      argument value.\n",
            "  \"\"\"\n",
            "  if not isinstance(model, Sequential):\n",
            "    raise ValueError('Expected `model` argument '\n",
            "                     'to be a `Sequential` model instance, '\n",
            "                     'but got:', model)\n",
            "\n",
            "  if not callable(layer_fn):\n",
            "    raise ValueError('Expected `layer_fn` argument to be a callable.')\n",
            "\n",
            "  # Use model._layers to ensure that all layers are cloned. The model's layers\n",
            "  # property will exclude the initial InputLayer (if it exists) in the model,\n",
            "  # resulting in a different Sequential model structure.\n",
            "  if input_tensors is None:\n",
            "    layers = []\n",
            "    for layer in model._layers:\n",
            "      if isinstance(layer, InputLayer):\n",
            "        layers.append(_clone_layer(layer))\n",
            "      else:\n",
            "        layers.append(layer_fn(layer))\n",
            "    return Sequential(layers=layers, name=model.name)\n",
            "  else:\n",
            "    # If input tensors are provided, the original model's InputLayer is\n",
            "    # overwritten with a different InputLayer.\n",
            "    layers = [\n",
            "        layer_fn(layer)\n",
            "        for layer in model._layers\n",
            "        if not isinstance(layer, InputLayer)\n",
            "    ]\n",
            "    if len(generic_utils.to_list(input_tensors)) != 1:\n",
            "      raise ValueError('To clone a `Sequential` model, we expect '\n",
            "                       ' at most one tensor '\n",
            "                       'as part of `input_tensors`.')\n",
            "\n",
            "    if isinstance(input_tensors, tuple):\n",
            "      input_tensors = list(input_tensors)\n",
            "    x = generic_utils.to_list(input_tensors)[0]\n",
            "    if K.is_keras_tensor(x):\n",
            "      origin_layer = x._keras_history.layer\n",
            "      if isinstance(origin_layer, InputLayer):\n",
            "        return Sequential(layers=[origin_layer] + layers, name=model.name)\n",
            "      else:\n",
            "        raise ValueError('Cannot clone a `Sequential` model on top '\n",
            "                         'of a tensor that comes from a Keras layer '\n",
            "                         'other than an `InputLayer`. '\n",
            "                         'Use the functional API instead.')\n",
            "    input_tensor = Input(tensor=x, name='input_wrapper_for_' + str(x.name))\n",
            "    input_layer = input_tensor._keras_history.layer\n",
            "    return Sequential(layers=[input_layer] + layers, name=model.name)\n",
            "\n",
            "\n",
            "@keras_export('keras.models.clone_model')\n",
            "def clone_model(model, input_tensors=None, clone_function=None):\n",
            "  \"\"\"Clone any `Model` instance.\n",
            "\n",
            "  Model cloning is similar to calling a model on new inputs,\n",
            "  except that it creates new layers (and thus new weights) instead\n",
            "  of sharing the weights of the existing layers.\n",
            "\n",
            "  Arguments:\n",
            "      model: Instance of `Model`\n",
            "          (could be a functional model or a Sequential model).\n",
            "      input_tensors: optional list of input tensors or InputLayer objects\n",
            "          to build the model upon. If not provided,\n",
            "          placeholders will be created.\n",
            "      clone_function: Callable to be used to clone each layer in the target\n",
            "          model (except `InputLayer` instances). It takes as argument the layer\n",
            "          instance to be cloned, and returns the corresponding layer instance to\n",
            "          be used in the model copy. If unspecified, this callable defaults to\n",
            "          the following serialization/deserialization function:\n",
            "          `lambda layer: layer.__class__.from_config(layer.get_config())`.\n",
            "          By passing a custom callable, you can customize your copy of the\n",
            "          model, e.g. by wrapping certain layers of interest (you might want to\n",
            "          replace all `LSTM` instances with equivalent\n",
            "          `Bidirectional(LSTM(...))` instances, for example).\n",
            "\n",
            "  Returns:\n",
            "      An instance of `Model` reproducing the behavior\n",
            "      of the original model, on top of new inputs tensors,\n",
            "      using newly instantiated weights. The cloned model might behave\n",
            "      differently from the original model if a custom clone_function\n",
            "      modifies the layer.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: in case of invalid `model` argument value.\n",
            "  \"\"\"\n",
            "  if clone_function is None:\n",
            "    clone_function = _clone_layer\n",
            "\n",
            "  if isinstance(model, Sequential):\n",
            "    return _clone_sequential_model(\n",
            "        model, input_tensors=input_tensors, layer_fn=clone_function)\n",
            "  else:\n",
            "    return _clone_functional_model(\n",
            "        model, input_tensors=input_tensors, layer_fn=clone_function)\n",
            "\n",
            "\n",
            "# \"Clone\" a subclassed model by reseting all of the attributes.\n",
            "def _in_place_subclassed_model_reset(model):\n",
            "  \"\"\"Substitute for model cloning that works for subclassed models.\n",
            "\n",
            "  Subclassed models cannot be cloned because their topology is not serializable.\n",
            "  To \"instantiate\" an identical model in a new TF graph, we reuse the original\n",
            "  model object, but we clear its state.\n",
            "\n",
            "  After calling this function on a model instance, you can use the model\n",
            "  instance as if it were a model clone (in particular you can use it in a new\n",
            "  graph).\n",
            "\n",
            "  This method clears the state of the input model. It is thus destructive.\n",
            "  However the original state can be restored fully by calling\n",
            "  `_in_place_subclassed_model_state_restoration`.\n",
            "\n",
            "  Args:\n",
            "    model: Instance of a Keras model created via subclassing.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: In case the model uses a subclassed model as inner layer.\n",
            "  \"\"\"\n",
            "  assert not model._is_graph_network  # Only makes sense for subclassed networks\n",
            "  # Retrieve all layers tracked by the model as well as their attribute names\n",
            "  attributes_cache = {}\n",
            "  for name in dir(model):\n",
            "    # Skip the check of methods in tf.Module since they basically\n",
            "    # recursively query all the other attributes within same module.\n",
            "    if name == 'submodules':\n",
            "      continue\n",
            "\n",
            "    try:\n",
            "      value = getattr(model, name)\n",
            "    except (AttributeError, ValueError, TypeError):\n",
            "      continue\n",
            "    if isinstance(value, Layer):\n",
            "      attributes_cache[name] = value\n",
            "      assert value in model.layers\n",
            "      if hasattr(value, 'layers') and value.layers:\n",
            "        raise ValueError('We do not support the use of nested layers '\n",
            "                         'in `model_to_estimator` at this time. Found nested '\n",
            "                         'layer: %s' % value)\n",
            "    elif isinstance(\n",
            "        value, (list, tuple)) and name not in ('layers', '_layers', 'metrics',\n",
            "                                               '_compile_metric_functions',\n",
            "                                               '_output_loss_metrics'):\n",
            "      # Handle case: list/tuple of layers (also tracked by the Network API).\n",
            "      if value and all(isinstance(val, Layer) for val in value):\n",
            "        raise ValueError('We do not support the use of list-of-layers '\n",
            "                         'attributes in subclassed models used with '\n",
            "                         '`model_to_estimator` at this time. Found list '\n",
            "                         'model: %s' % name)\n",
            "\n",
            "  # Replace layers on the model with fresh layers\n",
            "  layers_to_names = {value: key for key, value in attributes_cache.items()}\n",
            "  original_layers = model._layers[:]\n",
            "  setattr_tracking = model._setattr_tracking\n",
            "  model._setattr_tracking = False\n",
            "  model._layers = []\n",
            "  for layer in original_layers:  # We preserve layer order.\n",
            "    config = layer.get_config()\n",
            "    # This will not work for nested subclassed models used as layers.\n",
            "    # This would be theoretically possible to support, but would add complexity.\n",
            "    # Only do it if users complain.\n",
            "    if isinstance(layer, Network) and not layer._is_graph_network:\n",
            "      raise ValueError('We do not support the use of nested subclassed models '\n",
            "                       'in `model_to_estimator` at this time. Found nested '\n",
            "                       'model: %s' % layer)\n",
            "    fresh_layer = layer.__class__.from_config(config)\n",
            "    name = layers_to_names[layer]\n",
            "    setattr(model, name, fresh_layer)\n",
            "    model._layers.append(fresh_layer)\n",
            "\n",
            "  # Cache original model build attributes (in addition to layers)\n",
            "  if (not hasattr(model, '_original_attributes_cache') or\n",
            "      model._original_attributes_cache is None):\n",
            "    if model.built:\n",
            "      attributes_to_cache = [\n",
            "          'inputs',\n",
            "          'outputs',\n",
            "          'total_loss',\n",
            "          'optimizer',\n",
            "          'train_function',\n",
            "          'test_function',\n",
            "          'predict_function',\n",
            "          '_training_endpoints',\n",
            "          '_collected_trainable_weights',\n",
            "          '_feed_inputs',\n",
            "          '_feed_input_names',\n",
            "          '_feed_input_shapes',\n",
            "      ]\n",
            "      for name in attributes_to_cache:\n",
            "        attributes_cache[name] = getattr(model, name)\n",
            "  model._original_attributes_cache = attributes_cache\n",
            "  _reset_build_compile_trackers(model)\n",
            "  model._setattr_tracking = setattr_tracking\n",
            "\n",
            "\n",
            "def _reset_build_compile_trackers(model):\n",
            "  \"\"\"Reset state trackers for model.\n",
            "\n",
            "  Note that we do not actually zero out attributes such as optimizer,\n",
            "  but instead rely on the expectation that all of the attrs will be\n",
            "  over-written on calling build/compile/etc. This is somewhat fragile,\n",
            "  insofar as we check elsewhere for the presence of these attributes as\n",
            "  evidence of having been built/compiled/etc. Pending a better way to do this,\n",
            "  we reset key attributes here to allow building and compiling.\n",
            "\n",
            "  Args:\n",
            "    model: the model that is being reset\n",
            "  \"\"\"\n",
            "  # Reset build state\n",
            "  model.built = False\n",
            "  model.inputs = None\n",
            "  model.outputs = None\n",
            "  # Reset compile state\n",
            "  model._is_compiled = False  # pylint:disable=protected-access\n",
            "  model.optimizer = None\n",
            "\n",
            "\n",
            "def in_place_subclassed_model_state_restoration(model):\n",
            "  \"\"\"Restores the original state of a model after it was \"reset\".\n",
            "\n",
            "  This undoes this action of `_in_place_subclassed_model_reset`, which is called\n",
            "  in `clone_and_build_model` if `in_place_reset` is set to True.\n",
            "\n",
            "  Args:\n",
            "    model: Instance of a Keras model created via subclassing, on which\n",
            "      `_in_place_subclassed_model_reset` was previously called.\n",
            "  \"\"\"\n",
            "  assert not model._is_graph_network\n",
            "  # Restore layers and build attributes\n",
            "  if (hasattr(model, '_original_attributes_cache') and\n",
            "      model._original_attributes_cache is not None):\n",
            "    # Models have sticky attribute assignment, so we want to be careful to add\n",
            "    # back the previous attributes and track Layers by their original names\n",
            "    # without adding dependencies on \"utility\" attributes which Models exempt\n",
            "    # when they're constructed.\n",
            "    setattr_tracking = model._setattr_tracking\n",
            "    model._setattr_tracking = False\n",
            "    model._layers = []\n",
            "    for name, value in model._original_attributes_cache.items():\n",
            "      setattr(model, name, value)\n",
            "      if isinstance(value, Layer):\n",
            "        model._layers.append(value)\n",
            "    model._original_attributes_cache = None\n",
            "    model._setattr_tracking = setattr_tracking\n",
            "  else:\n",
            "    # Restore to the state of a never-called model.\n",
            "    _reset_build_compile_trackers(model)\n",
            "\n",
            "\n",
            "def clone_and_build_model(\n",
            "    model, input_tensors=None, target_tensors=None, custom_objects=None,\n",
            "    compile_clone=True, in_place_reset=False, optimizer_iterations=None,\n",
            "    optimizer_config=None):\n",
            "  \"\"\"Clone a `Model` and build/compile it with the same settings used before.\n",
            "\n",
            "  This function can be be run in the same graph or in a separate graph from the\n",
            "  model. When using a separate graph, `in_place_reset` must be `False`.\n",
            "\n",
            "  Note that, currently, the clone produced from this function may not work with\n",
            "  TPU DistributionStrategy. Try at your own risk.\n",
            "\n",
            "  Args:\n",
            "    model: `tf.keras.Model` object. Can be Functional, Sequential, or\n",
            "      sub-classed.\n",
            "    input_tensors: Optional list of input tensors to build the model upon. If\n",
            "      not provided, placeholders will be created.\n",
            "    target_tensors: Optional list of target tensors for compiling the model. If\n",
            "      not provided, placeholders will be created.\n",
            "    custom_objects: Optional dictionary mapping string names to custom classes\n",
            "      or functions.\n",
            "    compile_clone: Boolean, whether to compile model clone (default `True`).\n",
            "    in_place_reset: Boolean, whether to reset the model in place. Only used if\n",
            "      the model is a subclassed model. In the case of a subclassed model,\n",
            "      this argument must be set to `True` (default `False`). To restore the\n",
            "      original model, use the function\n",
            "      `in_place_subclassed_model_state_restoration(model)`.\n",
            "    optimizer_iterations: An iterations variable that will be incremented by the\n",
            "      optimizer if the clone is compiled. This argument is used when a Keras\n",
            "      model is cloned into an Estimator model function, because Estimators\n",
            "      create their own global step variable.\n",
            "    optimizer_config: Optimizer config dictionary returned from `get_config()`.\n",
            "      This argument should be defined if `clone_and_build_model` is called in\n",
            "      a different graph or session from the original model, and the optimizer is\n",
            "      an instance of `OptimizerV2`.\n",
            "\n",
            "  Returns:\n",
            "    Clone of the model.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: Cloning fails in the following cases\n",
            "      - cloning a subclassed model with `in_place_reset` set to False.\n",
            "      - compiling the clone when the original model has not been compiled.\n",
            "  \"\"\"\n",
            "  # Grab optimizer now, as we reset-in-place for subclassed models, but\n",
            "  # want to maintain access to the original optimizer.\n",
            "  orig_optimizer = model.optimizer\n",
            "  if compile_clone and not orig_optimizer:\n",
            "    raise ValueError(\n",
            "        'Error when cloning model: compile_clone was set to True, but the '\n",
            "        'original model has not been compiled.')\n",
            "\n",
            "  if model._is_graph_network or isinstance(model, Sequential):\n",
            "    if custom_objects:\n",
            "      with CustomObjectScope(custom_objects):\n",
            "        clone = clone_model(model, input_tensors=input_tensors)\n",
            "    else:\n",
            "      clone = clone_model(model, input_tensors=input_tensors)\n",
            "\n",
            "    if all([isinstance(clone, Sequential),\n",
            "            not clone._is_graph_network,\n",
            "            getattr(model, '_build_input_shape', None) is not None]):\n",
            "      # Set model inputs to build the model and add input/output properties.\n",
            "      # TODO(kathywu): Add multiple placeholders to handle edge case where\n",
            "      # sequential model has multiple inputs.\n",
            "      clone._set_inputs(\n",
            "          K.placeholder(model._build_input_shape, dtype=model.inputs[0].dtype))\n",
            "  else:\n",
            "    if not in_place_reset:\n",
            "      raise ValueError(\n",
            "          'This model is a subclassed model. '\n",
            "          'Such a model cannot be cloned, but there is a workaround where '\n",
            "          'the model is reset in-place. To use this, please set the argument '\n",
            "          '`in_place_reset` to `True`. This will reset the attributes in the '\n",
            "          'original model. To restore the attributes, call '\n",
            "          '`in_place_subclassed_model_state_restoration(model)`.')\n",
            "    clone = model\n",
            "    _in_place_subclassed_model_reset(clone)\n",
            "    if input_tensors is not None:\n",
            "      if isinstance(input_tensors, (list, tuple)) and len(input_tensors) == 1:\n",
            "        input_tensors = input_tensors[0]\n",
            "      clone._set_inputs(input_tensors)\n",
            "\n",
            "  if compile_clone:\n",
            "    if isinstance(orig_optimizer, optimizers.TFOptimizer):\n",
            "      optimizer = optimizers.TFOptimizer(\n",
            "          orig_optimizer.optimizer, optimizer_iterations)\n",
            "      K.track_tf_optimizer(optimizer)\n",
            "    else:\n",
            "      optimizer_config = optimizer_config or orig_optimizer.get_config()\n",
            "      optimizer = orig_optimizer.__class__.from_config(optimizer_config)\n",
            "      if optimizer_iterations is not None:\n",
            "        optimizer.iterations = optimizer_iterations\n",
            "\n",
            "    clone.compile(\n",
            "        optimizer,\n",
            "        model.loss,\n",
            "        metrics=metrics_module.clone_metrics(model._compile_metrics),\n",
            "        loss_weights=model.loss_weights,\n",
            "        sample_weight_mode=model.sample_weight_mode,\n",
            "        weighted_metrics=metrics_module.clone_metrics(\n",
            "            model._compile_weighted_metrics),\n",
            "        target_tensors=target_tensors)\n",
            "\n",
            "  return clone\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities for unit-testing Keras.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import threading\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "from tensorflow.python import keras\n",
            "from tensorflow.python import tf2\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.framework import tensor_shape\n",
            "from tensorflow.python.framework import test_util\n",
            "from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adagrad as adagrad_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adamax as adamax_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import nadam as nadam_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import rmsprop as rmsprop_v2\n",
            "from tensorflow.python.util import tf_contextlib\n",
            "from tensorflow.python.util import tf_inspect\n",
            "\n",
            "\n",
            "def get_test_data(train_samples,\n",
            "                  test_samples,\n",
            "                  input_shape,\n",
            "                  num_classes,\n",
            "                  random_seed=None):\n",
            "  \"\"\"Generates test data to train a model on.\n",
            "\n",
            "  Arguments:\n",
            "    train_samples: Integer, how many training samples to generate.\n",
            "    test_samples: Integer, how many test samples to generate.\n",
            "    input_shape: Tuple of integers, shape of the inputs.\n",
            "    num_classes: Integer, number of classes for the data and targets.\n",
            "    random_seed: Integer, random seed used by numpy to generate data.\n",
            "\n",
            "  Returns:\n",
            "    A tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
            "  \"\"\"\n",
            "  if random_seed is not None:\n",
            "    np.random.seed(random_seed)\n",
            "  num_sample = train_samples + test_samples\n",
            "  templates = 2 * num_classes * np.random.random((num_classes,) + input_shape)\n",
            "  y = np.random.randint(0, num_classes, size=(num_sample,))\n",
            "  x = np.zeros((num_sample,) + input_shape, dtype=np.float32)\n",
            "  for i in range(num_sample):\n",
            "    x[i] = templates[y[i]] + np.random.normal(loc=0, scale=1., size=input_shape)\n",
            "  return ((x[:train_samples], y[:train_samples]),\n",
            "          (x[train_samples:], y[train_samples:]))\n",
            "\n",
            "\n",
            "@test_util.use_deterministic_cudnn\n",
            "def layer_test(layer_cls, kwargs=None, input_shape=None, input_dtype=None,\n",
            "               input_data=None, expected_output=None,\n",
            "               expected_output_dtype=None):\n",
            "  \"\"\"Test routine for a layer with a single input and single output.\n",
            "\n",
            "  Arguments:\n",
            "    layer_cls: Layer class object.\n",
            "    kwargs: Optional dictionary of keyword arguments for instantiating the\n",
            "      layer.\n",
            "    input_shape: Input shape tuple.\n",
            "    input_dtype: Data type of the input data.\n",
            "    input_data: Numpy array of input data.\n",
            "    expected_output: Shape tuple for the expected shape of the output.\n",
            "    expected_output_dtype: Data type expected for the output.\n",
            "\n",
            "  Returns:\n",
            "    The output data (Numpy array) returned by the layer, for additional\n",
            "    checks to be done by the calling code.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if `input_shape is None`.\n",
            "  \"\"\"\n",
            "  if input_data is None:\n",
            "    if input_shape is None:\n",
            "      raise ValueError('input_shape is None')\n",
            "    if not input_dtype:\n",
            "      input_dtype = 'float32'\n",
            "    input_data_shape = list(input_shape)\n",
            "    for i, e in enumerate(input_data_shape):\n",
            "      if e is None:\n",
            "        input_data_shape[i] = np.random.randint(1, 4)\n",
            "    input_data = 10 * np.random.random(input_data_shape)\n",
            "    if input_dtype[:5] == 'float':\n",
            "      input_data -= 0.5\n",
            "    input_data = input_data.astype(input_dtype)\n",
            "  elif input_shape is None:\n",
            "    input_shape = input_data.shape\n",
            "  if input_dtype is None:\n",
            "    input_dtype = input_data.dtype\n",
            "  if expected_output_dtype is None:\n",
            "    expected_output_dtype = input_dtype\n",
            "\n",
            "  # instantiation\n",
            "  kwargs = kwargs or {}\n",
            "  layer = layer_cls(**kwargs)\n",
            "\n",
            "  # test get_weights , set_weights at layer level\n",
            "  weights = layer.get_weights()\n",
            "  layer.set_weights(weights)\n",
            "\n",
            "  # test and instantiation from weights\n",
            "  if 'weights' in tf_inspect.getargspec(layer_cls.__init__):\n",
            "    kwargs['weights'] = weights\n",
            "    layer = layer_cls(**kwargs)\n",
            "\n",
            "  # test in functional API\n",
            "  x = keras.layers.Input(shape=input_shape[1:], dtype=input_dtype)\n",
            "  y = layer(x)\n",
            "  if keras.backend.dtype(y) != expected_output_dtype:\n",
            "    raise AssertionError('When testing layer %s, for input %s, found output '\n",
            "                         'dtype=%s but expected to find %s.\\nFull kwargs: %s' %\n",
            "                         (layer_cls.__name__,\n",
            "                          x,\n",
            "                          keras.backend.dtype(y),\n",
            "                          expected_output_dtype,\n",
            "                          kwargs))\n",
            "  # check shape inference\n",
            "  model = keras.models.Model(x, y)\n",
            "  expected_output_shape = tuple(\n",
            "      layer.compute_output_shape(\n",
            "          tensor_shape.TensorShape(input_shape)).as_list())\n",
            "  actual_output = model.predict(input_data)\n",
            "  actual_output_shape = actual_output.shape\n",
            "  for expected_dim, actual_dim in zip(expected_output_shape,\n",
            "                                      actual_output_shape):\n",
            "    if expected_dim is not None:\n",
            "      if expected_dim != actual_dim:\n",
            "        raise AssertionError(\n",
            "            'When testing layer %s, for input %s, found output_shape='\n",
            "            '%s but expected to find %s.\\nFull kwargs: %s' %\n",
            "            (layer_cls.__name__,\n",
            "             x,\n",
            "             actual_output_shape,\n",
            "             expected_output_shape,\n",
            "             kwargs))\n",
            "  if expected_output is not None:\n",
            "    np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n",
            "\n",
            "  # test serialization, weight setting at model level\n",
            "  model_config = model.get_config()\n",
            "  recovered_model = keras.models.Model.from_config(model_config)\n",
            "  if model.weights:\n",
            "    weights = model.get_weights()\n",
            "    recovered_model.set_weights(weights)\n",
            "    output = recovered_model.predict(input_data)\n",
            "    np.testing.assert_allclose(output, actual_output, rtol=2e-3)\n",
            "\n",
            "  # test training mode (e.g. useful for dropout tests)\n",
            "  # Rebuild the model to avoid the graph being reused between predict() and\n",
            "  # train(). This was causing some error for layer with Defun as it body.\n",
            "  # See b/120160788 for more details. This should be mitigated after 2.0.\n",
            "  model = keras.models.Model(x, layer(x))\n",
            "  if _thread_local_data.run_eagerly is not None:\n",
            "    model.compile(\n",
            "        'rmsprop',\n",
            "        'mse',\n",
            "        weighted_metrics=['acc'],\n",
            "        run_eagerly=should_run_eagerly())\n",
            "  else:\n",
            "    model.compile('rmsprop', 'mse', weighted_metrics=['acc'])\n",
            "  model.train_on_batch(input_data, actual_output)\n",
            "\n",
            "  # test as first layer in Sequential API\n",
            "  layer_config = layer.get_config()\n",
            "  layer_config['batch_input_shape'] = input_shape\n",
            "  layer = layer.__class__.from_config(layer_config)\n",
            "\n",
            "  model = keras.models.Sequential()\n",
            "  model.add(layer)\n",
            "  actual_output = model.predict(input_data)\n",
            "  actual_output_shape = actual_output.shape\n",
            "  for expected_dim, actual_dim in zip(expected_output_shape,\n",
            "                                      actual_output_shape):\n",
            "    if expected_dim is not None:\n",
            "      if expected_dim != actual_dim:\n",
            "        raise AssertionError(\n",
            "            'When testing layer %s **after deserialization**, '\n",
            "            'for input %s, found output_shape='\n",
            "            '%s but expected to find inferred shape %s.\\nFull kwargs: %s' %\n",
            "            (layer_cls.__name__,\n",
            "             x,\n",
            "             actual_output_shape,\n",
            "             expected_output_shape,\n",
            "             kwargs))\n",
            "  if expected_output is not None:\n",
            "    np.testing.assert_allclose(actual_output, expected_output, rtol=1e-3)\n",
            "\n",
            "  # test serialization, weight setting at model level\n",
            "  model_config = model.get_config()\n",
            "  recovered_model = keras.models.Sequential.from_config(model_config)\n",
            "  if model.weights:\n",
            "    weights = model.get_weights()\n",
            "    recovered_model.set_weights(weights)\n",
            "    output = recovered_model.predict(input_data)\n",
            "    np.testing.assert_allclose(output, actual_output, rtol=2e-3)\n",
            "\n",
            "  # for further checks in the caller function\n",
            "  return actual_output\n",
            "\n",
            "\n",
            "_thread_local_data = threading.local()\n",
            "_thread_local_data.model_type = None\n",
            "_thread_local_data.run_eagerly = None\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def model_type_scope(value):\n",
            "  \"\"\"Provides a scope within which the model type to test is equal to `value`.\n",
            "\n",
            "  The model type gets restored to its original value upon exiting the scope.\n",
            "\n",
            "  Arguments:\n",
            "     value: model type value\n",
            "\n",
            "  Yields:\n",
            "    The provided value.\n",
            "  \"\"\"\n",
            "  previous_value = _thread_local_data.model_type\n",
            "  try:\n",
            "    _thread_local_data.model_type = value\n",
            "    yield value\n",
            "  finally:\n",
            "    # Restore model type to initial value.\n",
            "    _thread_local_data.model_type = previous_value\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def run_eagerly_scope(value):\n",
            "  \"\"\"Provides a scope within which we compile models to run eagerly or not.\n",
            "\n",
            "  The boolean gets restored to its original value upon exiting the scope.\n",
            "\n",
            "  Arguments:\n",
            "     value: Bool specifying if we should run models eagerly in the active test.\n",
            "     Should be True or False.\n",
            "\n",
            "  Yields:\n",
            "    The provided value.\n",
            "  \"\"\"\n",
            "  previous_value = _thread_local_data.run_eagerly\n",
            "  try:\n",
            "    _thread_local_data.run_eagerly = value\n",
            "    yield value\n",
            "  finally:\n",
            "    # Restore model type to initial value.\n",
            "    _thread_local_data.run_eagerly = previous_value\n",
            "\n",
            "\n",
            "def should_run_eagerly():\n",
            "  \"\"\"Returns whether the models we are testing should be run eagerly.\"\"\"\n",
            "  if _thread_local_data.run_eagerly is None:\n",
            "    raise ValueError('Cannot call `should_run_eagerly()` outside of a '\n",
            "                     '`run_eagerly_scope()` or `run_all_keras_modes` '\n",
            "                     'decorator.')\n",
            "\n",
            "  return _thread_local_data.run_eagerly and context.executing_eagerly()\n",
            "\n",
            "\n",
            "def get_model_type():\n",
            "  \"\"\"Gets the model type that should be tested.\"\"\"\n",
            "  if _thread_local_data.model_type is None:\n",
            "    raise ValueError('Cannot call `get_model_type()` outside of a '\n",
            "                     '`model_type_scope()` or `run_with_all_model_types` '\n",
            "                     'decorator.')\n",
            "\n",
            "  return _thread_local_data.model_type\n",
            "\n",
            "\n",
            "def get_small_sequential_mlp(num_hidden, num_classes, input_dim=None):\n",
            "  model = keras.models.Sequential()\n",
            "  if input_dim:\n",
            "    model.add(keras.layers.Dense(num_hidden, activation='relu',\n",
            "                                 input_dim=input_dim))\n",
            "  else:\n",
            "    model.add(keras.layers.Dense(num_hidden, activation='relu'))\n",
            "  activation = 'sigmoid' if num_classes == 1 else 'softmax'\n",
            "  model.add(keras.layers.Dense(num_classes, activation=activation))\n",
            "  return model\n",
            "\n",
            "\n",
            "def get_small_functional_mlp(num_hidden, num_classes, input_dim):\n",
            "  inputs = keras.Input(shape=(input_dim,))\n",
            "  outputs = keras.layers.Dense(num_hidden, activation='relu')(inputs)\n",
            "  activation = 'sigmoid' if num_classes == 1 else 'softmax'\n",
            "  outputs = keras.layers.Dense(num_classes, activation=activation)(outputs)\n",
            "  return keras.Model(inputs, outputs)\n",
            "\n",
            "\n",
            "class _SmallSubclassMLP(keras.Model):\n",
            "  \"\"\"A subclass model based small MLP.\"\"\"\n",
            "\n",
            "  def __init__(self, num_hidden, num_classes):\n",
            "    super(_SmallSubclassMLP, self).__init__()\n",
            "    self.layer_a = keras.layers.Dense(num_hidden, activation='relu')\n",
            "    activation = 'sigmoid' if num_classes == 1 else 'softmax'\n",
            "    self.layer_b = keras.layers.Dense(num_classes, activation=activation)\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    x = self.layer_a(inputs)\n",
            "    return self.layer_b(x)\n",
            "\n",
            "\n",
            "class _SmallSubclassMLPCustomBuild(keras.Model):\n",
            "  \"\"\"A subclass model small MLP that uses a custom build method.\"\"\"\n",
            "\n",
            "  def __init__(self, num_hidden, num_classes):\n",
            "    super(_SmallSubclassMLPCustomBuild, self).__init__()\n",
            "    self.layer_a = None\n",
            "    self.layer_b = None\n",
            "    self.num_hidden = num_hidden\n",
            "    self.num_classes = num_classes\n",
            "\n",
            "  def build(self, input_shape):\n",
            "    self.layer_a = keras.layers.Dense(self.num_hidden, activation='relu')\n",
            "    activation = 'sigmoid' if self.num_classes == 1 else 'softmax'\n",
            "    self.layer_b = keras.layers.Dense(self.num_classes, activation=activation)\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    x = self.layer_a(inputs)\n",
            "    return self.layer_b(x)\n",
            "\n",
            "\n",
            "def get_small_subclass_mlp(num_hidden, num_classes):\n",
            "  return _SmallSubclassMLP(num_hidden, num_classes)\n",
            "\n",
            "\n",
            "def get_small_subclass_mlp_with_custom_build(num_hidden, num_classes):\n",
            "  return _SmallSubclassMLPCustomBuild(num_hidden, num_classes)\n",
            "\n",
            "\n",
            "def get_small_mlp(num_hidden, num_classes, input_dim):\n",
            "  \"\"\"Get a small mlp of the model type specified by `get_model_type`.\"\"\"\n",
            "  model_type = get_model_type()\n",
            "  if model_type == 'subclass':\n",
            "    return get_small_subclass_mlp(num_hidden, num_classes)\n",
            "  if model_type == 'subclass_custom_build':\n",
            "    return get_small_subclass_mlp_with_custom_build(num_hidden, num_classes)\n",
            "  if model_type == 'sequential':\n",
            "    return get_small_sequential_mlp(num_hidden, num_classes, input_dim)\n",
            "  if model_type == 'functional':\n",
            "    return get_small_functional_mlp(num_hidden, num_classes, input_dim)\n",
            "  raise ValueError('Unknown model type {}'.format(model_type))\n",
            "\n",
            "\n",
            "class _SubclassModel(keras.Model):\n",
            "  \"\"\"A Keras subclass model.\"\"\"\n",
            "\n",
            "  def __init__(self, layers):\n",
            "    super(_SubclassModel, self).__init__()\n",
            "    # Note that clone and build doesn't support lists of layers in subclassed\n",
            "    # models. Adding each layer directly here.\n",
            "    for i, layer in enumerate(layers):\n",
            "      setattr(self, self._layer_name_for_i(i), layer)\n",
            "\n",
            "    self.num_layers = len(layers)\n",
            "\n",
            "  def _layer_name_for_i(self, i):\n",
            "    return 'layer{}'.format(i)\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    x = inputs\n",
            "    for i in range(self.num_layers):\n",
            "      layer = getattr(self, self._layer_name_for_i(i))\n",
            "      x = layer(x)\n",
            "    return x\n",
            "\n",
            "\n",
            "class _SubclassModelCustomBuild(keras.Model):\n",
            "  \"\"\"A Keras subclass model that uses a custom build method.\"\"\"\n",
            "\n",
            "  def __init__(self, layer_generating_func):\n",
            "    super(_SubclassModelCustomBuild, self).__init__()\n",
            "    self.all_layers = None\n",
            "    self._layer_generating_func = layer_generating_func\n",
            "\n",
            "  def build(self, input_shape):\n",
            "    layers = []\n",
            "    for layer in self._layer_generating_func():\n",
            "      layers.append(layer)\n",
            "    self.all_layers = layers\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    x = inputs\n",
            "    for layer in self.all_layers:\n",
            "      x = layer(x)\n",
            "    return x\n",
            "\n",
            "\n",
            "def get_model_from_layers(layers, input_shape=None, input_dtype=None):\n",
            "  \"\"\"Builds a model from a sequence of layers.\"\"\"\n",
            "  model_type = get_model_type()\n",
            "  if model_type == 'subclass':\n",
            "    return _SubclassModel(layers)\n",
            "\n",
            "  if model_type == 'subclass_custom_build':\n",
            "    layer_generating_func = lambda: layers\n",
            "    return _SubclassModelCustomBuild(layer_generating_func)\n",
            "\n",
            "  if model_type == 'sequential':\n",
            "    model = keras.models.Sequential()\n",
            "    if input_shape:\n",
            "      model.add(keras.layers.InputLayer(input_shape=input_shape,\n",
            "                                        dtype=input_dtype))\n",
            "    for layer in layers:\n",
            "      model.add(layer)\n",
            "    return model\n",
            "\n",
            "  if model_type == 'functional':\n",
            "    if not input_shape:\n",
            "      raise ValueError('Cannot create a functional model from layers with no '\n",
            "                       'input shape.')\n",
            "    inputs = keras.Input(shape=input_shape, dtype=input_dtype)\n",
            "    outputs = inputs\n",
            "    for layer in layers:\n",
            "      outputs = layer(outputs)\n",
            "    return keras.Model(inputs, outputs)\n",
            "\n",
            "  raise ValueError('Unknown model type {}'.format(model_type))\n",
            "\n",
            "\n",
            "class _MultiIOSubclassModel(keras.Model):\n",
            "  \"\"\"Multi IO Keras subclass model.\"\"\"\n",
            "\n",
            "  def __init__(self, branch_a, branch_b, shared_input_branch=None,\n",
            "               shared_output_branch=None):\n",
            "    super(_MultiIOSubclassModel, self).__init__()\n",
            "    self._shared_input_branch = shared_input_branch\n",
            "    self._branch_a = branch_a\n",
            "    self._branch_b = branch_b\n",
            "    self._shared_output_branch = shared_output_branch\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    if self._shared_input_branch:\n",
            "      for layer in self._shared_input_branch:\n",
            "        inputs = layer(inputs)\n",
            "      a = inputs\n",
            "      b = inputs\n",
            "    else:\n",
            "      a, b = inputs\n",
            "\n",
            "    for layer in self._branch_a:\n",
            "      a = layer(a)\n",
            "    for layer in self._branch_b:\n",
            "      b = layer(b)\n",
            "    outs = [a, b]\n",
            "\n",
            "    if self._shared_output_branch:\n",
            "      for layer in self._shared_output_branch:\n",
            "        outs = layer(outs)\n",
            "\n",
            "    return outs\n",
            "\n",
            "\n",
            "class _MultiIOSubclassModelCustomBuild(keras.Model):\n",
            "  \"\"\"Multi IO Keras subclass model that uses a custom build method.\"\"\"\n",
            "\n",
            "  def __init__(self, branch_a_func, branch_b_func,\n",
            "               shared_input_branch_func=None,\n",
            "               shared_output_branch_func=None):\n",
            "    super(_MultiIOSubclassModelCustomBuild, self).__init__()\n",
            "    self._shared_input_branch_func = shared_input_branch_func\n",
            "    self._branch_a_func = branch_a_func\n",
            "    self._branch_b_func = branch_b_func\n",
            "    self._shared_output_branch_func = shared_output_branch_func\n",
            "\n",
            "    self._shared_input_branch = None\n",
            "    self._branch_a = None\n",
            "    self._branch_b = None\n",
            "    self._shared_output_branch = None\n",
            "\n",
            "  def build(self, input_shape):\n",
            "    if self._shared_input_branch_func():\n",
            "      self._shared_input_branch = self._shared_input_branch_func()\n",
            "    self._branch_a = self._branch_a_func()\n",
            "    self._branch_b = self._branch_b_func()\n",
            "\n",
            "    if self._shared_output_branch_func():\n",
            "      self._shared_output_branch = self._shared_output_branch_func()\n",
            "\n",
            "  def call(self, inputs, **kwargs):\n",
            "    if self._shared_input_branch:\n",
            "      for layer in self._shared_input_branch:\n",
            "        inputs = layer(inputs)\n",
            "      a = inputs\n",
            "      b = inputs\n",
            "    else:\n",
            "      a, b = inputs\n",
            "\n",
            "    for layer in self._branch_a:\n",
            "      a = layer(a)\n",
            "    for layer in self._branch_b:\n",
            "      b = layer(b)\n",
            "    outs = a, b\n",
            "\n",
            "    if self._shared_output_branch:\n",
            "      for layer in self._shared_output_branch:\n",
            "        outs = layer(outs)\n",
            "\n",
            "    return outs\n",
            "\n",
            "\n",
            "def get_multi_io_model(\n",
            "    branch_a,\n",
            "    branch_b,\n",
            "    shared_input_branch=None,\n",
            "    shared_output_branch=None):\n",
            "  \"\"\"Builds a multi-io model that contains two branches.\n",
            "\n",
            "  The produced model will be of the type specified by `get_model_type`.\n",
            "\n",
            "  To build a two-input, two-output model:\n",
            "    Specify a list of layers for branch a and branch b, but do not specify any\n",
            "    shared input branch or shared output branch. The resulting model will apply\n",
            "    each branch to a different input, to produce two outputs.\n",
            "\n",
            "    The first value in branch_a must be the Keras 'Input' layer for branch a,\n",
            "    and the first value in branch_b must be the Keras 'Input' layer for\n",
            "    branch b.\n",
            "\n",
            "    example usage:\n",
            "    ```\n",
            "    branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]\n",
            "    branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]\n",
            "\n",
            "    model = get_multi_io_model(branch_a, branch_b)\n",
            "    ```\n",
            "\n",
            "  To build a two-input, one-output model:\n",
            "    Specify a list of layers for branch a and branch b, and specify a\n",
            "    shared output branch. The resulting model will apply\n",
            "    each branch to a different input. It will then apply the shared output\n",
            "    branch to a tuple containing the intermediate outputs of each branch,\n",
            "    to produce a single output. The first layer in the shared_output_branch\n",
            "    must be able to merge a tuple of two tensors.\n",
            "\n",
            "    The first value in branch_a must be the Keras 'Input' layer for branch a,\n",
            "    and the first value in branch_b must be the Keras 'Input' layer for\n",
            "    branch b.\n",
            "\n",
            "    example usage:\n",
            "    ```\n",
            "    input_branch_a = [Input(shape=(2,), name='a'), Dense(), Dense()]\n",
            "    input_branch_b = [Input(shape=(3,), name='b'), Dense(), Dense()]\n",
            "    shared_output_branch = [Concatenate(), Dense(), Dense()]\n",
            "\n",
            "    model = get_multi_io_model(input_branch_a, input_branch_b,\n",
            "                               shared_output_branch=shared_output_branch)\n",
            "    ```\n",
            "  To build a one-input, two-output model:\n",
            "    Specify a list of layers for branch a and branch b, and specify a\n",
            "    shared input branch. The resulting model will take one input, and apply\n",
            "    the shared input branch to it. It will then respectively apply each branch\n",
            "    to that intermediate result in parallel, to produce two outputs.\n",
            "\n",
            "    The first value in the shared_input_branch must be the Keras 'Input' layer\n",
            "    for the whole model. Branch a and branch b should not contain any Input\n",
            "    layers.\n",
            "\n",
            "    example usage:\n",
            "    ```\n",
            "    shared_input_branch = [Input(shape=(2,), name='in'), Dense(), Dense()]\n",
            "    output_branch_a = [Dense(), Dense()]\n",
            "    output_branch_b = [Dense(), Dense()]\n",
            "\n",
            "\n",
            "    model = get_multi_io_model(output__branch_a, output_branch_b,\n",
            "                               shared_input_branch=shared_input_branch)\n",
            "    ```\n",
            "\n",
            "  Args:\n",
            "    branch_a: A sequence of layers for branch a of the model.\n",
            "    branch_b: A sequence of layers for branch b of the model.\n",
            "    shared_input_branch: An optional sequence of layers to apply to a single\n",
            "      input, before applying both branches to that intermediate result. If set,\n",
            "      the model will take only one input instead of two. Defaults to None.\n",
            "    shared_output_branch: An optional sequence of layers to merge the\n",
            "      intermediate results produced by branch a and branch b. If set,\n",
            "      the model will produce only one output instead of two. Defaults to None.\n",
            "\n",
            "  Returns:\n",
            "    A multi-io model of the type specified by `get_model_type`, specified\n",
            "    by the different branches.\n",
            "  \"\"\"\n",
            "  # Extract the functional inputs from the layer lists\n",
            "  if shared_input_branch:\n",
            "    inputs = shared_input_branch[0]\n",
            "    shared_input_branch = shared_input_branch[1:]\n",
            "  else:\n",
            "    inputs = branch_a[0], branch_b[0]\n",
            "    branch_a = branch_a[1:]\n",
            "    branch_b = branch_b[1:]\n",
            "\n",
            "  model_type = get_model_type()\n",
            "  if model_type == 'subclass':\n",
            "    return _MultiIOSubclassModel(branch_a, branch_b, shared_input_branch,\n",
            "                                 shared_output_branch)\n",
            "\n",
            "  if model_type == 'subclass_custom_build':\n",
            "    return _MultiIOSubclassModelCustomBuild((lambda: branch_a),\n",
            "                                            (lambda: branch_b),\n",
            "                                            (lambda: shared_input_branch),\n",
            "                                            (lambda: shared_output_branch))\n",
            "\n",
            "  if model_type == 'sequential':\n",
            "    raise ValueError('Cannot use `get_multi_io_model` to construct '\n",
            "                     'sequential models')\n",
            "\n",
            "  if model_type == 'functional':\n",
            "    if shared_input_branch:\n",
            "      a_and_b = inputs\n",
            "      for layer in shared_input_branch:\n",
            "        a_and_b = layer(a_and_b)\n",
            "      a = a_and_b\n",
            "      b = a_and_b\n",
            "    else:\n",
            "      a, b = inputs\n",
            "\n",
            "    for layer in branch_a:\n",
            "      a = layer(a)\n",
            "    for layer in branch_b:\n",
            "      b = layer(b)\n",
            "    outputs = a, b\n",
            "\n",
            "    if shared_output_branch:\n",
            "      for layer in shared_output_branch:\n",
            "        outputs = layer(outputs)\n",
            "\n",
            "    return keras.Model(inputs, outputs)\n",
            "\n",
            "  raise ValueError('Unknown model type {}'.format(model_type))\n",
            "\n",
            "\n",
            "_V2_OPTIMIZER_MAP = {\n",
            "    'adadelta': adadelta_v2.Adadelta,\n",
            "    'adagrad': adagrad_v2.Adagrad,\n",
            "    'adam': adam_v2.Adam,\n",
            "    'adamax': adamax_v2.Adamax,\n",
            "    'nadam': nadam_v2.Nadam,\n",
            "    'rmsprop': rmsprop_v2.RMSprop,\n",
            "    'sgd': gradient_descent_v2.SGD\n",
            "}\n",
            "\n",
            "\n",
            "def get_v2_optimizer(name, **kwargs):\n",
            "  \"\"\"Get the v2 optimizer requested.\n",
            "\n",
            "  This is only necessary until v2 are the default, as we are testing in Eager,\n",
            "  and Eager + v1 optimizers fail tests. When we are in v2, the strings alone\n",
            "  should be sufficient, and this mapping can theoretically be removed.\n",
            "\n",
            "  Args:\n",
            "    name: string name of Keras v2 optimizer.\n",
            "    **kwargs: any kwargs to pass to the optimizer constructor.\n",
            "\n",
            "  Returns:\n",
            "    Initialized Keras v2 optimizer.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if an unknown name was passed.\n",
            "  \"\"\"\n",
            "  try:\n",
            "    return _V2_OPTIMIZER_MAP[name](**kwargs)\n",
            "  except KeyError:\n",
            "    raise ValueError(\n",
            "        'Could not find requested v2 optimizer: {}\\nValid choices: {}'.format(\n",
            "            name, list(_V2_OPTIMIZER_MAP.keys())))\n",
            "\n",
            "\n",
            "def get_expected_metric_variable_names(var_names, name_suffix=''):\n",
            "  \"\"\"Returns expected metric variable names given names and prefix/suffix.\"\"\"\n",
            "  if tf2.enabled() or context.executing_eagerly():\n",
            "    # In V1 eager mode and V2 variable names are not made unique.\n",
            "    return [n + ':0' for n in var_names]\n",
            "  # In V1 graph mode variable names are made unique using a suffix.\n",
            "  return [n + name_suffix + ':0' for n in var_names]\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Built-in loss functions.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import abc\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python.distribute import distribution_strategy_context\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.framework import smart_cond\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils import losses_utils\n",
            "from tensorflow.python.keras.utils import tf_utils\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import nn\n",
            "from tensorflow.python.ops.losses import losses_impl\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "from tensorflow.tools.docs import doc_controls\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.Loss')\n",
            "class Loss(object):\n",
            "  \"\"\"Loss base class.\n",
            "\n",
            "  To be implemented by subclasses:\n",
            "  * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`.\n",
            "\n",
            "  Example subclass implementation:\n",
            "  ```\n",
            "  class MeanSquaredError(Loss):\n",
            "    def call(self, y_true, y_pred):\n",
            "      y_pred = ops.convert_to_tensor(y_pred)\n",
            "      y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "      return K.mean(math_ops.square(y_pred - y_true), axis=-1)\n",
            "  ```\n",
            "\n",
            "  When used with `tf.distribute.Strategy`, outside of built-in training loops\n",
            "  such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction\n",
            "  types, and reduce losses explicitly in your training loop. Using 'AUTO' or\n",
            "  'SUM_OVER_BATCH_SIZE' will raise an error.\n",
            "\n",
            "  Please see\n",
            "  https://www.tensorflow.org/alpha/tutorials/distribute/training_loops for more\n",
            "  details on this.\n",
            "\n",
            "  You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n",
            "  ```\n",
            "  with strategy.scope():\n",
            "    loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
            "        reduction=tf.keras.losses.Reduction.NONE)\n",
            "    ....\n",
            "    loss = (tf.reduce_sum(loss_obj(labels, predictions)) *\n",
            "            (1. / global_batch_size))\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: Optional name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name=None):\n",
            "    losses_utils.ReductionV2.validate(reduction)\n",
            "    self.reduction = reduction\n",
            "    self.name = name\n",
            "\n",
            "  def __call__(self, y_true, y_pred, sample_weight=None):\n",
            "    \"\"\"Invokes the `Loss` instance.\n",
            "\n",
            "    Args:\n",
            "      y_true: Ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank\n",
            "        as `y_true`, or is broadcastable to `y_true`. `sample_weight` acts as a\n",
            "        coefficient for the loss. If a scalar is provided, then the loss is\n",
            "        simply scaled by the given value. If `sample_weight` is a tensor of size\n",
            "        `[batch_size]`, then the total loss for each sample of the batch is\n",
            "        rescaled by the corresponding element in the `sample_weight` vector. If\n",
            "        the shape of `sample_weight` matches the shape of `y_pred`, then the\n",
            "        loss of each measurable element of `y_pred` is scaled by the\n",
            "        corresponding value of `sample_weight`.\n",
            "\n",
            "    Returns:\n",
            "      Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same\n",
            "        shape as `y_true`; otherwise, it is scalar.\n",
            "\n",
            "    Raises:\n",
            "      ValueError: If the shape of `sample_weight` is invalid.\n",
            "    \"\"\"\n",
            "    # If we are wrapping a lambda function strip '<>' from the name as it is not\n",
            "    # accepted in scope name.\n",
            "    scope_name = 'lambda' if self.name == '<lambda>' else self.name\n",
            "    graph_ctx = tf_utils.graph_context_for_symbolic_tensors(\n",
            "        y_true, y_pred, sample_weight)\n",
            "    with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:\n",
            "      losses = self.call(y_true, y_pred)\n",
            "      return losses_utils.compute_weighted_loss(\n",
            "          losses, sample_weight, reduction=self._get_reduction())\n",
            "\n",
            "  @classmethod\n",
            "  def from_config(cls, config):\n",
            "    \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\n",
            "\n",
            "    Args:\n",
            "        config: Output of `get_config()`.\n",
            "\n",
            "    Returns:\n",
            "        A `Loss` instance.\n",
            "    \"\"\"\n",
            "    return cls(**config)\n",
            "\n",
            "  def get_config(self):\n",
            "    return {'reduction': self.reduction, 'name': self.name}\n",
            "\n",
            "  @abc.abstractmethod\n",
            "  @doc_controls.for_subclass_implementers\n",
            "  def call(self, y_true, y_pred):\n",
            "    \"\"\"Invokes the `Loss` instance.\n",
            "\n",
            "    Args:\n",
            "      y_true: Ground truth values, with the same shape as 'y_pred'.\n",
            "      y_pred: The predicted values.\n",
            "    \"\"\"\n",
            "    NotImplementedError('Must be implemented in subclasses.')\n",
            "\n",
            "  def _get_reduction(self):\n",
            "    \"\"\"Handles `AUTO` reduction cases and returns the reduction value.\"\"\"\n",
            "    if distribution_strategy_context.has_strategy() and (\n",
            "        self.reduction == losses_utils.ReductionV2.AUTO or\n",
            "        self.reduction == losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE):\n",
            "      raise ValueError(\n",
            "          'Please use `tf.keras.losses.Reduction.SUM` or '\n",
            "          '`tf.keras.losses.Reduction.NONE` for loss reduction when losses are '\n",
            "          'used with `tf.distribute.Strategy` outside of the built-in training '\n",
            "          'loops. You can implement '\n",
            "          '`tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch '\n",
            "          'size like:\\n```\\nwith strategy.scope():\\n'\n",
            "          '    loss_obj = tf.keras.losses.CategoricalCrossentropy('\n",
            "          'reduction=tf.keras.losses.reduction.None)\\n....\\n'\n",
            "          '    loss = tf.reduce_sum(loss_obj(labels, predictions)) * '\n",
            "          '(1. / global_batch_size)\\n```\\nPlease see '\n",
            "          'https://www.tensorflow.org/alpha/tutorials/distribute/training_loops'\n",
            "          ' for more details.')\n",
            "\n",
            "    if self.reduction == losses_utils.ReductionV2.AUTO:\n",
            "      return losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n",
            "    return self.reduction\n",
            "\n",
            "\n",
            "class LossFunctionWrapper(Loss):\n",
            "  \"\"\"Wraps a loss function in the `Loss` class.\n",
            "\n",
            "  Args:\n",
            "    fn: The loss function to wrap, with signature `fn(y_true, y_pred,\n",
            "      **kwargs)`.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: (Optional) name for the loss.\n",
            "    **kwargs: The keyword arguments that are passed on to `fn`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               fn,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name=None,\n",
            "               **kwargs):\n",
            "    super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n",
            "    self.fn = fn\n",
            "    self._fn_kwargs = kwargs\n",
            "\n",
            "  def call(self, y_true, y_pred):\n",
            "    \"\"\"Invokes the `LossFunctionWrapper` instance.\n",
            "\n",
            "    Args:\n",
            "      y_true: Ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "\n",
            "    Returns:\n",
            "      Loss values per sample.\n",
            "    \"\"\"\n",
            "    return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {}\n",
            "    for k, v in six.iteritems(self._fn_kwargs):\n",
            "      config[k] = K.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n",
            "    base_config = super(LossFunctionWrapper, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.MeanSquaredError')\n",
            "class MeanSquaredError(LossFunctionWrapper):\n",
            "  \"\"\"Computes the mean of squares of errors between labels and predictions.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n",
            "  then the mean squared error value is 3/4 (0.75).\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  mse = tf.keras.losses.MeanSquaredError()\n",
            "  loss = mse([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.MeanSquaredError())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='mean_squared_error'):\n",
            "    super(MeanSquaredError, self).__init__(\n",
            "        mean_squared_error, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.MeanAbsoluteError')\n",
            "class MeanAbsoluteError(LossFunctionWrapper):\n",
            "  \"\"\"Computes the mean of absolute difference between labels and predictions.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n",
            "  then the mean absolute error value is 3/4 (0.75).\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  mae = tf.keras.losses.MeanAbsoluteError()\n",
            "  loss = mae([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.75\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.MeanAbsoluteError())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='mean_absolute_error'):\n",
            "    super(MeanAbsoluteError, self).__init__(\n",
            "        mean_absolute_error, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.MeanAbsolutePercentageError')\n",
            "class MeanAbsolutePercentageError(LossFunctionWrapper):\n",
            "  \"\"\"Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n",
            "  then the mean absolute percentage error value is 5e+08.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
            "  loss = mape([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 5e+08\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.MeanAbsolutePercentageError())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='mean_absolute_percentage_error'):\n",
            "    super(MeanAbsolutePercentageError, self).__init__(\n",
            "        mean_absolute_percentage_error, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.MeanSquaredLogarithmicError')\n",
            "class MeanSquaredLogarithmicError(LossFunctionWrapper):\n",
            "  \"\"\"Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
            "\n",
            "  For example, if `y_true` is [0., 0., 1., 1.] and `y_pred` is [1., 1., 1., 0.]\n",
            "  then the mean squared logarithmic error value is 0.36034.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
            "  loss = msle([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.36034\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.MeanSquaredLogarithmicError())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='mean_squared_logarithmic_error'):\n",
            "    super(MeanSquaredLogarithmicError, self).__init__(\n",
            "        mean_squared_logarithmic_error, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.BinaryCrossentropy')\n",
            "class BinaryCrossentropy(LossFunctionWrapper):\n",
            "  \"\"\"Computes the cross-entropy loss between true labels and predicted labels.\n",
            "\n",
            "  Use this cross-entropy loss when there are only two label classes (assumed to\n",
            "  be 0 and 1). For each example, there should be a single floating-point value\n",
            "  per prediction.\n",
            "\n",
            "  In the snippet below, each of the four examples has only a single\n",
            "  floating-pointing value, and both `y_pred` and `y_true` have the shape\n",
            "  `[batch_size]`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  bce = tf.keras.losses.BinaryCrossentropy()\n",
            "  loss = bce([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 11.522857\n",
            "  ```\n",
            "\n",
            "  Usage with the `tf.keras` API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.BinaryCrossentropy())\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    from_logits: Whether to interpret `y_pred` as a tensor of\n",
            "      [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we assume\n",
            "        that `y_pred` contains probabilities (i.e., values in [0, 1]).\n",
            "    label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When > 0, we\n",
            "      compute the loss between the predicted labels and a smoothed version of\n",
            "      the true labels, where the smoothing squeezes the labels towards 0.5.\n",
            "      Larger values of `label_smoothing` correspond to heavier smoothing.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: (Optional) Name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               from_logits=False,\n",
            "               label_smoothing=0,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='binary_crossentropy'):\n",
            "    super(BinaryCrossentropy, self).__init__(\n",
            "        binary_crossentropy,\n",
            "        name=name,\n",
            "        reduction=reduction,\n",
            "        from_logits=from_logits,\n",
            "        label_smoothing=label_smoothing)\n",
            "    self.from_logits = from_logits\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.CategoricalCrossentropy')\n",
            "class CategoricalCrossentropy(LossFunctionWrapper):\n",
            "  \"\"\"Computes the crossentropy loss between the labels and predictions.\n",
            "\n",
            "  Use this crossentropy loss function when there are two or more label classes.\n",
            "  We expect labels to be provided in a `one_hot` representation. If you want to\n",
            "  provide labels as integers, please use `SparseCategoricalCrossentropy` loss.\n",
            "  There should be `# classes` floating point values per feature.\n",
            "\n",
            "  In the snippet below, there is `# classes` floating pointing values per\n",
            "  example. The shape of both `y_pred` and `y_true` are\n",
            "  `[batch_size, num_classes]`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  cce = tf.keras.losses.CategoricalCrossentropy()\n",
            "  loss = cce(\n",
            "    [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]],\n",
            "    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.3239\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.CategoricalCrossentropy())\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
            "      we assume that `y_pred` encodes a probability distribution.\n",
            "    label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n",
            "      meaning the confidence on label values are relaxed. e.g.\n",
            "      `label_smoothing=0.2` means that we will use a value of `0.1` for label\n",
            "      `0` and `0.9` for label `1`\"\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: Optional name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               from_logits=False,\n",
            "               label_smoothing=0,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='categorical_crossentropy'):\n",
            "    super(CategoricalCrossentropy, self).__init__(\n",
            "        categorical_crossentropy,\n",
            "        name=name,\n",
            "        reduction=reduction,\n",
            "        from_logits=from_logits,\n",
            "        label_smoothing=label_smoothing)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.SparseCategoricalCrossentropy')\n",
            "class SparseCategoricalCrossentropy(LossFunctionWrapper):\n",
            "  \"\"\"Computes the crossentropy loss between the labels and predictions.\n",
            "\n",
            "  Use this crossentropy loss function when there are two or more label classes.\n",
            "  We expect labels to be provided as integers. If you want to provide labels\n",
            "  using `one-hot` representation, please use `CategoricalCrossentropy` loss.\n",
            "  There should be `# classes` floating point values per feature for `y_pred`\n",
            "  and a single floating point value per feature for `y_true`.\n",
            "\n",
            "  In the snippet below, there is a single floating point value per example for\n",
            "  `y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
            "  The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
            "  `[batch_size, num_classes]`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
            "  loss = cce(\n",
            "    [0, 1, 2],\n",
            "    [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.3239\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
            "      we assume that `y_pred` encodes a probability distribution.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: Optional name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               from_logits=False,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name=None):\n",
            "    super(SparseCategoricalCrossentropy, self).__init__(\n",
            "        sparse_categorical_crossentropy,\n",
            "        name=name,\n",
            "        reduction=reduction,\n",
            "        from_logits=from_logits)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.Hinge')\n",
            "class Hinge(LossFunctionWrapper):\n",
            "  \"\"\"Computes the hinge loss between `y_true` and `y_pred`.\n",
            "\n",
            "  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
            "  provided we will convert them to -1 or 1.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  h = tf.keras.losses.Hinge()\n",
            "  loss = h([-1., 1., 1.], [0.6, -0.7, -0.5])\n",
            "\n",
            "  # loss = max(0, 1 - y_true * y_pred) = [1.6 + 1.7 + 1.5] / 3\n",
            "\n",
            "  print('Loss: ', loss.numpy())  # Loss: 1.6\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.Hinge())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name=None):\n",
            "    super(Hinge, self).__init__(hinge, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.SquaredHinge')\n",
            "class SquaredHinge(LossFunctionWrapper):\n",
            "  \"\"\"Computes the squared hinge loss between `y_true` and `y_pred`.\n",
            "\n",
            "  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
            "  provided we will convert them to -1 or 1.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  sh = tf.keras.losses.SquaredHinge()\n",
            "  loss = sh([-1., 1., 1.], [0.6, -0.7, -0.5])\n",
            "\n",
            "  # loss = (max(0, 1 - y_true * y_pred))^2 = [1.6^2 + 1.7^2 + 1.5^2] / 3\n",
            "\n",
            "  print('Loss: ', loss.numpy())  # Loss: 2.566666\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.SquaredHinge())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='squared_hinge'):\n",
            "    super(SquaredHinge, self).__init__(\n",
            "        squared_hinge, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.CategoricalHinge')\n",
            "class CategoricalHinge(LossFunctionWrapper):\n",
            "  \"\"\"Computes the categorical hinge loss between `y_true` and `y_pred`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  ch = tf.keras.losses.CategoricalHinge()\n",
            "  loss = ch([0., 1., 1.], [1., 0., 1.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 1.0\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.CategoricalHinge())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='categorical_hinge'):\n",
            "    super(CategoricalHinge, self).__init__(\n",
            "        categorical_hinge, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.Poisson')\n",
            "class Poisson(LossFunctionWrapper):\n",
            "  \"\"\"Computes the Poisson loss between `y_true` and `y_pred`.\n",
            "\n",
            "  `loss = y_pred - y_true * log(y_pred)`\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  p = tf.keras.losses.Poisson()\n",
            "  loss = p([1, 9, 2], [4, 8, 12])\n",
            "  print('Loss: ', loss.numpy())  # Loss: -4.63\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.Poisson())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name='poisson'):\n",
            "    super(Poisson, self).__init__(poisson, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.LogCosh')\n",
            "class LogCosh(LossFunctionWrapper):\n",
            "  \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
            "\n",
            "  `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred - y_true)\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  l = tf.keras.losses.LogCosh()\n",
            "  loss = l([0., 1., 1.], [1., 0., 1.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.289\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.LogCosh())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name='logcosh'):\n",
            "    super(LogCosh, self).__init__(logcosh, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.KLDivergence')\n",
            "class KLDivergence(LossFunctionWrapper):\n",
            "  \"\"\"Computes Kullback Leibler divergence loss between `y_true` and `y_pred`.\n",
            "\n",
            "  `loss = y_true * log(y_true / y_pred)`\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  k = tf.keras.losses.KLDivergence()\n",
            "  loss = k([.4, .9, .2], [.5, .8, .12])\n",
            "  print('Loss: ', loss.numpy())  # Loss: -0.043\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.KLDivergence())\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='kullback_leibler_divergence'):\n",
            "    super(KLDivergence, self).__init__(\n",
            "        kullback_leibler_divergence, name=name, reduction=reduction)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.Huber')\n",
            "class Huber(LossFunctionWrapper):\n",
            "  \"\"\"Computes the Huber loss between `y_true` and `y_pred`.\n",
            "\n",
            "  For each value x in `error=y_true-y_pred`, the following is calculated:\n",
            "\n",
            "  ```\n",
            "  0.5 * x^2                  if |x| <= d\n",
            "  0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
            "  ```\n",
            "  where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  l = tf.keras.losses.Huber()\n",
            "  loss = l([0., 1., 1.], [1., 0., 1.])\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.333\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.Huber())\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    delta: A float, the point where the Huber loss function changes from a\n",
            "      quadratic to linear.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: Optional name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               delta=1.0,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='huber_loss'):\n",
            "    super(Huber, self).__init__(\n",
            "        huber_loss, name=name, reduction=reduction, delta=delta)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.mean_squared_error',\n",
            "              'keras.metrics.mse',\n",
            "              'keras.metrics.MSE',\n",
            "              'keras.losses.mean_squared_error',\n",
            "              'keras.losses.mse',\n",
            "              'keras.losses.MSE')\n",
            "def mean_squared_error(y_true, y_pred):\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  return K.mean(math_ops.squared_difference(y_pred, y_true), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.mean_absolute_error',\n",
            "              'keras.metrics.mae',\n",
            "              'keras.metrics.MAE',\n",
            "              'keras.losses.mean_absolute_error',\n",
            "              'keras.losses.mae',\n",
            "              'keras.losses.MAE')\n",
            "def mean_absolute_error(y_true, y_pred):\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  return K.mean(math_ops.abs(y_pred - y_true), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.mean_absolute_percentage_error',\n",
            "              'keras.metrics.mape',\n",
            "              'keras.metrics.MAPE',\n",
            "              'keras.losses.mean_absolute_percentage_error',\n",
            "              'keras.losses.mape',\n",
            "              'keras.losses.MAPE')\n",
            "def mean_absolute_percentage_error(y_true, y_pred):  # pylint: disable=missing-docstring\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  diff = math_ops.abs(\n",
            "      (y_true - y_pred) / K.clip(math_ops.abs(y_true), K.epsilon(), None))\n",
            "  return 100. * K.mean(diff, axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.mean_squared_logarithmic_error',\n",
            "              'keras.metrics.msle',\n",
            "              'keras.metrics.MSLE',\n",
            "              'keras.losses.mean_squared_logarithmic_error',\n",
            "              'keras.losses.msle',\n",
            "              'keras.losses.MSLE')\n",
            "def mean_squared_logarithmic_error(y_true, y_pred):  # pylint: disable=missing-docstring\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  first_log = math_ops.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
            "  second_log = math_ops.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
            "  return K.mean(math_ops.squared_difference(first_log, second_log), axis=-1)\n",
            "\n",
            "\n",
            "def _maybe_convert_labels(y_true):\n",
            "  \"\"\"Converts binary labels into -1/1.\"\"\"\n",
            "  are_zeros = math_ops.equal(y_true, 0)\n",
            "  are_ones = math_ops.equal(y_true, 1)\n",
            "  is_binary = math_ops.reduce_all(math_ops.logical_or(are_zeros, are_ones))\n",
            "\n",
            "  def _convert_binary_labels():\n",
            "    # Convert the binary labels to -1 or 1.\n",
            "    return 2. * y_true - 1.\n",
            "\n",
            "  updated_y_true = smart_cond.smart_cond(is_binary,\n",
            "                                         _convert_binary_labels, lambda: y_true)\n",
            "  return updated_y_true\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.squared_hinge', 'keras.losses.squared_hinge')\n",
            "def squared_hinge(y_true, y_pred):\n",
            "  \"\"\"Computes the squared hinge loss between `y_true` and `y_pred`.\n",
            "\n",
            "  Args:\n",
            "    y_true: The ground truth values. `y_true` values are expected to be -1 or 1.\n",
            "      If binary (0 or 1) labels are provided we will convert them to -1 or 1.\n",
            "    y_pred: The predicted values.\n",
            "\n",
            "  Returns:\n",
            "    Tensor with one scalar loss entry per sample.\n",
            "  \"\"\"\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  y_true = _maybe_convert_labels(y_true)\n",
            "  return K.mean(\n",
            "      math_ops.square(math_ops.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.hinge', 'keras.losses.hinge')\n",
            "def hinge(y_true, y_pred):\n",
            "  \"\"\"Computes the hinge loss between `y_true` and `y_pred`.\n",
            "\n",
            "  Args:\n",
            "    y_true: The ground truth values. `y_true` values are expected to be -1 or 1.\n",
            "      If binary (0 or 1) labels are provided we will convert them to -1 or 1.\n",
            "    y_pred: The predicted values.\n",
            "\n",
            "  Returns:\n",
            "    Tensor with one scalar loss entry per sample.\n",
            "  \"\"\"\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  y_true = _maybe_convert_labels(y_true)\n",
            "  return K.mean(math_ops.maximum(1. - y_true * y_pred, 0.), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.categorical_hinge')\n",
            "def categorical_hinge(y_true, y_pred):\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  pos = math_ops.reduce_sum(y_true * y_pred, axis=-1)\n",
            "  neg = math_ops.reduce_max((1. - y_true) * y_pred, axis=-1)\n",
            "  return math_ops.maximum(0., neg - pos + 1.)\n",
            "\n",
            "\n",
            "def huber_loss(y_true, y_pred, delta=1.0):\n",
            "  \"\"\"Computes Huber loss value.\n",
            "\n",
            "  For each value x in `error=y_true-y_pred`, the following is calculated:\n",
            "\n",
            "  ```\n",
            "  0.5 * x^2                  if |x| <= d\n",
            "  0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
            "  ```\n",
            "  where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n",
            "\n",
            "  Args:\n",
            "    y_true: tensor of true targets.\n",
            "    y_pred: tensor of predicted targets.\n",
            "    delta: A float, the point where the Huber loss function changes from a\n",
            "      quadratic to linear.\n",
            "\n",
            "  Returns:\n",
            "    Tensor with one scalar loss entry per sample.\n",
            "  \"\"\"\n",
            "  y_pred = math_ops.cast(y_pred, dtype=K.floatx())\n",
            "  y_true = math_ops.cast(y_true, dtype=K.floatx())\n",
            "  error = math_ops.subtract(y_pred, y_true)\n",
            "  abs_error = math_ops.abs(error)\n",
            "  quadratic = math_ops.minimum(abs_error, delta)\n",
            "  linear = math_ops.subtract(abs_error, quadratic)\n",
            "  return math_ops.add(\n",
            "      math_ops.multiply(\n",
            "          ops.convert_to_tensor(0.5, dtype=quadratic.dtype),\n",
            "          math_ops.multiply(quadratic, quadratic)),\n",
            "      math_ops.multiply(delta, linear))\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.logcosh')\n",
            "def logcosh(y_true, y_pred):\n",
            "  \"\"\"Logarithm of the hyperbolic cosine of the prediction error.\n",
            "\n",
            "  `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n",
            "  to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n",
            "  like the mean squared error, but will not be so strongly affected by the\n",
            "  occasional wildly incorrect prediction.\n",
            "\n",
            "  Arguments:\n",
            "      y_true: tensor of true targets.\n",
            "      y_pred: tensor of predicted targets.\n",
            "\n",
            "  Returns:\n",
            "      Tensor with one scalar loss entry per sample.\n",
            "  \"\"\"\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "\n",
            "  def _logcosh(x):\n",
            "    return x + nn.softplus(-2. * x) - math_ops.log(2.)\n",
            "\n",
            "  return K.mean(_logcosh(y_pred - y_true), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.categorical_crossentropy',\n",
            "              'keras.losses.categorical_crossentropy')\n",
            "def categorical_crossentropy(y_true,\n",
            "                             y_pred,\n",
            "                             from_logits=False,\n",
            "                             label_smoothing=0):\n",
            "  \"\"\"Computes the categorical crossentropy loss.\n",
            "\n",
            "  Args:\n",
            "    y_true: tensor of true targets.\n",
            "    y_pred: tensor of predicted targets.\n",
            "    from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
            "      we assume that `y_pred` encodes a probability distribution.\n",
            "    label_smoothing: Float in [0, 1]. If > `0` then smooth the labels.\n",
            "\n",
            "  Returns:\n",
            "    Categorical crossentropy loss value.\n",
            "  \"\"\"\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  label_smoothing = ops.convert_to_tensor(label_smoothing, dtype=K.floatx())\n",
            "\n",
            "  def _smooth_labels():\n",
            "    num_classes = math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)\n",
            "    return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\n",
            "\n",
            "  y_true = smart_cond.smart_cond(label_smoothing,\n",
            "                                 _smooth_labels, lambda: y_true)\n",
            "  return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.sparse_categorical_crossentropy',\n",
            "              'keras.losses.sparse_categorical_crossentropy')\n",
            "def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n",
            "  return K.sparse_categorical_crossentropy(\n",
            "      y_true, y_pred, from_logits=from_logits, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.binary_crossentropy',\n",
            "              'keras.losses.binary_crossentropy')\n",
            "def binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0):  # pylint: disable=missing-docstring\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  label_smoothing = ops.convert_to_tensor(label_smoothing, dtype=K.floatx())\n",
            "\n",
            "  def _smooth_labels():\n",
            "    return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n",
            "\n",
            "  y_true = smart_cond.smart_cond(label_smoothing,\n",
            "                                 _smooth_labels, lambda: y_true)\n",
            "  return K.mean(\n",
            "      K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.kullback_leibler_divergence',\n",
            "              'keras.metrics.kld',\n",
            "              'keras.metrics.KLD',\n",
            "              'keras.losses.kullback_leibler_divergence',\n",
            "              'keras.losses.kld',\n",
            "              'keras.losses.KLD')\n",
            "def kullback_leibler_divergence(y_true, y_pred):  # pylint: disable=missing-docstring\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  y_true = K.clip(y_true, K.epsilon(), 1)\n",
            "  y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
            "  return math_ops.reduce_sum(y_true * math_ops.log(y_true / y_pred), axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.metrics.poisson', 'keras.losses.poisson')\n",
            "def poisson(y_true, y_pred):\n",
            "  y_pred = ops.convert_to_tensor(y_pred)\n",
            "  y_true = math_ops.cast(y_true, y_pred.dtype)\n",
            "  return K.mean(y_pred - y_true * math_ops.log(y_pred + K.epsilon()), axis=-1)\n",
            "\n",
            "\n",
            "# Retaining the legacy namespaces: 'cosine_proximity' and 'cosine'.\n",
            "# TODO(psv): Change name of this function to `cosine_similarity` after fixing\n",
            "# estimator test.\n",
            "@keras_export(\n",
            "    'keras.losses.cosine_similarity',\n",
            "    v1=[\n",
            "        'keras.metrics.cosine_proximity',\n",
            "        'keras.metrics.cosine',\n",
            "        'keras.losses.cosine_proximity',\n",
            "        'keras.losses.cosine',\n",
            "        'keras.losses.cosine_similarity',\n",
            "    ])\n",
            "def cosine_proximity(y_true, y_pred, axis=-1):\n",
            "  \"\"\"Computes the cosine similarity between labels and predictions.\"\"\"\n",
            "  y_true = nn.l2_normalize(y_true, axis=axis)\n",
            "  y_pred = nn.l2_normalize(y_pred, axis=axis)\n",
            "  return math_ops.reduce_sum(y_true * y_pred, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.CosineSimilarity')\n",
            "class CosineSimilarity(LossFunctionWrapper):\n",
            "  \"\"\"Computes the cosine similarity between `y_true` and `y_pred`.\n",
            "\n",
            "  Usage:\n",
            "\n",
            "  ```python\n",
            "  cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
            "  loss = cosine_loss([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n",
            "  # l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]\n",
            "  # l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]\n",
            "  # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n",
            "  # loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n",
            "         = ((0. + 0.) +  (0.5 + 0.5)) / 2\n",
            "\n",
            "  print('Loss: ', loss.numpy())  # Loss: 0.5\n",
            "  ```\n",
            "\n",
            "  Usage with tf.keras API:\n",
            "\n",
            "  ```python\n",
            "  model = tf.keras.Model(inputs, outputs)\n",
            "  model.compile('sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))\n",
            "  ```\n",
            "\n",
            "  Args:\n",
            "    axis: (Optional) Defaults to -1. The dimension along which the cosine\n",
            "      similarity is computed.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
            "      be determined by the usage context. For almost all cases this defaults to\n",
            "      `SUM_OVER_BATCH_SIZE`.\n",
            "      When used with `tf.distribute.Strategy`, outside of built-in training\n",
            "      loops such as `tf.keras` `compile` and `fit`, using `AUTO` or\n",
            "      `SUM_OVER_BATCH_SIZE` will raise an error. Please see\n",
            "      https://www.tensorflow.org/alpha/tutorials/distribute/training_loops\n",
            "      for more details on this.\n",
            "    name: Optional name for the op.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               axis=-1,\n",
            "               reduction=losses_utils.ReductionV2.AUTO,\n",
            "               name='cosine_similarity'):\n",
            "    super(CosineSimilarity, self).__init__(\n",
            "        cosine_similarity, reduction=reduction, name=name, axis=axis)\n",
            "\n",
            "\n",
            "# Aliases.\n",
            "\n",
            "mse = MSE = mean_squared_error\n",
            "mae = MAE = mean_absolute_error\n",
            "mape = MAPE = mean_absolute_percentage_error\n",
            "msle = MSLE = mean_squared_logarithmic_error\n",
            "kld = KLD = kullback_leibler_divergence\n",
            "cosine_similarity = cosine_proximity\n",
            "\n",
            "\n",
            "def is_categorical_crossentropy(loss):\n",
            "  result = ((isinstance(loss, CategoricalCrossentropy) or\n",
            "             (isinstance(loss, LossFunctionWrapper) and\n",
            "              loss.fn == categorical_crossentropy) or\n",
            "             (hasattr(loss, '__name__') and\n",
            "              loss.__name__ == 'categorical_crossentropy') or\n",
            "             (loss == 'categorical_crossentropy')))\n",
            "  return result\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.serialize')\n",
            "def serialize(loss):\n",
            "  return serialize_keras_object(loss)\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.deserialize')\n",
            "def deserialize(name, custom_objects=None):\n",
            "  return deserialize_keras_object(\n",
            "      name,\n",
            "      module_objects=globals(),\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='loss function')\n",
            "\n",
            "\n",
            "@keras_export('keras.losses.get')\n",
            "def get(identifier):\n",
            "  if identifier is None:\n",
            "    return None\n",
            "  if isinstance(identifier, six.string_types):\n",
            "    identifier = str(identifier)\n",
            "    return deserialize(identifier)\n",
            "  if isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret '\n",
            "                     'loss function identifier:', identifier)\n",
            "\n",
            "\n",
            "LABEL_DTYPES_FOR_LOSSES = {\n",
            "    losses_impl.sparse_softmax_cross_entropy: 'int32',\n",
            "    sparse_categorical_crossentropy: 'int32'\n",
            "}\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Built-in regularizers.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.Regularizer')\n",
            "class Regularizer(object):\n",
            "  \"\"\"Regularizer base class.\n",
            "  \"\"\"\n",
            "\n",
            "  def __call__(self, x):\n",
            "    return 0.\n",
            "\n",
            "  @classmethod\n",
            "  def from_config(cls, config):\n",
            "    return cls(**config)\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.L1L2')\n",
            "class L1L2(Regularizer):\n",
            "  \"\"\"Regularizer for L1 and L2 regularization.\n",
            "\n",
            "  Arguments:\n",
            "      l1: Float; L1 regularization factor.\n",
            "      l2: Float; L2 regularization factor.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, l1=0., l2=0.):  # pylint: disable=redefined-outer-name\n",
            "    self.l1 = K.cast_to_floatx(l1)\n",
            "    self.l2 = K.cast_to_floatx(l2)\n",
            "\n",
            "  def __call__(self, x):\n",
            "    if not self.l1 and not self.l2:\n",
            "      return K.constant(0.)\n",
            "    regularization = 0.\n",
            "    if self.l1:\n",
            "      regularization += self.l1 * math_ops.reduce_sum(math_ops.abs(x))\n",
            "    if self.l2:\n",
            "      regularization += self.l2 * math_ops.reduce_sum(math_ops.square(x))\n",
            "    return regularization\n",
            "\n",
            "  def get_config(self):\n",
            "    return {'l1': float(self.l1), 'l2': float(self.l2)}\n",
            "\n",
            "\n",
            "# Aliases.\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.l1')\n",
            "def l1(l=0.01):\n",
            "  return L1L2(l1=l)\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.l2')\n",
            "def l2(l=0.01):\n",
            "  return L1L2(l2=l)\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.l1_l2')\n",
            "def l1_l2(l1=0.01, l2=0.01):  # pylint: disable=redefined-outer-name\n",
            "  return L1L2(l1=l1, l2=l2)\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.serialize')\n",
            "def serialize(regularizer):\n",
            "  return serialize_keras_object(regularizer)\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.deserialize')\n",
            "def deserialize(config, custom_objects=None):\n",
            "  return deserialize_keras_object(\n",
            "      config,\n",
            "      module_objects=globals(),\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='regularizer')\n",
            "\n",
            "\n",
            "@keras_export('keras.regularizers.get')\n",
            "def get(identifier):\n",
            "  if identifier is None:\n",
            "    return None\n",
            "  if isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    identifier = str(identifier)\n",
            "    # We have to special-case functions that return classes.\n",
            "    # TODO(omalleyt): Turn these into classes or class aliases.\n",
            "    special_cases = ['l1', 'l2', 'l1_l2']\n",
            "    if identifier in special_cases:\n",
            "      # Treat like a class.\n",
            "      return deserialize({'class_name': identifier, 'config': {}})\n",
            "    return deserialize(str(identifier))\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret regularizer identifier:', identifier)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "# pylint: disable=redefined-outer-name\n",
            "# pylint: disable=redefined-builtin\n",
            "\"\"\"Keras backend API.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import collections\n",
            "import itertools\n",
            "import json\n",
            "import os\n",
            "import sys\n",
            "import threading\n",
            "import weakref\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "from tensorflow.core.protobuf import config_pb2\n",
            "from tensorflow.python.client import session as session_module\n",
            "from tensorflow.python.distribute import distribute_coordinator as dc\n",
            "from tensorflow.python.distribute import distribute_coordinator_context as dc_context\n",
            "from tensorflow.python.distribute import distribution_strategy_context\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.eager import function as eager_function\n",
            "from tensorflow.python.eager import lift_to_graph\n",
            "from tensorflow.python.framework import composite_tensor\n",
            "from tensorflow.python.framework import constant_op\n",
            "from tensorflow.python.framework import device as tfdev\n",
            "from tensorflow.python.framework import dtypes as dtypes_module\n",
            "from tensorflow.python.framework import func_graph\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.framework import sparse_tensor\n",
            "from tensorflow.python.framework import tensor_util\n",
            "from tensorflow.python.keras import backend_config\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import clip_ops\n",
            "from tensorflow.python.ops import control_flow_ops\n",
            "from tensorflow.python.ops import ctc_ops as ctc\n",
            "from tensorflow.python.ops import functional_ops\n",
            "from tensorflow.python.ops import gradients as gradients_module\n",
            "from tensorflow.python.ops import image_ops\n",
            "from tensorflow.python.ops import init_ops\n",
            "from tensorflow.python.ops import linalg_ops\n",
            "from tensorflow.python.ops import logging_ops\n",
            "from tensorflow.python.ops import map_fn as map_fn_lib\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import nn\n",
            "from tensorflow.python.ops import random_ops\n",
            "from tensorflow.python.ops import resource_variable_ops\n",
            "from tensorflow.python.ops import sparse_ops\n",
            "from tensorflow.python.ops import state_ops\n",
            "from tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\n",
            "from tensorflow.python.ops import tensor_array_ops\n",
            "from tensorflow.python.ops import variables as variables_module\n",
            "from tensorflow.python.training import server_lib\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util import tf_contextlib\n",
            "from tensorflow.python.util import tf_inspect\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "py_all = all\n",
            "py_sum = sum\n",
            "\n",
            "# INTERNAL UTILS\n",
            "\n",
            "# The internal graph maintained by Keras and used by the symbolic Keras APIs\n",
            "# while executing eagerly (such as the functional API for model-building).\n",
            "_GRAPH = None\n",
            "\n",
            "# A graph which is used for constructing functions in eager mode.\n",
            "_CURRENT_SCRATCH_GRAPH = None\n",
            "\n",
            "# This is a thread local object that will hold the default internal TF session\n",
            "# used by Keras. It can be set manually via `set_session(sess)`.\n",
            "_SESSION = threading.local()\n",
            "\n",
            "# This dictionary holds a mapping {graph: learning_phase}.\n",
            "# A learning phase is a bool tensor used to run Keras models in\n",
            "# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n",
            "_GRAPH_LEARNING_PHASES = weakref.WeakKeyDictionary()\n",
            "\n",
            "# This dictionary holds a mapping {graph: set_of_freezable_variables}.\n",
            "# Each set tracks objects created via `freezable_variable` in the graph.\n",
            "_FREEZABLE_VARS = weakref.WeakKeyDictionary()\n",
            "\n",
            "# _DUMMY_EAGER_GRAPH is used as a key in _GRAPH_LEARNING_PHASES.\n",
            "# We keep a separate reference to it to make sure it does not get removed from\n",
            "# _GRAPH_LEARNING_PHASES.\n",
            "_DUMMY_EAGER_GRAPH = threading.local()\n",
            "\n",
            "# This boolean flag can be set to True to leave variable initialization\n",
            "# up to the user.\n",
            "# Change its value via `manual_variable_initialization(value)`.\n",
            "_MANUAL_VAR_INIT = False\n",
            "\n",
            "# This list holds the available devices.\n",
            "# It is populated when `_get_available_gpus()` is called for the first time.\n",
            "# We assume our devices don't change henceforth.\n",
            "_LOCAL_DEVICES = None\n",
            "\n",
            "# This dictionary holds a mapping between a graph and variables to initialize\n",
            "# in the graph.\n",
            "_GRAPH_VARIABLES = weakref.WeakKeyDictionary()\n",
            "\n",
            "# This dictionary holds a mapping between a graph and TF optimizers created in\n",
            "# the graph.\n",
            "_GRAPH_TF_OPTIMIZERS = weakref.WeakKeyDictionary()\n",
            "\n",
            "# The below functions are kept accessible from backend for compatibility.\n",
            "epsilon = backend_config.epsilon\n",
            "floatx = backend_config.floatx\n",
            "image_data_format = backend_config.image_data_format\n",
            "set_epsilon = backend_config.set_epsilon\n",
            "set_floatx = backend_config.set_floatx\n",
            "set_image_data_format = backend_config.set_image_data_format\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.backend')\n",
            "def backend():\n",
            "  \"\"\"Publicly accessible method for determining the current backend.\n",
            "\n",
            "  Only exists for API compatibility with multi-backend Keras.\n",
            "\n",
            "  Returns:\n",
            "      The string \"tensorflow\".\n",
            "  \"\"\"\n",
            "  return 'tensorflow'\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.cast_to_floatx')\n",
            "def cast_to_floatx(x):\n",
            "  \"\"\"Cast a Numpy array to the default Keras float type.\n",
            "\n",
            "  Arguments:\n",
            "      x: Numpy array.\n",
            "\n",
            "  Returns:\n",
            "      The same Numpy array, cast to its new type.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> K.floatx()\n",
            "      'float32'\n",
            "      >>> arr = numpy.array([1.0, 2.0], dtype='float64')\n",
            "      >>> arr.dtype\n",
            "      dtype('float64')\n",
            "      >>> new_arr = K.cast_to_floatx(arr)\n",
            "      >>> new_arr\n",
            "      array([ 1.,  2.], dtype=float32)\n",
            "      >>> new_arr.dtype\n",
            "      dtype('float32')\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return np.asarray(x, dtype=floatx())\n",
            "\n",
            "\n",
            "# A global dictionary mapping graph objects to an index of counters used\n",
            "# for various layer/optimizer names in each graph.\n",
            "# Allows to give unique autogenerated names to layers, in a graph-specific way.\n",
            "PER_GRAPH_OBJECT_NAME_UIDS = weakref.WeakKeyDictionary()\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.get_uid')\n",
            "def get_uid(prefix=''):\n",
            "  \"\"\"Associates a string prefix with an integer counter in a TensorFlow graph.\n",
            "\n",
            "  Arguments:\n",
            "    prefix: String prefix to index.\n",
            "\n",
            "  Returns:\n",
            "    Unique integer ID.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```\n",
            "    >>> get_uid('dense')\n",
            "    1\n",
            "    >>> get_uid('dense')\n",
            "    2\n",
            "  ```\n",
            "  \"\"\"\n",
            "  graph = get_graph()\n",
            "  if graph not in PER_GRAPH_OBJECT_NAME_UIDS:\n",
            "    PER_GRAPH_OBJECT_NAME_UIDS[graph] = collections.defaultdict(int)\n",
            "  layer_name_uids = PER_GRAPH_OBJECT_NAME_UIDS[graph]\n",
            "  layer_name_uids[prefix] += 1\n",
            "  return layer_name_uids[prefix]\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.reset_uids')\n",
            "def reset_uids():\n",
            "  \"\"\"Resets graph identifiers.\n",
            "  \"\"\"\n",
            "  per_graph_object_name_uids = PER_GRAPH_OBJECT_NAME_UIDS\n",
            "  keys = list(per_graph_object_name_uids.keys())\n",
            "  for key in keys:\n",
            "    del per_graph_object_name_uids[key]\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.clear_session')\n",
            "def clear_session():\n",
            "  \"\"\"Destroys the current TF graph and creates a new one.\n",
            "\n",
            "  Useful to avoid clutter from old models / layers.\n",
            "  \"\"\"\n",
            "  global _SESSION\n",
            "  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n",
            "  global _GRAPH_VARIABLES  # pylint: disable=global-variable-not-assigned\n",
            "  global _GRAPH_TF_OPTIMIZERS  # pylint: disable=global-variable-not-assigned\n",
            "  global _GRAPH\n",
            "  global _FREEZABLE_VARS\n",
            "  _GRAPH = None\n",
            "  ops.reset_default_graph()\n",
            "  reset_uids()\n",
            "  _SESSION.session = None\n",
            "  graph = get_graph()\n",
            "  with graph.as_default():\n",
            "    with name_scope(''):\n",
            "      phase = array_ops.placeholder_with_default(\n",
            "          False, shape=(), name='keras_learning_phase')\n",
            "    _GRAPH_LEARNING_PHASES = {}\n",
            "    _GRAPH_LEARNING_PHASES[graph] = phase\n",
            "    _GRAPH_VARIABLES.pop(graph, None)\n",
            "    _GRAPH_TF_OPTIMIZERS.pop(graph, None)\n",
            "    _FREEZABLE_VARS.pop(graph, None)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.manual_variable_initialization')\n",
            "def manual_variable_initialization(value):\n",
            "  \"\"\"Sets the manual variable initialization flag.\n",
            "\n",
            "  This boolean flag determines whether\n",
            "  variables should be initialized\n",
            "  as they are instantiated (default), or if\n",
            "  the user should handle the initialization\n",
            "  (e.g. via `tf.compat.v1.initialize_all_variables()`).\n",
            "\n",
            "  Arguments:\n",
            "      value: Python boolean.\n",
            "  \"\"\"\n",
            "  global _MANUAL_VAR_INIT\n",
            "  _MANUAL_VAR_INIT = value\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.learning_phase')\n",
            "def learning_phase():\n",
            "  \"\"\"Returns the learning phase flag.\n",
            "\n",
            "  The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
            "  to be passed as input to any Keras function\n",
            "  that uses a different behavior at train time and test time.\n",
            "\n",
            "  Returns:\n",
            "      Learning phase (scalar integer tensor or Python integer).\n",
            "  \"\"\"\n",
            "  if ops.get_default_graph() is _GRAPH:\n",
            "    # Don't enter an init_scope for the learning phase if eager execution\n",
            "    # is enabled but we're inside the Keras workspace graph.\n",
            "    return symbolic_learning_phase()\n",
            "  with ops.init_scope():\n",
            "    # We always check & set the learning phase inside the init_scope,\n",
            "    # otherwise the wrong default_graph will be used to look up the learning\n",
            "    # phase inside of functions & defuns.\n",
            "    #\n",
            "    # This is because functions & defuns (both in graph & in eager mode)\n",
            "    # will always execute non-eagerly using a function-specific default\n",
            "    # subgraph.\n",
            "    if context.executing_eagerly():\n",
            "      if _DUMMY_EAGER_GRAPH not in _GRAPH_LEARNING_PHASES:\n",
            "        # Fallback to inference mode as default.\n",
            "        return 0\n",
            "      return _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\n",
            "    return symbolic_learning_phase()\n",
            "\n",
            "\n",
            "def symbolic_learning_phase():\n",
            "  graph = get_graph()\n",
            "  with graph.as_default():\n",
            "    if graph not in _GRAPH_LEARNING_PHASES:\n",
            "      with name_scope(''):\n",
            "        phase = array_ops.placeholder_with_default(\n",
            "            False, shape=(), name='keras_learning_phase')\n",
            "      _GRAPH_LEARNING_PHASES[graph] = phase\n",
            "    return _GRAPH_LEARNING_PHASES[graph]\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.set_learning_phase')\n",
            "def set_learning_phase(value):\n",
            "  \"\"\"Sets the learning phase to a fixed value.\n",
            "\n",
            "  Arguments:\n",
            "      value: Learning phase value, either 0 or 1 (integers).\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `value` is neither `0` nor `1`.\n",
            "  \"\"\"\n",
            "  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n",
            "  if value not in {0, 1}:\n",
            "    raise ValueError('Expected learning phase to be 0 or 1.')\n",
            "  with ops.init_scope():\n",
            "    if context.executing_eagerly():\n",
            "      # In an eager context, the learning phase values applies to both the eager\n",
            "      # context and the internal Keras graph.\n",
            "      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\n",
            "    _GRAPH_LEARNING_PHASES[get_graph()] = value\n",
            "\n",
            "\n",
            "def set_eager_learning_phase(value):\n",
            "  \"\"\"Internal utility that sets the learning phase in eager execution only.\n",
            "\n",
            "  Arguments:\n",
            "      value: Learning phase value, either 0 or 1 (integers).\n",
            "  \"\"\"\n",
            "  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n",
            "  assert value in {0, 1}\n",
            "  assert context.executing_eagerly()\n",
            "  _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.learning_phase_scope')\n",
            "@tf_contextlib.contextmanager\n",
            "def learning_phase_scope(value):\n",
            "  \"\"\"Provides a scope within which the learning phase is equal to `value`.\n",
            "\n",
            "  The learning phase gets restored to its original value upon exiting the scope.\n",
            "\n",
            "  Arguments:\n",
            "     value: Learning phase value, either 0 or 1 (integers).\n",
            "\n",
            "  Yields:\n",
            "    None.\n",
            "\n",
            "  Raises:\n",
            "     ValueError: if `value` is neither `0` nor `1`.\n",
            "  \"\"\"\n",
            "  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n",
            "  if value not in {0, 1}:\n",
            "    raise ValueError('Expected learning phase to be 0 or 1.')\n",
            "\n",
            "  with ops.init_scope():\n",
            "    if context.executing_eagerly():\n",
            "      previous_eager_value = _GRAPH_LEARNING_PHASES.get(\n",
            "          _DUMMY_EAGER_GRAPH, None)\n",
            "    previous_graph_value = _GRAPH_LEARNING_PHASES.get(get_graph(), None)\n",
            "\n",
            "  try:\n",
            "    set_learning_phase(value)\n",
            "    yield\n",
            "  finally:\n",
            "    # Restore learning phase to initial value.\n",
            "    with ops.init_scope():\n",
            "      if context.executing_eagerly():\n",
            "        if previous_eager_value is not None:\n",
            "          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_eager_value\n",
            "        elif _DUMMY_EAGER_GRAPH in _GRAPH_LEARNING_PHASES:\n",
            "          del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]\n",
            "\n",
            "      graph = get_graph()\n",
            "      if previous_graph_value is not None:\n",
            "        _GRAPH_LEARNING_PHASES[graph] = previous_graph_value\n",
            "      elif graph in _GRAPH_LEARNING_PHASES:\n",
            "        del _GRAPH_LEARNING_PHASES[graph]\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def eager_learning_phase_scope(value):\n",
            "  \"\"\"Internal scope that sets the learning phase in eager execution only.\n",
            "\n",
            "  Arguments:\n",
            "      value: Learning phase value, either 0 or 1 (integers).\n",
            "\n",
            "  Yields:\n",
            "    None.\n",
            "\n",
            "  Raises:\n",
            "     ValueError: if `value` is neither `0` nor `1`.\n",
            "  \"\"\"\n",
            "  global _GRAPH_LEARNING_PHASES  # pylint: disable=global-variable-not-assigned\n",
            "  assert value in {0, 1}\n",
            "  assert ops.executing_eagerly_outside_functions()\n",
            "  previous_value = learning_phase()\n",
            "  try:\n",
            "    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value\n",
            "    yield\n",
            "  finally:\n",
            "    # Restore learning phase to initial value.\n",
            "    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_value\n",
            "\n",
            "\n",
            "def _current_graph(op_input_list):\n",
            "  \"\"\"Return the graph members of `op_input_list`, or the current graph.\"\"\"\n",
            "  return ops._get_graph_from_inputs(op_input_list)\n",
            "\n",
            "\n",
            "def _get_session(op_input_list=()):\n",
            "  \"\"\"Returns the session object for the current thread.\"\"\"\n",
            "  global _SESSION\n",
            "  default_session = ops.get_default_session()\n",
            "  if default_session is not None:\n",
            "    session = default_session\n",
            "  else:\n",
            "    if ops.inside_function():\n",
            "      raise RuntimeError('Cannot get session inside Tensorflow graph function.')\n",
            "    # If we don't have a session, or that session does not match the current\n",
            "    # graph, create and cache a new session.\n",
            "    if (getattr(_SESSION, 'session', None) is None or\n",
            "        _SESSION.session.graph is not _current_graph(op_input_list)):\n",
            "      # If we are creating the Session inside a tf.distribute.Strategy scope,\n",
            "      # we ask the strategy for the right session options to use.\n",
            "      if distribution_strategy_context.has_strategy():\n",
            "        configure_and_create_distributed_session(\n",
            "            distribution_strategy_context.get_strategy())\n",
            "      else:\n",
            "        _SESSION.session = session_module.Session(\n",
            "            config=get_default_session_config())\n",
            "    session = _SESSION.session\n",
            "  return session\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.backend.get_session'])\n",
            "def get_session(op_input_list=()):\n",
            "  \"\"\"Returns the TF session to be used by the backend.\n",
            "\n",
            "  If a default TensorFlow session is available, we will return it.\n",
            "\n",
            "  Else, we will return the global Keras session assuming it matches\n",
            "  the current graph.\n",
            "\n",
            "  If no global Keras session exists at this point:\n",
            "  we will create a new global session.\n",
            "\n",
            "  Note that you can manually set the global session\n",
            "  via `K.set_session(sess)`.\n",
            "\n",
            "  Arguments:\n",
            "      op_input_list: An option sequence of tensors or ops, which will be used\n",
            "        to determine the current graph. Otherwise the default graph will be\n",
            "        used.\n",
            "\n",
            "  Returns:\n",
            "      A TensorFlow session.\n",
            "  \"\"\"\n",
            "  session = _get_session(op_input_list)\n",
            "  if not _MANUAL_VAR_INIT:\n",
            "    with session.graph.as_default():\n",
            "      _initialize_variables(session)\n",
            "  return session\n",
            "\n",
            "\n",
            "def get_graph():\n",
            "  if context.executing_eagerly():\n",
            "    global _GRAPH\n",
            "    if _GRAPH is None:\n",
            "      _GRAPH = func_graph.FuncGraph('keras_graph')\n",
            "    return _GRAPH\n",
            "  else:\n",
            "    return ops.get_default_graph()\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def _scratch_graph(graph=None):\n",
            "  \"\"\"Retrieve a shared and temporary func graph.\n",
            "\n",
            "  The eager execution path lifts a subgraph from the keras global graph into\n",
            "  a scratch graph in order to create a function. DistributionStrategies, in\n",
            "  turn, constructs multiple functions as well as a final combined function. In\n",
            "  order for that logic to work correctly, all of the functions need to be\n",
            "  created on the same scratch FuncGraph.\n",
            "\n",
            "  Args:\n",
            "    graph: A graph to be used as the current scratch graph. If not set then\n",
            "      a scratch graph will either be retrieved or created:\n",
            "\n",
            "  Yields:\n",
            "    The current scratch graph.\n",
            "  \"\"\"\n",
            "  global _CURRENT_SCRATCH_GRAPH\n",
            "  if (_CURRENT_SCRATCH_GRAPH is not None and graph is not None and\n",
            "      _CURRENT_SCRATCH_GRAPH is not graph):\n",
            "    raise ValueError('Multiple scratch graphs specified.')\n",
            "\n",
            "  if _CURRENT_SCRATCH_GRAPH:\n",
            "    yield _CURRENT_SCRATCH_GRAPH\n",
            "    return\n",
            "\n",
            "  graph = graph or func_graph.FuncGraph('keras_scratch_graph')\n",
            "  try:\n",
            "    _CURRENT_SCRATCH_GRAPH = graph\n",
            "    yield graph\n",
            "  finally:\n",
            "    _CURRENT_SCRATCH_GRAPH = None\n",
            "\n",
            "\n",
            "@keras_export(v1=['keras.backend.set_session'])\n",
            "def set_session(session):\n",
            "  \"\"\"Sets the global TensorFlow session.\n",
            "\n",
            "  Arguments:\n",
            "      session: A TF Session.\n",
            "  \"\"\"\n",
            "  global _SESSION\n",
            "  _SESSION.session = session\n",
            "\n",
            "\n",
            "def get_default_session_config():\n",
            "  if not os.environ.get('OMP_NUM_THREADS'):\n",
            "    config = config_pb2.ConfigProto(allow_soft_placement=True)\n",
            "  else:\n",
            "    num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n",
            "    config = config_pb2.ConfigProto(\n",
            "        intra_op_parallelism_threads=num_thread,\n",
            "        inter_op_parallelism_threads=num_thread,\n",
            "        allow_soft_placement=True)\n",
            "  return config\n",
            "\n",
            "\n",
            "def get_default_graph_uid_map():\n",
            "  graph = ops.get_default_graph()\n",
            "  name_uid_map = PER_GRAPH_OBJECT_NAME_UIDS.get(graph, None)\n",
            "  if name_uid_map is None:\n",
            "    name_uid_map = collections.defaultdict(int)\n",
            "    PER_GRAPH_OBJECT_NAME_UIDS[graph] = name_uid_map\n",
            "  return name_uid_map\n",
            "\n",
            "\n",
            "# DEVICE MANIPULATION\n",
            "\n",
            "\n",
            "class _TfDeviceCaptureOp(object):\n",
            "  \"\"\"Class for capturing the TF device scope.\"\"\"\n",
            "\n",
            "  def __init__(self):\n",
            "    self.device = None\n",
            "\n",
            "  def _set_device(self, device):\n",
            "    \"\"\"This method captures TF's explicit device scope setting.\"\"\"\n",
            "    if tfdev.is_device_spec(device):\n",
            "      device = device.to_string()\n",
            "    self.device = device\n",
            "\n",
            "  def _set_device_from_string(self, device_str):\n",
            "    self.device = device_str\n",
            "\n",
            "\n",
            "def _get_current_tf_device():\n",
            "  \"\"\"Return explicit device of current context, otherwise returns `None`.\n",
            "\n",
            "  Returns:\n",
            "      If the current device scope is explicitly set, it returns a string with\n",
            "      the device (`CPU` or `GPU`). If the scope is not explicitly set, it will\n",
            "      return `None`.\n",
            "  \"\"\"\n",
            "  graph = get_graph()\n",
            "  op = _TfDeviceCaptureOp()\n",
            "  graph._apply_device_functions(op)\n",
            "  return tfdev.DeviceSpec.from_string(op.device)\n",
            "\n",
            "\n",
            "def _is_current_explicit_device(device_type):\n",
            "  \"\"\"Check if the current device is explicitly set on the device type specified.\n",
            "\n",
            "  Arguments:\n",
            "      device_type: A string containing `GPU` or `CPU` (case-insensitive).\n",
            "\n",
            "  Returns:\n",
            "      A boolean indicating if the current device scope is explicitly set on the\n",
            "      device type.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If the `device_type` string indicates an unsupported device.\n",
            "  \"\"\"\n",
            "  device_type = device_type.upper()\n",
            "  if device_type not in ['CPU', 'GPU']:\n",
            "    raise ValueError('`device_type` should be either \"CPU\" or \"GPU\".')\n",
            "  device = _get_current_tf_device()\n",
            "  return device is not None and device.device_type == device_type.upper()\n",
            "\n",
            "\n",
            "def _get_available_gpus():\n",
            "  \"\"\"Get a list of available gpu devices (formatted as strings).\n",
            "\n",
            "  Returns:\n",
            "      A list of available GPU devices.\n",
            "  \"\"\"\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    # Returns names of devices directly.\n",
            "    return [name for name in context.list_devices() if 'GPU' in name]\n",
            "\n",
            "  global _LOCAL_DEVICES\n",
            "  if _LOCAL_DEVICES is None:\n",
            "    _LOCAL_DEVICES = get_session().list_devices()\n",
            "  return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']\n",
            "\n",
            "\n",
            "def _has_nchw_support():\n",
            "  \"\"\"Check whether the current scope supports NCHW ops.\n",
            "\n",
            "  TensorFlow does not support NCHW on CPU. Therefore we check if we are not\n",
            "  explicitly put on\n",
            "  CPU, and have GPUs available. In this case there will be soft-placing on the\n",
            "  GPU device.\n",
            "\n",
            "  Returns:\n",
            "      bool: if the current scope device placement would support nchw\n",
            "  \"\"\"\n",
            "  explicitly_on_cpu = _is_current_explicit_device('CPU')\n",
            "  gpus_available = bool(_get_available_gpus())\n",
            "  return not explicitly_on_cpu and gpus_available\n",
            "\n",
            "\n",
            "# VARIABLE MANIPULATION\n",
            "\n",
            "\n",
            "def _constant_to_tensor(x, dtype):\n",
            "  \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
            "\n",
            "  This is slightly faster than the _to_tensor function, at the cost of\n",
            "  handling fewer cases.\n",
            "\n",
            "  Arguments:\n",
            "      x: An object to be converted (numpy arrays, floats, ints and lists of\n",
            "        them).\n",
            "      dtype: The destination type.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return constant_op.constant(x, dtype=dtype)\n",
            "\n",
            "\n",
            "def _to_tensor(x, dtype):\n",
            "  \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
            "\n",
            "  Arguments:\n",
            "      x: An object to be converted (numpy array, list, tensors).\n",
            "      dtype: The destination type.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return ops.convert_to_tensor(x, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.is_sparse')\n",
            "def is_sparse(tensor):\n",
            "  \"\"\"Returns whether a tensor is a sparse tensor.\n",
            "\n",
            "  Arguments:\n",
            "      tensor: A tensor instance.\n",
            "\n",
            "  Returns:\n",
            "      A boolean.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> a = K.placeholder((2, 2), sparse=False)\n",
            "      >>> print(K.is_sparse(a))\n",
            "      False\n",
            "      >>> b = K.placeholder((2, 2), sparse=True)\n",
            "      >>> print(K.is_sparse(b))\n",
            "      True\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return isinstance(tensor, sparse_tensor.SparseTensor)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.to_dense')\n",
            "def to_dense(tensor):\n",
            "  \"\"\"Converts a sparse tensor into a dense tensor and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      tensor: A tensor instance (potentially sparse).\n",
            "\n",
            "  Returns:\n",
            "      A dense tensor.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> b = K.placeholder((2, 2), sparse=True)\n",
            "      >>> print(K.is_sparse(b))\n",
            "      True\n",
            "      >>> c = K.to_dense(b)\n",
            "      >>> print(K.is_sparse(c))\n",
            "      False\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if is_sparse(tensor):\n",
            "    return sparse_ops.sparse_tensor_to_dense(tensor)\n",
            "  else:\n",
            "    return tensor\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.name_scope', v1=[])\n",
            "def name_scope(name):\n",
            "  \"\"\"A context manager for use when defining a Python op.\n",
            "\n",
            "  This context manager pushes a name scope, which will make the name of all\n",
            "  operations added within it have a prefix.\n",
            "\n",
            "  For example, to define a new Python op called `my_op`:\n",
            "\n",
            "  ```python\n",
            "  def my_op(a):\n",
            "    with tf.name_scope(\"MyOp\") as scope:\n",
            "      a = tf.convert_to_tensor(a, name=\"a\")\n",
            "      # Define some computation that uses `a`.\n",
            "      return foo_op(..., name=scope)\n",
            "  ```\n",
            "\n",
            "  When executed, the Tensor `a` will have the name `MyOp/a`.\n",
            "\n",
            "  Args:\n",
            "    name: The prefix to use on all names created within the name scope.\n",
            "\n",
            "  Returns:\n",
            "    Name scope context manager.\n",
            "  \"\"\"\n",
            "  return ops.name_scope_v2(name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.variable')\n",
            "def variable(value, dtype=None, name=None, constraint=None):\n",
            "  \"\"\"Instantiates a variable and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      value: Numpy array, initial value of the tensor.\n",
            "      dtype: Tensor type.\n",
            "      name: Optional name string for the tensor.\n",
            "      constraint: Optional projection function to be\n",
            "          applied to the variable after an optimizer update.\n",
            "\n",
            "  Returns:\n",
            "      A variable instance (with Keras metadata included).\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> import numpy as np\n",
            "      >>> from keras import backend as K\n",
            "      >>> val = np.array([[1, 2], [3, 4]])\n",
            "      >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n",
            "      >>> K.dtype(kvar)\n",
            "      'float64'\n",
            "      >>> print(kvar)\n",
            "      example_var\n",
            "      >>> kvar.eval()\n",
            "      array([[ 1.,  2.],\n",
            "             [ 3.,  4.]])\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if hasattr(value, 'tocoo'):\n",
            "    sparse_coo = value.tocoo()\n",
            "    indices = np.concatenate((np.expand_dims(sparse_coo.row, 1), np.expand_dims(\n",
            "        sparse_coo.col, 1)), 1)\n",
            "    v = sparse_tensor.SparseTensor(\n",
            "        indices=indices, values=sparse_coo.data, dense_shape=sparse_coo.shape)\n",
            "    v._keras_shape = sparse_coo.shape\n",
            "    return v\n",
            "  v = resource_variable_ops.ResourceVariable(\n",
            "      value,\n",
            "      dtype=dtypes_module.as_dtype(dtype),\n",
            "      name=name,\n",
            "      constraint=constraint)\n",
            "  if isinstance(value, np.ndarray):\n",
            "    v._keras_shape = value.shape\n",
            "  elif hasattr(value, 'shape'):\n",
            "    v._keras_shape = int_shape(value)\n",
            "  track_variable(v)\n",
            "  return v\n",
            "\n",
            "\n",
            "def track_tf_optimizer(tf_optimizer):\n",
            "  \"\"\"Tracks the given TF optimizer for initialization of its variables.\"\"\"\n",
            "  if context.executing_eagerly():\n",
            "    return\n",
            "  graph = get_graph()\n",
            "  optimizers = _GRAPH_TF_OPTIMIZERS.setdefault(graph, weakref.WeakSet())\n",
            "  optimizers.add(tf_optimizer)\n",
            "\n",
            "\n",
            "def track_variable(v):\n",
            "  \"\"\"Tracks the given variable for initialization.\"\"\"\n",
            "  if context.executing_eagerly():\n",
            "    return\n",
            "  graph = v.graph if hasattr(v, 'graph') else get_graph()\n",
            "  if graph not in _GRAPH_VARIABLES:\n",
            "    _GRAPH_VARIABLES[graph] = weakref.WeakSet()\n",
            "  _GRAPH_VARIABLES[graph].add(v)\n",
            "\n",
            "\n",
            "def unique_object_name(name,\n",
            "                       name_uid_map=None,\n",
            "                       avoid_names=None,\n",
            "                       namespace='',\n",
            "                       zero_based=False):\n",
            "  \"\"\"Makes a object name (or arbitrary string) unique within a TensorFlow graph.\n",
            "\n",
            "  Arguments:\n",
            "    name: String name to make unique.\n",
            "    name_uid_map: An optional defaultdict(int) to use when creating unique\n",
            "      names. If None (default), uses a per-Graph dictionary.\n",
            "    avoid_names: An optional set or dict with names which should not be used. If\n",
            "      None (default) does not avoid any names.\n",
            "    namespace: Gets a name which is unique within the (graph, namespace). Layers\n",
            "      which are not Networks use a blank namespace and so get graph-global\n",
            "      names.\n",
            "    zero_based: If True, name sequences start with no suffix (e.g. \"dense\",\n",
            "      \"dense_1\"). If False, naming is one-based (\"dense_1\", \"dense_2\").\n",
            "\n",
            "  Returns:\n",
            "    Unique string name.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  _unique_layer_name('dense')  # dense_1\n",
            "  _unique_layer_name('dense')  # dense_2\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if name_uid_map is None:\n",
            "    name_uid_map = get_default_graph_uid_map()\n",
            "  if avoid_names is None:\n",
            "    avoid_names = set()\n",
            "  proposed_name = None\n",
            "  while proposed_name is None or proposed_name in avoid_names:\n",
            "    name_key = (namespace, name)\n",
            "    if zero_based:\n",
            "      number = name_uid_map[name_key]\n",
            "      if number:\n",
            "        proposed_name = name + '_' + str(number)\n",
            "      else:\n",
            "        proposed_name = name\n",
            "      name_uid_map[name_key] += 1\n",
            "    else:\n",
            "      name_uid_map[name_key] += 1\n",
            "      proposed_name = name + '_' + str(name_uid_map[name_key])\n",
            "  return proposed_name\n",
            "\n",
            "\n",
            "def _get_variables(graph=None):\n",
            "  \"\"\"Returns variables corresponding to the given graph for initialization.\"\"\"\n",
            "  assert not context.executing_eagerly()\n",
            "  variables = _GRAPH_VARIABLES.setdefault(graph, weakref.WeakSet())\n",
            "  for opt in _GRAPH_TF_OPTIMIZERS.get(graph, set()):\n",
            "    variables.update(opt.optimizer.variables())\n",
            "  return variables\n",
            "\n",
            "\n",
            "def _initialize_variables(session):\n",
            "  \"\"\"Utility to initialize uninitialized variables on the fly.\"\"\"\n",
            "  variables = _get_variables(get_graph())\n",
            "  candidate_vars = []\n",
            "  for v in variables:\n",
            "    if not getattr(v, '_keras_initialized', False):\n",
            "      candidate_vars.append(v)\n",
            "  if candidate_vars:\n",
            "    # This step is expensive, so we only run it on variables not already\n",
            "    # marked as initialized.\n",
            "    is_initialized = session.run(\n",
            "        [variables_module.is_variable_initialized(v) for v in candidate_vars])\n",
            "    uninitialized_vars = []\n",
            "    for flag, v in zip(is_initialized, candidate_vars):\n",
            "      if not flag:\n",
            "        uninitialized_vars.append(v)\n",
            "      v._keras_initialized = True\n",
            "    if uninitialized_vars:\n",
            "      session.run(variables_module.variables_initializer(uninitialized_vars))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.constant')\n",
            "def constant(value, dtype=None, shape=None, name=None):\n",
            "  \"\"\"Creates a constant tensor.\n",
            "\n",
            "  Arguments:\n",
            "      value: A constant value (or list)\n",
            "      dtype: The type of the elements of the resulting tensor.\n",
            "      shape: Optional dimensions of resulting tensor.\n",
            "      name: Optional name for the tensor.\n",
            "\n",
            "  Returns:\n",
            "      A Constant Tensor.\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "\n",
            "  return constant_op.constant(value, dtype=dtype, shape=shape, name=name)\n",
            "\n",
            "\n",
            "def is_keras_tensor(x):\n",
            "  \"\"\"Returns whether `x` is a Keras tensor.\n",
            "\n",
            "  A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n",
            "  (`Layer` class) or by `Input`.\n",
            "\n",
            "  Arguments:\n",
            "      x: A candidate tensor.\n",
            "\n",
            "  Returns:\n",
            "      A boolean: Whether the argument is a Keras tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In case `x` is not a symbolic tensor.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> import tensorflow as tf\n",
            "      >>> import numpy\n",
            "      >>> from keras import backend as K\n",
            "      >>> from keras.layers import Input, Dense\n",
            "      >>> np_var = numpy.array([1, 2])\n",
            "      >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n",
            "      ValueError\n",
            "      >>> k_var = tf.compat.v1.placeholder('float32', shape=(1,1))\n",
            "      >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of\n",
            "      keras is not a Keras tensor.\n",
            "      False\n",
            "      >>> keras_var = K.variable(np_var)\n",
            "      >>> K.is_keras_tensor(keras_var)  # A variable created with the keras\n",
            "      backend is not a Keras tensor.\n",
            "      False\n",
            "      >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n",
            "      >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras\n",
            "      tensor.\n",
            "      False\n",
            "      >>> keras_input = Input([10])\n",
            "      >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n",
            "      True\n",
            "      >>> keras_layer_output = Dense(10)(keras_input)\n",
            "      >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a\n",
            "      Keras tensor.\n",
            "      True\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if not isinstance(x, (ops.Tensor,\n",
            "                        variables_module.Variable,\n",
            "                        sparse_tensor.SparseTensor)):\n",
            "    raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) +\n",
            "                     '`. Expected a symbolic tensor instance.')\n",
            "  return hasattr(x, '_keras_history')\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.placeholder')\n",
            "def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n",
            "  \"\"\"Instantiates a placeholder tensor and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      shape: Shape of the placeholder\n",
            "          (integer tuple, may include `None` entries).\n",
            "      ndim: Number of axes of the tensor.\n",
            "          At least one of {`shape`, `ndim`} must be specified.\n",
            "          If both are specified, `shape` is used.\n",
            "      dtype: Placeholder type.\n",
            "      sparse: Boolean, whether the placeholder should have a sparse type.\n",
            "      name: Optional name string for the placeholder.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If called with eager execution.\n",
            "\n",
            "  Returns:\n",
            "      Tensor instance (with Keras metadata included).\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> input_ph = K.placeholder(shape=(2, 4, 5))\n",
            "      >>> input_ph\n",
            "      <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if not shape:\n",
            "    if ndim:\n",
            "      shape = tuple([None for _ in range(ndim)])\n",
            "  with get_graph().as_default():\n",
            "    if sparse:\n",
            "      x = array_ops.sparse_placeholder(dtype, shape=shape, name=name)\n",
            "    else:\n",
            "      x = array_ops.placeholder(dtype, shape=shape, name=name)\n",
            "  return x\n",
            "\n",
            "\n",
            "def is_placeholder(x):\n",
            "  \"\"\"Returns whether `x` is a placeholder.\n",
            "\n",
            "  Arguments:\n",
            "      x: A candidate placeholder.\n",
            "\n",
            "  Returns:\n",
            "      Boolean.\n",
            "  \"\"\"\n",
            "  try:\n",
            "    return x.op.type == 'Placeholder'\n",
            "  except AttributeError:\n",
            "    return False\n",
            "\n",
            "\n",
            "def freezable_variable(value, shape=None, name=None):\n",
            "  \"\"\"A tensor-like object whose value can be updated only up until execution.\n",
            "\n",
            "  After creating the freezable variable, you can update its value by calling\n",
            "  `var.update_value(new_value)` (similar to a regular variable).\n",
            "  Unlike an actual variable, the value used during execution is the current\n",
            "  value at the time the execution function (`backend.function()`) was created.\n",
            "\n",
            "  This is an internal API, expected to be temporary. It is used to implement a\n",
            "  mutable `trainable` property for `BatchNormalization` layers, with a frozen\n",
            "  value after model compilation.\n",
            "\n",
            "  We don't use a plain variable in this case because we need the value used\n",
            "  in a specific model to be frozen after `compile` has been called\n",
            "  (e.g. GAN use case).\n",
            "\n",
            "  Arguments:\n",
            "    value: The initial value for the tensor-like object.\n",
            "    shape: The shape for the tensor-like object (cannot be changed).\n",
            "    name: The name for the tensor-like object.\n",
            "\n",
            "  Returns:\n",
            "    A tensor-like object with a static value that can be updated via\n",
            "    `x.update_value(new_value)`, up until creating an execution function\n",
            "    (afterwards the value is fixed).\n",
            "  \"\"\"\n",
            "  graph = get_graph()\n",
            "  with graph.as_default():\n",
            "    x = array_ops.placeholder_with_default(\n",
            "        value, shape=shape, name=name)\n",
            "    x._initial_value = value\n",
            "    x._current_value = value\n",
            "\n",
            "    def update_value(new_value):\n",
            "      x._current_value = new_value\n",
            "\n",
            "    def get_value():\n",
            "      return x._current_value\n",
            "\n",
            "    x.update_value = update_value\n",
            "    x.get_value = get_value\n",
            "\n",
            "    global _FREEZABLE_VARS\n",
            "    if graph not in _FREEZABLE_VARS:\n",
            "      _FREEZABLE_VARS[graph] = weakref.WeakSet()\n",
            "    _FREEZABLE_VARS[graph].add(x)\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.shape')\n",
            "def shape(x):\n",
            "  \"\"\"Returns the symbolic shape of a tensor or variable.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A symbolic shape (which is itself a tensor).\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  ```python\n",
            "      # TensorFlow example\n",
            "      >>> from keras import backend as K\n",
            "      >>> tf_session = K.get_session()\n",
            "      >>> val = np.array([[1, 2], [3, 4]])\n",
            "      >>> kvar = K.variable(value=val)\n",
            "      >>> input = keras.backend.placeholder(shape=(2, 4, 5))\n",
            "      >>> K.shape(kvar)\n",
            "      <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n",
            "      >>> K.shape(input)\n",
            "      <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n",
            "      # To get integer shape (Instead, you can use K.int_shape(x))\n",
            "      >>> K.shape(kvar).eval(session=tf_session)\n",
            "      array([2, 2], dtype=int32)\n",
            "      >>> K.shape(input).eval(session=tf_session)\n",
            "      array([2, 4, 5], dtype=int32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return array_ops.shape(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.int_shape')\n",
            "def int_shape(x):\n",
            "  \"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tuple of integers (or None entries).\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> input = K.placeholder(shape=(2, 4, 5))\n",
            "      >>> K.int_shape(input)\n",
            "      (2, 4, 5)\n",
            "      >>> val = np.array([[1, 2], [3, 4]])\n",
            "      >>> kvar = K.variable(value=val)\n",
            "      >>> K.int_shape(kvar)\n",
            "      (2, 2)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  try:\n",
            "    shape = x.shape\n",
            "    if not isinstance(shape, tuple):\n",
            "      shape = tuple(shape.as_list())\n",
            "    return shape\n",
            "  except ValueError:\n",
            "    return None\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ndim')\n",
            "def ndim(x):\n",
            "  \"\"\"Returns the number of axes in a tensor, as an integer.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      Integer (scalar), number of axes.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> input = K.placeholder(shape=(2, 4, 5))\n",
            "      >>> val = np.array([[1, 2], [3, 4]])\n",
            "      >>> kvar = K.variable(value=val)\n",
            "      >>> K.ndim(input)\n",
            "      3\n",
            "      >>> K.ndim(kvar)\n",
            "      2\n",
            "  ```\n",
            "  \"\"\"\n",
            "  dims = x.shape._dims\n",
            "  if dims is not None:\n",
            "    return len(dims)\n",
            "  return None\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.dtype')\n",
            "def dtype(x):\n",
            "  \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      String, dtype of `x`.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> K.dtype(K.placeholder(shape=(2,4,5)))\n",
            "      'float32'\n",
            "      >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n",
            "      'float32'\n",
            "      >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n",
            "      'float64'\n",
            "      # Keras variable\n",
            "      >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n",
            "      >>> K.dtype(kvar)\n",
            "      'float32'\n",
            "      >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n",
            "      >>> K.dtype(kvar)\n",
            "      'float32'\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return x.dtype.base_dtype.name\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.eval')\n",
            "def eval(x):\n",
            "  \"\"\"Evaluates the value of a variable.\n",
            "\n",
            "  Arguments:\n",
            "      x: A variable.\n",
            "\n",
            "  Returns:\n",
            "      A Numpy array.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 1.,  2.],\n",
            "             [ 3.,  4.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return get_value(to_dense(x))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.zeros')\n",
            "def zeros(shape, dtype=None, name=None):\n",
            "  \"\"\"Instantiates an all-zeros variable and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      shape: Tuple of integers, shape of returned Keras variable\n",
            "      dtype: String, data type of returned Keras variable\n",
            "      name: String, name of returned Keras variable\n",
            "\n",
            "  Returns:\n",
            "      A variable (including Keras metadata), filled with `0.0`.\n",
            "      Note that if `shape` was symbolic, we cannot return a variable,\n",
            "      and will return a dynamically-shaped tensor instead.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.zeros((3,4))\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 0.,  0.,  0.,  0.],\n",
            "             [ 0.,  0.,  0.,  0.],\n",
            "             [ 0.,  0.,  0.,  0.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  with ops.init_scope():\n",
            "    if dtype is None:\n",
            "      dtype = floatx()\n",
            "    tf_dtype = dtypes_module.as_dtype(dtype)\n",
            "    v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)\n",
            "    if py_all(v.shape.as_list()):\n",
            "      return variable(v, dtype=dtype, name=name)\n",
            "    track_variable(v)\n",
            "    return v\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ones')\n",
            "def ones(shape, dtype=None, name=None):\n",
            "  \"\"\"Instantiates an all-ones variable and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      shape: Tuple of integers, shape of returned Keras variable.\n",
            "      dtype: String, data type of returned Keras variable.\n",
            "      name: String, name of returned Keras variable.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable, filled with `1.0`.\n",
            "      Note that if `shape` was symbolic, we cannot return a variable,\n",
            "      and will return a dynamically-shaped tensor instead.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.ones((3,4))\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 1.,  1.,  1.,  1.],\n",
            "             [ 1.,  1.,  1.,  1.],\n",
            "             [ 1.,  1.,  1.,  1.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  with ops.init_scope():\n",
            "    if dtype is None:\n",
            "      dtype = floatx()\n",
            "    tf_dtype = dtypes_module.as_dtype(dtype)\n",
            "    v = array_ops.ones(shape=shape, dtype=tf_dtype, name=name)\n",
            "    if py_all(v.shape.as_list()):\n",
            "      return variable(v, dtype=dtype, name=name)\n",
            "    track_variable(v)\n",
            "    return v\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.eye')\n",
            "def eye(size, dtype=None, name=None):\n",
            "  \"\"\"Instantiate an identity matrix and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      size: Integer, number of rows/columns.\n",
            "      dtype: String, data type of returned Keras variable.\n",
            "      name: String, name of returned Keras variable.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable, an identity matrix.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.eye(3)\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 1.,  0.,  0.],\n",
            "             [ 0.,  1.,  0.],\n",
            "             [ 0.,  0.,  1.]], dtype=float32)\n",
            "  ```\n",
            "\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  tf_dtype = dtypes_module.as_dtype(dtype)\n",
            "  return variable(linalg_ops.eye(size, dtype=tf_dtype), dtype, name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.zeros_like')\n",
            "def zeros_like(x, dtype=None, name=None):\n",
            "  \"\"\"Instantiates an all-zeros variable of the same shape as another tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Keras variable or Keras tensor.\n",
            "      dtype: String, dtype of returned Keras variable.\n",
            "           None uses the dtype of x.\n",
            "      name: String, name for the variable to create.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable with the shape of x filled with zeros.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.variable(np.random.random((2,3)))\n",
            "      >>> kvar_zeros = K.zeros_like(kvar)\n",
            "      >>> K.eval(kvar_zeros)\n",
            "      array([[ 0.,  0.,  0.],\n",
            "             [ 0.,  0.,  0.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return array_ops.zeros_like(x, dtype=dtype, name=name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ones_like')\n",
            "def ones_like(x, dtype=None, name=None):\n",
            "  \"\"\"Instantiates an all-ones variable of the same shape as another tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Keras variable or tensor.\n",
            "      dtype: String, dtype of returned Keras variable.\n",
            "           None uses the dtype of x.\n",
            "      name: String, name for the variable to create.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable with the shape of x filled with ones.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> from keras import backend as K\n",
            "      >>> kvar = K.variable(np.random.random((2,3)))\n",
            "      >>> kvar_ones = K.ones_like(kvar)\n",
            "      >>> K.eval(kvar_ones)\n",
            "      array([[ 1.,  1.,  1.],\n",
            "             [ 1.,  1.,  1.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return array_ops.ones_like(x, dtype=dtype, name=name)\n",
            "\n",
            "\n",
            "def identity(x, name=None):\n",
            "  \"\"\"Returns a tensor with the same content as the input tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: The input tensor.\n",
            "      name: String, name for the variable to create.\n",
            "\n",
            "  Returns:\n",
            "      A tensor of the same shape, type and content.\n",
            "  \"\"\"\n",
            "  return array_ops.identity(x, name=name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.random_uniform_variable')\n",
            "def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None):\n",
            "  \"\"\"Instantiates a variable with values drawn from a uniform distribution.\n",
            "\n",
            "  Arguments:\n",
            "      shape: Tuple of integers, shape of returned Keras variable.\n",
            "      low: Float, lower boundary of the output interval.\n",
            "      high: Float, upper boundary of the output interval.\n",
            "      dtype: String, dtype of returned Keras variable.\n",
            "      name: String, name of returned Keras variable.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable, filled with drawn samples.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      # TensorFlow example\n",
            "      >>> kvar = K.random_uniform_variable((2,3), 0, 1)\n",
            "      >>> kvar\n",
            "      <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 0.10940075,  0.10047495,  0.476143  ],\n",
            "             [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  tf_dtype = dtypes_module.as_dtype(dtype)\n",
            "  if seed is None:\n",
            "    # ensure that randomness is conditioned by the Numpy RNG\n",
            "    seed = np.random.randint(10e8)\n",
            "  value = init_ops.random_uniform_initializer(\n",
            "      low, high, dtype=tf_dtype, seed=seed)(shape)\n",
            "  return variable(value, dtype=dtype, name=name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.random_normal_variable')\n",
            "def random_normal_variable(shape, mean, scale, dtype=None, name=None,\n",
            "                           seed=None):\n",
            "  \"\"\"Instantiates a variable with values drawn from a normal distribution.\n",
            "\n",
            "  Arguments:\n",
            "      shape: Tuple of integers, shape of returned Keras variable.\n",
            "      mean: Float, mean of the normal distribution.\n",
            "      scale: Float, standard deviation of the normal distribution.\n",
            "      dtype: String, dtype of returned Keras variable.\n",
            "      name: String, name of returned Keras variable.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A Keras variable, filled with drawn samples.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      # TensorFlow example\n",
            "      >>> kvar = K.random_normal_variable((2,3), 0, 1)\n",
            "      >>> kvar\n",
            "      <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 1.19591331,  0.68685907, -0.63814116],\n",
            "             [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  tf_dtype = dtypes_module.as_dtype(dtype)\n",
            "  if seed is None:\n",
            "    # ensure that randomness is conditioned by the Numpy RNG\n",
            "    seed = np.random.randint(10e8)\n",
            "  value = init_ops.random_normal_initializer(\n",
            "      mean, scale, dtype=tf_dtype, seed=seed)(shape)\n",
            "  return variable(value, dtype=dtype, name=name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.count_params')\n",
            "def count_params(x):\n",
            "  \"\"\"Returns the static number of elements in a variable or tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Variable or tensor.\n",
            "\n",
            "  Returns:\n",
            "      Integer, the number of scalars in `x`.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "      >>> kvar = K.zeros((2,3))\n",
            "      >>> K.count_params(kvar)\n",
            "      6\n",
            "      >>> K.eval(kvar)\n",
            "      array([[ 0.,  0.,  0.],\n",
            "             [ 0.,  0.,  0.]], dtype=float32)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return np.prod(x.shape.as_list())\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.cast')\n",
            "def cast(x, dtype):\n",
            "  \"\"\"Casts a tensor to a different dtype and returns it.\n",
            "\n",
            "  You can cast a Keras variable but it still returns a Keras tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Keras tensor (or variable).\n",
            "      dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n",
            "\n",
            "  Returns:\n",
            "      Keras tensor with dtype `dtype`.\n",
            "\n",
            "  Examples:\n",
            "      Cast a float32 variable to a float64 tensor\n",
            "\n",
            "  ```python\n",
            "      >>> import tensorflow as tf\n",
            "      >>> from tensorflow.keras import backend as K\n",
            "      >>> input = K.ones(shape=(1,3))\n",
            "      >>> print(input)\n",
            "      >>> cast_input = K.cast(input, dtype='float64')\n",
            "      >>> print(cast_input)\n",
            "\n",
            "      <tf.Variable 'Variable:0' shape=(1, 3) dtype=float32,\n",
            "           numpy=array([[1., 1., 1.]], dtype=float32)>\n",
            "      tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float64)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return math_ops.cast(x, dtype)\n",
            "\n",
            "\n",
            "# UPDATES OPS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.update')\n",
            "def update(x, new_x):\n",
            "  return state_ops.assign(x, new_x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.update_add')\n",
            "def update_add(x, increment):\n",
            "  \"\"\"Update the value of `x` by adding `increment`.\n",
            "\n",
            "  Arguments:\n",
            "      x: A Variable.\n",
            "      increment: A tensor of same shape as `x`.\n",
            "\n",
            "  Returns:\n",
            "      The variable `x` updated.\n",
            "  \"\"\"\n",
            "  return state_ops.assign_add(x, increment)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.update_sub')\n",
            "def update_sub(x, decrement):\n",
            "  \"\"\"Update the value of `x` by subtracting `decrement`.\n",
            "\n",
            "  Arguments:\n",
            "      x: A Variable.\n",
            "      decrement: A tensor of same shape as `x`.\n",
            "\n",
            "  Returns:\n",
            "      The variable `x` updated.\n",
            "  \"\"\"\n",
            "  return state_ops.assign_sub(x, decrement)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.moving_average_update')\n",
            "def moving_average_update(x, value, momentum):\n",
            "  \"\"\"Compute the moving average of a variable.\n",
            "\n",
            "  Arguments:\n",
            "      x: A Variable.\n",
            "      value: A tensor with the same shape as `variable`.\n",
            "      momentum: The moving average momentum.\n",
            "\n",
            "  Returns:\n",
            "      An Operation to update the variable.\n",
            "  \"\"\"\n",
            "  # `training` is higher-up than the Keras backend in the abstraction hierarchy.\n",
            "  # In particular, `training` depends on layers, and thus on Keras.\n",
            "  # moving_averages, being low-level ops, should not be part of the training\n",
            "  # module.\n",
            "  from tensorflow.python.training import moving_averages  # pylint: disable=g-import-not-at-top\n",
            "  return moving_averages.assign_moving_average(\n",
            "      x, value, momentum, zero_debias=True)\n",
            "\n",
            "\n",
            "# LINEAR ALGEBRA\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.dot')\n",
            "def dot(x, y):\n",
            "  \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n",
            "\n",
            "  When attempting to multiply a nD tensor\n",
            "  with a nD tensor, it reproduces the Theano behavior.\n",
            "  (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, dot product of `x` and `y`.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      # dot product between tensors\n",
            "      >>> x = K.placeholder(shape=(2, 3))\n",
            "      >>> y = K.placeholder(shape=(3, 4))\n",
            "      >>> xy = K.dot(x, y)\n",
            "      >>> xy\n",
            "      <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n",
            "  ```\n",
            "\n",
            "  ```python\n",
            "      # dot product between tensors\n",
            "      >>> x = K.placeholder(shape=(32, 28, 3))\n",
            "      >>> y = K.placeholder(shape=(3, 4))\n",
            "      >>> xy = K.dot(x, y)\n",
            "      >>> xy\n",
            "      <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>\n",
            "  ```\n",
            "\n",
            "  ```python\n",
            "      # Theano-like behavior example\n",
            "      >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n",
            "      >>> y = K.ones((4, 3, 5))\n",
            "      >>> xy = K.dot(x, y)\n",
            "      >>> K.int_shape(xy)\n",
            "      (2, 4, 5)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n",
            "    x_shape = []\n",
            "    for i, s in zip(int_shape(x), array_ops.unstack(array_ops.shape(x))):\n",
            "      if i is not None:\n",
            "        x_shape.append(i)\n",
            "      else:\n",
            "        x_shape.append(s)\n",
            "    x_shape = tuple(x_shape)\n",
            "    y_shape = []\n",
            "    for i, s in zip(int_shape(y), array_ops.unstack(array_ops.shape(y))):\n",
            "      if i is not None:\n",
            "        y_shape.append(i)\n",
            "      else:\n",
            "        y_shape.append(s)\n",
            "    y_shape = tuple(y_shape)\n",
            "    y_permute_dim = list(range(ndim(y)))\n",
            "    y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\n",
            "    xt = array_ops.reshape(x, [-1, x_shape[-1]])\n",
            "    yt = array_ops.reshape(\n",
            "        array_ops.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\n",
            "    return array_ops.reshape(\n",
            "        math_ops.matmul(xt, yt), x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n",
            "  if is_sparse(x):\n",
            "    out = sparse_ops.sparse_tensor_dense_matmul(x, y)\n",
            "  else:\n",
            "    out = math_ops.matmul(x, y)\n",
            "  return out\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.batch_dot')\n",
            "def batch_dot(x, y, axes=None):\n",
            "  \"\"\"Batchwise dot product.\n",
            "\n",
            "  `batch_dot` is used to compute dot product of `x` and `y` when\n",
            "  `x` and `y` are data in batch, i.e. in a shape of\n",
            "  `(batch_size, :)`.\n",
            "  `batch_dot` results in a tensor or variable with less dimensions\n",
            "  than the input. If the number of dimensions is reduced to 1,\n",
            "  we use `expand_dims` to make sure that ndim is at least 2.\n",
            "\n",
            "  Arguments:\n",
            "      x: Keras tensor or variable with `ndim >= 2`.\n",
            "      y: Keras tensor or variable with `ndim >= 2`.\n",
            "      axes: list of (or single) int with target dimensions.\n",
            "          The lengths of `axes[0]` and `axes[1]` should be the same.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with shape equal to the concatenation of `x`'s shape\n",
            "      (less the dimension that was summed over) and `y`'s shape\n",
            "      (less the batch dimension and the dimension that was summed over).\n",
            "      If the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
            "\n",
            "  Examples:\n",
            "      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
            "      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
            "      of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
            "      elements.\n",
            "\n",
            "      Shape inference:\n",
            "      Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
            "      If `axes` is (1, 2), to find the output shape of resultant tensor,\n",
            "          loop through each dimension in `x`'s shape and `y`'s shape:\n",
            "\n",
            "      * `x.shape[0]` : 100 : append to output shape\n",
            "      * `x.shape[1]` : 20 : do not append to output shape,\n",
            "          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
            "      * `y.shape[0]` : 100 : do not append to output shape,\n",
            "          always ignore first dimension of `y`\n",
            "      * `y.shape[1]` : 30 : append to output shape\n",
            "      * `y.shape[2]` : 20 : do not append to output shape,\n",
            "          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
            "      `output_shape` = `(100, 30)`\n",
            "\n",
            "  ```python\n",
            "      >>> x_batch = K.ones(shape=(32, 20, 1))\n",
            "      >>> y_batch = K.ones(shape=(32, 30, 20))\n",
            "      >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
            "      >>> K.int_shape(xy_batch_dot)\n",
            "      (32, 1, 30)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  if isinstance(axes, int):\n",
            "    axes = (axes, axes)\n",
            "  x_ndim = ndim(x)\n",
            "  y_ndim = ndim(y)\n",
            "  if axes is None:\n",
            "    # behaves like tf.batch_matmul as default\n",
            "    axes = [x_ndim - 1, y_ndim - 2]\n",
            "  if x_ndim > y_ndim:\n",
            "    diff = x_ndim - y_ndim\n",
            "    y = array_ops.reshape(y,\n",
            "                          array_ops.concat(\n",
            "                              [array_ops.shape(y), [1] * (diff)], axis=0))\n",
            "  elif y_ndim > x_ndim:\n",
            "    diff = y_ndim - x_ndim\n",
            "    x = array_ops.reshape(x,\n",
            "                          array_ops.concat(\n",
            "                              [array_ops.shape(x), [1] * (diff)], axis=0))\n",
            "  else:\n",
            "    diff = 0\n",
            "  if ndim(x) == 2 and ndim(y) == 2:\n",
            "    if axes[0] == axes[1]:\n",
            "      out = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
            "    else:\n",
            "      out = math_ops.reduce_sum(\n",
            "          math_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
            "  else:\n",
            "    adj_x = None if axes[0] == ndim(x) - 1 else True\n",
            "    adj_y = True if axes[1] == ndim(y) - 1 else None\n",
            "    out = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
            "  if diff:\n",
            "    if x_ndim > y_ndim:\n",
            "      idx = x_ndim + y_ndim - 3\n",
            "    else:\n",
            "      idx = x_ndim - 1\n",
            "    out = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
            "  if ndim(out) == 1:\n",
            "    out = expand_dims(out, 1)\n",
            "  return out\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.transpose')\n",
            "def transpose(x):\n",
            "  \"\"\"Transposes a tensor and returns it.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "\n",
            "  Examples:\n",
            "  ```python\n",
            "      >>> var = K.variable([[1, 2, 3], [4, 5, 6]])\n",
            "      >>> K.eval(var)\n",
            "      array([[ 1.,  2.,  3.],\n",
            "             [ 4.,  5.,  6.]], dtype=float32)\n",
            "      >>> var_transposed = K.transpose(var)\n",
            "      >>> K.eval(var_transposed)\n",
            "      array([[ 1.,  4.],\n",
            "             [ 2.,  5.],\n",
            "             [ 3.,  6.]], dtype=float32)\n",
            "  ```\n",
            "\n",
            "  ```python\n",
            "      >>> input = K.placeholder((2, 3))\n",
            "      >>> input\n",
            "      <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n",
            "      >>> input_transposed = K.transpose(input)\n",
            "      >>> input_transposed\n",
            "      <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n",
            "\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return array_ops.transpose(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.gather')\n",
            "def gather(reference, indices):\n",
            "  \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n",
            "\n",
            "  Arguments:\n",
            "      reference: A tensor.\n",
            "      indices: An integer tensor of indices.\n",
            "\n",
            "  Returns:\n",
            "      A tensor of same type as `reference`.\n",
            "  \"\"\"\n",
            "  return array_ops.gather(reference, indices)\n",
            "\n",
            "\n",
            "# ELEMENT-WISE OPERATIONS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.max')\n",
            "def max(x, axis=None, keepdims=False):\n",
            "  \"\"\"Maximum value in a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to find maximum values.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with maximum values of `x`.\n",
            "  \"\"\"\n",
            "  return math_ops.reduce_max(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.min')\n",
            "def min(x, axis=None, keepdims=False):\n",
            "  \"\"\"Minimum value in a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to find minimum values.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with minimum values of `x`.\n",
            "  \"\"\"\n",
            "  return math_ops.reduce_min(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sum')\n",
            "def sum(x, axis=None, keepdims=False):\n",
            "  \"\"\"Sum of the values in a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to sum over.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with sum of `x`.\n",
            "  \"\"\"\n",
            "  return math_ops.reduce_sum(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.prod')\n",
            "def prod(x, axis=None, keepdims=False):\n",
            "  \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to compute the product.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with the product of elements of `x`.\n",
            "  \"\"\"\n",
            "  return math_ops.reduce_prod(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.cumsum')\n",
            "def cumsum(x, axis=0):\n",
            "  \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to compute the sum.\n",
            "\n",
            "  Returns:\n",
            "      A tensor of the cumulative sum of values of `x` along `axis`.\n",
            "  \"\"\"\n",
            "  return math_ops.cumsum(x, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.cumprod')\n",
            "def cumprod(x, axis=0):\n",
            "  \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to compute the product.\n",
            "\n",
            "  Returns:\n",
            "      A tensor of the cumulative product of values of `x` along `axis`.\n",
            "  \"\"\"\n",
            "  return math_ops.cumprod(x, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.var')\n",
            "def var(x, axis=None, keepdims=False):\n",
            "  \"\"\"Variance of a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to compute the variance.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with the variance of elements of `x`.\n",
            "  \"\"\"\n",
            "  if x.dtype.base_dtype == dtypes_module.bool:\n",
            "    x = math_ops.cast(x, floatx())\n",
            "  return math_ops.reduce_variance(x, axis=axis, keepdims=keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.std')\n",
            "def std(x, axis=None, keepdims=False):\n",
            "  \"\"\"Standard deviation of a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to compute the standard deviation.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`,\n",
            "          the reduced dimension is retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with the standard deviation of elements of `x`.\n",
            "  \"\"\"\n",
            "  if x.dtype.base_dtype == dtypes_module.bool:\n",
            "    x = math_ops.cast(x, floatx())\n",
            "  return math_ops.reduce_std(x, axis=axis, keepdims=keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.mean')\n",
            "def mean(x, axis=None, keepdims=False):\n",
            "  \"\"\"Mean of a tensor, alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: A list of integer. Axes to compute the mean.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1 for each entry in `axis`. If `keepdims` is `True`,\n",
            "          the reduced dimensions are retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with the mean of elements of `x`.\n",
            "  \"\"\"\n",
            "  if x.dtype.base_dtype == dtypes_module.bool:\n",
            "    x = math_ops.cast(x, floatx())\n",
            "  return math_ops.reduce_mean(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.any')\n",
            "def any(x, axis=None, keepdims=False):\n",
            "  \"\"\"Bitwise reduction (logical OR).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      axis: axis along which to perform the reduction.\n",
            "      keepdims: whether the drop or broadcast the reduction axes.\n",
            "\n",
            "  Returns:\n",
            "      A uint8 tensor (0s and 1s).\n",
            "  \"\"\"\n",
            "  x = math_ops.cast(x, dtypes_module.bool)\n",
            "  return math_ops.reduce_any(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.all')\n",
            "def all(x, axis=None, keepdims=False):\n",
            "  \"\"\"Bitwise reduction (logical AND).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      axis: axis along which to perform the reduction.\n",
            "      keepdims: whether the drop or broadcast the reduction axes.\n",
            "\n",
            "  Returns:\n",
            "      A uint8 tensor (0s and 1s).\n",
            "  \"\"\"\n",
            "  x = math_ops.cast(x, dtypes_module.bool)\n",
            "  return math_ops.reduce_all(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.argmax')\n",
            "def argmax(x, axis=-1):\n",
            "  \"\"\"Returns the index of the maximum value along an axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      axis: axis along which to perform the reduction.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.argmax(x, axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.argmin')\n",
            "def argmin(x, axis=-1):\n",
            "  \"\"\"Returns the index of the minimum value along an axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      axis: axis along which to perform the reduction.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.argmin(x, axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.square')\n",
            "def square(x):\n",
            "  \"\"\"Element-wise square.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.square(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.abs')\n",
            "def abs(x):\n",
            "  \"\"\"Element-wise absolute value.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.abs(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sqrt')\n",
            "def sqrt(x):\n",
            "  \"\"\"Element-wise square root.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  zero = _constant_to_tensor(0., x.dtype.base_dtype)\n",
            "  inf = _constant_to_tensor(np.inf, x.dtype.base_dtype)\n",
            "  x = clip_ops.clip_by_value(x, zero, inf)\n",
            "  return math_ops.sqrt(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.exp')\n",
            "def exp(x):\n",
            "  \"\"\"Element-wise exponential.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.exp(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.log')\n",
            "def log(x):\n",
            "  \"\"\"Element-wise log.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.log(x)\n",
            "\n",
            "\n",
            "def logsumexp(x, axis=None, keepdims=False):\n",
            "  \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n",
            "\n",
            "  This function is more numerically stable than log(sum(exp(x))).\n",
            "  It avoids overflows caused by taking the exp of large inputs and\n",
            "  underflows caused by taking the log of small inputs.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: An integer, the axis to reduce over.\n",
            "      keepdims: A boolean, whether to keep the dimensions or not.\n",
            "          If `keepdims` is `False`, the rank of the tensor is reduced\n",
            "          by 1. If `keepdims` is `True`, the reduced dimension is\n",
            "          retained with length 1.\n",
            "\n",
            "  Returns:\n",
            "      The reduced tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.reduce_logsumexp(x, axis, keepdims)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.round')\n",
            "def round(x):\n",
            "  \"\"\"Element-wise rounding to the closest integer.\n",
            "\n",
            "  In case of tie, the rounding mode used is \"half to even\".\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.round(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sign')\n",
            "def sign(x):\n",
            "  \"\"\"Element-wise sign.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.sign(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.pow')\n",
            "def pow(x, a):\n",
            "  \"\"\"Element-wise exponentiation.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      a: Python integer.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.pow(x, a)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.clip')\n",
            "def clip(x, min_value, max_value):\n",
            "  \"\"\"Element-wise value clipping.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      min_value: Python float or integer.\n",
            "      max_value: Python float or integer.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if max_value is not None and max_value < min_value:\n",
            "    max_value = min_value\n",
            "  if max_value is None:\n",
            "    max_value = np.inf\n",
            "  min_value = _constant_to_tensor(min_value, x.dtype.base_dtype)\n",
            "  max_value = _constant_to_tensor(max_value, x.dtype.base_dtype)\n",
            "  return clip_ops.clip_by_value(x, min_value, max_value)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.equal')\n",
            "def equal(x, y):\n",
            "  \"\"\"Element-wise equality between two tensors.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.equal(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.not_equal')\n",
            "def not_equal(x, y):\n",
            "  \"\"\"Element-wise inequality between two tensors.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.not_equal(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.greater')\n",
            "def greater(x, y):\n",
            "  \"\"\"Element-wise truth value of (x > y).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.greater(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.greater_equal')\n",
            "def greater_equal(x, y):\n",
            "  \"\"\"Element-wise truth value of (x >= y).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.greater_equal(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.less')\n",
            "def less(x, y):\n",
            "  \"\"\"Element-wise truth value of (x < y).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.less(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.less_equal')\n",
            "def less_equal(x, y):\n",
            "  \"\"\"Element-wise truth value of (x <= y).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A bool tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.less_equal(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.maximum')\n",
            "def maximum(x, y):\n",
            "  \"\"\"Element-wise maximum of two tensors.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.maximum(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.minimum')\n",
            "def minimum(x, y):\n",
            "  \"\"\"Element-wise minimum of two tensors.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      y: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.minimum(x, y)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sin')\n",
            "def sin(x):\n",
            "  \"\"\"Computes sin of x element-wise.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.sin(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.cos')\n",
            "def cos(x):\n",
            "  \"\"\"Computes cos of x element-wise.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return math_ops.cos(x)\n",
            "\n",
            "\n",
            "def _regular_normalize_batch_in_training(x,\n",
            "                                         gamma,\n",
            "                                         beta,\n",
            "                                         reduction_axes,\n",
            "                                         epsilon=1e-3):\n",
            "  \"\"\"Non-fused version of `normalize_batch_in_training`.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor or variable.\n",
            "      gamma: Tensor by which to scale the input.\n",
            "      beta: Tensor with which to center the input.\n",
            "      reduction_axes: iterable of integers,\n",
            "          axes over which to normalize.\n",
            "      epsilon: Fuzz factor.\n",
            "\n",
            "  Returns:\n",
            "      A tuple length of 3, `(normalized_tensor, mean, variance)`.\n",
            "  \"\"\"\n",
            "  mean, var = nn.moments(x, reduction_axes, None, None, False)\n",
            "  normed = nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n",
            "  return normed, mean, var\n",
            "\n",
            "\n",
            "def _broadcast_normalize_batch_in_training(x,\n",
            "                                           gamma,\n",
            "                                           beta,\n",
            "                                           reduction_axes,\n",
            "                                           epsilon=1e-3):\n",
            "  \"\"\"Non-fused, broadcast version of `normalize_batch_in_training`.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor or variable.\n",
            "      gamma: Tensor by which to scale the input.\n",
            "      beta: Tensor with which to center the input.\n",
            "      reduction_axes: iterable of integers,\n",
            "          axes over which to normalize.\n",
            "      epsilon: Fuzz factor.\n",
            "\n",
            "  Returns:\n",
            "      A tuple length of 3, `(normalized_tensor, mean, variance)`.\n",
            "  \"\"\"\n",
            "  mean, var = nn.moments(x, reduction_axes, None, None, False)\n",
            "  target_shape = []\n",
            "  for axis in range(ndim(x)):\n",
            "    if axis in reduction_axes:\n",
            "      target_shape.append(1)\n",
            "    else:\n",
            "      target_shape.append(array_ops.shape(x)[axis])\n",
            "  target_shape = array_ops.stack(target_shape)\n",
            "\n",
            "  broadcast_mean = array_ops.reshape(mean, target_shape)\n",
            "  broadcast_var = array_ops.reshape(var, target_shape)\n",
            "  if gamma is None:\n",
            "    broadcast_gamma = None\n",
            "  else:\n",
            "    broadcast_gamma = array_ops.reshape(gamma, target_shape)\n",
            "  if beta is None:\n",
            "    broadcast_beta = None\n",
            "  else:\n",
            "    broadcast_beta = array_ops.reshape(beta, target_shape)\n",
            "\n",
            "  normed = nn.batch_normalization(x, broadcast_mean, broadcast_var,\n",
            "                                  broadcast_beta, broadcast_gamma, epsilon)\n",
            "  return normed, mean, var\n",
            "\n",
            "\n",
            "def _fused_normalize_batch_in_training(x,\n",
            "                                       gamma,\n",
            "                                       beta,\n",
            "                                       reduction_axes,\n",
            "                                       epsilon=1e-3):\n",
            "  \"\"\"Fused version of `normalize_batch_in_training`.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor or variable.\n",
            "      gamma: Tensor by which to scale the input.\n",
            "      beta: Tensor with which to center the input.\n",
            "      reduction_axes: iterable of integers,\n",
            "          axes over which to normalize.\n",
            "      epsilon: Fuzz factor.\n",
            "\n",
            "  Returns:\n",
            "      A tuple length of 3, `(normalized_tensor, mean, variance)`.\n",
            "  \"\"\"\n",
            "  if list(reduction_axes) == [0, 1, 2]:\n",
            "    normalization_axis = 3\n",
            "    tf_data_format = 'NHWC'\n",
            "  else:\n",
            "    normalization_axis = 1\n",
            "    tf_data_format = 'NCHW'\n",
            "\n",
            "  if gamma is None:\n",
            "    gamma = constant_op.constant(\n",
            "        1.0, dtype=x.dtype, shape=[x.shape[normalization_axis]])\n",
            "  if beta is None:\n",
            "    beta = constant_op.constant(\n",
            "        0.0, dtype=x.dtype, shape=[x.shape[normalization_axis]])\n",
            "\n",
            "  return nn.fused_batch_norm(\n",
            "      x, gamma, beta, epsilon=epsilon, data_format=tf_data_format)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.normalize_batch_in_training')\n",
            "def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=1e-3):\n",
            "  \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor or variable.\n",
            "      gamma: Tensor by which to scale the input.\n",
            "      beta: Tensor with which to center the input.\n",
            "      reduction_axes: iterable of integers,\n",
            "          axes over which to normalize.\n",
            "      epsilon: Fuzz factor.\n",
            "\n",
            "  Returns:\n",
            "      A tuple length of 3, `(normalized_tensor, mean, variance)`.\n",
            "  \"\"\"\n",
            "  if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:\n",
            "    if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n",
            "      return _broadcast_normalize_batch_in_training(\n",
            "          x, gamma, beta, reduction_axes, epsilon=epsilon)\n",
            "    return _fused_normalize_batch_in_training(\n",
            "        x, gamma, beta, reduction_axes, epsilon=epsilon)\n",
            "  else:\n",
            "    if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:\n",
            "      return _regular_normalize_batch_in_training(\n",
            "          x, gamma, beta, reduction_axes, epsilon=epsilon)\n",
            "    else:\n",
            "      return _broadcast_normalize_batch_in_training(\n",
            "          x, gamma, beta, reduction_axes, epsilon=epsilon)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.batch_normalization')\n",
            "def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n",
            "  \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n",
            "\n",
            "  I.e. returns:\n",
            "  `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n",
            "\n",
            "  Arguments:\n",
            "      x: Input tensor or variable.\n",
            "      mean: Mean of batch.\n",
            "      var: Variance of batch.\n",
            "      beta: Tensor with which to center the input.\n",
            "      gamma: Tensor by which to scale the input.\n",
            "      axis: Integer, the axis that should be normalized.\n",
            "          (typically the features axis).\n",
            "      epsilon: Fuzz factor.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if ndim(x) == 4:\n",
            "    # The CPU implementation of `fused_batch_norm` only supports NHWC\n",
            "    if axis == 1 or axis == -3:\n",
            "      tf_data_format = 'NCHW'\n",
            "    elif axis == 3 or axis == -1:\n",
            "      tf_data_format = 'NHWC'\n",
            "    else:\n",
            "      tf_data_format = None\n",
            "\n",
            "    if (tf_data_format == 'NHWC' or\n",
            "        tf_data_format == 'NCHW' and _has_nchw_support()):\n",
            "      # The mean / var / beta / gamma tensors may be broadcasted\n",
            "      # so they may have extra axes of size 1, which should be squeezed.\n",
            "      if ndim(mean) > 1:\n",
            "        mean = array_ops.reshape(mean, [-1])\n",
            "      if ndim(var) > 1:\n",
            "        var = array_ops.reshape(var, [-1])\n",
            "      if beta is None:\n",
            "        beta = zeros_like(mean)\n",
            "      elif ndim(beta) > 1:\n",
            "        beta = array_ops.reshape(beta, [-1])\n",
            "      if gamma is None:\n",
            "        gamma = ones_like(mean)\n",
            "      elif ndim(gamma) > 1:\n",
            "        gamma = array_ops.reshape(gamma, [-1])\n",
            "    y, _, _ = nn.fused_batch_norm(\n",
            "        x,\n",
            "        gamma,\n",
            "        beta,\n",
            "        epsilon=epsilon,\n",
            "        mean=mean,\n",
            "        variance=var,\n",
            "        data_format=tf_data_format,\n",
            "        is_training=False\n",
            "    )\n",
            "    return y\n",
            "  return nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n",
            "\n",
            "\n",
            "# SHAPE OPERATIONS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.concatenate')\n",
            "def concatenate(tensors, axis=-1):\n",
            "  \"\"\"Concatenates a list of tensors alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      tensors: list of tensors to concatenate.\n",
            "      axis: concatenation axis.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if axis < 0:\n",
            "    rank = ndim(tensors[0])\n",
            "    if rank:\n",
            "      axis %= rank\n",
            "    else:\n",
            "      axis = 0\n",
            "\n",
            "  if py_all(is_sparse(x) for x in tensors):\n",
            "    return sparse_ops.sparse_concat(axis, tensors)\n",
            "  else:\n",
            "    return array_ops.concat([to_dense(x) for x in tensors], axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.reshape')\n",
            "def reshape(x, shape):\n",
            "  \"\"\"Reshapes a tensor to the specified shape.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      shape: Target shape tuple.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return array_ops.reshape(x, shape)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.permute_dimensions')\n",
            "def permute_dimensions(x, pattern):\n",
            "  \"\"\"Permutes axes in a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      pattern: A tuple of\n",
            "          dimension indices, e.g. `(0, 2, 1)`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return array_ops.transpose(x, perm=pattern)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.resize_images')\n",
            "def resize_images(x, height_factor, width_factor, data_format,\n",
            "                  interpolation='nearest'):\n",
            "  \"\"\"Resizes the images contained in a 4D tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable to resize.\n",
            "      height_factor: Positive integer.\n",
            "      width_factor: Positive integer.\n",
            "      data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n",
            "      interpolation: A string, one of `nearest` or `bilinear`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: in case of incorrect value for\n",
            "        `data_format` or `interpolation`.\n",
            "  \"\"\"\n",
            "  if data_format == 'channels_first':\n",
            "    rows, cols = 2, 3\n",
            "  elif data_format == 'channels_last':\n",
            "    rows, cols = 1, 2\n",
            "  else:\n",
            "    raise ValueError('Invalid `data_format` argument: %s' % (data_format,))\n",
            "\n",
            "  original_shape = int_shape(x)\n",
            "  new_shape = array_ops.shape(x)[rows:cols + 1]\n",
            "  new_shape *= constant_op.constant(\n",
            "      np.array([height_factor, width_factor], dtype='int32'))\n",
            "\n",
            "  if data_format == 'channels_first':\n",
            "    x = permute_dimensions(x, [0, 2, 3, 1])\n",
            "  if interpolation == 'nearest':\n",
            "    x = image_ops.resize_nearest_neighbor(x, new_shape)\n",
            "  elif interpolation == 'bilinear':\n",
            "    x = image_ops.resize_bilinear(x, new_shape)\n",
            "  else:\n",
            "    raise ValueError('interpolation should be one '\n",
            "                     'of \"nearest\" or \"bilinear\".')\n",
            "  if data_format == 'channels_first':\n",
            "    x = permute_dimensions(x, [0, 3, 1, 2])\n",
            "\n",
            "  if original_shape[rows] is None:\n",
            "    new_height = None\n",
            "  else:\n",
            "    new_height = original_shape[rows] * height_factor\n",
            "\n",
            "  if original_shape[cols] is None:\n",
            "    new_width = None\n",
            "  else:\n",
            "    new_width = original_shape[cols] * width_factor\n",
            "\n",
            "  if data_format == 'channels_first':\n",
            "    output_shape = (None, None, new_height, new_width)\n",
            "  else:\n",
            "    output_shape = (None, new_height, new_width, None)\n",
            "  x.set_shape(output_shape)\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.resize_volumes')\n",
            "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n",
            "  \"\"\"Resizes the volume contained in a 5D tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable to resize.\n",
            "      depth_factor: Positive integer.\n",
            "      height_factor: Positive integer.\n",
            "      width_factor: Positive integer.\n",
            "      data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither\n",
            "          `channels_last` or `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format == 'channels_first':\n",
            "    output = repeat_elements(x, depth_factor, axis=2)\n",
            "    output = repeat_elements(output, height_factor, axis=3)\n",
            "    output = repeat_elements(output, width_factor, axis=4)\n",
            "    return output\n",
            "  elif data_format == 'channels_last':\n",
            "    output = repeat_elements(x, depth_factor, axis=1)\n",
            "    output = repeat_elements(output, height_factor, axis=2)\n",
            "    output = repeat_elements(output, width_factor, axis=3)\n",
            "    return output\n",
            "  else:\n",
            "    raise ValueError('Invalid data_format: ' + str(data_format))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.repeat_elements')\n",
            "def repeat_elements(x, rep, axis):\n",
            "  \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n",
            "\n",
            "  If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\n",
            "  will have shape `(s1, s2 * rep, s3)`.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      rep: Python integer, number of times to repeat.\n",
            "      axis: Axis along which to repeat.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  x_shape = x.shape.as_list()\n",
            "  # For static axis\n",
            "  if x_shape[axis] is not None:\n",
            "    # slices along the repeat axis\n",
            "    splits = array_ops.split(value=x,\n",
            "                             num_or_size_splits=x_shape[axis],\n",
            "                             axis=axis)\n",
            "    # repeat each slice the given number of reps\n",
            "    x_rep = [s for s in splits for _ in range(rep)]\n",
            "    return concatenate(x_rep, axis)\n",
            "\n",
            "  # Here we use tf.tile to mimic behavior of np.repeat so that\n",
            "  # we can handle dynamic shapes (that include None).\n",
            "  # To do that, we need an auxiliary axis to repeat elements along\n",
            "  # it and then merge them along the desired axis.\n",
            "\n",
            "  # Repeating\n",
            "  auxiliary_axis = axis + 1\n",
            "  x_shape = array_ops.shape(x)\n",
            "  x_rep = array_ops.expand_dims(x, axis=auxiliary_axis)\n",
            "  reps = np.ones(len(x.shape) + 1)\n",
            "  reps[auxiliary_axis] = rep\n",
            "  x_rep = array_ops.tile(x_rep, reps)\n",
            "\n",
            "  # Merging\n",
            "  reps = np.delete(reps, auxiliary_axis)\n",
            "  reps[axis] = rep\n",
            "  reps = array_ops.constant(reps, dtype='int32')\n",
            "  x_shape *= reps\n",
            "  x_rep = array_ops.reshape(x_rep, x_shape)\n",
            "\n",
            "  # Fix shape representation\n",
            "  x_shape = x.shape.as_list()\n",
            "  x_rep.set_shape(x_shape)\n",
            "  x_rep._keras_shape = tuple(x_shape)\n",
            "  return x_rep\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.repeat')\n",
            "def repeat(x, n):\n",
            "  \"\"\"Repeats a 2D tensor.\n",
            "\n",
            "  if `x` has shape (samples, dim) and `n` is `2`,\n",
            "  the output will have shape `(samples, 2, dim)`.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      n: Python integer, number of times to repeat.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  assert ndim(x) == 2\n",
            "  x = array_ops.expand_dims(x, 1)\n",
            "  pattern = array_ops.stack([1, n, 1])\n",
            "  return array_ops.tile(x, pattern)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.arange')\n",
            "def arange(start, stop=None, step=1, dtype='int32'):\n",
            "  \"\"\"Creates a 1D tensor containing a sequence of integers.\n",
            "\n",
            "  The function arguments use the same convention as\n",
            "  Theano's arange: if only one argument is provided,\n",
            "  it is in fact the \"stop\" argument and \"start\" is 0.\n",
            "\n",
            "  The default type of the returned tensor is `'int32'` to\n",
            "  match TensorFlow's default.\n",
            "\n",
            "  Arguments:\n",
            "      start: Start value.\n",
            "      stop: Stop value.\n",
            "      step: Difference between two successive values.\n",
            "      dtype: Integer dtype to use.\n",
            "\n",
            "  Returns:\n",
            "      An integer tensor.\n",
            "\n",
            "  \"\"\"\n",
            "  # Match the behavior of numpy and Theano by returning an empty sequence.\n",
            "  if stop is None and start < 0:\n",
            "    start = 0\n",
            "  result = math_ops.range(start, limit=stop, delta=step, name='arange')\n",
            "  if dtype != 'int32':\n",
            "    result = cast(result, dtype)\n",
            "  return result\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.tile')\n",
            "def tile(x, n):\n",
            "  \"\"\"Creates a tensor by tiling `x` by `n`.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable\n",
            "      n: A list of integer. The length must be the same as the number of\n",
            "          dimensions in `x`.\n",
            "\n",
            "  Returns:\n",
            "      A tiled tensor.\n",
            "  \"\"\"\n",
            "  if isinstance(n, int):\n",
            "    n = [n]\n",
            "  return array_ops.tile(x, n)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.flatten')\n",
            "def flatten(x):\n",
            "  \"\"\"Flatten a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, reshaped into 1-D\n",
            "  \"\"\"\n",
            "  return array_ops.reshape(x, [-1])\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.batch_flatten')\n",
            "def batch_flatten(x):\n",
            "  \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n",
            "\n",
            "  In other words, it flattens each data samples of a batch.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "\n",
            "  Examples:\n",
            "    Flattening a 3D tensor to 2D by collapsing the last dimension.\n",
            "\n",
            "  ```python\n",
            "      >>> from tensorflow.keras import backend as K\n",
            "      >>> x_batch = K.ones(shape=(2, 3, 4, 5))\n",
            "      >>> x_batch_flatten = K.batch_flatten(x_batch)\n",
            "      >>> K.int_shape(x_batch_flatten)\n",
            "      (2, 60)\n",
            "  ```\n",
            "  \"\"\"\n",
            "  x = array_ops.reshape(x, array_ops.stack([-1, prod(shape(x)[1:])]))\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.expand_dims')\n",
            "def expand_dims(x, axis=-1):\n",
            "  \"\"\"Adds a 1-sized dimension at index \"axis\".\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: Position where to add a new axis.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with expanded dimensions.\n",
            "  \"\"\"\n",
            "  return array_ops.expand_dims(x, axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.squeeze')\n",
            "def squeeze(x, axis):\n",
            "  \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: Axis to drop.\n",
            "\n",
            "  Returns:\n",
            "      A tensor with the same data as `x` but reduced dimensions.\n",
            "  \"\"\"\n",
            "  return array_ops.squeeze(x, [axis])\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.temporal_padding')\n",
            "def temporal_padding(x, padding=(1, 1)):\n",
            "  \"\"\"Pads the middle dimension of a 3D tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      padding: Tuple of 2 integers, how many zeros to\n",
            "          add at the start and end of dim 1.\n",
            "\n",
            "  Returns:\n",
            "      A padded 3D tensor.\n",
            "  \"\"\"\n",
            "  assert len(padding) == 2\n",
            "  pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]\n",
            "  return array_ops.pad(x, pattern)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.spatial_2d_padding')\n",
            "def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n",
            "  \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      padding: Tuple of 2 tuples, padding pattern.\n",
            "      data_format: One of `channels_last` or `channels_first`.\n",
            "\n",
            "  Returns:\n",
            "      A padded 4D tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither\n",
            "          `channels_last` or `channels_first`.\n",
            "  \"\"\"\n",
            "  assert len(padding) == 2\n",
            "  assert len(padding[0]) == 2\n",
            "  assert len(padding[1]) == 2\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  if data_format == 'channels_first':\n",
            "    pattern = [[0, 0], [0, 0], list(padding[0]), list(padding[1])]\n",
            "  else:\n",
            "    pattern = [[0, 0], list(padding[0]), list(padding[1]), [0, 0]]\n",
            "  return array_ops.pad(x, pattern)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.spatial_3d_padding')\n",
            "def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n",
            "  \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n",
            "\n",
            "  Pads these dimensions with respectively\n",
            "  \"padding[0]\", \"padding[1]\" and \"padding[2]\" zeros left and right.\n",
            "\n",
            "  For 'channels_last' data_format,\n",
            "  the 2nd, 3rd and 4th dimension will be padded.\n",
            "  For 'channels_first' data_format,\n",
            "  the 3rd, 4th and 5th dimension will be padded.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      padding: Tuple of 3 tuples, padding pattern.\n",
            "      data_format: One of `channels_last` or `channels_first`.\n",
            "\n",
            "  Returns:\n",
            "      A padded 5D tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither\n",
            "          `channels_last` or `channels_first`.\n",
            "\n",
            "  \"\"\"\n",
            "  assert len(padding) == 3\n",
            "  assert len(padding[0]) == 2\n",
            "  assert len(padding[1]) == 2\n",
            "  assert len(padding[2]) == 2\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  if data_format == 'channels_first':\n",
            "    pattern = [[0, 0], [0, 0], [padding[0][0], padding[0][1]],\n",
            "               [padding[1][0], padding[1][1]], [padding[2][0], padding[2][1]]]\n",
            "  else:\n",
            "    pattern = [[0, 0], [padding[0][0], padding[0][1]],\n",
            "               [padding[1][0], padding[1][1]], [padding[2][0],\n",
            "                                                padding[2][1]], [0, 0]]\n",
            "  return array_ops.pad(x, pattern)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.stack')\n",
            "def stack(x, axis=0):\n",
            "  \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: List of tensors.\n",
            "      axis: Axis along which to perform stacking.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return array_ops.stack(x, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.one_hot')\n",
            "def one_hot(indices, num_classes):\n",
            "  \"\"\"Computes the one-hot representation of an integer tensor.\n",
            "\n",
            "  Arguments:\n",
            "      indices: nD integer tensor of shape\n",
            "          `(batch_size, dim1, dim2, ... dim(n-1))`\n",
            "      num_classes: Integer, number of classes to consider.\n",
            "\n",
            "  Returns:\n",
            "      (n + 1)D one hot representation of the input\n",
            "      with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n",
            "\n",
            "  Returns:\n",
            "      The one-hot tensor.\n",
            "  \"\"\"\n",
            "  return array_ops.one_hot(indices, depth=num_classes, axis=-1)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.reverse')\n",
            "def reverse(x, axes):\n",
            "  \"\"\"Reverse a tensor along the specified axes.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor to reverse.\n",
            "      axes: Integer or iterable of integers.\n",
            "          Axes to reverse.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if isinstance(axes, int):\n",
            "    axes = [axes]\n",
            "  return array_ops.reverse(x, axes)\n",
            "\n",
            "\n",
            "# VALUE MANIPULATION\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.get_value')\n",
            "def get_value(x):\n",
            "  \"\"\"Returns the value of a variable.\n",
            "\n",
            "  Arguments:\n",
            "      x: input variable.\n",
            "\n",
            "  Returns:\n",
            "      A Numpy array.\n",
            "  \"\"\"\n",
            "  if not tensor_util.is_tensor(x):\n",
            "    return x\n",
            "  if context.executing_eagerly():\n",
            "    return x.numpy()\n",
            "  if not getattr(x, '_in_graph_mode', True):\n",
            "    # This is a variable which was created in an eager context, but is being\n",
            "    # evaluated from a Graph.\n",
            "    with context.eager_mode():\n",
            "      return x.numpy()\n",
            "\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    # This method of evaluating works inside the Keras FuncGraph.\n",
            "    return function([], x)(x)\n",
            "\n",
            "  return x.eval(session=get_session((x,)))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.batch_get_value')\n",
            "def batch_get_value(tensors):\n",
            "  \"\"\"Returns the value of more than one tensor variable.\n",
            "\n",
            "  Arguments:\n",
            "      tensors: list of ops to run.\n",
            "\n",
            "  Returns:\n",
            "      A list of Numpy arrays.\n",
            "\n",
            "  Raises:\n",
            "      RuntimeError: If this method is called inside defun.\n",
            "  \"\"\"\n",
            "  if context.executing_eagerly():\n",
            "    return [x.numpy() for x in tensors]\n",
            "  elif ops.inside_function():  # pylint: disable=protected-access\n",
            "    raise RuntimeError('Cannot get value inside Tensorflow graph function.')\n",
            "  if tensors:\n",
            "    return get_session(tensors).run(tensors)\n",
            "  else:\n",
            "    return []\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.set_value')\n",
            "def set_value(x, value):\n",
            "  \"\"\"Sets the value of a variable, from a Numpy array.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor to set to a new value.\n",
            "      value: Value to set the tensor to, as a Numpy array\n",
            "          (of the same shape).\n",
            "  \"\"\"\n",
            "  value = np.asarray(value, dtype=dtype(x))\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    x.assign(value)\n",
            "  else:\n",
            "    with get_graph().as_default():\n",
            "      tf_dtype = dtypes_module.as_dtype(x.dtype.name.split('_')[0])\n",
            "      if hasattr(x, '_assign_placeholder'):\n",
            "        assign_placeholder = x._assign_placeholder\n",
            "        assign_op = x._assign_op\n",
            "      else:\n",
            "        assign_placeholder = array_ops.placeholder(tf_dtype, shape=value.shape)\n",
            "        assign_op = x.assign(assign_placeholder)\n",
            "        x._assign_placeholder = assign_placeholder\n",
            "        x._assign_op = assign_op\n",
            "      get_session().run(assign_op, feed_dict={assign_placeholder: value})\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.batch_set_value')\n",
            "def batch_set_value(tuples):\n",
            "  \"\"\"Sets the values of many tensor variables at once.\n",
            "\n",
            "  Arguments:\n",
            "      tuples: a list of tuples `(tensor, value)`.\n",
            "          `value` should be a Numpy array.\n",
            "  \"\"\"\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    for x, value in tuples:\n",
            "      x.assign(np.asarray(value, dtype=dtype(x)))\n",
            "  else:\n",
            "    with get_graph().as_default():\n",
            "      if tuples:\n",
            "        assign_ops = []\n",
            "        feed_dict = {}\n",
            "        for x, value in tuples:\n",
            "          value = np.asarray(value, dtype=dtype(x))\n",
            "          tf_dtype = dtypes_module.as_dtype(x.dtype.name.split('_')[0])\n",
            "          if hasattr(x, '_assign_placeholder'):\n",
            "            assign_placeholder = x._assign_placeholder\n",
            "            assign_op = x._assign_op\n",
            "          else:\n",
            "            assign_placeholder = array_ops.placeholder(tf_dtype,\n",
            "                                                       shape=value.shape)\n",
            "            assign_op = x.assign(assign_placeholder)\n",
            "            x._assign_placeholder = assign_placeholder\n",
            "            x._assign_op = assign_op\n",
            "          assign_ops.append(assign_op)\n",
            "          feed_dict[assign_placeholder] = value\n",
            "        get_session().run(assign_ops, feed_dict=feed_dict)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.print_tensor')\n",
            "def print_tensor(x, message=''):\n",
            "  \"\"\"Prints `message` and the tensor value when evaluated.\n",
            "\n",
            "  Note that `print_tensor` returns a new tensor identical to `x`\n",
            "  which should be used in the following code. Otherwise the\n",
            "  print operation is not taken into account during evaluation.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "     >>> x = K.print_tensor(x, message=\"x is: \")\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor to print.\n",
            "      message: Message to print jointly with the tensor.\n",
            "\n",
            "  Returns:\n",
            "      The same tensor `x`, unchanged.\n",
            "  \"\"\"\n",
            "  if isinstance(x, ops.Tensor) and hasattr(x, 'graph'):\n",
            "    with get_graph().as_default():\n",
            "      op = logging_ops.print_v2(message, x, output_stream=sys.stdout)\n",
            "      with ops.control_dependencies([op]):\n",
            "        return array_ops.identity(x)\n",
            "  else:\n",
            "    logging_ops.print_v2(message, x, output_stream=sys.stdout)\n",
            "    return x\n",
            "\n",
            "\n",
            "\n",
            "# GRAPH MANIPULATION\n",
            "\n",
            "\n",
            "class GraphExecutionFunction(object):\n",
            "  \"\"\"Runs a computation graph.\n",
            "\n",
            "  It's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.\n",
            "  In particular additional operations via `fetches` argument and additional\n",
            "  tensor substitutions via `feed_dict` arguments. Note that given\n",
            "  substitutions are merged with substitutions from `inputs`. Even though\n",
            "  `feed_dict` is passed once in the constructor (called in `model.compile()`)\n",
            "  we can modify the values in the dictionary. Through this feed_dict we can\n",
            "  provide additional substitutions besides Keras inputs.\n",
            "\n",
            "  Arguments:\n",
            "      inputs: Feed placeholders to the computation graph.\n",
            "      outputs: Output tensors to fetch.\n",
            "      updates: Additional update ops to be run at function call.\n",
            "      name: A name to help users identify what this function does.\n",
            "      session_kwargs: Arguments to `tf.Session.run()`:\n",
            "                      `fetches`, `feed_dict`, `options`, `run_metadata`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, inputs, outputs, updates=None, name=None,\n",
            "               **session_kwargs):\n",
            "    updates = updates or []\n",
            "    if not isinstance(updates, (list, tuple)):\n",
            "      raise TypeError('`updates` in a Keras backend function '\n",
            "                      'should be a list or tuple.')\n",
            "    self.inputs = nest.flatten(inputs)\n",
            "    self._outputs_structure = outputs\n",
            "    self.outputs = cast_variables_to_tensor(\n",
            "        nest.flatten(outputs, expand_composites=True))\n",
            "    # TODO(b/127668432): Consider using autograph to generate these\n",
            "    # dependencies in call.\n",
            "    # Index 0 = total loss or model output for `predict`.\n",
            "    with ops.control_dependencies([self.outputs[0]]):\n",
            "      updates_ops = []\n",
            "      for update in updates:\n",
            "        if isinstance(update, tuple):\n",
            "          p, new_p = update\n",
            "          updates_ops.append(state_ops.assign(p, new_p))\n",
            "        else:\n",
            "          # assumed already an op\n",
            "          updates_ops.append(update)\n",
            "      self.updates_op = control_flow_ops.group(*updates_ops)\n",
            "    self.name = name\n",
            "    # additional tensor substitutions\n",
            "    self.feed_dict = session_kwargs.pop('feed_dict', None)\n",
            "    # additional operations\n",
            "    self.fetches = session_kwargs.pop('fetches', [])\n",
            "    if not isinstance(self.fetches, list):\n",
            "      self.fetches = [self.fetches]\n",
            "    self.run_options = session_kwargs.pop('options', None)\n",
            "    self.run_metadata = session_kwargs.pop('run_metadata', None)\n",
            "    # The main use case of `fetches` being passed to a model is the ability\n",
            "    # to run custom updates\n",
            "    # This requires us to wrap fetches in `identity` ops.\n",
            "    self.fetches = [array_ops.identity(x) for x in self.fetches]\n",
            "    self.session_kwargs = session_kwargs\n",
            "    # This mapping keeps track of the function that should receive the\n",
            "    # output from a fetch in `fetches`: { fetch: function(fetch_output) }\n",
            "    # A Callback can use this to register a function with access to the\n",
            "    # output values for a fetch it added.\n",
            "    self.fetch_callbacks = {}\n",
            "\n",
            "    if session_kwargs:\n",
            "      raise ValueError('Some keys in session_kwargs are not supported at this '\n",
            "                       'time: %s' % (session_kwargs.keys(),))\n",
            "\n",
            "    self._callable_fn = None\n",
            "    self._feed_arrays = None\n",
            "    self._feed_symbols = None\n",
            "    self._symbol_vals = None\n",
            "    self._fetches = None\n",
            "    self._session = None\n",
            "\n",
            "  def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):\n",
            "    \"\"\"Generates a callable that runs the graph.\n",
            "\n",
            "    Arguments:\n",
            "      feed_arrays: List of input tensors to be fed Numpy arrays at runtime.\n",
            "      feed_symbols: List of input tensors to be fed symbolic tensors at runtime.\n",
            "      symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n",
            "      session: Session to use to generate the callable.\n",
            "\n",
            "    Returns:\n",
            "      Function that runs the graph according to the above options.\n",
            "    \"\"\"\n",
            "    # Prepare callable options.\n",
            "    callable_opts = config_pb2.CallableOptions()\n",
            "    # Handle external-data feed.\n",
            "    for x in feed_arrays:\n",
            "      callable_opts.feed.append(x.name)\n",
            "    if self.feed_dict:\n",
            "      for key in sorted(self.feed_dict.keys()):\n",
            "        callable_opts.feed.append(key.name)\n",
            "    # Handle symbolic feed.\n",
            "    for x, y in zip(feed_symbols, symbol_vals):\n",
            "      connection = callable_opts.tensor_connection.add()\n",
            "      if x.dtype != y.dtype:\n",
            "        y = math_ops.cast(y, dtype=x.dtype)\n",
            "      from_tensor = ops._as_graph_element(y)\n",
            "      if from_tensor is None:\n",
            "        from_tensor = y\n",
            "      connection.from_tensor = from_tensor.name  # Data tensor\n",
            "      connection.to_tensor = x.name  # Placeholder\n",
            "    # Handle fetches.\n",
            "    for x in self.outputs + self.fetches:\n",
            "      callable_opts.fetch.append(x.name)\n",
            "    # Handle updates.\n",
            "    callable_opts.target.append(self.updates_op.name)\n",
            "    # Handle run_options.\n",
            "    if self.run_options:\n",
            "      callable_opts.run_options.CopyFrom(self.run_options)\n",
            "    # Create callable.\n",
            "    callable_fn = session._make_callable_from_options(callable_opts)\n",
            "    # Cache parameters corresponding to the generated callable, so that\n",
            "    # we can detect future mismatches and refresh the callable.\n",
            "    self._callable_fn = callable_fn\n",
            "    self._feed_arrays = feed_arrays\n",
            "    self._feed_symbols = feed_symbols\n",
            "    self._symbol_vals = symbol_vals\n",
            "    self._fetches = list(self.fetches)\n",
            "    self._session = session\n",
            "\n",
            "  def _call_fetch_callbacks(self, fetches_output):\n",
            "    for fetch, output in zip(self._fetches, fetches_output):\n",
            "      if fetch in self.fetch_callbacks:\n",
            "        self.fetch_callbacks[fetch](output)\n",
            "\n",
            "  def _eval_if_composite(self, tensor):\n",
            "    \"\"\"Helper method which evaluates any CompositeTensors passed to it.\"\"\"\n",
            "    # We need to evaluate any composite tensor objects that have been\n",
            "    # reconstructed in 'pack_sequence_as', since otherwise they'll be output as\n",
            "    # actual CompositeTensor objects instead of the value(s) contained in the\n",
            "    # CompositeTensors. E.g., if output_structure contains a SparseTensor, then\n",
            "    # this ensures that we return its value as a SparseTensorValue rather than\n",
            "    # a SparseTensor.\n",
            "    if isinstance(tensor, composite_tensor.CompositeTensor):\n",
            "      return self._session.run(tensor)\n",
            "    else:\n",
            "      return tensor\n",
            "\n",
            "  def __call__(self, inputs):\n",
            "    inputs = nest.flatten(inputs)\n",
            "\n",
            "    session = get_session(inputs)\n",
            "    feed_arrays = []\n",
            "    array_vals = []\n",
            "    feed_symbols = []\n",
            "    symbol_vals = []\n",
            "    for tensor, value in zip(self.inputs, inputs):\n",
            "      if value is None:\n",
            "        continue\n",
            "      if is_sparse(tensor):\n",
            "        sparse_coo = value.tocoo()\n",
            "        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n",
            "                                  np.expand_dims(sparse_coo.col, 1)), 1)\n",
            "        value = (indices, sparse_coo.data, sparse_coo.shape)\n",
            "      if tensor_util.is_tensor(value):\n",
            "        # Case: feeding symbolic tensor.\n",
            "        feed_symbols.append(tensor)\n",
            "        symbol_vals.append(value)\n",
            "      else:\n",
            "        # Case: feeding Numpy array.\n",
            "        feed_arrays.append(tensor)\n",
            "        # We need to do array conversion and type casting at this level, since\n",
            "        # `callable_fn` only supports exact matches.\n",
            "        tensor_type = dtypes_module.as_dtype(tensor.dtype)\n",
            "        array_vals.append(np.asarray(value,\n",
            "                                     dtype=tensor_type.as_numpy_dtype))\n",
            "\n",
            "    if self.feed_dict:\n",
            "      for key in sorted(self.feed_dict.keys()):\n",
            "        array_vals.append(\n",
            "            np.asarray(self.feed_dict[key], dtype=key.dtype.base_dtype.name))\n",
            "\n",
            "    # Refresh callable if anything has changed.\n",
            "    if (self._callable_fn is None or feed_arrays != self._feed_arrays or\n",
            "        symbol_vals != self._symbol_vals or\n",
            "        feed_symbols != self._feed_symbols or self.fetches != self._fetches or\n",
            "        session != self._session):\n",
            "      self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n",
            "\n",
            "    fetched = self._callable_fn(*array_vals,\n",
            "                                run_metadata=self.run_metadata)\n",
            "    self._call_fetch_callbacks(fetched[-len(self._fetches):])\n",
            "    output_structure = nest.pack_sequence_as(\n",
            "        self._outputs_structure,\n",
            "        fetched[:len(self.outputs)],\n",
            "        expand_composites=True)\n",
            "    # We need to evaluate any composite tensor objects that have been\n",
            "    # reconstructed in 'pack_sequence_as', since otherwise they'll be output as\n",
            "    # actual CompositeTensor objects instead of the value(s) contained in the\n",
            "    # CompositeTensors. E.g., if output_structure contains a SparseTensor, then\n",
            "    # this ensures that we return its value as a SparseTensorValue rather than\n",
            "    # a SparseTensor.\n",
            "    return nest.map_structure(self._eval_if_composite, output_structure)\n",
            "\n",
            "\n",
            "class EagerExecutionFunction(object):\n",
            "  \"\"\"Helper class for constructing a TF graph function from the Keras graph.\n",
            "\n",
            "  Arguments:\n",
            "    inputs: Feed placeholders to the computation graph.\n",
            "    outputs: Output tensors to fetch.\n",
            "    updates: Additional update ops to be run at function call.\n",
            "    name: A name to help users identify what this function does.\n",
            "    session_kwargs: Unsupported.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, inputs, outputs, updates=None, name=None):\n",
            "    self.name = name\n",
            "    self._outputs_structure = outputs\n",
            "    inputs = nest.flatten(inputs)\n",
            "    outputs = nest.flatten(outputs, expand_composites=True)\n",
            "\n",
            "    updates = updates or []\n",
            "    if not isinstance(updates, (list, tuple)):\n",
            "      raise TypeError('`updates` in a Keras backend function '\n",
            "                      'should be a list or tuple.')\n",
            "\n",
            "    if updates and not outputs:\n",
            "      # Edge case; never happens in practice\n",
            "      raise ValueError('Cannot create a Keras backend function with updates'\n",
            "                       ' but no outputs during eager execution.')\n",
            "\n",
            "    graphs = {i.graph for i in nest.flatten([inputs, outputs, updates])\n",
            "              if hasattr(i, 'graph')}\n",
            "    if len(graphs) > 1:\n",
            "      raise ValueError('Cannot create an execution function which is comprised '\n",
            "                       'of elements from multiple graphs.')\n",
            "\n",
            "    source_graph = graphs.pop()\n",
            "    global_graph = get_graph()\n",
            "\n",
            "    updates_ops = []\n",
            "    legacy_update_ops = []\n",
            "    for update in updates:\n",
            "      # For legacy reasons it is allowed to pass an update as a tuple\n",
            "      # `(variable, new_value)` (this maps to an assign op). Otherwise it\n",
            "      # is assumed to already be an op -- we cannot control its execution\n",
            "      # order.\n",
            "      if isinstance(update, tuple):\n",
            "        legacy_update_ops.append(update)\n",
            "      else:\n",
            "        if hasattr(update, 'op'):\n",
            "          update = update.op\n",
            "        if update is not None:\n",
            "          # `update.op` may have been None in certain cases.\n",
            "          updates_ops.append(update)\n",
            "\n",
            "    self._freezable_vars_to_feed = []\n",
            "    self._freezable_vars_values = []\n",
            "    freezable_vars_from_keras_graph = _FREEZABLE_VARS.get(global_graph, {})\n",
            "    with _scratch_graph() as exec_graph:\n",
            "      global_graph = get_graph()\n",
            "      if source_graph not in (exec_graph, global_graph):\n",
            "        raise ValueError('Unknown graph. Aborting.')\n",
            "\n",
            "      if source_graph is global_graph and exec_graph is not global_graph:\n",
            "        init_tensors = (\n",
            "            outputs + updates_ops + [p for [p, _] in legacy_update_ops] +\n",
            "            [p_new for [_, p_new] in legacy_update_ops\n",
            "             if isinstance(p_new, ops.Tensor)])\n",
            "        lifted_map = lift_to_graph.lift_to_graph(\n",
            "            init_tensors=init_tensors, graph=exec_graph, sources=inputs,\n",
            "            add_sources=True, handle_captures=True, base_graph=source_graph)\n",
            "\n",
            "        inputs = [lifted_map[i] for i in inputs]\n",
            "        outputs = [lifted_map[i] for i in outputs]\n",
            "        updates_ops = [lifted_map[i] for i in updates_ops]\n",
            "        legacy_update_ops = [(lifted_map[p], lifted_map.get(p_new, p_new))\n",
            "                             for p, p_new in legacy_update_ops]\n",
            "\n",
            "        # Keep track of the value to feed to any \"freezable variables\"\n",
            "        # created in this graph.\n",
            "        for old_op, new_op in lifted_map.items():\n",
            "          if old_op in freezable_vars_from_keras_graph:\n",
            "            frozen_var = old_op\n",
            "            if frozen_var._initial_value != frozen_var._current_value:\n",
            "              # We only feed a frozen_variable if its value has changed;\n",
            "              # otherwise it can rely on the default value of the\n",
            "              # underlying placeholder_with_default.\n",
            "              self._freezable_vars_to_feed.append(new_op)\n",
            "              self._freezable_vars_values.append(frozen_var._current_value)\n",
            "\n",
            "    # Consolidate updates\n",
            "    with exec_graph.as_default():\n",
            "      outputs = cast_variables_to_tensor(outputs)\n",
            "      with ops.control_dependencies(outputs):\n",
            "        for p, p_new in legacy_update_ops:\n",
            "          updates_ops.append(state_ops.assign(p, p_new))\n",
            "\n",
            "      self.inputs, self.outputs = inputs, outputs\n",
            "      self._input_references = self.inputs + self._freezable_vars_to_feed\n",
            "      with ops.control_dependencies(updates_ops):\n",
            "        self.outputs[0] = array_ops.identity(self.outputs[0])\n",
            "\n",
            "      exec_graph.inputs = self._input_references + list(\n",
            "          exec_graph.captures.values())\n",
            "      exec_graph.outputs = self.outputs\n",
            "      graph_fn = eager_function.ConcreteFunction(exec_graph)\n",
            "\n",
            "    graph_fn._num_positional_args = len(self._input_references)\n",
            "    graph_fn._arg_keywords = []\n",
            "    self._graph_fn = graph_fn\n",
            "\n",
            "    # Handle placeholders with default\n",
            "    # (treated as required placeholder by graph functions)\n",
            "    self._placeholder_default_values = {}\n",
            "    with exec_graph.as_default():\n",
            "      for x in self.inputs:\n",
            "        if x.op.type == 'PlaceholderWithDefault':\n",
            "          self._placeholder_default_values[x] = tensor_util.constant_value(\n",
            "              x.op.inputs[0])\n",
            "\n",
            "  def __call__(self, inputs):\n",
            "    input_values = nest.flatten(inputs)\n",
            "    if self._freezable_vars_values:\n",
            "      input_values = input_values + self._freezable_vars_values\n",
            "    converted_inputs = []\n",
            "    for tensor, value in zip(self._input_references, input_values):\n",
            "      if value is None:\n",
            "        # Assume `value` is a placeholder with default\n",
            "        value = self._placeholder_default_values.get(tensor, None)\n",
            "        if value is None:\n",
            "          raise ValueError(\n",
            "              'You must feed a value for placeholder %s' % (tensor,))\n",
            "      if not isinstance(value, ops.Tensor):\n",
            "        value = ops.convert_to_tensor(value, dtype=tensor.dtype)\n",
            "      if value.dtype != tensor.dtype:\n",
            "        # Temporary workaround due to `convert_to_tensor` not casting floats.\n",
            "        # See b/119637405\n",
            "        value = math_ops.cast(value, tensor.dtype)\n",
            "      converted_inputs.append(value)\n",
            "    outputs = self._graph_fn(*converted_inputs)\n",
            "    return nest.pack_sequence_as(\n",
            "        self._outputs_structure, [x.numpy() for x in outputs],\n",
            "        expand_composites=True)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.function')\n",
            "def function(inputs, outputs, updates=None, name=None, **kwargs):\n",
            "  \"\"\"Instantiates a Keras function.\n",
            "\n",
            "  Arguments:\n",
            "      inputs: List of placeholder tensors.\n",
            "      outputs: List of output tensors.\n",
            "      updates: List of update ops.\n",
            "      name: String, name of function.\n",
            "      **kwargs: Passed to `tf.Session.run`.\n",
            "\n",
            "  Returns:\n",
            "      Output values as Numpy arrays.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if invalid kwargs are passed in or if in eager execution.\n",
            "  \"\"\"\n",
            "  if ops.executing_eagerly_outside_functions():\n",
            "    if kwargs:\n",
            "      raise ValueError('Session keyword arguments are not support during '\n",
            "                       'eager execution. You passed: %s' % (kwargs,))\n",
            "    return EagerExecutionFunction(inputs, outputs, updates=updates, name=name)\n",
            "\n",
            "  if kwargs:\n",
            "    for key in kwargs:\n",
            "      if (key not in tf_inspect.getfullargspec(session_module.Session.run)[0]\n",
            "          and key not in ['inputs', 'outputs', 'updates', 'name']):\n",
            "        msg = ('Invalid argument \"%s\" passed to K.function with TensorFlow '\n",
            "               'backend') % key\n",
            "        raise ValueError(msg)\n",
            "  return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.gradients')\n",
            "def gradients(loss, variables):\n",
            "  \"\"\"Returns the gradients of `loss` w.r.t. `variables`.\n",
            "\n",
            "  Arguments:\n",
            "      loss: Scalar tensor to minimize.\n",
            "      variables: List of variables.\n",
            "\n",
            "  Returns:\n",
            "      A gradients tensor.\n",
            "  \"\"\"\n",
            "  return gradients_module.gradients(\n",
            "      loss, variables, colocate_gradients_with_ops=True)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.stop_gradient')\n",
            "def stop_gradient(variables):\n",
            "  \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n",
            "\n",
            "  Arguments:\n",
            "      variables: Tensor or list of tensors to consider constant with respect\n",
            "        to any other variable.\n",
            "\n",
            "\n",
            "  Returns:\n",
            "      A single tensor or a list of tensors (depending on the passed argument)\n",
            "      that has no gradient with respect to any other variable.\n",
            "  \"\"\"\n",
            "  if isinstance(variables, (list, tuple)):\n",
            "    return map(array_ops.stop_gradient, variables)\n",
            "  return array_ops.stop_gradient(variables)\n",
            "\n",
            "\n",
            "# CONTROL FLOW\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.rnn')\n",
            "def rnn(step_function,\n",
            "        inputs,\n",
            "        initial_states,\n",
            "        go_backwards=False,\n",
            "        mask=None,\n",
            "        constants=None,\n",
            "        unroll=False,\n",
            "        input_length=None,\n",
            "        time_major=False,\n",
            "        zero_output_for_mask=False):\n",
            "  \"\"\"Iterates over the time dimension of a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      step_function: RNN step function.\n",
            "          Args;\n",
            "              input; Tensor with shape `(samples, ...)` (no time dimension),\n",
            "                  representing input for the batch of samples at a certain\n",
            "                  time step.\n",
            "              states; List of tensors.\n",
            "          Returns;\n",
            "              output; Tensor with shape `(samples, output_dim)`\n",
            "                  (no time dimension).\n",
            "              new_states; List of tensors, same length and shapes\n",
            "                  as 'states'. The first state in the list must be the\n",
            "                  output tensor at the previous timestep.\n",
            "      inputs: Tensor of temporal data of shape `(samples, time, ...)`\n",
            "          (at least 3D), or nested tensors, and each of which has shape\n",
            "          `(samples, time, ...)`.\n",
            "      initial_states: Tensor with shape `(samples, state_size)`\n",
            "          (no time dimension), containing the initial values for the states used\n",
            "          in the step function. In the case that state_size is in a nested\n",
            "          shape, the shape of initial_states will also follow the nested\n",
            "          structure.\n",
            "      go_backwards: Boolean. If True, do the iteration over the time\n",
            "          dimension in reverse order and return the reversed sequence.\n",
            "      mask: Binary tensor with shape `(samples, time, 1)`,\n",
            "          with a zero for every element that is masked.\n",
            "      constants: List of constant values passed at each step.\n",
            "      unroll: Whether to unroll the RNN or to use a symbolic `while_loop`.\n",
            "      input_length: If specified, assume time dimension is of this length.\n",
            "      time_major: Boolean. If true, the inputs and outputs will be in shape\n",
            "          `(timesteps, batch, ...)`, whereas in the False case, it will be\n",
            "          `(batch, timesteps, ...)`. Using `time_major = True` is a bit more\n",
            "          efficient because it avoids transposes at the beginning and end of the\n",
            "          RNN calculation. However, most TensorFlow data is batch-major, so by\n",
            "          default this function accepts input and emits output in batch-major\n",
            "          form.\n",
            "      zero_output_for_mask: Boolean. If True, the output for masked timestep\n",
            "          will be zeros, whereas in the False case, output from previous\n",
            "          timestep is returned.\n",
            "  Returns:\n",
            "      A tuple, `(last_output, outputs, new_states)`.\n",
            "          last_output: the latest output of the rnn, of shape `(samples, ...)`\n",
            "          outputs: tensor with shape `(samples, time, ...)` where each\n",
            "              entry `outputs[s, t]` is the output of the step function\n",
            "              at time `t` for sample `s`.\n",
            "          new_states: list of tensors, latest states returned by\n",
            "              the step function, of shape `(samples, ...)`.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if input dimension is less than 3.\n",
            "      ValueError: if `unroll` is `True` but input timestep is not a fixed\n",
            "      number.\n",
            "      ValueError: if `mask` is provided (not `None`) but states is not provided\n",
            "          (`len(states)` == 0).\n",
            "  \"\"\"\n",
            "\n",
            "  def swap_batch_timestep(input_t):\n",
            "    # Swap the batch and timestep dim for the incoming tensor.\n",
            "    axes = list(range(len(input_t.shape)))\n",
            "    axes[0], axes[1] = 1, 0\n",
            "    return array_ops.transpose(input_t, axes)\n",
            "\n",
            "  if not time_major:\n",
            "    inputs = nest.map_structure(swap_batch_timestep, inputs)\n",
            "\n",
            "  flatted_inputs = nest.flatten(inputs)\n",
            "  time_steps = flatted_inputs[0].shape[0]\n",
            "  batch = flatted_inputs[0].shape[1]\n",
            "  time_steps_t = array_ops.shape(flatted_inputs[0])[0]\n",
            "\n",
            "  for input_ in flatted_inputs:\n",
            "    input_.shape.with_rank_at_least(3)\n",
            "\n",
            "  if mask is not None:\n",
            "    if mask.dtype != dtypes_module.bool:\n",
            "      mask = math_ops.cast(mask, dtypes_module.bool)\n",
            "    if len(mask.shape) == 2:\n",
            "      mask = expand_dims(mask)\n",
            "    if not time_major:\n",
            "      mask = swap_batch_timestep(mask)\n",
            "\n",
            "  if constants is None:\n",
            "    constants = []\n",
            "\n",
            "  # tf.where needs its condition tensor to be the same shape as its two\n",
            "  # result tensors, but in our case the condition (mask) tensor is\n",
            "  # (nsamples, 1), and inputs are (nsamples, ndimensions) or even more.\n",
            "  # So we need to broadcast the mask to match the shape of inputs.\n",
            "  # That's what the tile call does, it just repeats the mask along its\n",
            "  # second dimension n times.\n",
            "  def _expand_mask(mask_t, input_t, fixed_dim=1):\n",
            "    assert not nest.is_sequence(mask_t)\n",
            "    assert not nest.is_sequence(input_t)\n",
            "    rank_diff = len(input_t.shape) - len(mask_t.shape)\n",
            "    for _ in range(rank_diff):\n",
            "      mask_t = array_ops.expand_dims(mask_t, -1)\n",
            "    multiples = [1] * fixed_dim + input_t.shape.as_list()[fixed_dim:]\n",
            "    return array_ops.tile(mask_t, multiples)\n",
            "\n",
            "  if unroll:\n",
            "    if not time_steps:\n",
            "      raise ValueError('Unrolling requires a fixed number of timesteps.')\n",
            "    states = tuple(initial_states)\n",
            "    successive_states = []\n",
            "    successive_outputs = []\n",
            "\n",
            "    # Process the input tensors. The input tensor need to be split on the\n",
            "    # time_step dim, and reverse if go_backwards is True. In the case of nested\n",
            "    # input, the input is flattened and then transformed individually.\n",
            "    # The result of this will be a tuple of lists, each of the item in tuple is\n",
            "    # list of the tensor with shape (batch, feature)\n",
            "    def _process_single_input_t(input_t):\n",
            "      input_t = array_ops.unstack(input_t)  # unstack for time_step dim\n",
            "      if go_backwards:\n",
            "        input_t.reverse()\n",
            "      return input_t\n",
            "\n",
            "    if nest.is_sequence(inputs):\n",
            "      processed_input = nest.map_structure(_process_single_input_t, inputs)\n",
            "    else:\n",
            "      processed_input = (_process_single_input_t(inputs),)\n",
            "\n",
            "    def _get_input_tensor(time):\n",
            "      inp = [t_[time] for t_ in processed_input]\n",
            "      return nest.pack_sequence_as(inputs, inp)\n",
            "\n",
            "    if mask is not None:\n",
            "      mask_list = array_ops.unstack(mask)\n",
            "      if go_backwards:\n",
            "        mask_list.reverse()\n",
            "\n",
            "      for i in range(time_steps):\n",
            "        inp = _get_input_tensor(i)\n",
            "        mask_t = mask_list[i]\n",
            "        output, new_states = step_function(inp,\n",
            "                                           tuple(states) + tuple(constants))\n",
            "        tiled_mask_t = _expand_mask(mask_t, output)\n",
            "\n",
            "        if not successive_outputs:\n",
            "          prev_output = zeros_like(output)\n",
            "        else:\n",
            "          prev_output = successive_outputs[-1]\n",
            "\n",
            "        output = array_ops.where(tiled_mask_t, output, prev_output)\n",
            "\n",
            "        return_states = []\n",
            "        for state, new_state in zip(states, new_states):\n",
            "          # (see earlier comment for tile explanation)\n",
            "          tiled_mask_t = _expand_mask(mask_t, new_state)\n",
            "          return_states.append(array_ops.where(tiled_mask_t, new_state, state))\n",
            "        states = return_states\n",
            "        successive_outputs.append(output)\n",
            "        successive_states.append(states)\n",
            "      last_output = successive_outputs[-1]\n",
            "      new_states = successive_states[-1]\n",
            "      outputs = array_ops.stack(successive_outputs)\n",
            "\n",
            "      if zero_output_for_mask:\n",
            "        last_output = array_ops.where(\n",
            "            _expand_mask(mask_list[-1], last_output),\n",
            "            last_output,\n",
            "            zeros_like(last_output))\n",
            "        outputs = array_ops.where(\n",
            "            _expand_mask(mask, outputs, fixed_dim=2),\n",
            "            outputs,\n",
            "            zeros_like(outputs))\n",
            "\n",
            "    else:\n",
            "      for i in range(time_steps):\n",
            "        inp = _get_input_tensor(i)\n",
            "        output, states = step_function(inp, tuple(states) + tuple(constants))\n",
            "        successive_outputs.append(output)\n",
            "        successive_states.append(states)\n",
            "      last_output = successive_outputs[-1]\n",
            "      new_states = successive_states[-1]\n",
            "      outputs = array_ops.stack(successive_outputs)\n",
            "\n",
            "  else:\n",
            "    states = tuple(initial_states)\n",
            "\n",
            "    # Create input tensor array, if the inputs is nested tensors, then it will\n",
            "    # be flattened first, and tensor array will be created one per flattened\n",
            "    # tensor.\n",
            "    input_ta = tuple(\n",
            "        tensor_array_ops.TensorArray(\n",
            "            dtype=inp.dtype,\n",
            "            size=time_steps_t,\n",
            "            tensor_array_name='input_ta_%s' % i)\n",
            "        for i, inp in enumerate(flatted_inputs))\n",
            "    input_ta = tuple(\n",
            "        ta.unstack(input_) if not go_backwards else ta\n",
            "        .unstack(reverse(input_, 0))\n",
            "        for ta, input_ in zip(input_ta, flatted_inputs))\n",
            "\n",
            "    # Get the time(0) input and compute the output for that, the output will be\n",
            "    # used to determine the dtype of output tensor array. Don't read from\n",
            "    # input_ta due to TensorArray clear_after_read default to True.\n",
            "    input_time_zero = nest.pack_sequence_as(inputs,\n",
            "                                            [inp[0] for inp in flatted_inputs])\n",
            "    # output_time_zero is used to determine the cell output shape and its dtype.\n",
            "    # the value is discarded.\n",
            "    output_time_zero, _ = step_function(\n",
            "        input_time_zero, tuple(initial_states) + tuple(constants))\n",
            "    output_ta = tuple(\n",
            "        tensor_array_ops.TensorArray(\n",
            "            dtype=out.dtype,\n",
            "            size=time_steps_t,\n",
            "            tensor_array_name='output_ta_%s' % i)\n",
            "        for i, out in enumerate(nest.flatten(output_time_zero)))\n",
            "\n",
            "    time = constant_op.constant(0, dtype='int32', name='time')\n",
            "\n",
            "    while_loop_kwargs = {\n",
            "        'cond': lambda time, *_: time < time_steps_t,\n",
            "        'maximum_iterations': input_length,\n",
            "        'parallel_iterations': 32,\n",
            "        'swap_memory': True,\n",
            "    }\n",
            "\n",
            "    if mask is not None:\n",
            "      if not states:\n",
            "        raise ValueError('No initial states provided! '\n",
            "                         'When using masking in an RNN, you should '\n",
            "                         'provide initial states '\n",
            "                         '(and your step function should return '\n",
            "                         'as its first state at time `t` '\n",
            "                         'the output at time `t-1`).')\n",
            "      if go_backwards:\n",
            "        mask = reverse(mask, 0)\n",
            "\n",
            "      mask_ta = tensor_array_ops.TensorArray(\n",
            "          dtype=dtypes_module.bool,\n",
            "          size=time_steps_t,\n",
            "          tensor_array_name='mask_ta')\n",
            "      mask_ta = mask_ta.unstack(mask)\n",
            "\n",
            "      # Mask for the T output will be base on the output of T - 1. In the case\n",
            "      # T = 0, a zero filled tensor will be used.\n",
            "      flat_zero_output = tuple(array_ops.zeros_like(o)\n",
            "                               for o in nest.flatten(output_time_zero))\n",
            "      def _step(time, output_ta_t, prev_output, *states):\n",
            "        \"\"\"RNN step function.\n",
            "\n",
            "        Arguments:\n",
            "            time: Current timestep value.\n",
            "            output_ta_t: TensorArray.\n",
            "            prev_output: tuple of outputs from time - 1.\n",
            "            *states: List of states.\n",
            "\n",
            "        Returns:\n",
            "            Tuple: `(time + 1, output_ta_t, output) + tuple(new_states)`\n",
            "        \"\"\"\n",
            "        current_input = tuple(ta.read(time) for ta in input_ta)\n",
            "        # maybe set shape.\n",
            "        current_input = nest.pack_sequence_as(inputs, current_input)\n",
            "        mask_t = mask_ta.read(time)\n",
            "        output, new_states = step_function(current_input,\n",
            "                                           tuple(states) + tuple(constants))\n",
            "        # mask output\n",
            "        flat_output = nest.flatten(output)\n",
            "        flat_mask_output = (flat_zero_output if zero_output_for_mask\n",
            "                            else nest.flatten(prev_output))\n",
            "        tiled_mask_t = tuple(_expand_mask(mask_t, o) for o in flat_output)\n",
            "        flat_new_output = tuple(\n",
            "            array_ops.where(m, o, zo) for m, o, zo in zip(\n",
            "                tiled_mask_t, flat_output, flat_mask_output))\n",
            "\n",
            "        # mask states\n",
            "        flat_state = nest.flatten(states)\n",
            "        flat_new_state = nest.flatten(new_states)\n",
            "        for state, new_state in zip(flat_state, flat_new_state):\n",
            "          if isinstance(new_state, ops.Tensor):\n",
            "            new_state.set_shape(state.shape)\n",
            "        tiled_mask_t = tuple(_expand_mask(mask_t, s) for s in flat_state)\n",
            "        flat_final_state = tuple(\n",
            "            array_ops.where(m, s, ps)\n",
            "            for m, s, ps in zip(tiled_mask_t, flat_new_state, flat_state))\n",
            "        new_states = nest.pack_sequence_as(new_states, flat_final_state)\n",
            "\n",
            "        output_ta_t = tuple(\n",
            "            ta.write(time, out)\n",
            "            for ta, out in zip(output_ta_t, flat_new_output))\n",
            "        return (time + 1, output_ta_t,\n",
            "                tuple(flat_new_output)) + tuple(new_states)\n",
            "\n",
            "      final_outputs = control_flow_ops.while_loop(\n",
            "          body=_step,\n",
            "          loop_vars=(time, output_ta, flat_zero_output) + states,\n",
            "          **while_loop_kwargs)\n",
            "      # Skip final_outputs[2] which is the output for final timestep.\n",
            "      new_states = final_outputs[3:]\n",
            "    else:\n",
            "      def _step(time, output_ta_t, *states):\n",
            "        \"\"\"RNN step function.\n",
            "\n",
            "        Arguments:\n",
            "            time: Current timestep value.\n",
            "            output_ta_t: TensorArray.\n",
            "            *states: List of states.\n",
            "\n",
            "        Returns:\n",
            "            Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n",
            "        \"\"\"\n",
            "        current_input = tuple(ta.read(time) for ta in input_ta)\n",
            "        current_input = nest.pack_sequence_as(inputs, current_input)\n",
            "        output, new_states = step_function(current_input,\n",
            "                                           tuple(states) + tuple(constants))\n",
            "        flat_state = nest.flatten(states)\n",
            "        flat_new_state = nest.flatten(new_states)\n",
            "        for state, new_state in zip(flat_state, flat_new_state):\n",
            "          if isinstance(new_state, ops.Tensor):\n",
            "            new_state.set_shape(state.shape)\n",
            "\n",
            "        flat_output = nest.flatten(output)\n",
            "        output_ta_t = tuple(\n",
            "            ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))\n",
            "        new_states = nest.pack_sequence_as(initial_states, flat_new_state)\n",
            "        return (time + 1, output_ta_t) + tuple(new_states)\n",
            "\n",
            "      final_outputs = control_flow_ops.while_loop(\n",
            "          body=_step,\n",
            "          loop_vars=(time, output_ta) + states,\n",
            "          **while_loop_kwargs)\n",
            "      new_states = final_outputs[2:]\n",
            "\n",
            "    output_ta = final_outputs[1]\n",
            "\n",
            "    outputs = tuple(o.stack() for o in output_ta)\n",
            "    last_output = tuple(o[-1] for o in outputs)\n",
            "\n",
            "    outputs = nest.pack_sequence_as(output_time_zero, outputs)\n",
            "    last_output = nest.pack_sequence_as(output_time_zero, last_output)\n",
            "\n",
            "  # static shape inference\n",
            "  def set_shape(output_):\n",
            "    if isinstance(output_, ops.Tensor):\n",
            "      shape = output_.shape.as_list()\n",
            "      shape[0] = time_steps\n",
            "      shape[1] = batch\n",
            "      output_.set_shape(shape)\n",
            "    return output_\n",
            "\n",
            "  outputs = nest.map_structure(set_shape, outputs)\n",
            "\n",
            "  if not time_major:\n",
            "    outputs = nest.map_structure(swap_batch_timestep, outputs)\n",
            "\n",
            "  return last_output, outputs, new_states\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.switch')\n",
            "def switch(condition, then_expression, else_expression):\n",
            "  \"\"\"Switches between two operations depending on a scalar value.\n",
            "\n",
            "  Note that both `then_expression` and `else_expression`\n",
            "  should be symbolic tensors of the *same shape*.\n",
            "\n",
            "  Arguments:\n",
            "      condition: tensor (`int` or `bool`).\n",
            "      then_expression: either a tensor, or a callable that returns a tensor.\n",
            "      else_expression: either a tensor, or a callable that returns a tensor.\n",
            "\n",
            "  Returns:\n",
            "      The selected tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If rank of `condition` is greater than rank of expressions.\n",
            "  \"\"\"\n",
            "  if condition.dtype != dtypes_module.bool:\n",
            "    condition = math_ops.cast(condition, 'bool')\n",
            "  cond_ndim = ndim(condition)\n",
            "  if not cond_ndim:\n",
            "    if not callable(then_expression):\n",
            "\n",
            "      def then_expression_fn():\n",
            "        return then_expression\n",
            "    else:\n",
            "      then_expression_fn = then_expression\n",
            "    if not callable(else_expression):\n",
            "\n",
            "      def else_expression_fn():\n",
            "        return else_expression\n",
            "    else:\n",
            "      else_expression_fn = else_expression\n",
            "    x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)\n",
            "  else:\n",
            "    # tf.where needs its condition tensor\n",
            "    # to be the same shape as its two\n",
            "    # result tensors\n",
            "    if callable(then_expression):\n",
            "      then_expression = then_expression()\n",
            "    if callable(else_expression):\n",
            "      else_expression = else_expression()\n",
            "    expr_ndim = ndim(then_expression)\n",
            "    if cond_ndim > expr_ndim:\n",
            "      raise ValueError('Rank of `condition` should be less than or'\n",
            "                       ' equal to rank of `then_expression` and '\n",
            "                       '`else_expression`. ndim(condition)=' + str(cond_ndim) +\n",
            "                       ', ndim(then_expression)'\n",
            "                       '=' + str(expr_ndim))\n",
            "    if cond_ndim > 1:\n",
            "      ndim_diff = expr_ndim - cond_ndim\n",
            "      cond_shape = array_ops.concat(\n",
            "          [array_ops.shape(condition), [1] * ndim_diff], axis=0)\n",
            "      condition = array_ops.reshape(condition, cond_shape)\n",
            "      expr_shape = array_ops.shape(then_expression)\n",
            "      shape_diff = expr_shape - cond_shape\n",
            "      tile_shape = array_ops.where(shape_diff > 0, expr_shape,\n",
            "                                   array_ops.ones_like(expr_shape))\n",
            "      condition = array_ops.tile(condition, tile_shape)\n",
            "    x = array_ops.where(condition, then_expression, else_expression)\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.in_train_phase')\n",
            "def in_train_phase(x, alt, training=None):\n",
            "  \"\"\"Selects `x` in train phase, and `alt` otherwise.\n",
            "\n",
            "  Note that `alt` should have the *same shape* as `x`.\n",
            "\n",
            "  Arguments:\n",
            "      x: What to return in train phase\n",
            "          (tensor or callable that returns a tensor).\n",
            "      alt: What to return otherwise\n",
            "          (tensor or callable that returns a tensor).\n",
            "      training: Optional scalar tensor\n",
            "          (or Python boolean, or Python integer)\n",
            "          specifying the learning phase.\n",
            "\n",
            "  Returns:\n",
            "      Either `x` or `alt` based on the `training` flag.\n",
            "      the `training` flag defaults to `K.learning_phase()`.\n",
            "  \"\"\"\n",
            "  if training is None:\n",
            "    training = learning_phase()\n",
            "\n",
            "  if training == 1 or training is True:\n",
            "    if callable(x):\n",
            "      return x()\n",
            "    else:\n",
            "      return x\n",
            "\n",
            "  elif training == 0 or training is False:\n",
            "    if callable(alt):\n",
            "      return alt()\n",
            "    else:\n",
            "      return alt\n",
            "\n",
            "  # else: assume learning phase is a placeholder tensor.\n",
            "  x = switch(training, x, alt)\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.in_test_phase')\n",
            "def in_test_phase(x, alt, training=None):\n",
            "  \"\"\"Selects `x` in test phase, and `alt` otherwise.\n",
            "\n",
            "  Note that `alt` should have the *same shape* as `x`.\n",
            "\n",
            "  Arguments:\n",
            "      x: What to return in test phase\n",
            "          (tensor or callable that returns a tensor).\n",
            "      alt: What to return otherwise\n",
            "          (tensor or callable that returns a tensor).\n",
            "      training: Optional scalar tensor\n",
            "          (or Python boolean, or Python integer)\n",
            "          specifying the learning phase.\n",
            "\n",
            "  Returns:\n",
            "      Either `x` or `alt` based on `K.learning_phase`.\n",
            "  \"\"\"\n",
            "  return in_train_phase(alt, x, training=training)\n",
            "\n",
            "\n",
            "# NN OPERATIONS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.relu')\n",
            "def relu(x, alpha=0., max_value=None, threshold=0):\n",
            "  \"\"\"Rectified linear unit.\n",
            "\n",
            "  With default values, it returns element-wise `max(x, 0)`.\n",
            "\n",
            "  Otherwise, it follows:\n",
            "  `f(x) = max_value` for `x >= max_value`,\n",
            "  `f(x) = x` for `threshold <= x < max_value`,\n",
            "  `f(x) = alpha * (x - threshold)` otherwise.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      alpha: A scalar, slope of negative section (default=`0.`).\n",
            "      max_value: float. Saturation threshold.\n",
            "      threshold: float. Threshold value for thresholded activation.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "\n",
            "  if alpha != 0.:\n",
            "    if max_value is None and threshold == 0:\n",
            "      return nn.leaky_relu(x, alpha=alpha)\n",
            "\n",
            "    if threshold != 0:\n",
            "      negative_part = nn.relu(-x + threshold)\n",
            "    else:\n",
            "      negative_part = nn.relu(-x)\n",
            "\n",
            "  clip_max = max_value is not None\n",
            "\n",
            "  if threshold != 0:\n",
            "    # computes x for x > threshold else 0\n",
            "    x = x * math_ops.cast(math_ops.greater(x, threshold), floatx())\n",
            "  elif max_value == 6:\n",
            "    # if no threshold, then can use nn.relu6 native TF op for performance\n",
            "    x = nn.relu6(x)\n",
            "    clip_max = False\n",
            "  else:\n",
            "    x = nn.relu(x)\n",
            "\n",
            "  if clip_max:\n",
            "    max_value = _constant_to_tensor(max_value, x.dtype.base_dtype)\n",
            "    zero = _constant_to_tensor(0., x.dtype.base_dtype)\n",
            "    x = clip_ops.clip_by_value(x, zero, max_value)\n",
            "\n",
            "  if alpha != 0.:\n",
            "    alpha = _to_tensor(alpha, x.dtype.base_dtype)\n",
            "    x -= alpha * negative_part\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.elu')\n",
            "def elu(x, alpha=1.):\n",
            "  \"\"\"Exponential linear unit.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable to compute the activation function for.\n",
            "      alpha: A scalar, slope of negative section.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  res = nn.elu(x)\n",
            "  if alpha == 1:\n",
            "    return res\n",
            "  else:\n",
            "    return array_ops.where(x > 0, res, alpha * res)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.softmax')\n",
            "def softmax(x, axis=-1):\n",
            "  \"\"\"Softmax of a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "      axis: The dimension softmax would be performed on.\n",
            "          The default is -1 which indicates the last dimension.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.softmax(x, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.softplus')\n",
            "def softplus(x):\n",
            "  \"\"\"Softplus of a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.softplus(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.softsign')\n",
            "def softsign(x):\n",
            "  \"\"\"Softsign of a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.softsign(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.categorical_crossentropy')\n",
            "def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n",
            "  \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n",
            "\n",
            "  Arguments:\n",
            "      target: A tensor of the same shape as `output`.\n",
            "      output: A tensor resulting from a softmax\n",
            "          (unless `from_logits` is True, in which\n",
            "          case `output` is expected to be the logits).\n",
            "      from_logits: Boolean, whether `output` is the\n",
            "          result of a softmax, or is a tensor of logits.\n",
            "      axis: Int specifying the channels axis. `axis=-1` corresponds to data\n",
            "          format `channels_last', and `axis=1` corresponds to data format\n",
            "          `channels_first`.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `axis` is neither -1 nor one of the axes of `output`.\n",
            "  \"\"\"\n",
            "  if not from_logits:\n",
            "    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n",
            "        output.op.type != 'Softmax'):\n",
            "      axis = axis % len(output.shape)\n",
            "      # scale preds so that the class probas of each sample sum to 1\n",
            "      output = output / math_ops.reduce_sum(output, axis, True)\n",
            "      # Compute cross entropy from probabilities.\n",
            "      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
            "      output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
            "      return -math_ops.reduce_sum(target * math_ops.log(output), axis)\n",
            "    else:\n",
            "      # When softmax activation function is used for output operation, we\n",
            "      # use logits from the softmax function directly to compute loss in order\n",
            "      # to prevent collapsing zero when training.\n",
            "      # See b/117284466\n",
            "      assert len(output.op.inputs) == 1\n",
            "      output = output.op.inputs[0]\n",
            "  return nn.softmax_cross_entropy_with_logits_v2(labels=target, logits=output)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sparse_categorical_crossentropy')\n",
            "def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n",
            "  \"\"\"Categorical crossentropy with integer targets.\n",
            "\n",
            "  Arguments:\n",
            "      target: An integer tensor.\n",
            "      output: A tensor resulting from a softmax\n",
            "          (unless `from_logits` is True, in which\n",
            "          case `output` is expected to be the logits).\n",
            "      from_logits: Boolean, whether `output` is the\n",
            "          result of a softmax, or is a tensor of logits.\n",
            "      axis: Int specifying the channels axis. `axis=-1` corresponds to data\n",
            "          format `channels_last', and `axis=1` corresponds to data format\n",
            "          `channels_first`.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `axis` is neither -1 nor one of the axes of `output`.\n",
            "  \"\"\"\n",
            "  if not from_logits:\n",
            "    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n",
            "        output.op.type != 'Softmax'):\n",
            "      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
            "      output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
            "      output = math_ops.log(output)\n",
            "    else:\n",
            "      # When softmax activation function is used for output operation, we\n",
            "      # use logits from the softmax function directly to compute loss in order\n",
            "      # to prevent collapsing zero when training.\n",
            "      # See b/117284466\n",
            "      assert len(output.op.inputs) == 1\n",
            "      output = output.op.inputs[0]\n",
            "\n",
            "  rank = len(output.shape)\n",
            "  axis = axis % rank\n",
            "  if axis != rank - 1:\n",
            "    permutation = list(range(axis)) + list(range(axis + 1, rank)) + [axis]\n",
            "    output = array_ops.transpose(output, perm=permutation)\n",
            "\n",
            "  output_shape = output.shape\n",
            "  targets = cast(flatten(target), 'int64')\n",
            "  logits = array_ops.reshape(output, [-1, int(output_shape[-1])])\n",
            "  res = nn.sparse_softmax_cross_entropy_with_logits(\n",
            "      labels=targets, logits=logits)\n",
            "  if len(output_shape) >= 3:\n",
            "    # If our output includes timesteps or spatial dimensions we need to reshape\n",
            "    return array_ops.reshape(res, array_ops.shape(output)[:-1])\n",
            "  else:\n",
            "    return res\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.binary_crossentropy')\n",
            "def binary_crossentropy(target, output, from_logits=False):\n",
            "  \"\"\"Binary crossentropy between an output tensor and a target tensor.\n",
            "\n",
            "  Arguments:\n",
            "      target: A tensor with the same shape as `output`.\n",
            "      output: A tensor.\n",
            "      from_logits: Whether `output` is expected to be a logits tensor.\n",
            "          By default, we consider that `output`\n",
            "          encodes a probability distribution.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if not from_logits:\n",
            "    if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n",
            "        output.op.type != 'Sigmoid'):\n",
            "      epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
            "      output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n",
            "\n",
            "      # Compute cross entropy from probabilities.\n",
            "      bce = target * math_ops.log(output + epsilon())\n",
            "      bce += (1 - target) * math_ops.log(1 - output + epsilon())\n",
            "      return -bce\n",
            "    else:\n",
            "      # When sigmoid activation function is used for output operation, we\n",
            "      # use logits from the sigmoid function directly to compute loss in order\n",
            "      # to prevent collapsing zero when training.\n",
            "      assert len(output.op.inputs) == 1\n",
            "      output = output.op.inputs[0]\n",
            "  return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.sigmoid')\n",
            "def sigmoid(x):\n",
            "  \"\"\"Element-wise sigmoid.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.sigmoid(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.hard_sigmoid')\n",
            "def hard_sigmoid(x):\n",
            "  \"\"\"Segment-wise linear approximation of sigmoid.\n",
            "\n",
            "  Faster than sigmoid.\n",
            "  Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n",
            "  In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  point_two = _constant_to_tensor(0.2, x.dtype.base_dtype)\n",
            "  point_five = _constant_to_tensor(0.5, x.dtype.base_dtype)\n",
            "  x = math_ops.mul(x, point_two)\n",
            "  x = math_ops.add(x, point_five)\n",
            "  x = clip_ops.clip_by_value(x, 0., 1.)\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.tanh')\n",
            "def tanh(x):\n",
            "  \"\"\"Element-wise tanh.\n",
            "\n",
            "  Arguments:\n",
            "      x: A tensor or variable.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.tanh(x)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.dropout')\n",
            "def dropout(x, level, noise_shape=None, seed=None):\n",
            "  \"\"\"Sets entries in `x` to zero at random, while scaling the entire tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: tensor\n",
            "      level: fraction of the entries in the tensor\n",
            "          that will be set to 0.\n",
            "      noise_shape: shape for randomly generated keep/drop flags,\n",
            "          must be broadcastable to the shape of `x`\n",
            "      seed: random seed to ensure determinism.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if seed is None:\n",
            "    seed = np.random.randint(10e6)\n",
            "  return nn.dropout_v2(x, rate=level, noise_shape=noise_shape, seed=seed)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.l2_normalize')\n",
            "def l2_normalize(x, axis=None):\n",
            "  \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      axis: axis along which to perform normalization.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  return nn.l2_normalize(x, axis=axis)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.in_top_k')\n",
            "def in_top_k(predictions, targets, k):\n",
            "  \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n",
            "\n",
            "  Arguments:\n",
            "      predictions: A tensor of shape `(batch_size, classes)` and type `float32`.\n",
            "      targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.\n",
            "      k: An `int`, number of top elements to consider.\n",
            "\n",
            "  Returns:\n",
            "      A 1D tensor of length `batch_size` and type `bool`.\n",
            "      `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`\n",
            "      values of `predictions[i]`.\n",
            "  \"\"\"\n",
            "  return nn.in_top_k(predictions, targets, k)\n",
            "\n",
            "\n",
            "# CONVOLUTIONS\n",
            "\n",
            "\n",
            "def _preprocess_conv1d_input(x, data_format):\n",
            "  \"\"\"Transpose and cast the input before the conv1d.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  tf_data_format = 'NWC'  # to pass TF Conv2dNative operations\n",
            "  if data_format == 'channels_first':\n",
            "    if not _has_nchw_support():\n",
            "      x = array_ops.transpose(x, (0, 2, 1))  # NCW -> NWC\n",
            "    else:\n",
            "      tf_data_format = 'NCW'\n",
            "  return x, tf_data_format\n",
            "\n",
            "\n",
            "def _preprocess_conv2d_input(x, data_format, force_transpose=False):\n",
            "  \"\"\"Transpose and cast the input before the conv2d.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      force_transpose: Boolean. If True, the input will always be transposed\n",
            "          from NCHW to NHWC if `data_format` is `\"channels_first\"`.\n",
            "          If False, the transposition only occurs on CPU (GPU ops are\n",
            "          assumed to support NCHW).\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  tf_data_format = 'NHWC'\n",
            "  if data_format == 'channels_first':\n",
            "    if not _has_nchw_support() or force_transpose:\n",
            "      x = array_ops.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n",
            "    else:\n",
            "      tf_data_format = 'NCHW'\n",
            "  return x, tf_data_format\n",
            "\n",
            "\n",
            "def _preprocess_conv3d_input(x, data_format):\n",
            "  \"\"\"Transpose and cast the input before the conv3d.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  tf_data_format = 'NDHWC'\n",
            "  if data_format == 'channels_first':\n",
            "    if not _has_nchw_support():\n",
            "      x = array_ops.transpose(x, (0, 2, 3, 4, 1))\n",
            "    else:\n",
            "      tf_data_format = 'NCDHW'\n",
            "  return x, tf_data_format\n",
            "\n",
            "\n",
            "def _preprocess_padding(padding):\n",
            "  \"\"\"Convert keras' padding to TensorFlow's padding.\n",
            "\n",
            "  Arguments:\n",
            "      padding: string, one of 'same' , 'valid'\n",
            "\n",
            "  Returns:\n",
            "      a string, one of 'SAME', 'VALID'.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if invalid `padding'`\n",
            "  \"\"\"\n",
            "  if padding == 'same':\n",
            "    padding = 'SAME'\n",
            "  elif padding == 'valid':\n",
            "    padding = 'VALID'\n",
            "  else:\n",
            "    raise ValueError('Invalid padding: ' + str(padding))\n",
            "  return padding\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.conv1d')\n",
            "def conv1d(x,\n",
            "           kernel,\n",
            "           strides=1,\n",
            "           padding='valid',\n",
            "           data_format=None,\n",
            "           dilation_rate=1):\n",
            "  \"\"\"1D convolution.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      kernel: kernel tensor.\n",
            "      strides: stride integer.\n",
            "      padding: string, `\"same\"`, `\"causal\"` or `\"valid\"`.\n",
            "      data_format: string, one of \"channels_last\", \"channels_first\".\n",
            "      dilation_rate: integer dilate rate.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of 1D convolution.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  kernel_shape = kernel.shape.as_list()\n",
            "  if padding == 'causal':\n",
            "    # causal (dilated) convolution:\n",
            "    left_pad = dilation_rate * (kernel_shape[0] - 1)\n",
            "    x = temporal_padding(x, (left_pad, 0))\n",
            "    padding = 'valid'\n",
            "  padding = _preprocess_padding(padding)\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n",
            "  x = nn.convolution(\n",
            "      input=x,\n",
            "      filter=kernel,\n",
            "      dilation_rate=dilation_rate,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NWC':\n",
            "    x = array_ops.transpose(x, (0, 2, 1))  # NWC -> NCW\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.conv2d')\n",
            "def conv2d(x,\n",
            "           kernel,\n",
            "           strides=(1, 1),\n",
            "           padding='valid',\n",
            "           data_format=None,\n",
            "           dilation_rate=(1, 1)):\n",
            "  \"\"\"2D convolution.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      kernel: kernel tensor.\n",
            "      strides: strides tuple.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: tuple of 2 integers.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of 2D convolution.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  x = nn.convolution(\n",
            "      input=x,\n",
            "      filter=kernel,\n",
            "      dilation_rate=dilation_rate,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.conv2d_transpose')\n",
            "def conv2d_transpose(x,\n",
            "                     kernel,\n",
            "                     output_shape,\n",
            "                     strides=(1, 1),\n",
            "                     padding='valid',\n",
            "                     data_format=None,\n",
            "                     dilation_rate=(1, 1)):\n",
            "  \"\"\"2D deconvolution (i.e.\n",
            "\n",
            "  transposed convolution).\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      kernel: kernel tensor.\n",
            "      output_shape: 1D int tensor for the output shape.\n",
            "      strides: strides tuple.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: Tuple of 2 integers.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of transposed 2D convolution.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  # `atrous_conv2d_transpose` only supports NHWC format, even on GPU.\n",
            "  if data_format == 'channels_first' and dilation_rate != (1, 1):\n",
            "    force_transpose = True\n",
            "  else:\n",
            "    force_transpose = False\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)\n",
            "\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    output_shape = (output_shape[0], output_shape[2], output_shape[3],\n",
            "                    output_shape[1])\n",
            "  if output_shape[0] is None:\n",
            "    output_shape = (shape(x)[0],) + tuple(output_shape[1:])\n",
            "\n",
            "  if isinstance(output_shape, (tuple, list)):\n",
            "    output_shape = array_ops.stack(list(output_shape))\n",
            "\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if tf_data_format == 'NHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "\n",
            "  if dilation_rate == (1, 1):\n",
            "    x = nn.conv2d_transpose(x, kernel, output_shape, strides,\n",
            "                            padding=padding,\n",
            "                            data_format=tf_data_format)\n",
            "  else:\n",
            "    assert dilation_rate[0] == dilation_rate[1]\n",
            "    x = nn.atrous_conv2d_transpose(\n",
            "        x,\n",
            "        kernel,\n",
            "        output_shape,\n",
            "        rate=dilation_rate[0],\n",
            "        padding=padding)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
            "  return x\n",
            "\n",
            "\n",
            "def separable_conv1d(x,\n",
            "                     depthwise_kernel,\n",
            "                     pointwise_kernel,\n",
            "                     strides=1,\n",
            "                     padding='valid',\n",
            "                     data_format=None,\n",
            "                     dilation_rate=1):\n",
            "  \"\"\"1D convolution with separable filters.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor\n",
            "      depthwise_kernel: convolution kernel for the depthwise convolution.\n",
            "      pointwise_kernel: kernel for the 1x1 convolution.\n",
            "      strides: stride integer.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: integer dilation rate.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  if isinstance(strides, int):\n",
            "    strides = (strides,)\n",
            "  if isinstance(dilation_rate, int):\n",
            "    dilation_rate = (dilation_rate,)\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if not isinstance(strides, tuple):\n",
            "    strides = tuple(strides)\n",
            "  if tf_data_format == 'NWC':\n",
            "    spatial_start_dim = 1\n",
            "    strides = (1,) + strides * 2 + (1,)\n",
            "  else:\n",
            "    spatial_start_dim = 2\n",
            "    strides = (1, 1) + strides * 2\n",
            "  x = array_ops.expand_dims(x, spatial_start_dim)\n",
            "  depthwise_kernel = array_ops.expand_dims(depthwise_kernel, 0)\n",
            "  pointwise_kernel = array_ops.expand_dims(pointwise_kernel, 0)\n",
            "  dilation_rate = (1,) + dilation_rate\n",
            "\n",
            "  x = nn.separable_conv2d(\n",
            "      x,\n",
            "      depthwise_kernel,\n",
            "      pointwise_kernel,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      rate=dilation_rate,\n",
            "      data_format=tf_data_format)\n",
            "\n",
            "  x = array_ops.squeeze(x, [spatial_start_dim])\n",
            "\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NWC':\n",
            "    x = array_ops.transpose(x, (0, 2, 1))  # NWC -> NCW\n",
            "\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.separable_conv2d')\n",
            "def separable_conv2d(x,\n",
            "                     depthwise_kernel,\n",
            "                     pointwise_kernel,\n",
            "                     strides=(1, 1),\n",
            "                     padding='valid',\n",
            "                     data_format=None,\n",
            "                     dilation_rate=(1, 1)):\n",
            "  \"\"\"2D convolution with separable filters.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor\n",
            "      depthwise_kernel: convolution kernel for the depthwise convolution.\n",
            "      pointwise_kernel: kernel for the 1x1 convolution.\n",
            "      strides: strides tuple (length 2).\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: tuple of integers,\n",
            "          dilation rates for the separable convolution.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "      ValueError: if `strides` is not a tuple of 2 integers.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "  if len(strides) != 2:\n",
            "    raise ValueError('`strides` must be a tuple of 2 integers.')\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if not isinstance(strides, tuple):\n",
            "    strides = tuple(strides)\n",
            "  if tf_data_format == 'NHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "\n",
            "  x = nn.separable_conv2d(\n",
            "      x,\n",
            "      depthwise_kernel,\n",
            "      pointwise_kernel,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      rate=dilation_rate,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
            "  return x\n",
            "\n",
            "\n",
            "def depthwise_conv2d(x,\n",
            "                     depthwise_kernel,\n",
            "                     strides=(1, 1),\n",
            "                     padding='valid',\n",
            "                     data_format=None,\n",
            "                     dilation_rate=(1, 1)):\n",
            "  \"\"\"2D convolution with separable filters.\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor\n",
            "      depthwise_kernel: convolution kernel for the depthwise convolution.\n",
            "      strides: strides tuple (length 2).\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: tuple of integers,\n",
            "          dilation rates for the separable convolution.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if tf_data_format == 'NHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "\n",
            "  x = nn.depthwise_conv2d(\n",
            "      x,\n",
            "      depthwise_kernel,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      rate=dilation_rate,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.conv3d')\n",
            "def conv3d(x,\n",
            "           kernel,\n",
            "           strides=(1, 1, 1),\n",
            "           padding='valid',\n",
            "           data_format=None,\n",
            "           dilation_rate=(1, 1, 1)):\n",
            "  \"\"\"3D convolution.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      kernel: kernel tensor.\n",
            "      strides: strides tuple.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      dilation_rate: tuple of 3 integers.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of 3D convolution.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  x = nn.convolution(\n",
            "      input=x,\n",
            "      filter=kernel,\n",
            "      dilation_rate=dilation_rate,\n",
            "      strides=strides,\n",
            "      padding=padding,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n",
            "    x = array_ops.transpose(x, (0, 4, 1, 2, 3))\n",
            "  return x\n",
            "\n",
            "\n",
            "def conv3d_transpose(x,\n",
            "                     kernel,\n",
            "                     output_shape,\n",
            "                     strides=(1, 1, 1),\n",
            "                     padding='valid',\n",
            "                     data_format=None):\n",
            "  \"\"\"3D deconvolution (i.e.\n",
            "\n",
            "  transposed convolution).\n",
            "\n",
            "  Arguments:\n",
            "      x: input tensor.\n",
            "      kernel: kernel tensor.\n",
            "      output_shape: 1D int tensor for the output shape.\n",
            "      strides: strides tuple.\n",
            "      padding: string, \"same\" or \"valid\".\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of transposed 3D convolution.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `channels_last` or\n",
            "      `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "  if isinstance(output_shape, (tuple, list)):\n",
            "    output_shape = array_ops.stack(output_shape)\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n",
            "\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n",
            "    output_shape = (output_shape[0], output_shape[2], output_shape[3],\n",
            "                    output_shape[4], output_shape[1])\n",
            "  if output_shape[0] is None:\n",
            "    output_shape = (array_ops.shape(x)[0],) + tuple(output_shape[1:])\n",
            "    output_shape = array_ops.stack(list(output_shape))\n",
            "\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if tf_data_format == 'NDHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "\n",
            "  x = nn.conv3d_transpose(\n",
            "      x,\n",
            "      kernel,\n",
            "      output_shape,\n",
            "      strides,\n",
            "      padding=padding,\n",
            "      data_format=tf_data_format)\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n",
            "    x = array_ops.transpose(x, (0, 4, 1, 2, 3))\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.pool2d')\n",
            "def pool2d(x,\n",
            "           pool_size,\n",
            "           strides=(1, 1),\n",
            "           padding='valid',\n",
            "           data_format=None,\n",
            "           pool_mode='max'):\n",
            "  \"\"\"2D Pooling.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      pool_size: tuple of 2 integers.\n",
            "      strides: tuple of 2 integers.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      pool_mode: string, `\"max\"` or `\"avg\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of 2D pooling.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `\"channels_last\"` or\n",
            "      `\"channels_first\"`.\n",
            "      ValueError: if `pool_size` is not a tuple of 2 integers.\n",
            "      ValueError: if `strides` is not a tuple of 2 integers.\n",
            "      ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "  if len(pool_size) != 2:\n",
            "    raise ValueError('`pool_size` must be a tuple of 2 integers.')\n",
            "  if len(strides) != 2:\n",
            "    raise ValueError('`strides` must be a tuple of 2 integers.')\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if tf_data_format == 'NHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "    pool_size = (1,) + pool_size + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "    pool_size = (1, 1) + pool_size\n",
            "\n",
            "  if pool_mode == 'max':\n",
            "    x = nn.max_pool(\n",
            "        x, pool_size, strides, padding=padding, data_format=tf_data_format)\n",
            "  elif pool_mode == 'avg':\n",
            "    x = nn.avg_pool(\n",
            "        x, pool_size, strides, padding=padding, data_format=tf_data_format)\n",
            "  else:\n",
            "    raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n",
            "\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NHWC':\n",
            "    x = array_ops.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n",
            "  return x\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.pool3d')\n",
            "def pool3d(x,\n",
            "           pool_size,\n",
            "           strides=(1, 1, 1),\n",
            "           padding='valid',\n",
            "           data_format=None,\n",
            "           pool_mode='max'):\n",
            "  \"\"\"3D Pooling.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      pool_size: tuple of 3 integers.\n",
            "      strides: tuple of 3 integers.\n",
            "      padding: string, `\"same\"` or `\"valid\"`.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "      pool_mode: string, `\"max\"` or `\"avg\"`.\n",
            "\n",
            "  Returns:\n",
            "      A tensor, result of 3D pooling.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither `\"channels_last\"` or\n",
            "      `\"channels_first\"`.\n",
            "      ValueError: if `pool_mode` is neither `\"max\"` or `\"avg\"`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n",
            "  padding = _preprocess_padding(padding)\n",
            "  if tf_data_format == 'NDHWC':\n",
            "    strides = (1,) + strides + (1,)\n",
            "    pool_size = (1,) + pool_size + (1,)\n",
            "  else:\n",
            "    strides = (1, 1) + strides\n",
            "    pool_size = (1, 1) + pool_size\n",
            "\n",
            "  if pool_mode == 'max':\n",
            "    x = nn.max_pool3d(\n",
            "        x, pool_size, strides, padding=padding, data_format=tf_data_format)\n",
            "  elif pool_mode == 'avg':\n",
            "    x = nn.avg_pool3d(\n",
            "        x, pool_size, strides, padding=padding, data_format=tf_data_format)\n",
            "  else:\n",
            "    raise ValueError('Invalid pooling mode: ' + str(pool_mode))\n",
            "\n",
            "  if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n",
            "    x = array_ops.transpose(x, (0, 4, 1, 2, 3))\n",
            "  return x\n",
            "\n",
            "\n",
            "def local_conv(inputs,\n",
            "               kernel,\n",
            "               kernel_size,\n",
            "               strides,\n",
            "               output_shape,\n",
            "               data_format=None):\n",
            "  \"\"\"Apply N-D convolution with un-shared weights.\n",
            "\n",
            "  Arguments:\n",
            "      inputs: (N+2)-D tensor with shape\n",
            "          (batch_size, channels_in, d_in1, ..., d_inN)\n",
            "          if data_format='channels_first', or\n",
            "          (batch_size, d_in1, ..., d_inN, channels_in)\n",
            "          if data_format='channels_last'.\n",
            "      kernel: the unshared weight for N-D convolution,\n",
            "          with shape (output_items, feature_dim, channels_out), where\n",
            "          feature_dim = np.prod(kernel_size) * channels_in,\n",
            "          output_items = np.prod(output_shape).\n",
            "      kernel_size: a tuple of N integers, specifying the\n",
            "          spatial dimensions of the N-D convolution window.\n",
            "      strides: a tuple of N integers, specifying the strides\n",
            "          of the convolution along the spatial dimensions.\n",
            "      output_shape: a tuple of (d_out1, ..., d_outN) specifying the spatial\n",
            "          dimensionality of the output.\n",
            "      data_format: string, \"channels_first\" or \"channels_last\".\n",
            "\n",
            "  Returns:\n",
            "      An (N+2)-D tensor with shape:\n",
            "      (batch_size, channels_out) + output_shape\n",
            "      if data_format='channels_first', or:\n",
            "      (batch_size,) + output_shape + (channels_out,)\n",
            "      if data_format='channels_last'.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if `data_format` is neither\n",
            "      `channels_last` nor `channels_first`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "\n",
            "  kernel_shape = int_shape(kernel)\n",
            "  feature_dim = kernel_shape[1]\n",
            "  channels_out = kernel_shape[-1]\n",
            "  ndims = len(output_shape)\n",
            "  spatial_dimensions = list(range(ndims))\n",
            "\n",
            "  xs = []\n",
            "  output_axes_ticks = [range(axis_max) for axis_max in output_shape]\n",
            "  for position in itertools.product(*output_axes_ticks):\n",
            "    slices = [slice(None)]\n",
            "\n",
            "    if data_format == 'channels_first':\n",
            "      slices.append(slice(None))\n",
            "\n",
            "    slices.extend([slice(position[d] * strides[d],\n",
            "                         position[d] * strides[d] + kernel_size[d])\n",
            "                   for d in spatial_dimensions])\n",
            "\n",
            "    if data_format == 'channels_last':\n",
            "      slices.append(slice(None))\n",
            "\n",
            "    xs.append(reshape(inputs[slices], (1, -1, feature_dim)))\n",
            "\n",
            "  x_aggregate = concatenate(xs, axis=0)\n",
            "  output = batch_dot(x_aggregate, kernel)\n",
            "  output = reshape(output, output_shape + (-1, channels_out))\n",
            "\n",
            "  if data_format == 'channels_first':\n",
            "    permutation = [ndims, ndims + 1] + spatial_dimensions\n",
            "  else:\n",
            "    permutation = [ndims] + spatial_dimensions + [ndims + 1]\n",
            "\n",
            "  return permute_dimensions(output, permutation)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.local_conv1d')\n",
            "def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n",
            "  \"\"\"Apply 1D conv with un-shared weights.\n",
            "\n",
            "  Arguments:\n",
            "      inputs: 3D tensor with shape:\n",
            "          (batch_size, steps, input_dim)\n",
            "          if data_format is \"channels_last\" or\n",
            "          (batch_size, input_dim, steps)\n",
            "          if data_format is \"channels_first\".\n",
            "      kernel: the unshared weight for convolution,\n",
            "          with shape (output_length, feature_dim, filters).\n",
            "      kernel_size: a tuple of a single integer,\n",
            "          specifying the length of the 1D convolution window.\n",
            "      strides: a tuple of a single integer,\n",
            "          specifying the stride length of the convolution.\n",
            "      data_format: the data format, channels_first or channels_last.\n",
            "\n",
            "  Returns:\n",
            "      A 3d tensor with shape:\n",
            "      (batch_size, output_length, filters)\n",
            "      if data_format='channels_first'\n",
            "      or 3D tensor with shape:\n",
            "      (batch_size, filters, output_length)\n",
            "      if data_format='channels_last'.\n",
            "  \"\"\"\n",
            "  output_shape = (kernel.shape[0],)\n",
            "  return local_conv(inputs,\n",
            "                    kernel,\n",
            "                    kernel_size,\n",
            "                    strides,\n",
            "                    output_shape,\n",
            "                    data_format)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.local_conv2d')\n",
            "def local_conv2d(inputs,\n",
            "                 kernel,\n",
            "                 kernel_size,\n",
            "                 strides,\n",
            "                 output_shape,\n",
            "                 data_format=None):\n",
            "  \"\"\"Apply 2D conv with un-shared weights.\n",
            "\n",
            "  Arguments:\n",
            "      inputs: 4D tensor with shape:\n",
            "          (batch_size, filters, new_rows, new_cols)\n",
            "          if data_format='channels_first'\n",
            "          or 4D tensor with shape:\n",
            "          (batch_size, new_rows, new_cols, filters)\n",
            "          if data_format='channels_last'.\n",
            "      kernel: the unshared weight for convolution,\n",
            "          with shape (output_items, feature_dim, filters).\n",
            "      kernel_size: a tuple of 2 integers, specifying the\n",
            "          width and height of the 2D convolution window.\n",
            "      strides: a tuple of 2 integers, specifying the strides\n",
            "          of the convolution along the width and height.\n",
            "      output_shape: a tuple with (output_row, output_col).\n",
            "      data_format: the data format, channels_first or channels_last.\n",
            "\n",
            "  Returns:\n",
            "      A 4D tensor with shape:\n",
            "      (batch_size, filters, new_rows, new_cols)\n",
            "      if data_format='channels_first'\n",
            "      or 4D tensor with shape:\n",
            "      (batch_size, new_rows, new_cols, filters)\n",
            "      if data_format='channels_last'.\n",
            "  \"\"\"\n",
            "  return local_conv(inputs,\n",
            "                    kernel,\n",
            "                    kernel_size,\n",
            "                    strides,\n",
            "                    output_shape,\n",
            "                    data_format)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.bias_add')\n",
            "def bias_add(x, bias, data_format=None):\n",
            "  \"\"\"Adds a bias vector to a tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: Tensor or variable.\n",
            "      bias: Bias tensor to add.\n",
            "      data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n",
            "\n",
            "  Returns:\n",
            "      Output tensor.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In one of the two cases below:\n",
            "                  1. invalid `data_format` argument.\n",
            "                  2. invalid bias shape.\n",
            "                     the bias should be either a vector or\n",
            "                     a tensor with ndim(x) - 1 dimension\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = image_data_format()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "  bias_shape = int_shape(bias)\n",
            "  if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n",
            "    raise ValueError(\n",
            "        'Unexpected bias dimensions %d, expect to be 1 or %d dimensions' %\n",
            "        (len(bias_shape), ndim(x)))\n",
            "  # pylint: disable=g-no-augmented-assignment\n",
            "  if ndim(x) == 5:\n",
            "    if data_format == 'channels_first':\n",
            "      if len(bias_shape) == 1:\n",
            "        x = x + reshape(bias, (1, bias_shape[0], 1, 1, 1))\n",
            "      else:\n",
            "        x = x + reshape(bias, (1, bias_shape[3]) + bias_shape[:3])\n",
            "    elif data_format == 'channels_last':\n",
            "      if len(bias_shape) == 1:\n",
            "        x = x + reshape(bias, (1, 1, 1, bias_shape[0]))\n",
            "      else:\n",
            "        x = x + reshape(bias, (1,) + bias_shape)\n",
            "  elif ndim(x) == 4:\n",
            "    if data_format == 'channels_first':\n",
            "      if len(bias_shape) == 1:\n",
            "        if _has_nchw_support():\n",
            "          x = nn.bias_add(x, bias, data_format='NCHW')\n",
            "        else:\n",
            "          x = x + reshape(bias, (1, bias_shape[0], 1, 1))\n",
            "      else:\n",
            "        x = x + reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n",
            "    elif data_format == 'channels_last':\n",
            "      if len(bias_shape) == 1:\n",
            "        x = nn.bias_add(x, bias, data_format='NHWC')\n",
            "      else:\n",
            "        x = x + reshape(bias, (1,) + bias_shape)\n",
            "  elif ndim(x) == 3:\n",
            "    if data_format == 'channels_first':\n",
            "      if len(bias_shape) == 1:\n",
            "        x = x + reshape(bias, (1, bias_shape[0], 1))\n",
            "      else:\n",
            "        x = x + reshape(bias, (1, bias_shape[1], bias_shape[0]))\n",
            "    elif data_format == 'channels_last':\n",
            "      if len(bias_shape) == 1:\n",
            "        x = x + reshape(bias, (1, 1, bias_shape[0]))\n",
            "      else:\n",
            "        x = x + reshape(bias, (1,) + bias_shape)\n",
            "  else:\n",
            "    x = nn.bias_add(x, bias)\n",
            "  # pylint: enable=g-no-augmented-assignment\n",
            "  return x\n",
            "\n",
            "\n",
            "# RANDOMNESS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.random_normal')\n",
            "def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n",
            "  \"\"\"Returns a tensor with normal distribution of values.\n",
            "\n",
            "  Arguments:\n",
            "      shape: A tuple of integers, the shape of tensor to create.\n",
            "      mean: A float, mean of the normal distribution to draw samples.\n",
            "      stddev: A float, standard deviation of the normal distribution\n",
            "          to draw samples.\n",
            "      dtype: String, dtype of returned tensor.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if seed is None:\n",
            "    seed = np.random.randint(10e6)\n",
            "  return random_ops.random_normal(\n",
            "      shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.random_uniform')\n",
            "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n",
            "  \"\"\"Returns a tensor with uniform distribution of values.\n",
            "\n",
            "  Arguments:\n",
            "      shape: A tuple of integers, the shape of tensor to create.\n",
            "      minval: A float, lower boundary of the uniform distribution\n",
            "          to draw samples.\n",
            "      maxval: A float, upper boundary of the uniform distribution\n",
            "          to draw samples.\n",
            "      dtype: String, dtype of returned tensor.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if seed is None:\n",
            "    seed = np.random.randint(10e6)\n",
            "  return random_ops.random_uniform(\n",
            "      shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.random_binomial')\n",
            "def random_binomial(shape, p=0.0, dtype=None, seed=None):\n",
            "  \"\"\"Returns a tensor with random binomial distribution of values.\n",
            "\n",
            "  The binomial distribution with parameters `n` and `p` is the probability\n",
            "  distribution of the number of successful Bernoulli process. Only supports\n",
            "  `n` = 1 for now.\n",
            "\n",
            "  Arguments:\n",
            "      shape: A tuple of integers, the shape of tensor to create.\n",
            "      p: A float, `0. <= p <= 1`, probability of binomial distribution.\n",
            "      dtype: String, dtype of returned tensor.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if seed is None:\n",
            "    seed = np.random.randint(10e6)\n",
            "  return array_ops.where(\n",
            "      random_ops.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n",
            "      array_ops.ones(shape, dtype=dtype), array_ops.zeros(shape, dtype=dtype))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.truncated_normal')\n",
            "def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n",
            "  \"\"\"Returns a tensor with truncated random normal distribution of values.\n",
            "\n",
            "  The generated values follow a normal distribution\n",
            "  with specified mean and standard deviation,\n",
            "  except that values whose magnitude is more than\n",
            "  two standard deviations from the mean are dropped and re-picked.\n",
            "\n",
            "  Arguments:\n",
            "      shape: A tuple of integers, the shape of tensor to create.\n",
            "      mean: Mean of the values.\n",
            "      stddev: Standard deviation of the values.\n",
            "      dtype: String, dtype of returned tensor.\n",
            "      seed: Integer, random seed.\n",
            "\n",
            "  Returns:\n",
            "      A tensor.\n",
            "  \"\"\"\n",
            "  if dtype is None:\n",
            "    dtype = floatx()\n",
            "  if seed is None:\n",
            "    seed = np.random.randint(10e6)\n",
            "  return random_ops.truncated_normal(\n",
            "      shape, mean, stddev, dtype=dtype, seed=seed)\n",
            "\n",
            "\n",
            "# CTC\n",
            "# TensorFlow has a native implementation, but it uses sparse tensors\n",
            "# and therefore requires a wrapper for Keras. The functions below convert\n",
            "# dense to sparse tensors and also wraps up the beam search code that is\n",
            "# in TensorFlow's CTC implementation\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ctc_label_dense_to_sparse')\n",
            "def ctc_label_dense_to_sparse(labels, label_lengths):\n",
            "  \"\"\"Converts CTC labels from dense to sparse.\n",
            "\n",
            "  Arguments:\n",
            "      labels: dense CTC labels.\n",
            "      label_lengths: length of the labels.\n",
            "\n",
            "  Returns:\n",
            "      A sparse tensor representation of the labels.\n",
            "  \"\"\"\n",
            "  label_shape = array_ops.shape(labels)\n",
            "  num_batches_tns = array_ops.stack([label_shape[0]])\n",
            "  max_num_labels_tns = array_ops.stack([label_shape[1]])\n",
            "\n",
            "  def range_less_than(_, current_input):\n",
            "    return array_ops.expand_dims(\n",
            "        math_ops.range(label_shape[1]), 0) < array_ops.fill(\n",
            "            max_num_labels_tns, current_input)\n",
            "\n",
            "  init = math_ops.cast(\n",
            "      array_ops.fill([1, label_shape[1]], 0), dtypes_module.bool)\n",
            "  dense_mask = functional_ops.scan(\n",
            "      range_less_than, label_lengths, initializer=init, parallel_iterations=1)\n",
            "  dense_mask = dense_mask[:, 0, :]\n",
            "\n",
            "  label_array = array_ops.reshape(\n",
            "      array_ops.tile(math_ops.range(0, label_shape[1]), num_batches_tns),\n",
            "      label_shape)\n",
            "  label_ind = array_ops.boolean_mask(label_array, dense_mask)\n",
            "\n",
            "  batch_array = array_ops.transpose(\n",
            "      array_ops.reshape(\n",
            "          array_ops.tile(math_ops.range(0, label_shape[0]), max_num_labels_tns),\n",
            "          reverse(label_shape, 0)))\n",
            "  batch_ind = array_ops.boolean_mask(batch_array, dense_mask)\n",
            "  indices = array_ops.transpose(\n",
            "      array_ops.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n",
            "\n",
            "  vals_sparse = array_ops.gather_nd(labels, indices)\n",
            "\n",
            "  return sparse_tensor.SparseTensor(\n",
            "      math_ops.cast(indices, dtypes_module.int64), vals_sparse,\n",
            "      math_ops.cast(label_shape, dtypes_module.int64))\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ctc_batch_cost')\n",
            "def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n",
            "  \"\"\"Runs CTC loss algorithm on each batch element.\n",
            "\n",
            "  Arguments:\n",
            "      y_true: tensor `(samples, max_string_length)`\n",
            "          containing the truth labels.\n",
            "      y_pred: tensor `(samples, time_steps, num_categories)`\n",
            "          containing the prediction, or output of the softmax.\n",
            "      input_length: tensor `(samples, 1)` containing the sequence length for\n",
            "          each batch item in `y_pred`.\n",
            "      label_length: tensor `(samples, 1)` containing the sequence length for\n",
            "          each batch item in `y_true`.\n",
            "\n",
            "  Returns:\n",
            "      Tensor with shape (samples,1) containing the\n",
            "          CTC loss of each element.\n",
            "  \"\"\"\n",
            "  label_length = math_ops.cast(\n",
            "      array_ops.squeeze(label_length, axis=-1), dtypes_module.int32)\n",
            "  input_length = math_ops.cast(\n",
            "      array_ops.squeeze(input_length, axis=-1), dtypes_module.int32)\n",
            "  sparse_labels = math_ops.cast(\n",
            "      ctc_label_dense_to_sparse(y_true, label_length), dtypes_module.int32)\n",
            "\n",
            "  y_pred = math_ops.log(array_ops.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n",
            "\n",
            "  return array_ops.expand_dims(\n",
            "      ctc.ctc_loss(\n",
            "          inputs=y_pred, labels=sparse_labels, sequence_length=input_length), 1)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.ctc_decode')\n",
            "def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):\n",
            "  \"\"\"Decodes the output of a softmax.\n",
            "\n",
            "  Can use either greedy search (also known as best path)\n",
            "  or a constrained dictionary search.\n",
            "\n",
            "  Arguments:\n",
            "      y_pred: tensor `(samples, time_steps, num_categories)`\n",
            "          containing the prediction, or output of the softmax.\n",
            "      input_length: tensor `(samples, )` containing the sequence length for\n",
            "          each batch item in `y_pred`.\n",
            "      greedy: perform much faster best-path search if `true`.\n",
            "          This does not use a dictionary.\n",
            "      beam_width: if `greedy` is `false`: a beam search decoder will be used\n",
            "          with a beam of this width.\n",
            "      top_paths: if `greedy` is `false`,\n",
            "          how many of the most probable paths will be returned.\n",
            "\n",
            "  Returns:\n",
            "      Tuple:\n",
            "          List: if `greedy` is `true`, returns a list of one element that\n",
            "              contains the decoded sequence.\n",
            "              If `false`, returns the `top_paths` most probable\n",
            "              decoded sequences.\n",
            "              Important: blank labels are returned as `-1`.\n",
            "          Tensor `(top_paths, )` that contains\n",
            "              the log probability of each decoded sequence.\n",
            "  \"\"\"\n",
            "  y_pred = math_ops.log(array_ops.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n",
            "  input_length = math_ops.cast(input_length, dtypes_module.int32)\n",
            "\n",
            "  if greedy:\n",
            "    (decoded, log_prob) = ctc.ctc_greedy_decoder(\n",
            "        inputs=y_pred, sequence_length=input_length)\n",
            "  else:\n",
            "    (decoded, log_prob) = ctc.ctc_beam_search_decoder(\n",
            "        inputs=y_pred,\n",
            "        sequence_length=input_length,\n",
            "        beam_width=beam_width,\n",
            "        top_paths=top_paths)\n",
            "  decoded_dense = [\n",
            "      sparse_ops.sparse_to_dense(\n",
            "          st.indices, st.dense_shape, st.values, default_value=-1)\n",
            "      for st in decoded\n",
            "  ]\n",
            "  return (decoded_dense, log_prob)\n",
            "\n",
            "\n",
            "# HIGH ORDER FUNCTIONS\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.map_fn')\n",
            "def map_fn(fn, elems, name=None, dtype=None):\n",
            "  \"\"\"Map the function fn over the elements elems and return the outputs.\n",
            "\n",
            "  Arguments:\n",
            "      fn: Callable that will be called upon each element in elems\n",
            "      elems: tensor\n",
            "      name: A string name for the map node in the graph\n",
            "      dtype: Output data type.\n",
            "\n",
            "  Returns:\n",
            "      Tensor with dtype `dtype`.\n",
            "  \"\"\"\n",
            "  return map_fn_lib.map_fn(fn, elems, name=name, dtype=dtype)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.foldl')\n",
            "def foldl(fn, elems, initializer=None, name=None):\n",
            "  \"\"\"Reduce elems using fn to combine them from left to right.\n",
            "\n",
            "  Arguments:\n",
            "      fn: Callable that will be called upon each element in elems and an\n",
            "          accumulator, for instance `lambda acc, x: acc + x`\n",
            "      elems: tensor\n",
            "      initializer: The first value used (`elems[0]` in case of None)\n",
            "      name: A string name for the foldl node in the graph\n",
            "\n",
            "  Returns:\n",
            "      Tensor with same type and shape as `initializer`.\n",
            "  \"\"\"\n",
            "  return functional_ops.foldl(fn, elems, initializer=initializer, name=name)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.foldr')\n",
            "def foldr(fn, elems, initializer=None, name=None):\n",
            "  \"\"\"Reduce elems using fn to combine them from right to left.\n",
            "\n",
            "  Arguments:\n",
            "      fn: Callable that will be called upon each element in elems and an\n",
            "          accumulator, for instance `lambda acc, x: acc + x`\n",
            "      elems: tensor\n",
            "      initializer: The first value used (`elems[-1]` in case of None)\n",
            "      name: A string name for the foldr node in the graph\n",
            "\n",
            "  Returns:\n",
            "      Same type and shape as initializer\n",
            "  \"\"\"\n",
            "  return functional_ops.foldr(fn, elems, initializer=initializer, name=name)\n",
            "\n",
            "# Load Keras default configuration from config file if present.\n",
            "# Set Keras base dir path given KERAS_HOME env variable, if applicable.\n",
            "# Otherwise either ~/.keras or /tmp.\n",
            "if 'KERAS_HOME' in os.environ:\n",
            "  _keras_dir = os.environ.get('KERAS_HOME')\n",
            "else:\n",
            "  _keras_base_dir = os.path.expanduser('~')\n",
            "  _keras_dir = os.path.join(_keras_base_dir, '.keras')\n",
            "_config_path = os.path.expanduser(os.path.join(_keras_dir, 'keras.json'))\n",
            "if os.path.exists(_config_path):\n",
            "  try:\n",
            "    _config = json.load(open(_config_path))\n",
            "  except ValueError:\n",
            "    _config = {}\n",
            "  _floatx = _config.get('floatx', floatx())\n",
            "  assert _floatx in {'float16', 'float32', 'float64'}\n",
            "  _epsilon = _config.get('epsilon', epsilon())\n",
            "  assert isinstance(_epsilon, float)\n",
            "  _image_data_format = _config.get('image_data_format', image_data_format())\n",
            "  assert _image_data_format in {'channels_last', 'channels_first'}\n",
            "  set_floatx(_floatx)\n",
            "  set_epsilon(_epsilon)\n",
            "  set_image_data_format(_image_data_format)\n",
            "\n",
            "# Save config file.\n",
            "if not os.path.exists(_keras_dir):\n",
            "  try:\n",
            "    os.makedirs(_keras_dir)\n",
            "  except OSError:\n",
            "    # Except permission denied and potential race conditions\n",
            "    # in multi-threaded environments.\n",
            "    pass\n",
            "\n",
            "if not os.path.exists(_config_path):\n",
            "  _config = {\n",
            "      'floatx': floatx(),\n",
            "      'epsilon': epsilon(),\n",
            "      'backend': 'tensorflow',\n",
            "      'image_data_format': image_data_format()\n",
            "  }\n",
            "  try:\n",
            "    with open(_config_path, 'w') as f:\n",
            "      f.write(json.dumps(_config, indent=4))\n",
            "  except IOError:\n",
            "    # Except permission denied.\n",
            "    pass\n",
            "\n",
            "\n",
            "def in_multi_worker_mode():\n",
            "  \"\"\"Whether we are operating in a Multi-Worker setting.\"\"\"\n",
            "  # TODO(rchao): Consider a warning if user uses multiple `model` method\n",
            "  # calls in multi-worker setting.\n",
            "  tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
            "  cluster_spec = server_lib.ClusterSpec(tf_config.get('cluster', {}))\n",
            "  return tf_config and 'master' not in cluster_spec.jobs\n",
            "\n",
            "\n",
            "def configure_and_create_distributed_session(distribution_strategy):\n",
            "  \"\"\"Configure session config and create a session with it.\"\"\"\n",
            "\n",
            "  def _create_session(distribution_strategy):\n",
            "    \"\"\"Create the Distributed Strategy session.\"\"\"\n",
            "    session_config = get_default_session_config()\n",
            "\n",
            "    # If a session already exists, merge in its config; in the case there is a\n",
            "    # conflict, take values of the existing config.\n",
            "    global _SESSION\n",
            "    if getattr(_SESSION, 'session', None) and _SESSION.session._config:\n",
            "      session_config.MergeFrom(_SESSION.session._config)\n",
            "\n",
            "    if is_tpu_strategy(distribution_strategy):\n",
            "      # TODO(priyag, yuefengz): Remove this workaround when Distribute\n",
            "      # Coordinator is integrated with keras and we can create a session from\n",
            "      # there.\n",
            "      distribution_strategy.configure(session_config)\n",
            "      master = distribution_strategy.extended._tpu_cluster_resolver.master()  # pylint: disable=protected-access\n",
            "      session = session_module.Session(config=session_config, target=master)\n",
            "    else:\n",
            "      worker_context = dc_context.get_current_worker_context()\n",
            "      if worker_context:\n",
            "        dc_session_config = worker_context.session_config\n",
            "        # Merge the default session config to the one from distribute\n",
            "        # coordinator, which is fine for now since they don't have\n",
            "        # conflicting configurations.\n",
            "        dc_session_config.MergeFrom(session_config)\n",
            "        session = session_module.Session(\n",
            "            config=dc_session_config, target=worker_context.master_target)\n",
            "      else:\n",
            "        distribution_strategy.configure(session_config)\n",
            "        session = session_module.Session(config=session_config)\n",
            "\n",
            "    set_session(session)\n",
            "\n",
            "  if in_multi_worker_mode():\n",
            "    dc.run_distribute_coordinator(\n",
            "        _create_session,\n",
            "        distribution_strategy,\n",
            "        mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\n",
            "  else:\n",
            "    _create_session(distribution_strategy)\n",
            "\n",
            "\n",
            "def is_tpu_strategy(strategy):\n",
            "  \"\"\"We're executing TPU Strategy.\"\"\"\n",
            "  return (strategy is not None and\n",
            "          strategy.__class__.__name__.startswith('TPUStrategy'))\n",
            "\n",
            "\n",
            "def cast_variables_to_tensor(tensors):\n",
            "\n",
            "  def _cast_variables_to_tensor(tensor):\n",
            "    if isinstance(tensor, variables_module.Variable):\n",
            "      return array_ops.identity(tensor)\n",
            "    return tensor\n",
            "\n",
            "  return nest.map_structure(_cast_variables_to_tensor, tensors)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities for unit-testing Keras.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import collections\n",
            "import functools\n",
            "import itertools\n",
            "import unittest\n",
            "\n",
            "from absl.testing import parameterized\n",
            "\n",
            "from tensorflow.python import keras\n",
            "from tensorflow.python import tf2\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.keras import testing_utils\n",
            "from tensorflow.python.platform import test\n",
            "from tensorflow.python.util import nest\n",
            "\n",
            "\n",
            "class TestCase(test.TestCase, parameterized.TestCase):\n",
            "\n",
            "  def tearDown(self):\n",
            "    keras.backend.clear_session()\n",
            "    super(TestCase, self).tearDown()\n",
            "\n",
            "\n",
            "# TODO(kaftan): Possibly enable 'subclass_custom_build' when tests begin to pass\n",
            "# it. Or perhaps make 'subclass' always use a custom build method.\n",
            "def run_with_all_model_types(\n",
            "    test_or_class=None,\n",
            "    exclude_models=None):\n",
            "  \"\"\"Execute the decorated test with all Keras model types.\n",
            "\n",
            "  This decorator is intended to be applied either to individual test methods in\n",
            "  a `keras_parameterized.TestCase` class, or directly to a test class that\n",
            "  extends it. Doing so will cause the contents of the individual test\n",
            "  method (or all test methods in the class) to be executed multiple times - once\n",
            "  for each Keras model type.\n",
            "\n",
            "  The Keras model types are: ['functional', 'subclass', 'sequential']\n",
            "\n",
            "  Note: if stacking this decorator with absl.testing's parameterized decorators,\n",
            "  those should be at the bottom of the stack.\n",
            "\n",
            "  Various methods in `testing_utils` to get models will auto-generate a model\n",
            "  of the currently active Keras model type. This allows unittests to confirm\n",
            "  the equivalence between different Keras models.\n",
            "\n",
            "  For example, consider the following unittest:\n",
            "\n",
            "  ```python\n",
            "  class MyTests(testing_utils.KerasTestCase):\n",
            "\n",
            "    @testing_utils.run_with_all_model_types(\n",
            "      exclude_models = ['sequential'])\n",
            "    def test_foo(self):\n",
            "      model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n",
            "      optimizer = RMSPropOptimizer(learning_rate=0.001)\n",
            "      loss = 'mse'\n",
            "      metrics = ['mae']\n",
            "      model.compile(optimizer, loss, metrics=metrics)\n",
            "\n",
            "      inputs = np.zeros((10, 3))\n",
            "      targets = np.zeros((10, 4))\n",
            "      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))\n",
            "      dataset = dataset.repeat(100)\n",
            "      dataset = dataset.batch(10)\n",
            "\n",
            "      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n",
            "\n",
            "  if __name__ == \"__main__\":\n",
            "    tf.test.main()\n",
            "  ```\n",
            "\n",
            "  This test tries building a small mlp as both a functional model and as a\n",
            "  subclass model.\n",
            "\n",
            "  We can also annotate the whole class if we want this to apply to all tests in\n",
            "  the class:\n",
            "  ```python\n",
            "  @testing_utils.run_with_all_model_types(exclude_models = ['sequential'])\n",
            "  class MyTests(testing_utils.KerasTestCase):\n",
            "\n",
            "    def test_foo(self):\n",
            "      model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n",
            "      optimizer = RMSPropOptimizer(learning_rate=0.001)\n",
            "      loss = 'mse'\n",
            "      metrics = ['mae']\n",
            "      model.compile(optimizer, loss, metrics=metrics)\n",
            "\n",
            "      inputs = np.zeros((10, 3))\n",
            "      targets = np.zeros((10, 4))\n",
            "      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))\n",
            "      dataset = dataset.repeat(100)\n",
            "      dataset = dataset.batch(10)\n",
            "\n",
            "      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n",
            "\n",
            "  if __name__ == \"__main__\":\n",
            "    tf.test.main()\n",
            "  ```\n",
            "\n",
            "\n",
            "  Args:\n",
            "    test_or_class: test method or class to be annotated. If None,\n",
            "      this method returns a decorator that can be applied to a test method or\n",
            "      test class. If it is not None this returns the decorator applied to the\n",
            "      test or class.\n",
            "    exclude_models: A collection of Keras model types to not run.\n",
            "      (May also be a single model type not wrapped in a collection).\n",
            "      Defaults to None.\n",
            "\n",
            "  Returns:\n",
            "    Returns a decorator that will run the decorated test method multiple times:\n",
            "    once for each desired Keras model type.\n",
            "\n",
            "  Raises:\n",
            "    ImportError: If abseil parameterized is not installed or not included as\n",
            "      a target dependency.\n",
            "  \"\"\"\n",
            "  model_types = ['functional', 'subclass', 'sequential']\n",
            "  params = [('_%s' % model, model) for model in model_types\n",
            "            if model not in nest.flatten(exclude_models)]\n",
            "\n",
            "  def single_method_decorator(f):\n",
            "    \"\"\"Decorator that constructs the test cases.\"\"\"\n",
            "    # Use named_parameters so it can be individually run from the command line\n",
            "    @parameterized.named_parameters(*params)\n",
            "    @functools.wraps(f)\n",
            "    def decorated(self, model_type, *args, **kwargs):\n",
            "      \"\"\"A run of a single test case w/ the specified model type.\"\"\"\n",
            "      if model_type == 'functional':\n",
            "        _test_functional_model_type(f, self, *args, **kwargs)\n",
            "      elif model_type == 'subclass':\n",
            "        _test_subclass_model_type(f, self, *args, **kwargs)\n",
            "      elif model_type == 'sequential':\n",
            "        _test_sequential_model_type(f, self, *args, **kwargs)\n",
            "      else:\n",
            "        raise ValueError('Unknown model type: %s' % (model_type,))\n",
            "    return decorated\n",
            "\n",
            "  return _test_or_class_decorator(test_or_class, single_method_decorator)\n",
            "\n",
            "\n",
            "def _test_functional_model_type(f, test_or_class, *args, **kwargs):\n",
            "  with testing_utils.model_type_scope('functional'):\n",
            "    f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def _test_subclass_model_type(f, test_or_class, *args, **kwargs):\n",
            "  with testing_utils.model_type_scope('subclass'):\n",
            "    f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def _test_sequential_model_type(f, test_or_class, *args, **kwargs):\n",
            "  with testing_utils.model_type_scope('sequential'):\n",
            "    f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def run_all_keras_modes(\n",
            "    test_or_class=None,\n",
            "    config=None,\n",
            "    always_skip_v1=False):\n",
            "  \"\"\"Execute the decorated test with all keras execution modes.\n",
            "\n",
            "  This decorator is intended to be applied either to individual test methods in\n",
            "  a `keras_parameterized.TestCase` class, or directly to a test class that\n",
            "  extends it. Doing so will cause the contents of the individual test\n",
            "  method (or all test methods in the class) to be executed multiple times -\n",
            "  once executing in legacy graph mode, once running eagerly and with\n",
            "  `should_run_eagerly` returning True, and once running eagerly with\n",
            "  `should_run_eagerly` returning False.\n",
            "\n",
            "  If Tensorflow v2 behavior is enabled, legacy graph mode will be skipped, and\n",
            "  the test will only run twice.\n",
            "\n",
            "  Note: if stacking this decorator with absl.testing's parameterized decorators,\n",
            "  those should be at the bottom of the stack.\n",
            "\n",
            "  For example, consider the following unittest:\n",
            "\n",
            "  ```python\n",
            "  class MyTests(testing_utils.KerasTestCase):\n",
            "\n",
            "    @testing_utils.run_all_keras_modes\n",
            "    def test_foo(self):\n",
            "      model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n",
            "      optimizer = RMSPropOptimizer(learning_rate=0.001)\n",
            "      loss = 'mse'\n",
            "      metrics = ['mae']\n",
            "      model.compile(optimizer, loss, metrics=metrics,\n",
            "                    run_eagerly=testing_utils.should_run_eagerly())\n",
            "\n",
            "      inputs = np.zeros((10, 3))\n",
            "      targets = np.zeros((10, 4))\n",
            "      dataset = dataset_ops.Dataset.from_tensor_slices((inputs, targets))\n",
            "      dataset = dataset.repeat(100)\n",
            "      dataset = dataset.batch(10)\n",
            "\n",
            "      model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n",
            "\n",
            "  if __name__ == \"__main__\":\n",
            "    tf.test.main()\n",
            "  ```\n",
            "\n",
            "  This test will try compiling & fitting the small functional mlp using all\n",
            "  three Keras execution modes.\n",
            "\n",
            "  Args:\n",
            "    test_or_class: test method or class to be annotated. If None,\n",
            "      this method returns a decorator that can be applied to a test method or\n",
            "      test class. If it is not None this returns the decorator applied to the\n",
            "      test or class.\n",
            "    config: An optional config_pb2.ConfigProto to use to configure the\n",
            "      session when executing graphs.\n",
            "    always_skip_v1: If True, does not try running the legacy graph mode even\n",
            "      when Tensorflow v2 behavior is not enabled.\n",
            "\n",
            "  Returns:\n",
            "    Returns a decorator that will run the decorated test method multiple times.\n",
            "\n",
            "  Raises:\n",
            "    ImportError: If abseil parameterized is not installed or not included as\n",
            "      a target dependency.\n",
            "  \"\"\"\n",
            "  params = [('_v2_eager', 'v2_eager'),\n",
            "            ('_v2_function', 'v2_function')]\n",
            "  if not (always_skip_v1 or tf2.enabled()):\n",
            "    params.append(('_v1_graph', 'v1_graph'))\n",
            "\n",
            "  def single_method_decorator(f):\n",
            "    \"\"\"Decorator that constructs the test cases.\"\"\"\n",
            "\n",
            "    # Use named_parameters so it can be individually run from the command line\n",
            "    @parameterized.named_parameters(*params)\n",
            "    @functools.wraps(f)\n",
            "    def decorated(self, run_mode, *args, **kwargs):\n",
            "      \"\"\"A run of a single test case w/ specified run mode.\"\"\"\n",
            "      if run_mode == 'v1_graph':\n",
            "        _v1_graph_test(f, self, config, *args, **kwargs)\n",
            "      elif run_mode == 'v2_function':\n",
            "        _v2_graph_functions_test(f, self, *args, **kwargs)\n",
            "      elif run_mode == 'v2_eager':\n",
            "        _v2_eager_test(f, self, *args, **kwargs)\n",
            "      else:\n",
            "        return ValueError('Unknown run mode %s' % run_mode)\n",
            "\n",
            "    return decorated\n",
            "\n",
            "  return _test_or_class_decorator(test_or_class, single_method_decorator)\n",
            "\n",
            "\n",
            "def _v1_graph_test(f, test_or_class, config, *args, **kwargs):\n",
            "  with context.graph_mode(), testing_utils.run_eagerly_scope(False):\n",
            "    with test_or_class.test_session(use_gpu=True, config=config):\n",
            "      f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def _v2_graph_functions_test(f, test_or_class, *args, **kwargs):\n",
            "  with context.eager_mode():\n",
            "    with testing_utils.run_eagerly_scope(False):\n",
            "      f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def _v2_eager_test(f, test_or_class, *args, **kwargs):\n",
            "  with context.eager_mode():\n",
            "    with testing_utils.run_eagerly_scope(True):\n",
            "      f(test_or_class, *args, **kwargs)\n",
            "\n",
            "\n",
            "def _test_or_class_decorator(test_or_class, single_method_decorator):\n",
            "  \"\"\"Decorate a test or class with a decorator intended for one method.\n",
            "\n",
            "  If the test_or_class is a class:\n",
            "    This will apply the decorator to all test methods in the class.\n",
            "\n",
            "  If the test_or_class is an iterable of already-parameterized test cases:\n",
            "    This will apply the decorator to all the cases, and then flatten the\n",
            "    resulting cross-product of test cases. This allows stacking the Keras\n",
            "    parameterized decorators w/ each other, and to apply them to test methods\n",
            "    that have already been marked with an absl parameterized decorator.\n",
            "\n",
            "  Otherwise, treat the obj as a single method and apply the decorator directly.\n",
            "\n",
            "  Args:\n",
            "    test_or_class: A test method (that may have already been decorated with a\n",
            "      parameterized decorator, or a test class that extends\n",
            "      keras_parameterized.TestCase\n",
            "    single_method_decorator:\n",
            "      A parameterized decorator intended for a single test method.\n",
            "  Returns:\n",
            "    The decorated result.\n",
            "  \"\"\"\n",
            "  def _decorate_test_or_class(obj):\n",
            "    if isinstance(obj, collections.Iterable):\n",
            "      return itertools.chain.from_iterable(\n",
            "          single_method_decorator(method) for method in obj)\n",
            "    if isinstance(obj, type):\n",
            "      cls = obj\n",
            "      for name, value in cls.__dict__.copy().items():\n",
            "        if callable(value) and name.startswith(\n",
            "            unittest.TestLoader.testMethodPrefix):\n",
            "          setattr(cls, name, single_method_decorator(value))\n",
            "\n",
            "      cls = type(cls).__new__(type(cls), cls.__name__, cls.__bases__,\n",
            "                              cls.__dict__.copy())\n",
            "      return cls\n",
            "\n",
            "    return single_method_decorator(obj)\n",
            "\n",
            "  if test_or_class is not None:\n",
            "    return _decorate_test_or_class(test_or_class)\n",
            "\n",
            "  return _decorate_test_or_class\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras backend config API.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "# The type of float to use throughout a session.\n",
            "_FLOATX = 'float32'\n",
            "\n",
            "# Epsilon fuzz factor used throughout the codebase.\n",
            "_EPSILON = 1e-7\n",
            "\n",
            "# Default image data format, one of \"channels_last\", \"channels_first\".\n",
            "_IMAGE_DATA_FORMAT = 'channels_last'\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.epsilon')\n",
            "def epsilon():\n",
            "  \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n",
            "\n",
            "  Returns:\n",
            "      A float.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "  keras.backend.epsilon() >>>1e-07\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return _EPSILON\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.set_epsilon')\n",
            "def set_epsilon(value):\n",
            "  \"\"\"Sets the value of the fuzz factor used in numeric expressions.\n",
            "\n",
            "  Arguments:\n",
            "      value: float. New value of epsilon.\n",
            "  Example: ```python from keras import backend as K K.epsilon() >>> 1e-07\n",
            "    K.set_epsilon(1e-05) K.epsilon() >>> 1e-05 ```\n",
            "  \"\"\"\n",
            "  global _EPSILON\n",
            "  _EPSILON = value\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.floatx')\n",
            "def floatx():\n",
            "  \"\"\"Returns the default float type, as a string.\n",
            "\n",
            "  E.g. 'float16', 'float32', 'float64'.\n",
            "\n",
            "  Returns:\n",
            "      String, the current default float type.\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "  keras.backend.floatx() >>> 'float32'\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return _FLOATX\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.set_floatx')\n",
            "def set_floatx(value):\n",
            "  \"\"\"Sets the default float type.\n",
            "\n",
            "  Arguments:\n",
            "      value: String; 'float16', 'float32', or 'float64'.\n",
            "  Example: ```python from keras import backend as K K.floatx() >>> 'float32'\n",
            "    K.set_floatx('float16') K.floatx() >>> 'float16' ```\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In case of invalid value.\n",
            "  \"\"\"\n",
            "  global _FLOATX\n",
            "  if value not in {'float16', 'float32', 'float64'}:\n",
            "    raise ValueError('Unknown floatx type: ' + str(value))\n",
            "  _FLOATX = str(value)\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.image_data_format')\n",
            "def image_data_format():\n",
            "  \"\"\"Returns the default image data format convention.\n",
            "\n",
            "  Returns:\n",
            "      A string, either `'channels_first'` or `'channels_last'`\n",
            "\n",
            "  Example:\n",
            "  ```python\n",
            "  keras.backend.image_data_format() >>> 'channels_first'\n",
            "  ```\n",
            "  \"\"\"\n",
            "  return _IMAGE_DATA_FORMAT\n",
            "\n",
            "\n",
            "@keras_export('keras.backend.set_image_data_format')\n",
            "def set_image_data_format(data_format):\n",
            "  \"\"\"Sets the value of the image data format convention.\n",
            "\n",
            "  Arguments:\n",
            "      data_format: string. `'channels_first'` or `'channels_last'`.\n",
            "  Example: ```python from keras import backend as K K.image_data_format() >>>\n",
            "    'channels_first' K.set_image_data_format('channels_last')\n",
            "    K.image_data_format() >>> 'channels_last' ```\n",
            "\n",
            "  Raises:\n",
            "      ValueError: In case of invalid `data_format` value.\n",
            "  \"\"\"\n",
            "  global _IMAGE_DATA_FORMAT\n",
            "  if data_format not in {'channels_last', 'channels_first'}:\n",
            "    raise ValueError('Unknown data_format: ' + str(data_format))\n",
            "  _IMAGE_DATA_FORMAT = str(data_format)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=invalid-name\n",
            "\"\"\"Constraints: functions that impose constraints on weight values.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.Constraint')\n",
            "class Constraint(object):\n",
            "\n",
            "  def __call__(self, w):\n",
            "    return w\n",
            "\n",
            "  def get_config(self):\n",
            "    return {}\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.MaxNorm', 'keras.constraints.max_norm')\n",
            "class MaxNorm(Constraint):\n",
            "  \"\"\"MaxNorm weight constraint.\n",
            "\n",
            "  Constrains the weights incident to each hidden unit\n",
            "  to have a norm less than or equal to a desired value.\n",
            "\n",
            "  Arguments:\n",
            "      m: the maximum norm for the incoming weights.\n",
            "      axis: integer, axis along which to calculate weight norms.\n",
            "          For instance, in a `Dense` layer the weight matrix\n",
            "          has shape `(input_dim, output_dim)`,\n",
            "          set `axis` to `0` to constrain each weight vector\n",
            "          of length `(input_dim,)`.\n",
            "          In a `Conv2D` layer with `data_format=\"channels_last\"`,\n",
            "          the weight tensor has shape\n",
            "          `(rows, cols, input_depth, output_depth)`,\n",
            "          set `axis` to `[0, 1, 2]`\n",
            "          to constrain the weights of each filter tensor of size\n",
            "          `(rows, cols, input_depth)`.\n",
            "\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, max_value=2, axis=0):\n",
            "    self.max_value = max_value\n",
            "    self.axis = axis\n",
            "\n",
            "  def __call__(self, w):\n",
            "    norms = K.sqrt(\n",
            "        math_ops.reduce_sum(math_ops.square(w), axis=self.axis, keepdims=True))\n",
            "    desired = K.clip(norms, 0, self.max_value)\n",
            "    return w * (desired / (K.epsilon() + norms))\n",
            "\n",
            "  def get_config(self):\n",
            "    return {'max_value': self.max_value, 'axis': self.axis}\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.NonNeg', 'keras.constraints.non_neg')\n",
            "class NonNeg(Constraint):\n",
            "  \"\"\"Constrains the weights to be non-negative.\n",
            "  \"\"\"\n",
            "\n",
            "  def __call__(self, w):\n",
            "    return w * math_ops.cast(math_ops.greater_equal(w, 0.), K.floatx())\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.UnitNorm', 'keras.constraints.unit_norm')\n",
            "class UnitNorm(Constraint):\n",
            "  \"\"\"Constrains the weights incident to each hidden unit to have unit norm.\n",
            "\n",
            "  Arguments:\n",
            "      axis: integer, axis along which to calculate weight norms.\n",
            "          For instance, in a `Dense` layer the weight matrix\n",
            "          has shape `(input_dim, output_dim)`,\n",
            "          set `axis` to `0` to constrain each weight vector\n",
            "          of length `(input_dim,)`.\n",
            "          In a `Conv2D` layer with `data_format=\"channels_last\"`,\n",
            "          the weight tensor has shape\n",
            "          `(rows, cols, input_depth, output_depth)`,\n",
            "          set `axis` to `[0, 1, 2]`\n",
            "          to constrain the weights of each filter tensor of size\n",
            "          `(rows, cols, input_depth)`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, axis=0):\n",
            "    self.axis = axis\n",
            "\n",
            "  def __call__(self, w):\n",
            "    return w / (\n",
            "        K.epsilon() + K.sqrt(\n",
            "            math_ops.reduce_sum(\n",
            "                math_ops.square(w), axis=self.axis, keepdims=True)))\n",
            "\n",
            "  def get_config(self):\n",
            "    return {'axis': self.axis}\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.MinMaxNorm', 'keras.constraints.min_max_norm')\n",
            "class MinMaxNorm(Constraint):\n",
            "  \"\"\"MinMaxNorm weight constraint.\n",
            "\n",
            "  Constrains the weights incident to each hidden unit\n",
            "  to have the norm between a lower bound and an upper bound.\n",
            "\n",
            "  Arguments:\n",
            "      min_value: the minimum norm for the incoming weights.\n",
            "      max_value: the maximum norm for the incoming weights.\n",
            "      rate: rate for enforcing the constraint: weights will be\n",
            "          rescaled to yield\n",
            "          `(1 - rate) * norm + rate * norm.clip(min_value, max_value)`.\n",
            "          Effectively, this means that rate=1.0 stands for strict\n",
            "          enforcement of the constraint, while rate<1.0 means that\n",
            "          weights will be rescaled at each step to slowly move\n",
            "          towards a value inside the desired interval.\n",
            "      axis: integer, axis along which to calculate weight norms.\n",
            "          For instance, in a `Dense` layer the weight matrix\n",
            "          has shape `(input_dim, output_dim)`,\n",
            "          set `axis` to `0` to constrain each weight vector\n",
            "          of length `(input_dim,)`.\n",
            "          In a `Conv2D` layer with `data_format=\"channels_last\"`,\n",
            "          the weight tensor has shape\n",
            "          `(rows, cols, input_depth, output_depth)`,\n",
            "          set `axis` to `[0, 1, 2]`\n",
            "          to constrain the weights of each filter tensor of size\n",
            "          `(rows, cols, input_depth)`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, min_value=0.0, max_value=1.0, rate=1.0, axis=0):\n",
            "    self.min_value = min_value\n",
            "    self.max_value = max_value\n",
            "    self.rate = rate\n",
            "    self.axis = axis\n",
            "\n",
            "  def __call__(self, w):\n",
            "    norms = K.sqrt(\n",
            "        math_ops.reduce_sum(math_ops.square(w), axis=self.axis, keepdims=True))\n",
            "    desired = (\n",
            "        self.rate * K.clip(norms, self.min_value, self.max_value) +\n",
            "        (1 - self.rate) * norms)\n",
            "    return w * (desired / (K.epsilon() + norms))\n",
            "\n",
            "  def get_config(self):\n",
            "    return {\n",
            "        'min_value': self.min_value,\n",
            "        'max_value': self.max_value,\n",
            "        'rate': self.rate,\n",
            "        'axis': self.axis\n",
            "    }\n",
            "\n",
            "\n",
            "# Aliases.\n",
            "\n",
            "max_norm = MaxNorm\n",
            "non_neg = NonNeg\n",
            "unit_norm = UnitNorm\n",
            "min_max_norm = MinMaxNorm\n",
            "\n",
            "# Legacy aliases.\n",
            "maxnorm = max_norm\n",
            "nonneg = non_neg\n",
            "unitnorm = unit_norm\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.serialize')\n",
            "def serialize(constraint):\n",
            "  return serialize_keras_object(constraint)\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.deserialize')\n",
            "def deserialize(config, custom_objects=None):\n",
            "  return deserialize_keras_object(\n",
            "      config,\n",
            "      module_objects=globals(),\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='constraint')\n",
            "\n",
            "\n",
            "@keras_export('keras.constraints.get')\n",
            "def get(identifier):\n",
            "  if identifier is None:\n",
            "    return None\n",
            "  if isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    config = {'class_name': str(identifier), 'config': {}}\n",
            "    return deserialize(config)\n",
            "  elif callable(identifier):\n",
            "    return identifier\n",
            "  else:\n",
            "    raise ValueError('Could not interpret constraint identifier: ' +\n",
            "                     str(identifier))\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=invalid-name\n",
            "\"\"\"Built-in optimizer classes.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "from six.moves import zip  # pylint: disable=redefined-builtin\n",
            "\n",
            "from tensorflow.python.distribute import distribution_strategy_context\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.optimizer_v2 import adadelta as adadelta_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adagrad as adagrad_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import adamax as adamax_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import ftrl\n",
            "from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import nadam as nadam_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
            "from tensorflow.python.keras.optimizer_v2 import rmsprop as rmsprop_v2\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.ops import clip_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import state_ops\n",
            "from tensorflow.python.training import optimizer as tf_optimizer_module\n",
            "from tensorflow.python.training import training_util\n",
            "from tensorflow.python.training.tracking import base as trackable\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "class Optimizer(object):\n",
            "  \"\"\"Abstract optimizer base class.\n",
            "\n",
            "  Note: this is the parent class of all optimizers, not an actual optimizer\n",
            "  that can be used for training models.\n",
            "\n",
            "  All Keras optimizers support the following keyword arguments:\n",
            "\n",
            "      clipnorm: float >= 0. Gradients will be clipped\n",
            "          when their L2 norm exceeds this value.\n",
            "      clipvalue: float >= 0. Gradients will be clipped\n",
            "          when their absolute value exceeds this value.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, **kwargs):\n",
            "    allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
            "    for k in kwargs:\n",
            "      if k not in allowed_kwargs:\n",
            "        raise TypeError('Unexpected keyword argument '\n",
            "                        'passed to optimizer: ' + str(k))\n",
            "      # checks that clipnorm >= 0 and clipvalue >= 0\n",
            "      if kwargs[k] < 0:\n",
            "        raise ValueError('Expected {} >= 0, received: {}'.format(k, kwargs[k]))\n",
            "    self.__dict__.update(kwargs)\n",
            "    self.updates = []\n",
            "    self.weights = []\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    raise NotImplementedError\n",
            "\n",
            "  def get_gradients(self, loss, params):\n",
            "    \"\"\"Returns gradients of `loss` with respect to `params`.\n",
            "\n",
            "    Arguments:\n",
            "        loss: Loss tensor.\n",
            "        params: List of variables.\n",
            "\n",
            "    Returns:\n",
            "        List of gradient tensors.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
            "          function not implemented).\n",
            "    \"\"\"\n",
            "    grads = K.gradients(loss, params)\n",
            "    if None in grads:\n",
            "      raise ValueError('An operation has `None` for gradient. '\n",
            "                       'Please make sure that all of your ops have a '\n",
            "                       'gradient defined (i.e. are differentiable). '\n",
            "                       'Common ops without gradient: '\n",
            "                       'K.argmax, K.round, K.eval.')\n",
            "    if hasattr(self, 'clipnorm'):\n",
            "      grads = [clip_ops.clip_by_norm(g, self.clipnorm) for g in grads]\n",
            "    if hasattr(self, 'clipvalue'):\n",
            "      grads = [\n",
            "          clip_ops.clip_by_value(g, -self.clipvalue, self.clipvalue)\n",
            "          for g in grads\n",
            "      ]\n",
            "    return grads\n",
            "\n",
            "  def set_weights(self, weights):\n",
            "    \"\"\"Sets the weights of the optimizer, from Numpy arrays.\n",
            "\n",
            "    Should only be called after computing the gradients\n",
            "    (otherwise the optimizer has no weights).\n",
            "\n",
            "    Arguments:\n",
            "        weights: a list of Numpy arrays. The number of arrays and their shape\n",
            "          must match number of the dimensions of the weights of the optimizer\n",
            "          (i.e. it should match the output of `get_weights`).\n",
            "\n",
            "    Raises:\n",
            "        ValueError: in case of incompatible weight shapes.\n",
            "    \"\"\"\n",
            "    params = self.weights\n",
            "    if len(params) != len(weights):\n",
            "      raise ValueError('Length of the specified weight list (' +\n",
            "                       str(len(weights)) +\n",
            "                       ') does not match the number of weights '\n",
            "                       'of the optimizer (' + str(len(params)) + ')')\n",
            "    weight_value_tuples = []\n",
            "    param_values = K.batch_get_value(params)\n",
            "    for pv, p, w in zip(param_values, params, weights):\n",
            "      if pv.shape != w.shape:\n",
            "        raise ValueError('Optimizer weight shape ' + str(pv.shape) +\n",
            "                         ' not compatible with '\n",
            "                         'provided weight shape ' + str(w.shape))\n",
            "      weight_value_tuples.append((p, w))\n",
            "    K.batch_set_value(weight_value_tuples)\n",
            "\n",
            "  def get_weights(self):\n",
            "    \"\"\"Returns the current value of the weights of the optimizer.\n",
            "\n",
            "    Returns:\n",
            "        A list of numpy arrays.\n",
            "    \"\"\"\n",
            "    return K.batch_get_value(self.weights)\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {}\n",
            "    if hasattr(self, 'clipnorm'):\n",
            "      config['clipnorm'] = self.clipnorm\n",
            "    if hasattr(self, 'clipvalue'):\n",
            "      config['clipvalue'] = self.clipvalue\n",
            "    return config\n",
            "\n",
            "  @classmethod\n",
            "  def from_config(cls, config):\n",
            "    return cls(**config)\n",
            "\n",
            "\n",
            "class SGD(Optimizer):\n",
            "  \"\"\"Stochastic gradient descent optimizer.\n",
            "\n",
            "  Includes support for momentum,\n",
            "  learning rate decay, and Nesterov momentum.\n",
            "\n",
            "  Arguments:\n",
            "      lr: float >= 0. Learning rate.\n",
            "      momentum: float >= 0. Parameter that accelerates SGD in the relevant\n",
            "        direction and dampens oscillations.\n",
            "      decay: float >= 0. Learning rate decay over each update.\n",
            "      nesterov: boolean. Whether to apply Nesterov momentum.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, **kwargs):\n",
            "    super(SGD, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.momentum = K.variable(momentum, name='momentum')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "    self.initial_decay = decay\n",
            "    self.nesterov = nesterov\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "    # momentum\n",
            "    shapes = [K.int_shape(p) for p in params]\n",
            "    moments = [K.zeros(shape) for shape in shapes]\n",
            "    self.weights = [self.iterations] + moments\n",
            "    for p, g, m in zip(params, grads, moments):\n",
            "      v = self.momentum * m - lr * g  # velocity\n",
            "      self.updates.append(state_ops.assign(m, v))\n",
            "\n",
            "      if self.nesterov:\n",
            "        new_p = p + self.momentum * v - lr * g\n",
            "      else:\n",
            "        new_p = p + v\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'momentum': float(K.get_value(self.momentum)),\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'nesterov': self.nesterov\n",
            "    }\n",
            "    base_config = super(SGD, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class RMSprop(Optimizer):\n",
            "  \"\"\"RMSProp optimizer.\n",
            "\n",
            "  It is recommended to leave the parameters of this optimizer\n",
            "  at their default values\n",
            "  (except the learning rate, which can be freely tuned).\n",
            "\n",
            "  Arguments:\n",
            "      lr: float >= 0. Learning rate.\n",
            "      rho: float >= 0.\n",
            "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
            "      decay: float >= 0. Learning rate decay over each update.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0., **kwargs):\n",
            "    super(RMSprop, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.rho = K.variable(rho, name='rho')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.epsilon = epsilon\n",
            "    self.initial_decay = decay\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
            "    self.weights = accumulators\n",
            "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "\n",
            "    for p, g, a in zip(params, grads, accumulators):\n",
            "      # update accumulator\n",
            "      new_a = self.rho * a + (1. - self.rho) * math_ops.square(g)\n",
            "      self.updates.append(state_ops.assign(a, new_a))\n",
            "      new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'rho': float(K.get_value(self.rho)),\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'epsilon': self.epsilon\n",
            "    }\n",
            "    base_config = super(RMSprop, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class Adagrad(Optimizer):\n",
            "  \"\"\"Adagrad optimizer.\n",
            "\n",
            "  Adagrad is an optimizer with parameter-specific learning rates,\n",
            "  which are adapted relative to how frequently a parameter gets\n",
            "  updated during training. The more updates a parameter receives,\n",
            "  the smaller the updates.\n",
            "\n",
            "  It is recommended to leave the parameters of this optimizer\n",
            "  at their default values.\n",
            "\n",
            "  # Arguments\n",
            "      lr: float >= 0. Initial learning rate.\n",
            "      epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.\n",
            "      decay: float >= 0. Learning rate decay over each update.\n",
            "\n",
            "  # References\n",
            "      - [Adaptive Subgradient Methods for Online Learning and Stochastic\n",
            "      Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n",
            "    super(Adagrad, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.epsilon = epsilon\n",
            "    self.initial_decay = decay\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    shapes = [K.int_shape(p) for p in params]\n",
            "    accumulators = [K.zeros(shape) for shape in shapes]\n",
            "    self.weights = accumulators\n",
            "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "\n",
            "    for p, g, a in zip(params, grads, accumulators):\n",
            "      new_a = a + math_ops.square(g)  # update accumulator\n",
            "      self.updates.append(state_ops.assign(a, new_a))\n",
            "      new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'epsilon': self.epsilon\n",
            "    }\n",
            "    base_config = super(Adagrad, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class Adadelta(Optimizer):\n",
            "  \"\"\"Adadelta optimizer.\n",
            "\n",
            "  Adadelta is a more robust extension of Adagrad\n",
            "  that adapts learning rates based on a moving window of gradient updates,\n",
            "  instead of accumulating all past gradients. This way, Adadelta continues\n",
            "  learning even when many updates have been done. Compared to Adagrad, in the\n",
            "  original version of Adadelta you don't have to set an initial learning\n",
            "  rate. In this version, initial learning rate and decay factor can\n",
            "  be set, as in most other Keras optimizers.\n",
            "\n",
            "  It is recommended to leave the parameters of this optimizer\n",
            "  at their default values.\n",
            "\n",
            "  # Arguments\n",
            "      lr: float >= 0. Initial learning rate, defaults to 1.\n",
            "          It is recommended to leave it at the default value.\n",
            "      rho: float >= 0. Adadelta decay factor, corresponding to fraction of\n",
            "          gradient to keep at each time step.\n",
            "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
            "      decay: float >= 0. Initial learning rate decay.\n",
            "\n",
            "  # References\n",
            "      - [Adadelta - an adaptive learning rate\n",
            "      method](http://arxiv.org/abs/1212.5701)\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0., **kwargs):\n",
            "    super(Adadelta, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.rho = rho\n",
            "    self.epsilon = epsilon\n",
            "    self.initial_decay = decay\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    shapes = [K.int_shape(p) for p in params]\n",
            "    accumulators = [K.zeros(shape) for shape in shapes]\n",
            "    delta_accumulators = [K.zeros(shape) for shape in shapes]\n",
            "    self.weights = accumulators + delta_accumulators\n",
            "    self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "\n",
            "    for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n",
            "      # update accumulator\n",
            "      new_a = self.rho * a + (1. - self.rho) * math_ops.square(g)\n",
            "      self.updates.append(state_ops.assign(a, new_a))\n",
            "\n",
            "      # use the new accumulator and the *old* delta_accumulator\n",
            "      update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n",
            "      new_p = p - lr * update\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "\n",
            "      # update delta_accumulator\n",
            "      new_d_a = self.rho * d_a + (1 - self.rho) * math_ops.square(update)\n",
            "      self.updates.append(state_ops.assign(d_a, new_d_a))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'rho': self.rho,\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'epsilon': self.epsilon\n",
            "    }\n",
            "    base_config = super(Adadelta, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class Adam(Optimizer):\n",
            "  \"\"\"Adam optimizer.\n",
            "\n",
            "  Default parameters follow those provided in the original paper.\n",
            "\n",
            "  Arguments:\n",
            "      lr: float >= 0. Learning rate.\n",
            "      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
            "      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
            "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
            "      decay: float >= 0. Learning rate decay over each update.\n",
            "      amsgrad: boolean. Whether to apply the AMSGrad variant of this algorithm\n",
            "        from the paper \"On the Convergence of Adam and Beyond\".\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               lr=0.001,\n",
            "               beta_1=0.9,\n",
            "               beta_2=0.999,\n",
            "               epsilon=None,\n",
            "               decay=0.,\n",
            "               amsgrad=False,\n",
            "               **kwargs):\n",
            "    super(Adam, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.beta_1 = K.variable(beta_1, name='beta_1')\n",
            "      self.beta_2 = K.variable(beta_2, name='beta_2')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.epsilon = epsilon\n",
            "    self.initial_decay = decay\n",
            "    self.amsgrad = amsgrad\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    self.updates = []\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "\n",
            "    with ops.control_dependencies([state_ops.assign_add(self.iterations, 1)]):\n",
            "      t = math_ops.cast(self.iterations, K.floatx())\n",
            "    lr_t = lr * (\n",
            "        K.sqrt(1. - math_ops.pow(self.beta_2, t)) /\n",
            "        (1. - math_ops.pow(self.beta_1, t)))\n",
            "\n",
            "    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
            "    vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
            "    if self.amsgrad:\n",
            "      vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
            "    else:\n",
            "      vhats = [K.zeros(1) for _ in params]\n",
            "    self.weights = [self.iterations] + ms + vs + vhats\n",
            "\n",
            "    for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
            "      m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
            "      v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g)\n",
            "      if self.amsgrad:\n",
            "        vhat_t = math_ops.maximum(vhat, v_t)\n",
            "        p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
            "        self.updates.append(state_ops.assign(vhat, vhat_t))\n",
            "      else:\n",
            "        p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
            "\n",
            "      self.updates.append(state_ops.assign(m, m_t))\n",
            "      self.updates.append(state_ops.assign(v, v_t))\n",
            "      new_p = p_t\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'beta_1': float(K.get_value(self.beta_1)),\n",
            "        'beta_2': float(K.get_value(self.beta_2)),\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'epsilon': self.epsilon,\n",
            "        'amsgrad': self.amsgrad\n",
            "    }\n",
            "    base_config = super(Adam, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class Adamax(Optimizer):\n",
            "  \"\"\"Adamax optimizer from Adam paper's Section 7.\n",
            "\n",
            "  It is a variant of Adam based on the infinity norm.\n",
            "  Default parameters follow those provided in the paper.\n",
            "\n",
            "  Arguments:\n",
            "      lr: float >= 0. Learning rate.\n",
            "      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
            "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
            "      decay: float >= 0. Learning rate decay over each update.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               lr=0.002,\n",
            "               beta_1=0.9,\n",
            "               beta_2=0.999,\n",
            "               epsilon=None,\n",
            "               decay=0.,\n",
            "               **kwargs):\n",
            "    super(Adamax, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.beta_1 = K.variable(beta_1, name='beta_1')\n",
            "      self.beta_2 = K.variable(beta_2, name='beta_2')\n",
            "      self.decay = K.variable(decay, name='decay')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.epsilon = epsilon\n",
            "    self.initial_decay = decay\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    self.updates = []\n",
            "\n",
            "    lr = self.lr\n",
            "    if self.initial_decay > 0:\n",
            "      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
            "          1. /\n",
            "          (1. +\n",
            "           self.decay * math_ops.cast(self.iterations, K.dtype(self.decay))))\n",
            "\n",
            "    with ops.control_dependencies([state_ops.assign_add(self.iterations, 1)]):\n",
            "      t = math_ops.cast(self.iterations, K.floatx())\n",
            "    lr_t = lr / (1. - math_ops.pow(self.beta_1, t))\n",
            "\n",
            "    shapes = [K.int_shape(p) for p in params]\n",
            "    # zero init of 1st moment\n",
            "    ms = [K.zeros(shape) for shape in shapes]\n",
            "    # zero init of exponentially weighted infinity norm\n",
            "    us = [K.zeros(shape) for shape in shapes]\n",
            "    self.weights = [self.iterations] + ms + us\n",
            "\n",
            "    for p, g, m, u in zip(params, grads, ms, us):\n",
            "\n",
            "      m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
            "      u_t = math_ops.maximum(self.beta_2 * u, math_ops.abs(g))\n",
            "      p_t = p - lr_t * m_t / (u_t + self.epsilon)\n",
            "\n",
            "      self.updates.append(state_ops.assign(m, m_t))\n",
            "      self.updates.append(state_ops.assign(u, u_t))\n",
            "      new_p = p_t\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'beta_1': float(K.get_value(self.beta_1)),\n",
            "        'beta_2': float(K.get_value(self.beta_2)),\n",
            "        'decay': float(K.get_value(self.decay)),\n",
            "        'epsilon': self.epsilon\n",
            "    }\n",
            "    base_config = super(Adamax, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class Nadam(Optimizer):\n",
            "  \"\"\"Nesterov Adam optimizer.\n",
            "\n",
            "  Much like Adam is essentially RMSprop with momentum,\n",
            "  Nadam is Adam RMSprop with Nesterov momentum.\n",
            "\n",
            "  Default parameters follow those provided in the paper.\n",
            "  It is recommended to leave the parameters of this optimizer\n",
            "  at their default values.\n",
            "\n",
            "  Arguments:\n",
            "      lr: float >= 0. Learning rate.\n",
            "      beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
            "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               lr=0.002,\n",
            "               beta_1=0.9,\n",
            "               beta_2=0.999,\n",
            "               epsilon=None,\n",
            "               schedule_decay=0.004,\n",
            "               **kwargs):\n",
            "    super(Nadam, self).__init__(**kwargs)\n",
            "    with K.name_scope(self.__class__.__name__):\n",
            "      self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "      self.m_schedule = K.variable(1., name='m_schedule')\n",
            "      self.lr = K.variable(lr, name='lr')\n",
            "      self.beta_1 = K.variable(beta_1, name='beta_1')\n",
            "      self.beta_2 = K.variable(beta_2, name='beta_2')\n",
            "    if epsilon is None:\n",
            "      epsilon = K.epsilon()\n",
            "    self.epsilon = epsilon\n",
            "    self.schedule_decay = schedule_decay\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    grads = self.get_gradients(loss, params)\n",
            "    self.updates = []\n",
            "\n",
            "    with ops.control_dependencies([state_ops.assign_add(self.iterations, 1)]):\n",
            "      t = math_ops.cast(self.iterations, K.floatx())\n",
            "\n",
            "    # Due to the recommendations in [2], i.e. warming momentum schedule\n",
            "    momentum_cache_t = self.beta_1 * (\n",
            "        1. - 0.5 *\n",
            "        (math_ops.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n",
            "    momentum_cache_t_1 = self.beta_1 * (\n",
            "        1. - 0.5 *\n",
            "        (math_ops.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n",
            "    m_schedule_new = self.m_schedule * momentum_cache_t\n",
            "    m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n",
            "    self.updates.append((self.m_schedule, m_schedule_new))\n",
            "\n",
            "    shapes = [K.int_shape(p) for p in params]\n",
            "    ms = [K.zeros(shape) for shape in shapes]\n",
            "    vs = [K.zeros(shape) for shape in shapes]\n",
            "\n",
            "    self.weights = [self.iterations, self.m_schedule] + ms + vs\n",
            "\n",
            "    for p, g, m, v in zip(params, grads, ms, vs):\n",
            "      # the following equations given in [1]\n",
            "      g_prime = g / (1. - m_schedule_new)\n",
            "      m_t = self.beta_1 * m + (1. - self.beta_1) * g\n",
            "      m_t_prime = m_t / (1. - m_schedule_next)\n",
            "      v_t = self.beta_2 * v + (1. - self.beta_2) * math_ops.square(g)\n",
            "      v_t_prime = v_t / (1. - math_ops.pow(self.beta_2, t))\n",
            "      m_t_bar = (1. -\n",
            "                 momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n",
            "\n",
            "      self.updates.append(state_ops.assign(m, m_t))\n",
            "      self.updates.append(state_ops.assign(v, v_t))\n",
            "\n",
            "      p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n",
            "      new_p = p_t\n",
            "\n",
            "      # Apply constraints.\n",
            "      if getattr(p, 'constraint', None) is not None:\n",
            "        new_p = p.constraint(new_p)\n",
            "\n",
            "      self.updates.append(state_ops.assign(p, new_p))\n",
            "    return self.updates\n",
            "\n",
            "  def get_config(self):\n",
            "    config = {\n",
            "        'lr': float(K.get_value(self.lr)),\n",
            "        'beta_1': float(K.get_value(self.beta_1)),\n",
            "        'beta_2': float(K.get_value(self.beta_2)),\n",
            "        'epsilon': self.epsilon,\n",
            "        'schedule_decay': self.schedule_decay\n",
            "    }\n",
            "    base_config = super(Nadam, self).get_config()\n",
            "    return dict(list(base_config.items()) + list(config.items()))\n",
            "\n",
            "\n",
            "class TFOptimizer(Optimizer, trackable.Trackable):\n",
            "  \"\"\"Wrapper class for native TensorFlow optimizers.\"\"\"\n",
            "\n",
            "  def __init__(self, optimizer, iterations=None):  # pylint: disable=super-init-not-called\n",
            "    self.optimizer = optimizer\n",
            "    self._track_trackable(optimizer, name='optimizer')\n",
            "    if iterations is None:\n",
            "      with K.name_scope(self.__class__.__name__):\n",
            "        self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
            "    else:\n",
            "      self.iterations = iterations\n",
            "    self._track_trackable(self.iterations, name='global_step')\n",
            "\n",
            "  def apply_gradients(self, grads):\n",
            "    self.optimizer.apply_gradients(grads, global_step=self.iterations)\n",
            "\n",
            "  def get_grads(self, loss, params):\n",
            "    return self.optimizer.compute_gradients(loss, params)\n",
            "\n",
            "  def get_updates(self, loss, params):\n",
            "    if distribution_strategy_context.has_strategy():\n",
            "      self.updates = []\n",
            "\n",
            "      if not params:\n",
            "        # After the model vars have been created, the second call to get_updates\n",
            "        # is called with params as an empty list. This ensures that we call\n",
            "        # compute_gradients with params=None.\n",
            "        grads = self.optimizer.compute_gradients(loss)\n",
            "      else:\n",
            "        grads = self.optimizer.compute_gradients(loss, params)\n",
            "      global_step = training_util.get_global_step()\n",
            "      opt_update = self.optimizer.apply_gradients(grads, global_step)\n",
            "    else:\n",
            "      if not params:\n",
            "        self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
            "        return self.updates\n",
            "\n",
            "      # Updates list starts out empty because the iterations variable is\n",
            "      # incremented in optimizer.apply_gradients()\n",
            "      self.updates = []\n",
            "      grads = self.optimizer.compute_gradients(loss, params)\n",
            "      opt_update = self.optimizer.apply_gradients(\n",
            "          grads, global_step=self.iterations)\n",
            "\n",
            "    self.updates.append(opt_update)\n",
            "    return self.updates\n",
            "\n",
            "  @property\n",
            "  def weights(self):\n",
            "    raise NotImplementedError\n",
            "\n",
            "  def get_config(self):\n",
            "    raise NotImplementedError\n",
            "\n",
            "  def from_config(self, config):\n",
            "    raise NotImplementedError\n",
            "\n",
            "\n",
            "# Aliases.\n",
            "\n",
            "sgd = SGD\n",
            "rmsprop = RMSprop\n",
            "adagrad = Adagrad\n",
            "adadelta = Adadelta\n",
            "adam = Adam\n",
            "adamax = Adamax\n",
            "nadam = Nadam\n",
            "\n",
            "\n",
            "@keras_export('keras.optimizers.serialize')\n",
            "def serialize(optimizer):\n",
            "  return serialize_keras_object(optimizer)\n",
            "\n",
            "\n",
            "@keras_export('keras.optimizers.deserialize')\n",
            "def deserialize(config, custom_objects=None):\n",
            "  \"\"\"Inverse of the `serialize` function.\n",
            "\n",
            "  Arguments:\n",
            "      config: Optimizer configuration dictionary.\n",
            "      custom_objects: Optional dictionary mapping names (strings) to custom\n",
            "        objects (classes and functions) to be considered during deserialization.\n",
            "\n",
            "  Returns:\n",
            "      A Keras Optimizer instance.\n",
            "  \"\"\"\n",
            "  all_classes = {\n",
            "      'adadelta': adadelta_v2.Adadelta,\n",
            "      'adagrad': adagrad_v2.Adagrad,\n",
            "      'adam': adam_v2.Adam,\n",
            "      'adamax': adamax_v2.Adamax,\n",
            "      'nadam': nadam_v2.Nadam,\n",
            "      'rmsprop': rmsprop_v2.RMSprop,\n",
            "      'sgd': gradient_descent_v2.SGD,\n",
            "      'ftrl': ftrl.Ftrl\n",
            "  }\n",
            "\n",
            "  # Make deserialization case-insensitive for built-in optimizers.\n",
            "  if config['class_name'].lower() in all_classes:\n",
            "    config['class_name'] = config['class_name'].lower()\n",
            "  return deserialize_keras_object(\n",
            "      config,\n",
            "      module_objects=all_classes,\n",
            "      custom_objects=custom_objects,\n",
            "      printable_module_name='optimizer')\n",
            "\n",
            "\n",
            "@keras_export('keras.optimizers.get')\n",
            "def get(identifier):\n",
            "  \"\"\"Retrieves a Keras Optimizer instance.\n",
            "\n",
            "  Arguments:\n",
            "      identifier: Optimizer identifier, one of\n",
            "          - String: name of an optimizer\n",
            "          - Dictionary: configuration dictionary. - Keras Optimizer instance (it\n",
            "            will be returned unchanged). - TensorFlow Optimizer instance (it\n",
            "            will be wrapped as a Keras Optimizer).\n",
            "\n",
            "  Returns:\n",
            "      A Keras Optimizer instance.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If `identifier` cannot be interpreted.\n",
            "  \"\"\"\n",
            "  if isinstance(identifier, (Optimizer, optimizer_v2.OptimizerV2)):\n",
            "    return identifier\n",
            "  # Wrap TF optimizer instances\n",
            "  elif isinstance(identifier, tf_optimizer_module.Optimizer):\n",
            "    opt = TFOptimizer(identifier)\n",
            "    K.track_tf_optimizer(opt)\n",
            "    return opt\n",
            "  elif isinstance(identifier, dict):\n",
            "    return deserialize(identifier)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    config = {'class_name': str(identifier), 'config': {}}\n",
            "    return deserialize(config)\n",
            "  else:\n",
            "    raise ValueError('Could not interpret optimizer identifier:', identifier)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Module for exporting TensorFlow ops under tf.keras.*.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.ops import init_ops\n",
            "from tensorflow.python.ops import init_ops_v2\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "# pylint: disable=bad-continuation\n",
            "keras_export(v1=[\"keras.initializers.Initializer\"])(\n",
            "    init_ops.Initializer)\n",
            "keras_export(v1=[\"keras.initializers.Zeros\", \"keras.initializers.zeros\"])(\n",
            "    init_ops.Zeros)\n",
            "keras_export(v1=[\"keras.initializers.Ones\", \"keras.initializers.ones\"])(\n",
            "    init_ops.Ones)\n",
            "keras_export(v1=[\"keras.initializers.Constant\", \"keras.initializers.constant\"])(\n",
            "    init_ops.Constant)\n",
            "keras_export(v1=[\"keras.initializers.VarianceScaling\"])(\n",
            "    init_ops.VarianceScaling)\n",
            "keras_export(v1=[\"keras.initializers.Orthogonal\",\n",
            "                 \"keras.initializers.orthogonal\"])(\n",
            "    init_ops.Orthogonal)\n",
            "keras_export(v1=[\"keras.initializers.Identity\",\n",
            "                 \"keras.initializers.identity\"])(\n",
            "    init_ops.Identity)\n",
            "keras_export(v1=[\"keras.initializers.glorot_uniform\"])(\n",
            "    init_ops.GlorotUniform)\n",
            "keras_export(v1=[\"keras.initializers.glorot_normal\"])(\n",
            "    init_ops.GlorotNormal)\n",
            "keras_export(v1=[\"keras.initializers.lecun_normal\"])(\n",
            "    init_ops.lecun_normal)\n",
            "keras_export(v1=[\"keras.initializers.lecun_uniform\"])(\n",
            "    init_ops.lecun_uniform)\n",
            "keras_export(v1=[\"keras.initializers.he_normal\"])(\n",
            "    init_ops.he_normal)\n",
            "keras_export(v1=[\"keras.initializers.he_uniform\"])(\n",
            "    init_ops.he_uniform)\n",
            "\n",
            "keras_export(\"keras.initializers.Initializer\", v1=[])(\n",
            "    init_ops_v2.Initializer)\n",
            "keras_export(\n",
            "    \"keras.initializers.Zeros\", \"keras.initializers.zeros\", v1=[])(\n",
            "        init_ops_v2.Zeros)\n",
            "keras_export(\n",
            "    \"keras.initializers.Ones\", \"keras.initializers.ones\", v1=[])(\n",
            "        init_ops_v2.Ones)\n",
            "keras_export(\n",
            "    \"keras.initializers.Constant\", \"keras.initializers.constant\", v1=[])(\n",
            "        init_ops_v2.Constant)\n",
            "keras_export(\"keras.initializers.VarianceScaling\", v1=[])(\n",
            "    init_ops_v2.VarianceScaling)\n",
            "keras_export(\n",
            "    \"keras.initializers.Orthogonal\", \"keras.initializers.orthogonal\", v1=[])(\n",
            "        init_ops_v2.Orthogonal)\n",
            "keras_export(\n",
            "    \"keras.initializers.Identity\", \"keras.initializers.identity\", v1=[])(\n",
            "        init_ops_v2.Identity)\n",
            "keras_export(\n",
            "    \"keras.initializers.GlorotUniform\",\n",
            "    \"keras.initializers.glorot_uniform\",\n",
            "    v1=[])(\n",
            "        init_ops_v2.GlorotUniform)\n",
            "keras_export(\n",
            "    \"keras.initializers.GlorotNormal\",\n",
            "    \"keras.initializers.glorot_normal\",\n",
            "    v1=[])(\n",
            "        init_ops_v2.GlorotNormal)\n",
            "keras_export(\"keras.initializers.lecun_normal\", v1=[])(\n",
            "    init_ops_v2.lecun_normal)\n",
            "keras_export(\"keras.initializers.lecun_uniform\", v1=[])(\n",
            "    init_ops_v2.lecun_uniform)\n",
            "keras_export(\"keras.initializers.he_normal\", v1=[])(\n",
            "    init_ops_v2.he_normal)\n",
            "keras_export(\"keras.initializers.he_uniform\", v1=[])(\n",
            "    init_ops_v2.he_uniform)\n",
            "keras_export(\"keras.initializers.RandomNormal\", v1=[])(\n",
            "    init_ops_v2.RandomNormal)\n",
            "keras_export(\"keras.initializers.RandomUniform\", v1=[])(\n",
            "    init_ops_v2.RandomUniform)\n",
            "keras_export(\"keras.initializers.TruncatedNormal\", v1=[])(\n",
            "    init_ops_v2.TruncatedNormal)\n",
            "# pylint: enable=bad-continuation\n",
            "\n",
            "keras_export(v1=[\"keras.backend.name_scope\"])(ops.name_scope)\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing\n",
            "Directories ['__pycache__']\n",
            "Files ['__init__.py', 'text.py', 'image.py', 'sequence.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras data preprocessing utils.\"\"\"\n",
            "# pylint: disable=g-import-not-at-top\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import keras_preprocessing\n",
            "\n",
            "from tensorflow.python.keras import backend\n",
            "from tensorflow.python.keras import utils\n",
            "\n",
            "# This exists for compatibility with prior version of keras_preprocessing.\n",
            "# TODO(fchollet): remove in the future.\n",
            "keras_preprocessing.set_keras_submodules(backend=backend, utils=utils)\n",
            "\n",
            "from tensorflow.python.keras.preprocessing import image\n",
            "from tensorflow.python.keras.preprocessing import sequence\n",
            "from tensorflow.python.keras.preprocessing import text\n",
            "\n",
            "del absolute_import\n",
            "del division\n",
            "del print_function\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities for text input preprocessing.\n",
            "\"\"\"\n",
            "# pylint: disable=invalid-name\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from keras_preprocessing import text\n",
            "\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "text_to_word_sequence = text.text_to_word_sequence\n",
            "one_hot = text.one_hot\n",
            "hashing_trick = text.hashing_trick\n",
            "Tokenizer = text.Tokenizer\n",
            "\n",
            "keras_export(\n",
            "    'keras.preprocessing.text.text_to_word_sequence')(text_to_word_sequence)\n",
            "keras_export('keras.preprocessing.text.one_hot')(one_hot)\n",
            "keras_export('keras.preprocessing.text.hashing_trick')(hashing_trick)\n",
            "keras_export('keras.preprocessing.text.Tokenizer')(Tokenizer)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=invalid-name\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Set of tools for real-time data augmentation on image data.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from keras_preprocessing import image\n",
            "try:\n",
            "  from scipy import linalg  # pylint: disable=unused-import\n",
            "  from scipy import ndimage  # pylint: disable=unused-import\n",
            "except ImportError:\n",
            "  pass\n",
            "\n",
            "from tensorflow.python.keras import backend\n",
            "from tensorflow.python.keras import utils\n",
            "from tensorflow.python.util import tf_inspect\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "random_rotation = image.random_rotation\n",
            "random_shift = image.random_shift\n",
            "random_shear = image.random_shear\n",
            "random_zoom = image.random_zoom\n",
            "apply_channel_shift = image.apply_channel_shift\n",
            "random_channel_shift = image.random_channel_shift\n",
            "apply_brightness_shift = image.apply_brightness_shift\n",
            "random_brightness = image.random_brightness\n",
            "apply_affine_transform = image.apply_affine_transform\n",
            "load_img = image.load_img\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.array_to_img')\n",
            "def array_to_img(x, data_format=None, scale=True, dtype=None):\n",
            "  \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n",
            "\n",
            "  Arguments:\n",
            "      x: Input Numpy array.\n",
            "      data_format: Image data format.\n",
            "          either \"channels_first\" or \"channels_last\".\n",
            "      scale: Whether to rescale image values\n",
            "          to be within `[0, 255]`.\n",
            "      dtype: Dtype to use.\n",
            "\n",
            "  Returns:\n",
            "      A PIL Image instance.\n",
            "\n",
            "  Raises:\n",
            "      ImportError: if PIL is not available.\n",
            "      ValueError: if invalid `x` or `data_format` is passed.\n",
            "  \"\"\"\n",
            "\n",
            "  if data_format is None:\n",
            "    data_format = backend.image_data_format()\n",
            "  kwargs = {}\n",
            "  if 'dtype' in tf_inspect.getfullargspec(image.array_to_img)[0]:\n",
            "    if dtype is None:\n",
            "      dtype = backend.floatx()\n",
            "    kwargs['dtype'] = dtype\n",
            "  return image.array_to_img(x, data_format=data_format, scale=scale, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.img_to_array')\n",
            "def img_to_array(img, data_format=None, dtype=None):\n",
            "  \"\"\"Converts a PIL Image instance to a Numpy array.\n",
            "\n",
            "  Arguments:\n",
            "      img: PIL Image instance.\n",
            "      data_format: Image data format,\n",
            "          either \"channels_first\" or \"channels_last\".\n",
            "      dtype: Dtype to use for the returned array.\n",
            "\n",
            "  Returns:\n",
            "      A 3D Numpy array.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: if invalid `img` or `data_format` is passed.\n",
            "  \"\"\"\n",
            "\n",
            "  if data_format is None:\n",
            "    data_format = backend.image_data_format()\n",
            "  kwargs = {}\n",
            "  if 'dtype' in tf_inspect.getfullargspec(image.img_to_array)[0]:\n",
            "    if dtype is None:\n",
            "      dtype = backend.floatx()\n",
            "    kwargs['dtype'] = dtype\n",
            "  return image.img_to_array(img, data_format=data_format, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.save_img')\n",
            "def save_img(path,\n",
            "             x,\n",
            "             data_format=None,\n",
            "             file_format=None,\n",
            "             scale=True,\n",
            "             **kwargs):\n",
            "  \"\"\"Saves an image stored as a Numpy array to a path or file object.\n",
            "\n",
            "  Arguments:\n",
            "      path: Path or file object.\n",
            "      x: Numpy array.\n",
            "      data_format: Image data format,\n",
            "          either \"channels_first\" or \"channels_last\".\n",
            "      file_format: Optional file format override. If omitted, the\n",
            "          format to use is determined from the filename extension.\n",
            "          If a file object was used instead of a filename, this\n",
            "          parameter should always be used.\n",
            "      scale: Whether to rescale image values to be within `[0, 255]`.\n",
            "      **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n",
            "  \"\"\"\n",
            "  if data_format is None:\n",
            "    data_format = backend.image_data_format()\n",
            "  image.save_img(path,\n",
            "                 x,\n",
            "                 data_format=data_format,\n",
            "                 file_format=file_format,\n",
            "                 scale=scale, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.Iterator')\n",
            "class Iterator(image.Iterator, utils.Sequence):\n",
            "  pass\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.DirectoryIterator')\n",
            "class DirectoryIterator(image.DirectoryIterator, Iterator):\n",
            "  \"\"\"Iterator capable of reading images from a directory on disk.\n",
            "\n",
            "  Arguments:\n",
            "      directory: Path to the directory to read images from.\n",
            "          Each subdirectory in this directory will be\n",
            "          considered to contain images from one class,\n",
            "          or alternatively you could specify class subdirectories\n",
            "          via the `classes` argument.\n",
            "      image_data_generator: Instance of `ImageDataGenerator`\n",
            "          to use for random transformations and normalization.\n",
            "      target_size: tuple of integers, dimensions to resize input images to.\n",
            "      color_mode: One of `\"rgb\"`, `\"rgba\"`, `\"grayscale\"`.\n",
            "          Color mode to read images.\n",
            "      classes: Optional list of strings, names of subdirectories\n",
            "          containing images from each class (e.g. `[\"dogs\", \"cats\"]`).\n",
            "          It will be computed automatically if not set.\n",
            "      class_mode: Mode for yielding the targets:\n",
            "          `\"binary\"`: binary targets (if there are only two classes),\n",
            "          `\"categorical\"`: categorical targets,\n",
            "          `\"sparse\"`: integer targets,\n",
            "          `\"input\"`: targets are images identical to input images (mainly\n",
            "              used to work with autoencoders),\n",
            "          `None`: no targets get yielded (only input images are yielded).\n",
            "      batch_size: Integer, size of a batch.\n",
            "      shuffle: Boolean, whether to shuffle the data between epochs.\n",
            "      seed: Random seed for data shuffling.\n",
            "      data_format: String, one of `channels_first`, `channels_last`.\n",
            "      save_to_dir: Optional directory where to save the pictures\n",
            "          being yielded, in a viewable format. This is useful\n",
            "          for visualizing the random transformations being\n",
            "          applied, for debugging purposes.\n",
            "      save_prefix: String prefix to use for saving sample\n",
            "          images (if `save_to_dir` is set).\n",
            "      save_format: Format to use for saving sample images\n",
            "          (if `save_to_dir` is set).\n",
            "      subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
            "          validation_split is set in ImageDataGenerator.\n",
            "      interpolation: Interpolation method used to resample the image if the\n",
            "          target size is different from that of the loaded image.\n",
            "          Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n",
            "          If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n",
            "          supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n",
            "          \"hamming\" are also supported. By default, \"nearest\" is used.\n",
            "      dtype: Dtype to use for generated arrays.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, directory, image_data_generator,\n",
            "               target_size=(256, 256),\n",
            "               color_mode='rgb',\n",
            "               classes=None,\n",
            "               class_mode='categorical',\n",
            "               batch_size=32,\n",
            "               shuffle=True,\n",
            "               seed=None,\n",
            "               data_format=None,\n",
            "               save_to_dir=None,\n",
            "               save_prefix='',\n",
            "               save_format='png',\n",
            "               follow_links=False,\n",
            "               subset=None,\n",
            "               interpolation='nearest',\n",
            "               dtype=None):\n",
            "    if data_format is None:\n",
            "      data_format = backend.image_data_format()\n",
            "    kwargs = {}\n",
            "    if 'dtype' in tf_inspect.getfullargspec(\n",
            "        image.ImageDataGenerator.__init__)[0]:\n",
            "      if dtype is None:\n",
            "        dtype = backend.floatx()\n",
            "      kwargs['dtype'] = dtype\n",
            "    super(DirectoryIterator, self).__init__(\n",
            "        directory, image_data_generator,\n",
            "        target_size=target_size,\n",
            "        color_mode=color_mode,\n",
            "        classes=classes,\n",
            "        class_mode=class_mode,\n",
            "        batch_size=batch_size,\n",
            "        shuffle=shuffle,\n",
            "        seed=seed,\n",
            "        data_format=data_format,\n",
            "        save_to_dir=save_to_dir,\n",
            "        save_prefix=save_prefix,\n",
            "        save_format=save_format,\n",
            "        follow_links=follow_links,\n",
            "        subset=subset,\n",
            "        interpolation=interpolation,\n",
            "        **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.NumpyArrayIterator')\n",
            "class NumpyArrayIterator(image.NumpyArrayIterator, Iterator):\n",
            "  \"\"\"Iterator yielding data from a Numpy array.\n",
            "\n",
            "  Arguments:\n",
            "      x: Numpy array of input data or tuple.\n",
            "          If tuple, the second elements is either\n",
            "          another numpy array or a list of numpy arrays,\n",
            "          each of which gets passed\n",
            "          through as an output without any modifications.\n",
            "      y: Numpy array of targets data.\n",
            "      image_data_generator: Instance of `ImageDataGenerator`\n",
            "          to use for random transformations and normalization.\n",
            "      batch_size: Integer, size of a batch.\n",
            "      shuffle: Boolean, whether to shuffle the data between epochs.\n",
            "      sample_weight: Numpy array of sample weights.\n",
            "      seed: Random seed for data shuffling.\n",
            "      data_format: String, one of `channels_first`, `channels_last`.\n",
            "      save_to_dir: Optional directory where to save the pictures\n",
            "          being yielded, in a viewable format. This is useful\n",
            "          for visualizing the random transformations being\n",
            "          applied, for debugging purposes.\n",
            "      save_prefix: String prefix to use for saving sample\n",
            "          images (if `save_to_dir` is set).\n",
            "      save_format: Format to use for saving sample images\n",
            "          (if `save_to_dir` is set).\n",
            "      subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
            "          validation_split is set in ImageDataGenerator.\n",
            "      dtype: Dtype to use for the generated arrays.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, x, y, image_data_generator,\n",
            "               batch_size=32,\n",
            "               shuffle=False,\n",
            "               sample_weight=None,\n",
            "               seed=None,\n",
            "               data_format=None,\n",
            "               save_to_dir=None,\n",
            "               save_prefix='',\n",
            "               save_format='png',\n",
            "               subset=None,\n",
            "               dtype=None):\n",
            "    if data_format is None:\n",
            "      data_format = backend.image_data_format()\n",
            "    kwargs = {}\n",
            "    if 'dtype' in tf_inspect.getfullargspec(\n",
            "        image.NumpyArrayIterator.__init__)[0]:\n",
            "      if dtype is None:\n",
            "        dtype = backend.floatx()\n",
            "      kwargs['dtype'] = dtype\n",
            "    super(NumpyArrayIterator, self).__init__(\n",
            "        x, y, image_data_generator,\n",
            "        batch_size=batch_size,\n",
            "        shuffle=shuffle,\n",
            "        sample_weight=sample_weight,\n",
            "        seed=seed,\n",
            "        data_format=data_format,\n",
            "        save_to_dir=save_to_dir,\n",
            "        save_prefix=save_prefix,\n",
            "        save_format=save_format,\n",
            "        subset=subset,\n",
            "        **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.image.ImageDataGenerator')\n",
            "class ImageDataGenerator(image.ImageDataGenerator):\n",
            "  \"\"\"Generate batches of tensor image data with real-time data augmentation.\n",
            "\n",
            "   The data will be looped over (in batches).\n",
            "\n",
            "  Arguments:\n",
            "      featurewise_center: Boolean.\n",
            "          Set input mean to 0 over the dataset, feature-wise.\n",
            "      samplewise_center: Boolean. Set each sample mean to 0.\n",
            "      featurewise_std_normalization: Boolean.\n",
            "          Divide inputs by std of the dataset, feature-wise.\n",
            "      samplewise_std_normalization: Boolean. Divide each input by its std.\n",
            "      zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
            "      zca_whitening: Boolean. Apply ZCA whitening.\n",
            "      rotation_range: Int. Degree range for random rotations.\n",
            "      width_shift_range: Float, 1-D array-like or int\n",
            "          - float: fraction of total width, if < 1, or pixels if >= 1.\n",
            "          - 1-D array-like: random elements from the array.\n",
            "          - int: integer number of pixels from interval\n",
            "              `(-width_shift_range, +width_shift_range)`\n",
            "          - With `width_shift_range=2` possible values\n",
            "              are integers `[-1, 0, +1]`,\n",
            "              same as with `width_shift_range=[-1, 0, +1]`,\n",
            "              while with `width_shift_range=1.0` possible values are floats\n",
            "              in the interval [-1.0, +1.0).\n",
            "      height_shift_range: Float, 1-D array-like or int\n",
            "          - float: fraction of total height, if < 1, or pixels if >= 1.\n",
            "          - 1-D array-like: random elements from the array.\n",
            "          - int: integer number of pixels from interval\n",
            "              `(-height_shift_range, +height_shift_range)`\n",
            "          - With `height_shift_range=2` possible values\n",
            "              are integers `[-1, 0, +1]`,\n",
            "              same as with `height_shift_range=[-1, 0, +1]`,\n",
            "              while with `height_shift_range=1.0` possible values are floats\n",
            "              in the interval [-1.0, +1.0).\n",
            "      brightness_range: Tuple or list of two floats. Range for picking\n",
            "          a brightness shift value from.\n",
            "      shear_range: Float. Shear Intensity\n",
            "          (Shear angle in counter-clockwise direction in degrees)\n",
            "      zoom_range: Float or [lower, upper]. Range for random zoom.\n",
            "          If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
            "      channel_shift_range: Float. Range for random channel shifts.\n",
            "      fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
            "          Default is 'nearest'.\n",
            "          Points outside the boundaries of the input are filled\n",
            "          according to the given mode:\n",
            "          - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
            "          - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
            "          - 'reflect':  abcddcba|abcd|dcbaabcd\n",
            "          - 'wrap':  abcdabcd|abcd|abcdabcd\n",
            "      cval: Float or Int.\n",
            "          Value used for points outside the boundaries\n",
            "          when `fill_mode = \"constant\"`.\n",
            "      horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
            "      vertical_flip: Boolean. Randomly flip inputs vertically.\n",
            "      rescale: rescaling factor. Defaults to None.\n",
            "          If None or 0, no rescaling is applied,\n",
            "          otherwise we multiply the data by the value provided\n",
            "          (after applying all other transformations).\n",
            "      preprocessing_function: function that will be implied on each input.\n",
            "          The function will run after the image is resized and augmented.\n",
            "          The function should take one argument:\n",
            "          one image (Numpy tensor with rank 3),\n",
            "          and should output a Numpy tensor with the same shape.\n",
            "      data_format: Image data format,\n",
            "          either \"channels_first\" or \"channels_last\".\n",
            "          \"channels_last\" mode means that the images should have shape\n",
            "          `(samples, height, width, channels)`,\n",
            "          \"channels_first\" mode means that the images should have shape\n",
            "          `(samples, channels, height, width)`.\n",
            "          It defaults to the `image_data_format` value found in your\n",
            "          Keras config file at `~/.keras/keras.json`.\n",
            "          If you never set it, then it will be \"channels_last\".\n",
            "      validation_split: Float. Fraction of images reserved for validation\n",
            "          (strictly between 0 and 1).\n",
            "      dtype: Dtype to use for the generated arrays.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  Example of using `.flow(x, y)`:\n",
            "\n",
            "  ```python\n",
            "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
            "  y_train = np_utils.to_categorical(y_train, num_classes)\n",
            "  y_test = np_utils.to_categorical(y_test, num_classes)\n",
            "  datagen = ImageDataGenerator(\n",
            "      featurewise_center=True,\n",
            "      featurewise_std_normalization=True,\n",
            "      rotation_range=20,\n",
            "      width_shift_range=0.2,\n",
            "      height_shift_range=0.2,\n",
            "      horizontal_flip=True)\n",
            "  # compute quantities required for featurewise normalization\n",
            "  # (std, mean, and principal components if ZCA whitening is applied)\n",
            "  datagen.fit(x_train)\n",
            "  # fits the model on batches with real-time data augmentation:\n",
            "  model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
            "                      steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
            "  # here's a more \"manual\" example\n",
            "  for e in range(epochs):\n",
            "      print('Epoch', e)\n",
            "      batches = 0\n",
            "      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
            "          model.fit(x_batch, y_batch)\n",
            "          batches += 1\n",
            "          if batches >= len(x_train) / 32:\n",
            "              # we need to break the loop by hand because\n",
            "              # the generator loops indefinitely\n",
            "              break\n",
            "  ```\n",
            "\n",
            "  Example of using `.flow_from_directory(directory)`:\n",
            "\n",
            "  ```python\n",
            "  train_datagen = ImageDataGenerator(\n",
            "          rescale=1./255,\n",
            "          shear_range=0.2,\n",
            "          zoom_range=0.2,\n",
            "          horizontal_flip=True)\n",
            "  test_datagen = ImageDataGenerator(rescale=1./255)\n",
            "  train_generator = train_datagen.flow_from_directory(\n",
            "          'data/train',\n",
            "          target_size=(150, 150),\n",
            "          batch_size=32,\n",
            "          class_mode='binary')\n",
            "  validation_generator = test_datagen.flow_from_directory(\n",
            "          'data/validation',\n",
            "          target_size=(150, 150),\n",
            "          batch_size=32,\n",
            "          class_mode='binary')\n",
            "  model.fit_generator(\n",
            "          train_generator,\n",
            "          steps_per_epoch=2000,\n",
            "          epochs=50,\n",
            "          validation_data=validation_generator,\n",
            "          validation_steps=800)\n",
            "  ```\n",
            "\n",
            "  Example of transforming images and masks together.\n",
            "\n",
            "  ```python\n",
            "  # we create two instances with the same arguments\n",
            "  data_gen_args = dict(featurewise_center=True,\n",
            "                       featurewise_std_normalization=True,\n",
            "                       rotation_range=90,\n",
            "                       width_shift_range=0.1,\n",
            "                       height_shift_range=0.1,\n",
            "                       zoom_range=0.2)\n",
            "  image_datagen = ImageDataGenerator(**data_gen_args)\n",
            "  mask_datagen = ImageDataGenerator(**data_gen_args)\n",
            "  # Provide the same seed and keyword arguments to the fit and flow methods\n",
            "  seed = 1\n",
            "  image_datagen.fit(images, augment=True, seed=seed)\n",
            "  mask_datagen.fit(masks, augment=True, seed=seed)\n",
            "  image_generator = image_datagen.flow_from_directory(\n",
            "      'data/images',\n",
            "      class_mode=None,\n",
            "      seed=seed)\n",
            "  mask_generator = mask_datagen.flow_from_directory(\n",
            "      'data/masks',\n",
            "      class_mode=None,\n",
            "      seed=seed)\n",
            "  # combine generators into one which yields image and masks\n",
            "  train_generator = zip(image_generator, mask_generator)\n",
            "  model.fit_generator(\n",
            "      train_generator,\n",
            "      steps_per_epoch=2000,\n",
            "      epochs=50)\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self,\n",
            "               featurewise_center=False,\n",
            "               samplewise_center=False,\n",
            "               featurewise_std_normalization=False,\n",
            "               samplewise_std_normalization=False,\n",
            "               zca_whitening=False,\n",
            "               zca_epsilon=1e-6,\n",
            "               rotation_range=0,\n",
            "               width_shift_range=0.,\n",
            "               height_shift_range=0.,\n",
            "               brightness_range=None,\n",
            "               shear_range=0.,\n",
            "               zoom_range=0.,\n",
            "               channel_shift_range=0.,\n",
            "               fill_mode='nearest',\n",
            "               cval=0.,\n",
            "               horizontal_flip=False,\n",
            "               vertical_flip=False,\n",
            "               rescale=None,\n",
            "               preprocessing_function=None,\n",
            "               data_format=None,\n",
            "               validation_split=0.0,\n",
            "               dtype=None):\n",
            "    if data_format is None:\n",
            "      data_format = backend.image_data_format()\n",
            "    kwargs = {}\n",
            "    if 'dtype' in tf_inspect.getfullargspec(\n",
            "        image.ImageDataGenerator.__init__)[0]:\n",
            "      if dtype is None:\n",
            "        dtype = backend.floatx()\n",
            "      kwargs['dtype'] = dtype\n",
            "    super(ImageDataGenerator, self).__init__(\n",
            "        featurewise_center=featurewise_center,\n",
            "        samplewise_center=samplewise_center,\n",
            "        featurewise_std_normalization=featurewise_std_normalization,\n",
            "        samplewise_std_normalization=samplewise_std_normalization,\n",
            "        zca_whitening=zca_whitening,\n",
            "        zca_epsilon=zca_epsilon,\n",
            "        rotation_range=rotation_range,\n",
            "        width_shift_range=width_shift_range,\n",
            "        height_shift_range=height_shift_range,\n",
            "        brightness_range=brightness_range,\n",
            "        shear_range=shear_range,\n",
            "        zoom_range=zoom_range,\n",
            "        channel_shift_range=channel_shift_range,\n",
            "        fill_mode=fill_mode,\n",
            "        cval=cval,\n",
            "        horizontal_flip=horizontal_flip,\n",
            "        vertical_flip=vertical_flip,\n",
            "        rescale=rescale,\n",
            "        preprocessing_function=preprocessing_function,\n",
            "        data_format=data_format,\n",
            "        validation_split=validation_split,\n",
            "        **kwargs)\n",
            "\n",
            "keras_export('keras.preprocessing.image.random_rotation')(random_rotation)\n",
            "keras_export('keras.preprocessing.image.random_shift')(random_shift)\n",
            "keras_export('keras.preprocessing.image.random_shear')(random_shear)\n",
            "keras_export('keras.preprocessing.image.random_zoom')(random_zoom)\n",
            "keras_export(\n",
            "    'keras.preprocessing.image.apply_channel_shift')(apply_channel_shift)\n",
            "keras_export(\n",
            "    'keras.preprocessing.image.random_channel_shift')(random_channel_shift)\n",
            "keras_export(\n",
            "    'keras.preprocessing.image.apply_brightness_shift')(apply_brightness_shift)\n",
            "keras_export('keras.preprocessing.image.random_brightness')(random_brightness)\n",
            "keras_export(\n",
            "    'keras.preprocessing.image.apply_affine_transform')(apply_affine_transform)\n",
            "keras_export('keras.preprocessing.image.load_img')(load_img)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities for preprocessing sequence data.\n",
            "\"\"\"\n",
            "# pylint: disable=invalid-name\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from keras_preprocessing import sequence\n",
            "\n",
            "from tensorflow.python.keras import utils\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "pad_sequences = sequence.pad_sequences\n",
            "make_sampling_table = sequence.make_sampling_table\n",
            "skipgrams = sequence.skipgrams\n",
            "# TODO(fchollet): consider making `_remove_long_seq` public.\n",
            "_remove_long_seq = sequence._remove_long_seq  # pylint: disable=protected-access\n",
            "\n",
            "\n",
            "@keras_export('keras.preprocessing.sequence.TimeseriesGenerator')\n",
            "class TimeseriesGenerator(sequence.TimeseriesGenerator, utils.Sequence):\n",
            "  \"\"\"Utility class for generating batches of temporal data.\n",
            "  This class takes in a sequence of data-points gathered at\n",
            "  equal intervals, along with time series parameters such as\n",
            "  stride, length of history, etc., to produce batches for\n",
            "  training/validation.\n",
            "  # Arguments\n",
            "      data: Indexable generator (such as list or Numpy array)\n",
            "          containing consecutive data points (timesteps).\n",
            "          The data should be at 2D, and axis 0 is expected\n",
            "          to be the time dimension.\n",
            "      targets: Targets corresponding to timesteps in `data`.\n",
            "          It should have same length as `data`.\n",
            "      length: Length of the output sequences (in number of timesteps).\n",
            "      sampling_rate: Period between successive individual timesteps\n",
            "          within sequences. For rate `r`, timesteps\n",
            "          `data[i]`, `data[i-r]`, ... `data[i - length]`\n",
            "          are used for create a sample sequence.\n",
            "      stride: Period between successive output sequences.\n",
            "          For stride `s`, consecutive output samples would\n",
            "          be centered around `data[i]`, `data[i+s]`, `data[i+2*s]`, etc.\n",
            "      start_index: Data points earlier than `start_index` will not be used\n",
            "          in the output sequences. This is useful to reserve part of the\n",
            "          data for test or validation.\n",
            "      end_index: Data points later than `end_index` will not be used\n",
            "          in the output sequences. This is useful to reserve part of the\n",
            "          data for test or validation.\n",
            "      shuffle: Whether to shuffle output samples,\n",
            "          or instead draw them in chronological order.\n",
            "      reverse: Boolean: if `true`, timesteps in each output sample will be\n",
            "          in reverse chronological order.\n",
            "      batch_size: Number of timeseries samples in each batch\n",
            "          (except maybe the last one).\n",
            "  # Returns\n",
            "      A [Sequence](/utils/#sequence) instance.\n",
            "  # Examples\n",
            "  ```python\n",
            "  from keras.preprocessing.sequence import TimeseriesGenerator\n",
            "  import numpy as np\n",
            "  data = np.array([[i] for i in range(50)])\n",
            "  targets = np.array([[i] for i in range(50)])\n",
            "  data_gen = TimeseriesGenerator(data, targets,\n",
            "                                 length=10, sampling_rate=2,\n",
            "                                 batch_size=2)\n",
            "  assert len(data_gen) == 20\n",
            "  batch_0 = data_gen[0]\n",
            "  x, y = batch_0\n",
            "  assert np.array_equal(x,\n",
            "                        np.array([[[0], [2], [4], [6], [8]],\n",
            "                                  [[1], [3], [5], [7], [9]]]))\n",
            "  assert np.array_equal(y,\n",
            "                        np.array([[10], [11]]))\n",
            "  ```\n",
            "  \"\"\"\n",
            "  pass\n",
            "\n",
            "\n",
            "keras_export('keras.preprocessing.sequence.pad_sequences')(pad_sequences)\n",
            "keras_export(\n",
            "    'keras.preprocessing.sequence.make_sampling_table')(make_sampling_table)\n",
            "keras_export('keras.preprocessing.sequence.skipgrams')(skipgrams)\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/preprocessing/__pycache__\n",
            "Directories []\n",
            "Files ['image.cpython-36.pyc', 'sequence.cpython-36.pyc', '__init__.cpython-36.pyc', 'text.cpython-36.pyc']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xdd in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils\n",
            "Directories ['__pycache__']\n",
            "Files ['__init__.py', 'layer_utils.py', 'tf_utils.py', 'vis_utils.py', 'generic_utils.py', 'data_utils.py', 'metrics_utils.py', 'multi_gpu_utils.py', 'losses_utils.py', 'conv_utils.py', 'io_utils.py', 'np_utils.py', 'kernelized_utils.py', 'mode_keys.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras utilities.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.keras.utils.data_utils import GeneratorEnqueuer\n",
            "from tensorflow.python.keras.utils.data_utils import get_file\n",
            "from tensorflow.python.keras.utils.data_utils import OrderedEnqueuer\n",
            "from tensorflow.python.keras.utils.data_utils import Sequence\n",
            "from tensorflow.python.keras.utils.data_utils import SequenceEnqueuer\n",
            "from tensorflow.python.keras.utils.generic_utils import class_and_config_for_serialized_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import custom_object_scope\n",
            "from tensorflow.python.keras.utils.generic_utils import CustomObjectScope\n",
            "from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\n",
            "from tensorflow.python.keras.utils.generic_utils import get_custom_objects\n",
            "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_class_and_config\n",
            "from tensorflow.python.keras.utils.generic_utils import serialize_keras_object\n",
            "from tensorflow.python.keras.utils.io_utils import HDF5Matrix\n",
            "from tensorflow.python.keras.utils.layer_utils import convert_all_kernels_in_model\n",
            "from tensorflow.python.keras.utils.layer_utils import get_source_inputs\n",
            "from tensorflow.python.keras.utils.layer_utils import print_summary\n",
            "from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions\n",
            "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
            "from tensorflow.python.keras.utils.np_utils import normalize\n",
            "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
            "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
            "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
            "\n",
            "del absolute_import\n",
            "del division\n",
            "del print_function\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "\"\"\"Utilities related to layer/model functionality.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.utils.conv_utils import convert_kernel\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.get_source_inputs')\n",
            "def get_source_inputs(tensor, layer=None, node_index=None):\n",
            "  \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n",
            "\n",
            "  Output will always be a list of tensors\n",
            "  (potentially with 1 element).\n",
            "\n",
            "  Arguments:\n",
            "      tensor: The tensor to start from.\n",
            "      layer: Origin layer of the tensor. Will be\n",
            "          determined via tensor._keras_history if not provided.\n",
            "      node_index: Origin node index of the tensor.\n",
            "\n",
            "  Returns:\n",
            "      List of input tensors.\n",
            "  \"\"\"\n",
            "  if not hasattr(tensor, '_keras_history'):\n",
            "    return tensor\n",
            "\n",
            "  if layer is None or node_index:\n",
            "    layer, node_index, _ = tensor._keras_history\n",
            "  if not layer._inbound_nodes:\n",
            "    return [tensor]\n",
            "  else:\n",
            "    node = layer._inbound_nodes[node_index]\n",
            "    if not node.inbound_layers:\n",
            "      # Reached an Input layer, stop recursion.\n",
            "      return nest.flatten(node.input_tensors)\n",
            "    else:\n",
            "      source_tensors = []\n",
            "      for layer, node_index, _, tensor in node.iterate_inbound():\n",
            "        previous_sources = get_source_inputs(tensor, layer, node_index)\n",
            "        # Avoid input redundancy.\n",
            "        for x in previous_sources:\n",
            "          if x not in source_tensors:\n",
            "            source_tensors.append(x)\n",
            "      return source_tensors\n",
            "\n",
            "\n",
            "def count_params(weights):\n",
            "  \"\"\"Count the total number of scalars composing the weights.\n",
            "\n",
            "  Arguments:\n",
            "      weights: An iterable containing the weights on which to compute params\n",
            "\n",
            "  Returns:\n",
            "      The total number of scalars composing the weights\n",
            "  \"\"\"\n",
            "  return int(sum(np.prod(p.shape.as_list()) for p in set(weights)))\n",
            "\n",
            "\n",
            "def print_summary(model, line_length=None, positions=None, print_fn=None):\n",
            "  \"\"\"Prints a summary of a model.\n",
            "\n",
            "  Arguments:\n",
            "      model: Keras model instance.\n",
            "      line_length: Total length of printed lines\n",
            "          (e.g. set this to adapt the display to different\n",
            "          terminal window sizes).\n",
            "      positions: Relative or absolute positions of log elements in each line.\n",
            "          If not provided, defaults to `[.33, .55, .67, 1.]`.\n",
            "      print_fn: Print function to use.\n",
            "          It will be called on each line of the summary.\n",
            "          You can set it to a custom function\n",
            "          in order to capture the string summary.\n",
            "          It defaults to `print` (prints to stdout).\n",
            "  \"\"\"\n",
            "  if print_fn is None:\n",
            "    print_fn = print\n",
            "\n",
            "  if model.__class__.__name__ == 'Sequential':\n",
            "    sequential_like = True\n",
            "  elif not model._is_graph_network:\n",
            "    # We treat subclassed models as a simple sequence of layers, for logging\n",
            "    # purposes.\n",
            "    sequential_like = True\n",
            "  else:\n",
            "    sequential_like = True\n",
            "    nodes_by_depth = model._nodes_by_depth.values()\n",
            "    nodes = []\n",
            "    for v in nodes_by_depth:\n",
            "      if (len(v) > 1) or (len(v) == 1 and\n",
            "                          len(nest.flatten(v[0].inbound_layers)) > 1):\n",
            "        # if the model has multiple nodes\n",
            "        # or if the nodes have multiple inbound_layers\n",
            "        # the model is no longer sequential\n",
            "        sequential_like = False\n",
            "        break\n",
            "      nodes += v\n",
            "    if sequential_like:\n",
            "      # search for shared layers\n",
            "      for layer in model.layers:\n",
            "        flag = False\n",
            "        for node in layer._inbound_nodes:\n",
            "          if node in nodes:\n",
            "            if flag:\n",
            "              sequential_like = False\n",
            "              break\n",
            "            else:\n",
            "              flag = True\n",
            "        if not sequential_like:\n",
            "          break\n",
            "\n",
            "  if sequential_like:\n",
            "    line_length = line_length or 65\n",
            "    positions = positions or [.45, .85, 1.]\n",
            "    if positions[-1] <= 1:\n",
            "      positions = [int(line_length * p) for p in positions]\n",
            "    # header names for the different log elements\n",
            "    to_display = ['Layer (type)', 'Output Shape', 'Param #']\n",
            "  else:\n",
            "    line_length = line_length or 98\n",
            "    positions = positions or [.33, .55, .67, 1.]\n",
            "    if positions[-1] <= 1:\n",
            "      positions = [int(line_length * p) for p in positions]\n",
            "    # header names for the different log elements\n",
            "    to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Connected to']\n",
            "    relevant_nodes = []\n",
            "    for v in model._nodes_by_depth.values():\n",
            "      relevant_nodes += v\n",
            "\n",
            "  def print_row(fields, positions):\n",
            "    line = ''\n",
            "    for i in range(len(fields)):\n",
            "      if i > 0:\n",
            "        line = line[:-1] + ' '\n",
            "      line += str(fields[i])\n",
            "      line = line[:positions[i]]\n",
            "      line += ' ' * (positions[i] - len(line))\n",
            "    print_fn(line)\n",
            "\n",
            "  print_fn('Model: \"{}\"'.format(model.name))\n",
            "  print_fn('_' * line_length)\n",
            "  print_row(to_display, positions)\n",
            "  print_fn('=' * line_length)\n",
            "\n",
            "  def print_layer_summary(layer):\n",
            "    \"\"\"Prints a summary for a single layer.\n",
            "\n",
            "    Arguments:\n",
            "        layer: target layer.\n",
            "    \"\"\"\n",
            "    try:\n",
            "      output_shape = layer.output_shape\n",
            "    except AttributeError:\n",
            "      output_shape = 'multiple'\n",
            "    except RuntimeError:  # output_shape unknown in Eager mode.\n",
            "      output_shape = '?'\n",
            "    name = layer.name\n",
            "    cls_name = layer.__class__.__name__\n",
            "    fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]\n",
            "    print_row(fields, positions)\n",
            "\n",
            "  def print_layer_summary_with_connections(layer):\n",
            "    \"\"\"Prints a summary for a single layer (including topological connections).\n",
            "\n",
            "    Arguments:\n",
            "        layer: target layer.\n",
            "    \"\"\"\n",
            "    try:\n",
            "      output_shape = layer.output_shape\n",
            "    except AttributeError:\n",
            "      output_shape = 'multiple'\n",
            "    connections = []\n",
            "    for node in layer._inbound_nodes:\n",
            "      if relevant_nodes and node not in relevant_nodes:\n",
            "        # node is not part of the current network\n",
            "        continue\n",
            "\n",
            "      for inbound_layer, node_index, tensor_index, _ in node.iterate_inbound():\n",
            "        connections.append('{}[{}][{}]'.format(inbound_layer.name, node_index,\n",
            "                                               tensor_index))\n",
            "\n",
            "    name = layer.name\n",
            "    cls_name = layer.__class__.__name__\n",
            "    if not connections:\n",
            "      first_connection = ''\n",
            "    else:\n",
            "      first_connection = connections[0]\n",
            "    fields = [\n",
            "        name + ' (' + cls_name + ')', output_shape,\n",
            "        layer.count_params(), first_connection\n",
            "    ]\n",
            "    print_row(fields, positions)\n",
            "    if len(connections) > 1:\n",
            "      for i in range(1, len(connections)):\n",
            "        fields = ['', '', '', connections[i]]\n",
            "        print_row(fields, positions)\n",
            "\n",
            "  layers = model.layers\n",
            "  for i in range(len(layers)):\n",
            "    if sequential_like:\n",
            "      print_layer_summary(layers[i])\n",
            "    else:\n",
            "      print_layer_summary_with_connections(layers[i])\n",
            "    if i == len(layers) - 1:\n",
            "      print_fn('=' * line_length)\n",
            "    else:\n",
            "      print_fn('_' * line_length)\n",
            "\n",
            "  model._check_trainable_weights_consistency()\n",
            "  if hasattr(model, '_collected_trainable_weights'):\n",
            "    trainable_count = count_params(model._collected_trainable_weights)\n",
            "  else:\n",
            "    trainable_count = count_params(model.trainable_weights)\n",
            "\n",
            "  non_trainable_count = count_params(model.non_trainable_weights)\n",
            "\n",
            "  print_fn('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
            "  print_fn('Trainable params: {:,}'.format(trainable_count))\n",
            "  print_fn('Non-trainable params: {:,}'.format(non_trainable_count))\n",
            "  print_fn('_' * line_length)\n",
            "\n",
            "\n",
            "def gather_trainable_weights(trainable, sub_layers, extra_variables):\n",
            "  \"\"\"Lists the trainable weights for an object with sub-layers.\n",
            "\n",
            "  Args:\n",
            "    trainable: Whether the object collecting the variables is trainable.\n",
            "    sub_layers: A flat list of Layer objects owned by this object, to collect\n",
            "      variables from.\n",
            "    extra_variables: Any extra variables to include. Their `.trainable` property\n",
            "      is used to categorize them.\n",
            "\n",
            "  Returns:\n",
            "    A list of collected trainable weights/variables.\n",
            "  \"\"\"\n",
            "  if not trainable:\n",
            "    return []\n",
            "  weights = []\n",
            "  for layer in sub_layers:\n",
            "    weights += layer.trainable_weights\n",
            "  trainable_extra_variables = [\n",
            "      v for v in extra_variables if v.trainable]\n",
            "  return weights + trainable_extra_variables\n",
            "\n",
            "\n",
            "def gather_non_trainable_weights(trainable, sub_layers, extra_variables):\n",
            "  \"\"\"Lists the non-trainable weights for an object with sub-layers.\n",
            "\n",
            "  Args:\n",
            "    trainable: Whether the object collecting the variables is trainable.\n",
            "    sub_layers: A flat list of Layer objects owned by this object, to collect\n",
            "      variables from.\n",
            "    extra_variables: Any extra variables to include. Their `.trainable` property\n",
            "      is used to categorize them.\n",
            "\n",
            "  Returns:\n",
            "    A list of collected non-trainable weights/variables.\n",
            "  \"\"\"\n",
            "  trainable_extra_variables = []\n",
            "  non_trainable_extra_variables = []\n",
            "  for v in extra_variables:\n",
            "    if v.trainable:\n",
            "      trainable_extra_variables.append(v)\n",
            "    else:\n",
            "      non_trainable_extra_variables.append(v)\n",
            "  weights = []\n",
            "  for layer in sub_layers:\n",
            "    weights += layer.non_trainable_weights\n",
            "  if not trainable:\n",
            "    trainable_weights = []\n",
            "    for layer in sub_layers:\n",
            "      trainable_weights += layer.trainable_weights\n",
            "    return (trainable_weights + trainable_extra_variables\n",
            "            + weights + non_trainable_extra_variables)\n",
            "  return weights + non_trainable_extra_variables\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.convert_all_kernels_in_model')\n",
            "def convert_all_kernels_in_model(model):\n",
            "  \"\"\"Converts all convolution kernels in a model from Theano to TensorFlow.\n",
            "\n",
            "  Also works from TensorFlow to Theano.\n",
            "\n",
            "  Arguments:\n",
            "      model: target model for the conversion.\n",
            "  \"\"\"\n",
            "  # Note: SeparableConvolution not included\n",
            "  # since only supported by TF.\n",
            "  conv_classes = {\n",
            "      'Conv1D',\n",
            "      'Conv2D',\n",
            "      'Conv3D',\n",
            "      'Conv2DTranspose',\n",
            "  }\n",
            "  to_assign = []\n",
            "  for layer in model.layers:\n",
            "    if layer.__class__.__name__ in conv_classes:\n",
            "      original_kernel = K.get_value(layer.kernel)\n",
            "      converted_kernel = convert_kernel(original_kernel)\n",
            "      to_assign.append((layer.kernel, converted_kernel))\n",
            "  K.batch_set_value(to_assign)\n",
            "\n",
            "\n",
            "def convert_dense_weights_data_format(dense,\n",
            "                                      previous_feature_map_shape,\n",
            "                                      target_data_format='channels_first'):\n",
            "  \"\"\"Utility useful when changing a convnet's `data_format`.\n",
            "\n",
            "  When porting the weights of a convnet from one data format to the other,\n",
            "  if the convnet includes a `Flatten` layer\n",
            "  (applied to the last convolutional feature map)\n",
            "  followed by a `Dense` layer, the weights of that `Dense` layer\n",
            "  should be updated to reflect the new dimension ordering.\n",
            "\n",
            "  Arguments:\n",
            "      dense: The target `Dense` layer.\n",
            "      previous_feature_map_shape: A shape tuple of 3 integers,\n",
            "          e.g. `(512, 7, 7)`. The shape of the convolutional\n",
            "          feature map right before the `Flatten` layer that\n",
            "          came before the target `Dense` layer.\n",
            "      target_data_format: One of \"channels_last\", \"channels_first\".\n",
            "          Set it \"channels_last\"\n",
            "          if converting a \"channels_first\" model to \"channels_last\",\n",
            "          or reciprocally.\n",
            "  \"\"\"\n",
            "  assert target_data_format in {'channels_last', 'channels_first'}\n",
            "  kernel, bias = dense.get_weights()\n",
            "  for i in range(kernel.shape[1]):\n",
            "    if target_data_format == 'channels_first':\n",
            "      c, h, w = previous_feature_map_shape\n",
            "      original_fm_shape = (h, w, c)\n",
            "      ki = kernel[:, i].reshape(original_fm_shape)\n",
            "      ki = np.transpose(ki, (2, 0, 1))  # last -> first\n",
            "    else:\n",
            "      h, w, c = previous_feature_map_shape\n",
            "      original_fm_shape = (c, h, w)\n",
            "      ki = kernel[:, i].reshape(original_fm_shape)\n",
            "      ki = np.transpose(ki, (1, 2, 0))  # first -> last\n",
            "    kernel[:, i] = np.reshape(ki, (np.prod(previous_feature_map_shape),))\n",
            "  dense.set_weights([kernel, bias])\n",
            "\n",
            "\n",
            "def is_builtin_layer(layer):\n",
            "  if not getattr(layer, '_keras_api_names', None):\n",
            "    return False\n",
            "\n",
            "  # Subclasses of `Layer` that are not exported inherit the export name\n",
            "  # of the base layer class.\n",
            "  return (layer._keras_api_names != ('keras.layers.Layer',) and\n",
            "          layer._keras_api_names_v1 != ('keras.layers.Layer',))\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"TensorFlow-related utilities.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import six\n",
            "\n",
            "from tensorflow.python.eager import context\n",
            "from tensorflow.python.framework import composite_tensor\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.framework import smart_cond as smart_module\n",
            "from tensorflow.python.framework import tensor_shape\n",
            "from tensorflow.python.framework import tensor_util\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.ops import control_flow_ops\n",
            "from tensorflow.python.ops import variables\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util import tf_contextlib\n",
            "\n",
            "\n",
            "def smart_cond(pred, true_fn=None, false_fn=None, name=None):\n",
            "  \"\"\"Return either `true_fn()` if predicate `pred` is true else `false_fn()`.\n",
            "\n",
            "  If `pred` is a bool or has a constant value, we return either `true_fn()`\n",
            "  or `false_fn()`, otherwise we use `tf.cond` to dynamically route to both.\n",
            "\n",
            "  Arguments:\n",
            "    pred: A scalar determining whether to return the result of `true_fn` or\n",
            "      `false_fn`.\n",
            "    true_fn: The callable to be performed if pred is true.\n",
            "    false_fn: The callable to be performed if pred is false.\n",
            "    name: Optional name prefix when using `tf.cond`.\n",
            "\n",
            "  Returns:\n",
            "    Tensors returned by the call to either `true_fn` or `false_fn`.\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If `true_fn` or `false_fn` is not callable.\n",
            "  \"\"\"\n",
            "  if isinstance(pred, variables.Variable):\n",
            "    return control_flow_ops.cond(\n",
            "        pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
            "  return smart_module.smart_cond(\n",
            "      pred, true_fn=true_fn, false_fn=false_fn, name=name)\n",
            "\n",
            "\n",
            "def constant_value(pred):\n",
            "  \"\"\"Return the bool value for `pred`, or None if `pred` had a dynamic value.\n",
            "\n",
            "  Arguments:\n",
            "    pred: A scalar, either a Python bool or a TensorFlow boolean variable\n",
            "      or tensor, or the Python integer 1 or 0.\n",
            "\n",
            "  Returns:\n",
            "    True or False if `pred` has a constant boolean value, None otherwise.\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If `pred` is not a Variable, Tensor or bool, or Python\n",
            "      integer 1 or 0.\n",
            "  \"\"\"\n",
            "  # Allow integer booleans.\n",
            "  if isinstance(pred, int):\n",
            "    if pred == 1:\n",
            "      pred = True\n",
            "    elif pred == 0:\n",
            "      pred = False\n",
            "\n",
            "  if isinstance(pred, variables.Variable):\n",
            "    return None\n",
            "  return smart_module.smart_constant_value(pred)\n",
            "\n",
            "\n",
            "def is_tensor_or_tensor_list(v):\n",
            "  v = nest.flatten(v)\n",
            "  if v and isinstance(v[0], ops.Tensor):\n",
            "    return True\n",
            "  else:\n",
            "    return False\n",
            "\n",
            "\n",
            "def get_reachable_from_inputs(inputs, targets=None):\n",
            "  \"\"\"Returns the set of tensors/ops reachable from `inputs`.\n",
            "\n",
            "  Stops if all targets have been found (target is optional).\n",
            "\n",
            "  Only valid in Symbolic mode, not Eager mode.\n",
            "\n",
            "  Args:\n",
            "    inputs: List of tensors.\n",
            "    targets: List of tensors.\n",
            "\n",
            "  Returns:\n",
            "    A set of tensors reachable from the inputs (includes the inputs themselves).\n",
            "  \"\"\"\n",
            "  inputs = nest.flatten(inputs)\n",
            "  reachable = set(inputs)\n",
            "  if targets:\n",
            "    targets = set(targets)\n",
            "  queue = inputs[:]\n",
            "\n",
            "  while queue:\n",
            "    x = queue.pop()\n",
            "    if isinstance(x, tuple(_user_convertible_tensor_types)):\n",
            "      # Can't find consumers of user-specific types.\n",
            "      continue\n",
            "\n",
            "    if isinstance(x, ops.Operation):\n",
            "      outputs = x.outputs[:] or []\n",
            "      outputs += x._control_outputs  # pylint: disable=protected-access\n",
            "    elif isinstance(x, variables.Variable):\n",
            "      try:\n",
            "        outputs = [x.op]\n",
            "      except AttributeError:\n",
            "        # Variables can be created in an Eager context.\n",
            "        outputs = []\n",
            "    elif tensor_util.is_tensor(x):\n",
            "      outputs = x.consumers()\n",
            "    else:\n",
            "      raise TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))\n",
            "\n",
            "    for y in outputs:\n",
            "      if y not in reachable:\n",
            "        reachable.add(y)\n",
            "        queue.insert(0, y)\n",
            "\n",
            "    if targets and targets.issubset(reachable):\n",
            "      return reachable\n",
            "  return reachable\n",
            "\n",
            "\n",
            "# This function needs access to private functions of `nest`.\n",
            "#  pylint: disable=protected-access\n",
            "def map_structure_with_atomic(is_atomic_fn, map_fn, nested):\n",
            "  \"\"\"Maps the atomic elements of a nested structure.\n",
            "\n",
            "  Arguments:\n",
            "    is_atomic_fn: A function that determines if an element of `nested` is\n",
            "      atomic.\n",
            "    map_fn: The function to apply to atomic elements of `nested`.\n",
            "    nested: A nested structure.\n",
            "\n",
            "  Returns:\n",
            "    The nested structure, with atomic elements mapped according to `map_fn`.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: If an element that is neither atomic nor a sequence is\n",
            "      encountered.\n",
            "  \"\"\"\n",
            "  if is_atomic_fn(nested):\n",
            "    return map_fn(nested)\n",
            "\n",
            "  # Recursively convert.\n",
            "  if not nest.is_sequence(nested):\n",
            "    raise ValueError(\n",
            "        'Received non-atomic and non-sequence element: {}'.format(nested))\n",
            "  if nest._is_mapping(nested):\n",
            "    values = [nested[k] for k in nest._sorted(nested)]\n",
            "  else:\n",
            "    values = nested\n",
            "  mapped_values = [\n",
            "      map_structure_with_atomic(is_atomic_fn, map_fn, ele) for ele in values\n",
            "  ]\n",
            "  return nest._sequence_like(nested, mapped_values)\n",
            "\n",
            "\n",
            "#  pylint: enable=protected-access\n",
            "\n",
            "\n",
            "def convert_shapes(input_shape, to_tuples=True):\n",
            "  \"\"\"Converts nested shape representations to desired format.\n",
            "\n",
            "  Performs:\n",
            "\n",
            "  TensorShapes -> tuples if `to_tuples=True`.\n",
            "  tuples of int or None -> TensorShapes if `to_tuples=False`.\n",
            "\n",
            "  Valid objects to be converted are:\n",
            "  - TensorShapes\n",
            "  - tuples with elements of type int or None.\n",
            "  - ints\n",
            "  - None\n",
            "\n",
            "  Arguments:\n",
            "    input_shape: A nested structure of objects to be converted to TensorShapes.\n",
            "    to_tuples: If `True`, converts all TensorShape to tuples. Otherwise converts\n",
            "      all tuples representing shapes to TensorShapes.\n",
            "\n",
            "  Returns:\n",
            "    Nested structure of shapes in desired format.\n",
            "  \"\"\"\n",
            "\n",
            "  def _is_shape_component(value):\n",
            "    return value is None or isinstance(value, (int, tensor_shape.Dimension))\n",
            "\n",
            "  def _is_atomic_shape(input_shape):\n",
            "    # Ex: TensorShape or (None, 10, 32) or 5 or `None`\n",
            "    if _is_shape_component(input_shape):\n",
            "      return True\n",
            "    if isinstance(input_shape, tensor_shape.TensorShape):\n",
            "      return True\n",
            "    if (isinstance(input_shape, (tuple, list)) and\n",
            "        all(_is_shape_component(ele) for ele in input_shape)):\n",
            "      return True\n",
            "    return False\n",
            "\n",
            "  def _convert_shape(input_shape):\n",
            "    input_shape = tensor_shape.TensorShape(input_shape)\n",
            "    if to_tuples:\n",
            "      input_shape = tuple(input_shape.as_list())\n",
            "    return input_shape\n",
            "\n",
            "  return map_structure_with_atomic(_is_atomic_shape, _convert_shape,\n",
            "                                   input_shape)\n",
            "\n",
            "\n",
            "class ListWrapper(object):\n",
            "  \"\"\"A wrapper for lists to be treated as elements for `nest`.\"\"\"\n",
            "\n",
            "  def __init__(self, list_to_wrap):\n",
            "    self._list = list_to_wrap\n",
            "\n",
            "  def as_list(self):\n",
            "    return self._list\n",
            "\n",
            "\n",
            "def convert_inner_node_data(nested, wrap=False):\n",
            "  \"\"\"Either wraps or unwraps innermost node data lists in `ListWrapper` objects.\n",
            "\n",
            "  Arguments:\n",
            "    nested: A nested data structure.\n",
            "    wrap: If `True`, wrap innermost lists in `ListWrapper` objects. If `False`,\n",
            "      unwraps `ListWrapper` objects into lists.\n",
            "\n",
            "  Returns:\n",
            "    Structure of same type as nested, with lists wrapped/unwrapped.\n",
            "  \"\"\"\n",
            "\n",
            "  def _is_atomic_nested(nested):\n",
            "    \"\"\"Returns `True` if `nested` is a list representing node data.\"\"\"\n",
            "    if isinstance(nested, ListWrapper):\n",
            "      return True\n",
            "    # Node data can be of form `[layer_name, node_id, tensor_id]` or\n",
            "    # `[layer_name, node_id, tensor_id, kwargs]`.\n",
            "    if (isinstance(nested, list) and (len(nested) in [3, 4]) and\n",
            "        isinstance(nested[0], six.string_types)):\n",
            "      return True\n",
            "    return False\n",
            "\n",
            "  def _convert_object_or_list(nested):\n",
            "    \"\"\"Convert b/t `ListWrapper` object and list representations.\"\"\"\n",
            "    if wrap:\n",
            "      if isinstance(nested, ListWrapper):\n",
            "        return nested\n",
            "      return ListWrapper(nested)\n",
            "    else:\n",
            "      if isinstance(nested, ListWrapper):\n",
            "        return nested.as_list()\n",
            "      return nested\n",
            "\n",
            "  return map_structure_with_atomic(_is_atomic_nested, _convert_object_or_list,\n",
            "                                   nested)\n",
            "\n",
            "\n",
            "def shape_type_conversion(fn):\n",
            "  \"\"\"Decorator that handles tuple/TensorShape conversion.\n",
            "\n",
            "  Used in `compute_output_shape` and `build`.\n",
            "\n",
            "  Arguments:\n",
            "    fn: function to wrap.\n",
            "\n",
            "  Returns:\n",
            "    Wrapped function.\n",
            "  \"\"\"\n",
            "\n",
            "  def wrapper(instance, input_shape):\n",
            "    # Pass shapes as tuples to `fn`\n",
            "    # This preserves compatibility with external Keras.\n",
            "    if input_shape is not None:\n",
            "      input_shape = convert_shapes(input_shape, to_tuples=True)\n",
            "    output_shape = fn(instance, input_shape)\n",
            "    # Return shapes from `fn` as TensorShapes.\n",
            "    if output_shape is not None:\n",
            "      output_shape = convert_shapes(output_shape, to_tuples=False)\n",
            "    return output_shape\n",
            "\n",
            "  return wrapper\n",
            "\n",
            "\n",
            "def are_all_symbolic_tensors(tensors):\n",
            "  return all(is_symbolic_tensor(tensor) for tensor in tensors)\n",
            "\n",
            "\n",
            "_user_convertible_tensor_types = set()\n",
            "\n",
            "\n",
            "def is_symbolic_tensor(tensor):\n",
            "  \"\"\"Returns whether a tensor is symbolic (from a TF graph) or an eager tensor.\n",
            "\n",
            "  A Variable can be seen as either: it is considered symbolic\n",
            "  when we are in a graph scope, and eager when we are in an eager scope.\n",
            "\n",
            "  Arguments:\n",
            "    tensor: A tensor instance to test.\n",
            "\n",
            "  Returns:\n",
            "    True for symbolic tensors, False for eager tensors.\n",
            "  \"\"\"\n",
            "  if isinstance(tensor, tuple(_user_convertible_tensor_types)):\n",
            "    tensor = ops.convert_to_tensor_or_composite(tensor)\n",
            "  if isinstance(tensor, variables.Variable):\n",
            "    # Variables that are output of a Keras Layer in Functional API mode\n",
            "    # should be considered symbolic.\n",
            "    # TODO(omalleyt): We need a better way to check this in order to\n",
            "    # enable `run_eagerly=True` for Models containing Layers that\n",
            "    # return Variables as outputs.\n",
            "    return (getattr(tensor, '_keras_history', False) or\n",
            "            not context.executing_eagerly())\n",
            "  if isinstance(tensor, composite_tensor.CompositeTensor):\n",
            "    return tensor._is_graph_tensor  # pylint: disable=protected-access\n",
            "  if isinstance(tensor, ops.Tensor):\n",
            "    return hasattr(tensor, 'graph')\n",
            "  return False\n",
            "\n",
            "\n",
            "def register_symbolic_tensor_type(cls):\n",
            "  \"\"\"Allows users to specify types regarded as symbolic `Tensor`s.\n",
            "\n",
            "  Used in conjunction with `tf.register_tensor_conversion_function`, calling\n",
            "  `tf.keras.utils.register_symbolic_tensor_type(cls)` allows non-`Tensor`\n",
            "  objects to be plumbed through Keras layers.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "  # One-time setup.\n",
            "  class Foo(object):\n",
            "    def __init__(self, input_):\n",
            "      self._input = input_\n",
            "    def value(self):\n",
            "      return tf.constant(42.)\n",
            "\n",
            "  tf.register_tensor_conversion_function(\n",
            "      Foo, lambda x, *args, **kwargs: x.value())\n",
            "\n",
            "  tf.keras.utils.register_symbolic_tensor_type(Foo)\n",
            "\n",
            "  # User-land.\n",
            "  layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "    cls: A `class` type which shall be regarded as a symbolic `Tensor`.\n",
            "  \"\"\"\n",
            "  global _user_convertible_tensor_types\n",
            "  _user_convertible_tensor_types.add(cls)\n",
            "\n",
            "\n",
            "def is_tensor_or_variable(x):\n",
            "  return tensor_util.is_tensor(x) or isinstance(x, variables.Variable)\n",
            "\n",
            "\n",
            "def assert_no_legacy_layers(layers):\n",
            "  \"\"\"Prevent tf.layers.Layers from being used with Keras.\n",
            "\n",
            "  Certain legacy layers inherit from their keras analogs; however they are\n",
            "  not supported with keras and can lead to subtle and hard to diagnose bugs.\n",
            "\n",
            "  Args:\n",
            "    layers: A list of layers to check\n",
            "\n",
            "  Raises:\n",
            "    TypeError: If any elements of layers are tf.layers.Layers\n",
            "  \"\"\"\n",
            "\n",
            "  # isinstance check for tf.layers.Layer introduces a circular dependency.\n",
            "  legacy_layers = [l for l in layers if getattr(l, '_is_legacy_layer', None)]\n",
            "  if legacy_layers:\n",
            "    layer_str = '\\n'.join(['  ' + str(l) for l in legacy_layers])\n",
            "    raise TypeError(\n",
            "        'The following are legacy tf.layers.Layers:\\n{}\\nTo use keras as a '\n",
            "        'framework (for instance using the Network, Model, or Sequential '\n",
            "        'classes), please use the tf.keras.layers implementation instead. '\n",
            "        '(Or, if writing custom layers, subclass from tf.keras.layers rather '\n",
            "        'than tf.layers)'.format(layer_str))\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def maybe_init_scope(layer):\n",
            "  \"\"\"Open an `init_scope` if in V2 mode and using the keras graph.\n",
            "\n",
            "  Arguments:\n",
            "    layer: The Layer/Model that is currently active.\n",
            "\n",
            "  Yields:\n",
            "    None\n",
            "  \"\"\"\n",
            "  # Don't open an init_scope in V1 mode or when using legacy tf.layers.\n",
            "  if (ops.executing_eagerly_outside_functions() and\n",
            "      getattr(layer, '_keras_style', True)):\n",
            "    with ops.init_scope():\n",
            "      yield\n",
            "  else:\n",
            "    yield\n",
            "\n",
            "\n",
            "@tf_contextlib.contextmanager\n",
            "def graph_context_for_symbolic_tensors(*args, **kwargs):\n",
            "  \"\"\"Returns graph context manager if any of the inputs is a symbolic tensor.\"\"\"\n",
            "  if any(is_symbolic_tensor(v) for v in list(args) + list(kwargs.values())):\n",
            "    with K.get_graph().as_default():\n",
            "      yield\n",
            "  else:\n",
            "    yield\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Utilities related to model visualization.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import os\n",
            "import sys\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "try:\n",
            "  # pydot-ng is a fork of pydot that is better maintained.\n",
            "  import pydot_ng as pydot\n",
            "except ImportError:\n",
            "  # pydotplus is an improved version of pydot\n",
            "  try:\n",
            "    import pydotplus as pydot\n",
            "  except ImportError:\n",
            "    # Fall back on pydot if necessary.\n",
            "    try:\n",
            "      import pydot\n",
            "    except ImportError:\n",
            "      pydot = None\n",
            "\n",
            "\n",
            "def _check_pydot():\n",
            "  try:\n",
            "    # Attempt to create an image of a blank graph\n",
            "    # to check the pydot/graphviz installation.\n",
            "    pydot.Dot.create(pydot.Dot())\n",
            "    return True\n",
            "  except Exception:  # pylint: disable=broad-except\n",
            "    # pydot raises a generic Exception here,\n",
            "    # so no specific class can be caught.\n",
            "    return False\n",
            "\n",
            "\n",
            "def model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir='TB'):\n",
            "  \"\"\"Convert a Keras model to dot format.\n",
            "\n",
            "  Arguments:\n",
            "      model: A Keras model instance.\n",
            "      show_shapes: whether to display shape information.\n",
            "      show_layer_names: whether to display layer names.\n",
            "      rankdir: `rankdir` argument passed to PyDot,\n",
            "          a string specifying the format of the plot:\n",
            "          'TB' creates a vertical plot;\n",
            "          'LR' creates a horizontal plot.\n",
            "\n",
            "  Returns:\n",
            "      A `pydot.Dot` instance representing the Keras model (or None if the Dot\n",
            "      file could not be generated).\n",
            "\n",
            "  Raises:\n",
            "    ImportError: if graphviz or pydot are not available.\n",
            "  \"\"\"\n",
            "  from tensorflow.python.keras.layers.wrappers import Wrapper\n",
            "  from tensorflow.python.keras.models import Sequential\n",
            "  from tensorflow.python.util import nest\n",
            "\n",
            "  check = _check_pydot()\n",
            "  if not check:\n",
            "    if 'IPython.core.magics.namespace' in sys.modules:\n",
            "      # We don't raise an exception here in order to avoid crashing notebook\n",
            "      # tests where graphviz is not available.\n",
            "      print('Failed to import pydot. You must install pydot'\n",
            "            ' and graphviz for `pydotprint` to work.')\n",
            "      return\n",
            "    else:\n",
            "      raise ImportError('Failed to import pydot. You must install pydot'\n",
            "                        ' and graphviz for `pydotprint` to work.')\n",
            "\n",
            "  dot = pydot.Dot()\n",
            "  dot.set('rankdir', rankdir)\n",
            "  dot.set('concentrate', True)\n",
            "  dot.set_node_defaults(shape='record')\n",
            "\n",
            "  if isinstance(model, Sequential):\n",
            "    if not model.built:\n",
            "      model.build()\n",
            "  layers = model._layers\n",
            "\n",
            "  # Create graph nodes.\n",
            "  for layer in layers:\n",
            "    layer_id = str(id(layer))\n",
            "\n",
            "    # Append a wrapped layer's label to node's label, if it exists.\n",
            "    layer_name = layer.name\n",
            "    class_name = layer.__class__.__name__\n",
            "    if isinstance(layer, Wrapper):\n",
            "      layer_name = '{}({})'.format(layer_name, layer.layer.name)\n",
            "      child_class_name = layer.layer.__class__.__name__\n",
            "      class_name = '{}({})'.format(class_name, child_class_name)\n",
            "\n",
            "    # Create node's label.\n",
            "    if show_layer_names:\n",
            "      label = '{}: {}'.format(layer_name, class_name)\n",
            "    else:\n",
            "      label = class_name\n",
            "\n",
            "    # Rebuild the label as a table including input/output shapes.\n",
            "    if show_shapes:\n",
            "      try:\n",
            "        outputlabels = str(layer.output_shape)\n",
            "      except AttributeError:\n",
            "        outputlabels = 'multiple'\n",
            "      if hasattr(layer, 'input_shape'):\n",
            "        inputlabels = str(layer.input_shape)\n",
            "      elif hasattr(layer, 'input_shapes'):\n",
            "        inputlabels = ', '.join([str(ishape) for ishape in layer.input_shapes])\n",
            "      else:\n",
            "        inputlabels = 'multiple'\n",
            "      label = '%s\\n|{input:|output:}|{{%s}|{%s}}' % (label, inputlabels,\n",
            "                                                     outputlabels)\n",
            "    node = pydot.Node(layer_id, label=label)\n",
            "    dot.add_node(node)\n",
            "\n",
            "  # Connect nodes with edges.\n",
            "  for layer in layers:\n",
            "    layer_id = str(id(layer))\n",
            "    for i, node in enumerate(layer._inbound_nodes):\n",
            "      node_key = layer.name + '_ib-' + str(i)\n",
            "      if node_key in model._network_nodes:  # pylint: disable=protected-access\n",
            "        for inbound_layer in nest.flatten(node.inbound_layers):\n",
            "          inbound_layer_id = str(id(inbound_layer))\n",
            "          layer_id = str(id(layer))\n",
            "          dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n",
            "  return dot\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.plot_model')\n",
            "def plot_model(model,\n",
            "               to_file='model.png',\n",
            "               show_shapes=False,\n",
            "               show_layer_names=True,\n",
            "               rankdir='TB'):\n",
            "  \"\"\"Converts a Keras model to dot format and save to a file.\n",
            "\n",
            "  Arguments:\n",
            "      model: A Keras model instance\n",
            "      to_file: File name of the plot image.\n",
            "      show_shapes: whether to display shape information.\n",
            "      show_layer_names: whether to display layer names.\n",
            "      rankdir: `rankdir` argument passed to PyDot,\n",
            "          a string specifying the format of the plot:\n",
            "          'TB' creates a vertical plot;\n",
            "          'LR' creates a horizontal plot.\n",
            "\n",
            "  Returns:\n",
            "      A Jupyter notebook Image object if Jupyter is installed.\n",
            "      This enables in-line display of the model plots in notebooks.\n",
            "  \"\"\"\n",
            "  dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)\n",
            "  if dot is None:\n",
            "    return\n",
            "  _, extension = os.path.splitext(to_file)\n",
            "  if not extension:\n",
            "    extension = 'png'\n",
            "  else:\n",
            "    extension = extension[1:]\n",
            "  # Save image to disk.\n",
            "  dot.write(to_file, format=extension)\n",
            "  # Return the image as a Jupyter Image object, to be displayed in-line.\n",
            "  # Note that we cannot easily detect whether the code is running in a\n",
            "  # notebook, and thus we always return the Image if Jupyter is available.\n",
            "  try:\n",
            "    from IPython import display\n",
            "    return display.Image(filename=to_file)\n",
            "  except ImportError:\n",
            "    pass\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Python utilities required by Keras.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import binascii\n",
            "import codecs\n",
            "import marshal\n",
            "import os\n",
            "import re\n",
            "import sys\n",
            "import time\n",
            "import types as python_types\n",
            "\n",
            "import numpy as np\n",
            "import six\n",
            "\n",
            "from tensorflow.python.util import nest\n",
            "from tensorflow.python.util import tf_decorator\n",
            "from tensorflow.python.util import tf_inspect\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "_GLOBAL_CUSTOM_OBJECTS = {}\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.CustomObjectScope')\n",
            "class CustomObjectScope(object):\n",
            "  \"\"\"Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n",
            "\n",
            "  Code within a `with` statement will be able to access custom objects\n",
            "  by name. Changes to global custom objects persist\n",
            "  within the enclosing `with` statement. At end of the `with` statement,\n",
            "  global custom objects are reverted to state\n",
            "  at beginning of the `with` statement.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  Consider a custom object `MyObject` (e.g. a class):\n",
            "\n",
            "  ```python\n",
            "      with CustomObjectScope({'MyObject':MyObject}):\n",
            "          layer = Dense(..., kernel_regularizer='MyObject')\n",
            "          # save, load, etc. will recognize custom object by name\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, *args):\n",
            "    self.custom_objects = args\n",
            "    self.backup = None\n",
            "\n",
            "  def __enter__(self):\n",
            "    self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()\n",
            "    for objects in self.custom_objects:\n",
            "      _GLOBAL_CUSTOM_OBJECTS.update(objects)\n",
            "    return self\n",
            "\n",
            "  def __exit__(self, *args, **kwargs):\n",
            "    _GLOBAL_CUSTOM_OBJECTS.clear()\n",
            "    _GLOBAL_CUSTOM_OBJECTS.update(self.backup)\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.custom_object_scope')\n",
            "def custom_object_scope(*args):\n",
            "  \"\"\"Provides a scope that changes to `_GLOBAL_CUSTOM_OBJECTS` cannot escape.\n",
            "\n",
            "  Convenience wrapper for `CustomObjectScope`.\n",
            "  Code within a `with` statement will be able to access custom objects\n",
            "  by name. Changes to global custom objects persist\n",
            "  within the enclosing `with` statement. At end of the `with` statement,\n",
            "  global custom objects are reverted to state\n",
            "  at beginning of the `with` statement.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  Consider a custom object `MyObject`\n",
            "\n",
            "  ```python\n",
            "      with custom_object_scope({'MyObject':MyObject}):\n",
            "          layer = Dense(..., kernel_regularizer='MyObject')\n",
            "          # save, load, etc. will recognize custom object by name\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      *args: Variable length list of dictionaries of name,\n",
            "          class pairs to add to custom objects.\n",
            "\n",
            "  Returns:\n",
            "      Object of type `CustomObjectScope`.\n",
            "  \"\"\"\n",
            "  return CustomObjectScope(*args)\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.get_custom_objects')\n",
            "def get_custom_objects():\n",
            "  \"\"\"Retrieves a live reference to the global dictionary of custom objects.\n",
            "\n",
            "  Updating and clearing custom objects using `custom_object_scope`\n",
            "  is preferred, but `get_custom_objects` can\n",
            "  be used to directly access `_GLOBAL_CUSTOM_OBJECTS`.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "      get_custom_objects().clear()\n",
            "      get_custom_objects()['MyObject'] = MyObject\n",
            "  ```\n",
            "\n",
            "  Returns:\n",
            "      Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).\n",
            "  \"\"\"\n",
            "  return _GLOBAL_CUSTOM_OBJECTS\n",
            "\n",
            "\n",
            "def serialize_keras_class_and_config(cls_name, cls_config):\n",
            "  \"\"\"Returns the serialization of the class with the given config.\"\"\"\n",
            "  return {'class_name': cls_name, 'config': cls_config}\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.serialize_keras_object')\n",
            "def serialize_keras_object(instance):\n",
            "  _, instance = tf_decorator.unwrap(instance)\n",
            "  if instance is None:\n",
            "    return None\n",
            "  if hasattr(instance, 'get_config'):\n",
            "    return serialize_keras_class_and_config(instance.__class__.__name__,\n",
            "                                            instance.get_config())\n",
            "  if hasattr(instance, '__name__'):\n",
            "    return instance.__name__\n",
            "  else:\n",
            "    raise ValueError('Cannot serialize', instance)\n",
            "\n",
            "\n",
            "def class_and_config_for_serialized_keras_object(\n",
            "    config,\n",
            "    module_objects=None,\n",
            "    custom_objects=None,\n",
            "    printable_module_name='object'):\n",
            "  \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n",
            "  if (not isinstance(config, dict) or 'class_name' not in config or\n",
            "      'config' not in config):\n",
            "    raise ValueError('Improper config format: ' + str(config))\n",
            "\n",
            "  class_name = config['class_name']\n",
            "  if custom_objects and class_name in custom_objects:\n",
            "    cls = custom_objects[class_name]\n",
            "  elif class_name in _GLOBAL_CUSTOM_OBJECTS:\n",
            "    cls = _GLOBAL_CUSTOM_OBJECTS[class_name]\n",
            "  else:\n",
            "    module_objects = module_objects or {}\n",
            "    cls = module_objects.get(class_name)\n",
            "    if cls is None:\n",
            "      raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\n",
            "  return (cls, config['config'])\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.deserialize_keras_object')\n",
            "def deserialize_keras_object(identifier,\n",
            "                             module_objects=None,\n",
            "                             custom_objects=None,\n",
            "                             printable_module_name='object'):\n",
            "  if identifier is None:\n",
            "    return None\n",
            "  if isinstance(identifier, dict):\n",
            "    # In this case we are dealing with a Keras config dictionary.\n",
            "    config = identifier\n",
            "    (cls, cls_config) = class_and_config_for_serialized_keras_object(\n",
            "        config, module_objects, custom_objects, printable_module_name)\n",
            "\n",
            "    if hasattr(cls, 'from_config'):\n",
            "      arg_spec = tf_inspect.getfullargspec(cls.from_config)\n",
            "      custom_objects = custom_objects or {}\n",
            "\n",
            "      if 'custom_objects' in arg_spec.args:\n",
            "        return cls.from_config(\n",
            "            cls_config,\n",
            "            custom_objects=dict(\n",
            "                list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n",
            "                list(custom_objects.items())))\n",
            "      with CustomObjectScope(custom_objects):\n",
            "        return cls.from_config(cls_config)\n",
            "    else:\n",
            "      # Then `cls` may be a function returning a class.\n",
            "      # in this case by convention `config` holds\n",
            "      # the kwargs of the function.\n",
            "      custom_objects = custom_objects or {}\n",
            "      with CustomObjectScope(custom_objects):\n",
            "        return cls(**cls_config)\n",
            "  elif isinstance(identifier, six.string_types):\n",
            "    object_name = identifier\n",
            "    if custom_objects and object_name in custom_objects:\n",
            "      obj = custom_objects.get(object_name)\n",
            "    elif object_name in _GLOBAL_CUSTOM_OBJECTS:\n",
            "      obj = _GLOBAL_CUSTOM_OBJECTS[object_name]\n",
            "    else:\n",
            "      obj = module_objects.get(object_name)\n",
            "      if obj is None:\n",
            "        raise ValueError('Unknown ' + printable_module_name + ':' + object_name)\n",
            "    # Classes passed by name are instantiated with no args, functions are\n",
            "    # returned as-is.\n",
            "    if tf_inspect.isclass(obj):\n",
            "      return obj()\n",
            "    return obj\n",
            "  else:\n",
            "    raise ValueError('Could not interpret serialized ' + printable_module_name +\n",
            "                     ': ' + identifier)\n",
            "\n",
            "\n",
            "def func_dump(func):\n",
            "  \"\"\"Serializes a user defined function.\n",
            "\n",
            "  Arguments:\n",
            "      func: the function to serialize.\n",
            "\n",
            "  Returns:\n",
            "      A tuple `(code, defaults, closure)`.\n",
            "  \"\"\"\n",
            "  if os.name == 'nt':\n",
            "    raw_code = marshal.dumps(func.__code__).replace(b'\\\\', b'/')\n",
            "    code = codecs.encode(raw_code, 'base64').decode('ascii')\n",
            "  else:\n",
            "    raw_code = marshal.dumps(func.__code__)\n",
            "    code = codecs.encode(raw_code, 'base64').decode('ascii')\n",
            "  defaults = func.__defaults__\n",
            "  if func.__closure__:\n",
            "    closure = tuple(c.cell_contents for c in func.__closure__)\n",
            "  else:\n",
            "    closure = None\n",
            "  return code, defaults, closure\n",
            "\n",
            "\n",
            "def func_load(code, defaults=None, closure=None, globs=None):\n",
            "  \"\"\"Deserializes a user defined function.\n",
            "\n",
            "  Arguments:\n",
            "      code: bytecode of the function.\n",
            "      defaults: defaults of the function.\n",
            "      closure: closure of the function.\n",
            "      globs: dictionary of global objects.\n",
            "\n",
            "  Returns:\n",
            "      A function object.\n",
            "  \"\"\"\n",
            "  if isinstance(code, (tuple, list)):  # unpack previous dump\n",
            "    code, defaults, closure = code\n",
            "    if isinstance(defaults, list):\n",
            "      defaults = tuple(defaults)\n",
            "\n",
            "  def ensure_value_to_cell(value):\n",
            "    \"\"\"Ensures that a value is converted to a python cell object.\n",
            "\n",
            "    Arguments:\n",
            "        value: Any value that needs to be casted to the cell type\n",
            "\n",
            "    Returns:\n",
            "        A value wrapped as a cell object (see function \"func_load\")\n",
            "    \"\"\"\n",
            "    def dummy_fn():\n",
            "      # pylint: disable=pointless-statement\n",
            "      value  # just access it so it gets captured in .__closure__\n",
            "\n",
            "    cell_value = dummy_fn.__closure__[0]\n",
            "    if not isinstance(value, type(cell_value)):\n",
            "      return cell_value\n",
            "    else:\n",
            "      return value\n",
            "\n",
            "  if closure is not None:\n",
            "    closure = tuple(ensure_value_to_cell(_) for _ in closure)\n",
            "  try:\n",
            "    raw_code = codecs.decode(code.encode('ascii'), 'base64')\n",
            "  except (UnicodeEncodeError, binascii.Error):\n",
            "    raw_code = code.encode('raw_unicode_escape')\n",
            "  code = marshal.loads(raw_code)\n",
            "  if globs is None:\n",
            "    globs = globals()\n",
            "  return python_types.FunctionType(\n",
            "      code, globs, name=code.co_name, argdefs=defaults, closure=closure)\n",
            "\n",
            "\n",
            "def has_arg(fn, name, accept_all=False):\n",
            "  \"\"\"Checks if a callable accepts a given keyword argument.\n",
            "\n",
            "  Arguments:\n",
            "      fn: Callable to inspect.\n",
            "      name: Check if `fn` can be called with `name` as a keyword argument.\n",
            "      accept_all: What to return if there is no parameter called `name`\n",
            "                  but the function accepts a `**kwargs` argument.\n",
            "\n",
            "  Returns:\n",
            "      bool, whether `fn` accepts a `name` keyword argument.\n",
            "  \"\"\"\n",
            "  arg_spec = tf_inspect.getfullargspec(fn)\n",
            "  if accept_all and arg_spec.varkw is not None:\n",
            "    return True\n",
            "  return name in arg_spec.args\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.Progbar')\n",
            "class Progbar(object):\n",
            "  \"\"\"Displays a progress bar.\n",
            "\n",
            "  Arguments:\n",
            "      target: Total number of steps expected, None if unknown.\n",
            "      width: Progress bar width on screen.\n",
            "      verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n",
            "      stateful_metrics: Iterable of string names of metrics that\n",
            "          should *not* be averaged over time. Metrics in this list\n",
            "          will be displayed as-is. All others will be averaged\n",
            "          by the progbar before display.\n",
            "      interval: Minimum visual progress update interval (in seconds).\n",
            "      unit_name: Display name for step counts (usually \"step\" or \"sample\").\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, target, width=30, verbose=1, interval=0.05,\n",
            "               stateful_metrics=None, unit_name='step'):\n",
            "    self.target = target\n",
            "    self.width = width\n",
            "    self.verbose = verbose\n",
            "    self.interval = interval\n",
            "    self.unit_name = unit_name\n",
            "    if stateful_metrics:\n",
            "      self.stateful_metrics = set(stateful_metrics)\n",
            "    else:\n",
            "      self.stateful_metrics = set()\n",
            "\n",
            "    self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n",
            "                              sys.stdout.isatty()) or\n",
            "                             'ipykernel' in sys.modules or\n",
            "                             'posix' in sys.modules)\n",
            "    self._total_width = 0\n",
            "    self._seen_so_far = 0\n",
            "    # We use a dict + list to avoid garbage collection\n",
            "    # issues found in OrderedDict\n",
            "    self._values = {}\n",
            "    self._values_order = []\n",
            "    self._start = time.time()\n",
            "    self._last_update = 0\n",
            "\n",
            "  def update(self, current, values=None):\n",
            "    \"\"\"Updates the progress bar.\n",
            "\n",
            "    Arguments:\n",
            "        current: Index of current step.\n",
            "        values: List of tuples:\n",
            "            `(name, value_for_last_step)`.\n",
            "            If `name` is in `stateful_metrics`,\n",
            "            `value_for_last_step` will be displayed as-is.\n",
            "            Else, an average of the metric over time will be displayed.\n",
            "    \"\"\"\n",
            "    values = values or []\n",
            "    for k, v in values:\n",
            "      if k not in self._values_order:\n",
            "        self._values_order.append(k)\n",
            "      if k not in self.stateful_metrics:\n",
            "        if k not in self._values:\n",
            "          self._values[k] = [v * (current - self._seen_so_far),\n",
            "                             current - self._seen_so_far]\n",
            "        else:\n",
            "          self._values[k][0] += v * (current - self._seen_so_far)\n",
            "          self._values[k][1] += (current - self._seen_so_far)\n",
            "      else:\n",
            "        # Stateful metrics output a numeric value. This representation\n",
            "        # means \"take an average from a single value\" but keeps the\n",
            "        # numeric formatting.\n",
            "        self._values[k] = [v, 1]\n",
            "    self._seen_so_far = current\n",
            "\n",
            "    now = time.time()\n",
            "    info = ' - %.0fs' % (now - self._start)\n",
            "    if self.verbose == 1:\n",
            "      if (now - self._last_update < self.interval and\n",
            "          self.target is not None and current < self.target):\n",
            "        return\n",
            "\n",
            "      prev_total_width = self._total_width\n",
            "      if self._dynamic_display:\n",
            "        sys.stdout.write('\\b' * prev_total_width)\n",
            "        sys.stdout.write('\\r')\n",
            "      else:\n",
            "        sys.stdout.write('\\n')\n",
            "\n",
            "      if self.target is not None:\n",
            "        numdigits = int(np.log10(self.target)) + 1\n",
            "        bar = ('%' + str(numdigits) + 'd/%d [') % (current, self.target)\n",
            "        prog = float(current) / self.target\n",
            "        prog_width = int(self.width * prog)\n",
            "        if prog_width > 0:\n",
            "          bar += ('=' * (prog_width - 1))\n",
            "          if current < self.target:\n",
            "            bar += '>'\n",
            "          else:\n",
            "            bar += '='\n",
            "        bar += ('.' * (self.width - prog_width))\n",
            "        bar += ']'\n",
            "      else:\n",
            "        bar = '%7d/Unknown' % current\n",
            "\n",
            "      self._total_width = len(bar)\n",
            "      sys.stdout.write(bar)\n",
            "\n",
            "      if current:\n",
            "        time_per_unit = (now - self._start) / current\n",
            "      else:\n",
            "        time_per_unit = 0\n",
            "      if self.target is not None and current < self.target:\n",
            "        eta = time_per_unit * (self.target - current)\n",
            "        if eta > 3600:\n",
            "          eta_format = '%d:%02d:%02d' % (eta // 3600,\n",
            "                                         (eta % 3600) // 60,\n",
            "                                         eta % 60)\n",
            "        elif eta > 60:\n",
            "          eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
            "        else:\n",
            "          eta_format = '%ds' % eta\n",
            "\n",
            "        info = ' - ETA: %s' % eta_format\n",
            "      else:\n",
            "        if time_per_unit >= 1 or time_per_unit == 0:\n",
            "          info += ' %.0fs/%s' % (time_per_unit, self.unit_name)\n",
            "        elif time_per_unit >= 1e-3:\n",
            "          info += ' %.0fms/%s' % (time_per_unit * 1e3, self.unit_name)\n",
            "        else:\n",
            "          info += ' %.0fus/%s' % (time_per_unit * 1e6, self.unit_name)\n",
            "\n",
            "      for k in self._values_order:\n",
            "        info += ' - %s:' % k\n",
            "        if isinstance(self._values[k], list):\n",
            "          avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
            "          if abs(avg) > 1e-3:\n",
            "            info += ' %.4f' % avg\n",
            "          else:\n",
            "            info += ' %.4e' % avg\n",
            "        else:\n",
            "          info += ' %s' % self._values[k]\n",
            "\n",
            "      self._total_width += len(info)\n",
            "      if prev_total_width > self._total_width:\n",
            "        info += (' ' * (prev_total_width - self._total_width))\n",
            "\n",
            "      if self.target is not None and current >= self.target:\n",
            "        info += '\\n'\n",
            "\n",
            "      sys.stdout.write(info)\n",
            "      sys.stdout.flush()\n",
            "\n",
            "    elif self.verbose == 2:\n",
            "      if self.target is not None and current >= self.target:\n",
            "        numdigits = int(np.log10(self.target)) + 1\n",
            "        count = ('%' + str(numdigits) + 'd/%d') % (current, self.target)\n",
            "        info = count + info\n",
            "        for k in self._values_order:\n",
            "          info += ' - %s:' % k\n",
            "          avg = np.mean(self._values[k][0] / max(1, self._values[k][1]))\n",
            "          if avg > 1e-3:\n",
            "            info += ' %.4f' % avg\n",
            "          else:\n",
            "            info += ' %.4e' % avg\n",
            "        info += '\\n'\n",
            "\n",
            "        sys.stdout.write(info)\n",
            "        sys.stdout.flush()\n",
            "\n",
            "    self._last_update = now\n",
            "\n",
            "  def add(self, n, values=None):\n",
            "    self.update(self._seen_so_far + n, values)\n",
            "\n",
            "\n",
            "def make_batches(size, batch_size):\n",
            "  \"\"\"Returns a list of batch indices (tuples of indices).\n",
            "\n",
            "  Arguments:\n",
            "      size: Integer, total size of the data to slice into batches.\n",
            "      batch_size: Integer, batch size.\n",
            "\n",
            "  Returns:\n",
            "      A list of tuples of array indices.\n",
            "  \"\"\"\n",
            "  num_batches = int(np.ceil(size / float(batch_size)))\n",
            "  return [(i * batch_size, min(size, (i + 1) * batch_size))\n",
            "          for i in range(0, num_batches)]\n",
            "\n",
            "\n",
            "def slice_arrays(arrays, start=None, stop=None):\n",
            "  \"\"\"Slice an array or list of arrays.\n",
            "\n",
            "  This takes an array-like, or a list of\n",
            "  array-likes, and outputs:\n",
            "      - arrays[start:stop] if `arrays` is an array-like\n",
            "      - [x[start:stop] for x in arrays] if `arrays` is a list\n",
            "\n",
            "  Can also work on list/array of indices: `slice_arrays(x, indices)`\n",
            "\n",
            "  Arguments:\n",
            "      arrays: Single array or list of arrays.\n",
            "      start: can be an integer index (start index)\n",
            "          or a list/array of indices\n",
            "      stop: integer (stop index); should be None if\n",
            "          `start` was a list.\n",
            "\n",
            "  Returns:\n",
            "      A slice of the array(s).\n",
            "\n",
            "  Raises:\n",
            "      ValueError: If the value of start is a list and stop is not None.\n",
            "  \"\"\"\n",
            "  if arrays is None:\n",
            "    return [None]\n",
            "  if isinstance(start, list) and stop is not None:\n",
            "    raise ValueError('The stop argument has to be None if the value of start '\n",
            "                     'is a list.')\n",
            "  elif isinstance(arrays, list):\n",
            "    if hasattr(start, '__len__'):\n",
            "      # hdf5 datasets only support list objects as indices\n",
            "      if hasattr(start, 'shape'):\n",
            "        start = start.tolist()\n",
            "      return [None if x is None else x[start] for x in arrays]\n",
            "    else:\n",
            "      return [None if x is None else x[start:stop] for x in arrays]\n",
            "  else:\n",
            "    if hasattr(start, '__len__'):\n",
            "      if hasattr(start, 'shape'):\n",
            "        start = start.tolist()\n",
            "      return arrays[start]\n",
            "    elif hasattr(start, '__getitem__'):\n",
            "      return arrays[start:stop]\n",
            "    else:\n",
            "      return [None]\n",
            "\n",
            "\n",
            "def to_list(x):\n",
            "  \"\"\"Normalizes a list/tensor into a list.\n",
            "\n",
            "  If a tensor is passed, we return\n",
            "  a list of size 1 containing the tensor.\n",
            "\n",
            "  Arguments:\n",
            "      x: target object to be normalized.\n",
            "\n",
            "  Returns:\n",
            "      A list.\n",
            "  \"\"\"\n",
            "  if isinstance(x, list):\n",
            "    return x\n",
            "  return [x]\n",
            "\n",
            "\n",
            "def object_list_uid(object_list):\n",
            "  \"\"\"Creates a single string from object ids.\"\"\"\n",
            "  object_list = nest.flatten(object_list)\n",
            "  return ', '.join([str(abs(id(x))) for x in object_list])\n",
            "\n",
            "\n",
            "def to_snake_case(name):\n",
            "  intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n",
            "  insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n",
            "  # If the class is private the name starts with \"_\" which is not secure\n",
            "  # for creating scopes. We prefix the name with \"private\" in this case.\n",
            "  if insecure[0] != '_':\n",
            "    return insecure\n",
            "  return 'private' + insecure\n",
            "\n",
            "\n",
            "def is_all_none(structure):\n",
            "  iterable = nest.flatten(structure)\n",
            "  # We cannot use Python's `any` because the iterable may return Tensors.\n",
            "  for element in iterable:\n",
            "    if element is not None:\n",
            "      return False\n",
            "  return True\n",
            "\n",
            "\n",
            "def check_for_unexpected_keys(name, input_dict, expected_values):\n",
            "  unknown = set(input_dict.keys()).difference(expected_values)\n",
            "  if unknown:\n",
            "    raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n",
            "                     'following keys: {}'.format(name, list(unknown),\n",
            "                                                 expected_values))\n",
            "\n",
            "\n",
            "def validate_kwargs(kwargs, allowed_kwargs,\n",
            "                    error_message='Keyword argument not understood:'):\n",
            "  \"\"\"Checks that all keyword arguments are in the set of allowed keys.\"\"\"\n",
            "  for kwarg in kwargs:\n",
            "    if kwarg not in allowed_kwargs:\n",
            "      raise TypeError(error_message, kwarg)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Utilities for file download and caching.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from abc import abstractmethod\n",
            "from contextlib import closing\n",
            "import gc\n",
            "import hashlib\n",
            "import multiprocessing\n",
            "from multiprocessing.pool import ThreadPool\n",
            "import os\n",
            "import random\n",
            "import shutil\n",
            "import signal\n",
            "import sys\n",
            "import tarfile\n",
            "import threading\n",
            "import time\n",
            "import weakref\n",
            "import zipfile\n",
            "\n",
            "import numpy as np\n",
            "import six\n",
            "from six.moves.urllib.error import HTTPError\n",
            "from six.moves.urllib.error import URLError\n",
            "from six.moves.urllib.request import urlopen\n",
            "\n",
            "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
            "from tensorflow.python.util import tf_inspect\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "try:\n",
            "  import queue\n",
            "except ImportError:\n",
            "  import Queue as queue\n",
            "\n",
            "\n",
            "if sys.version_info[0] == 2:\n",
            "\n",
            "  def urlretrieve(url, filename, reporthook=None, data=None):\n",
            "    \"\"\"Replacement for `urlretrive` for Python 2.\n",
            "\n",
            "    Under Python 2, `urlretrieve` relies on `FancyURLopener` from legacy\n",
            "    `urllib` module, known to have issues with proxy management.\n",
            "\n",
            "    Arguments:\n",
            "        url: url to retrieve.\n",
            "        filename: where to store the retrieved data locally.\n",
            "        reporthook: a hook function that will be called once\n",
            "            on establishment of the network connection and once\n",
            "            after each block read thereafter.\n",
            "            The hook will be passed three arguments;\n",
            "            a count of blocks transferred so far,\n",
            "            a block size in bytes, and the total size of the file.\n",
            "        data: `data` argument passed to `urlopen`.\n",
            "    \"\"\"\n",
            "\n",
            "    def chunk_read(response, chunk_size=8192, reporthook=None):\n",
            "      content_type = response.info().get('Content-Length')\n",
            "      total_size = -1\n",
            "      if content_type is not None:\n",
            "        total_size = int(content_type.strip())\n",
            "      count = 0\n",
            "      while True:\n",
            "        chunk = response.read(chunk_size)\n",
            "        count += 1\n",
            "        if reporthook is not None:\n",
            "          reporthook(count, chunk_size, total_size)\n",
            "        if chunk:\n",
            "          yield chunk\n",
            "        else:\n",
            "          break\n",
            "\n",
            "    response = urlopen(url, data)\n",
            "    with open(filename, 'wb') as fd:\n",
            "      for chunk in chunk_read(response, reporthook=reporthook):\n",
            "        fd.write(chunk)\n",
            "else:\n",
            "  from six.moves.urllib.request import urlretrieve\n",
            "\n",
            "\n",
            "def is_generator_or_sequence(x):\n",
            "  \"\"\"Check if `x` is a Keras generator type.\"\"\"\n",
            "  return tf_inspect.isgenerator(x) or isinstance(x, Sequence)\n",
            "\n",
            "\n",
            "def _extract_archive(file_path, path='.', archive_format='auto'):\n",
            "  \"\"\"Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.\n",
            "\n",
            "  Arguments:\n",
            "      file_path: path to the archive file\n",
            "      path: path to extract the archive file\n",
            "      archive_format: Archive format to try for extracting the file.\n",
            "          Options are 'auto', 'tar', 'zip', and None.\n",
            "          'tar' includes tar, tar.gz, and tar.bz files.\n",
            "          The default 'auto' is ['tar', 'zip'].\n",
            "          None or an empty list will return no matches found.\n",
            "\n",
            "  Returns:\n",
            "      True if a match was found and an archive extraction was completed,\n",
            "      False otherwise.\n",
            "  \"\"\"\n",
            "  if archive_format is None:\n",
            "    return False\n",
            "  if archive_format == 'auto':\n",
            "    archive_format = ['tar', 'zip']\n",
            "  if isinstance(archive_format, six.string_types):\n",
            "    archive_format = [archive_format]\n",
            "\n",
            "  for archive_type in archive_format:\n",
            "    if archive_type == 'tar':\n",
            "      open_fn = tarfile.open\n",
            "      is_match_fn = tarfile.is_tarfile\n",
            "    if archive_type == 'zip':\n",
            "      open_fn = zipfile.ZipFile\n",
            "      is_match_fn = zipfile.is_zipfile\n",
            "\n",
            "    if is_match_fn(file_path):\n",
            "      with open_fn(file_path) as archive:\n",
            "        try:\n",
            "          archive.extractall(path)\n",
            "        except (tarfile.TarError, RuntimeError, KeyboardInterrupt):\n",
            "          if os.path.exists(path):\n",
            "            if os.path.isfile(path):\n",
            "              os.remove(path)\n",
            "            else:\n",
            "              shutil.rmtree(path)\n",
            "          raise\n",
            "      return True\n",
            "  return False\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.get_file')\n",
            "def get_file(fname,\n",
            "             origin,\n",
            "             untar=False,\n",
            "             md5_hash=None,\n",
            "             file_hash=None,\n",
            "             cache_subdir='datasets',\n",
            "             hash_algorithm='auto',\n",
            "             extract=False,\n",
            "             archive_format='auto',\n",
            "             cache_dir=None):\n",
            "  \"\"\"Downloads a file from a URL if it not already in the cache.\n",
            "\n",
            "  By default the file at the url `origin` is downloaded to the\n",
            "  cache_dir `~/.keras`, placed in the cache_subdir `datasets`,\n",
            "  and given the filename `fname`. The final location of a file\n",
            "  `example.txt` would therefore be `~/.keras/datasets/example.txt`.\n",
            "\n",
            "  Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.\n",
            "  Passing a hash will verify the file after download. The command line\n",
            "  programs `shasum` and `sha256sum` can compute the hash.\n",
            "\n",
            "  Arguments:\n",
            "      fname: Name of the file. If an absolute path `/path/to/file.txt` is\n",
            "          specified the file will be saved at that location.\n",
            "      origin: Original URL of the file.\n",
            "      untar: Deprecated in favor of 'extract'.\n",
            "          boolean, whether the file should be decompressed\n",
            "      md5_hash: Deprecated in favor of 'file_hash'.\n",
            "          md5 hash of the file for verification\n",
            "      file_hash: The expected hash string of the file after download.\n",
            "          The sha256 and md5 hash algorithms are both supported.\n",
            "      cache_subdir: Subdirectory under the Keras cache dir where the file is\n",
            "          saved. If an absolute path `/path/to/folder` is\n",
            "          specified the file will be saved at that location.\n",
            "      hash_algorithm: Select the hash algorithm to verify the file.\n",
            "          options are 'md5', 'sha256', and 'auto'.\n",
            "          The default 'auto' detects the hash algorithm in use.\n",
            "      extract: True tries extracting the file as an Archive, like tar or zip.\n",
            "      archive_format: Archive format to try for extracting the file.\n",
            "          Options are 'auto', 'tar', 'zip', and None.\n",
            "          'tar' includes tar, tar.gz, and tar.bz files.\n",
            "          The default 'auto' is ['tar', 'zip'].\n",
            "          None or an empty list will return no matches found.\n",
            "      cache_dir: Location to store cached files, when None it\n",
            "          defaults to the [Keras\n",
            "            Directory](/faq/#where-is-the-keras-configuration-filed-stored).\n",
            "\n",
            "  Returns:\n",
            "      Path to the downloaded file\n",
            "  \"\"\"\n",
            "  if cache_dir is None:\n",
            "    cache_dir = os.path.join(os.path.expanduser('~'), '.keras')\n",
            "  if md5_hash is not None and file_hash is None:\n",
            "    file_hash = md5_hash\n",
            "    hash_algorithm = 'md5'\n",
            "  datadir_base = os.path.expanduser(cache_dir)\n",
            "  if not os.access(datadir_base, os.W_OK):\n",
            "    datadir_base = os.path.join('/tmp', '.keras')\n",
            "  datadir = os.path.join(datadir_base, cache_subdir)\n",
            "  if not os.path.exists(datadir):\n",
            "    os.makedirs(datadir)\n",
            "\n",
            "  if untar:\n",
            "    untar_fpath = os.path.join(datadir, fname)\n",
            "    fpath = untar_fpath + '.tar.gz'\n",
            "  else:\n",
            "    fpath = os.path.join(datadir, fname)\n",
            "\n",
            "  download = False\n",
            "  if os.path.exists(fpath):\n",
            "    # File found; verify integrity if a hash was provided.\n",
            "    if file_hash is not None:\n",
            "      if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
            "        print('A local file was found, but it seems to be '\n",
            "              'incomplete or outdated because the ' + hash_algorithm +\n",
            "              ' file hash does not match the original value of ' + file_hash +\n",
            "              ' so we will re-download the data.')\n",
            "        download = True\n",
            "  else:\n",
            "    download = True\n",
            "\n",
            "  if download:\n",
            "    print('Downloading data from', origin)\n",
            "\n",
            "    class ProgressTracker(object):\n",
            "      # Maintain progbar for the lifetime of download.\n",
            "      # This design was chosen for Python 2.7 compatibility.\n",
            "      progbar = None\n",
            "\n",
            "    def dl_progress(count, block_size, total_size):\n",
            "      if ProgressTracker.progbar is None:\n",
            "        if total_size == -1:\n",
            "          total_size = None\n",
            "        ProgressTracker.progbar = Progbar(total_size)\n",
            "      else:\n",
            "        ProgressTracker.progbar.update(count * block_size)\n",
            "\n",
            "    error_msg = 'URL fetch failure on {}: {} -- {}'\n",
            "    try:\n",
            "      try:\n",
            "        urlretrieve(origin, fpath, dl_progress)\n",
            "      except HTTPError as e:\n",
            "        raise Exception(error_msg.format(origin, e.code, e.msg))\n",
            "      except URLError as e:\n",
            "        raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
            "    except (Exception, KeyboardInterrupt) as e:\n",
            "      if os.path.exists(fpath):\n",
            "        os.remove(fpath)\n",
            "      raise\n",
            "    ProgressTracker.progbar = None\n",
            "\n",
            "  if untar:\n",
            "    if not os.path.exists(untar_fpath):\n",
            "      _extract_archive(fpath, datadir, archive_format='tar')\n",
            "    return untar_fpath\n",
            "\n",
            "  if extract:\n",
            "    _extract_archive(fpath, datadir, archive_format)\n",
            "\n",
            "  return fpath\n",
            "\n",
            "\n",
            "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n",
            "  \"\"\"Calculates a file sha256 or md5 hash.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "      >>> from keras.data_utils import _hash_file\n",
            "      >>> _hash_file('/path/to/file.zip')\n",
            "      'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n",
            "  ```\n",
            "\n",
            "  Arguments:\n",
            "      fpath: path to the file being validated\n",
            "      algorithm: hash algorithm, one of 'auto', 'sha256', or 'md5'.\n",
            "          The default 'auto' detects the hash algorithm in use.\n",
            "      chunk_size: Bytes to read at a time, important for large files.\n",
            "\n",
            "  Returns:\n",
            "      The file hash\n",
            "  \"\"\"\n",
            "  if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):\n",
            "    hasher = hashlib.sha256()\n",
            "  else:\n",
            "    hasher = hashlib.md5()\n",
            "\n",
            "  with open(fpath, 'rb') as fpath_file:\n",
            "    for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n",
            "      hasher.update(chunk)\n",
            "\n",
            "  return hasher.hexdigest()\n",
            "\n",
            "\n",
            "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n",
            "  \"\"\"Validates a file against a sha256 or md5 hash.\n",
            "\n",
            "  Arguments:\n",
            "      fpath: path to the file being validated\n",
            "      file_hash:  The expected hash string of the file.\n",
            "          The sha256 and md5 hash algorithms are both supported.\n",
            "      algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'.\n",
            "          The default 'auto' detects the hash algorithm in use.\n",
            "      chunk_size: Bytes to read at a time, important for large files.\n",
            "\n",
            "  Returns:\n",
            "      Whether the file is valid\n",
            "  \"\"\"\n",
            "  if (algorithm == 'sha256') or (algorithm == 'auto' and len(file_hash) == 64):\n",
            "    hasher = 'sha256'\n",
            "  else:\n",
            "    hasher = 'md5'\n",
            "\n",
            "  if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n",
            "    return True\n",
            "  else:\n",
            "    return False\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.Sequence')\n",
            "class Sequence(object):\n",
            "  \"\"\"Base object for fitting to a sequence of data, such as a dataset.\n",
            "\n",
            "  Every `Sequence` must implement the `__getitem__` and the `__len__` methods.\n",
            "  If you want to modify your dataset between epochs you may implement\n",
            "  `on_epoch_end`.\n",
            "  The method `__getitem__` should return a complete batch.\n",
            "\n",
            "  Notes:\n",
            "\n",
            "  `Sequence` are a safer way to do multiprocessing. This structure guarantees\n",
            "  that the network will only train once\n",
            "   on each sample per epoch which is not the case with generators.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  ```python\n",
            "      from skimage.io import imread\n",
            "      from skimage.transform import resize\n",
            "      import numpy as np\n",
            "      import math\n",
            "\n",
            "      # Here, `x_set` is list of path to the images\n",
            "      # and `y_set` are the associated classes.\n",
            "\n",
            "      class CIFAR10Sequence(Sequence):\n",
            "\n",
            "          def __init__(self, x_set, y_set, batch_size):\n",
            "              self.x, self.y = x_set, y_set\n",
            "              self.batch_size = batch_size\n",
            "\n",
            "          def __len__(self):\n",
            "              return math.ceil(len(self.x) / self.batch_size)\n",
            "\n",
            "          def __getitem__(self, idx):\n",
            "              batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
            "              self.batch_size]\n",
            "              batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
            "              self.batch_size]\n",
            "\n",
            "              return np.array([\n",
            "                  resize(imread(file_name), (200, 200))\n",
            "                     for file_name in batch_x]), np.array(batch_y)\n",
            "  ```\n",
            "  \"\"\"\n",
            "\n",
            "  @abstractmethod\n",
            "  def __getitem__(self, index):\n",
            "    \"\"\"Gets batch at position `index`.\n",
            "\n",
            "    Arguments:\n",
            "        index: position of the batch in the Sequence.\n",
            "\n",
            "    Returns:\n",
            "        A batch\n",
            "    \"\"\"\n",
            "    raise NotImplementedError\n",
            "\n",
            "  @abstractmethod\n",
            "  def __len__(self):\n",
            "    \"\"\"Number of batch in the Sequence.\n",
            "\n",
            "    Returns:\n",
            "        The number of batches in the Sequence.\n",
            "    \"\"\"\n",
            "    raise NotImplementedError\n",
            "\n",
            "  def on_epoch_end(self):\n",
            "    \"\"\"Method called at the end of every epoch.\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "  def __iter__(self):\n",
            "    \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n",
            "    for item in (self[i] for i in range(len(self))):\n",
            "      yield item\n",
            "\n",
            "\n",
            "def iter_sequence_infinite(seq):\n",
            "  \"\"\"Iterates indefinitely over a Sequence.\n",
            "\n",
            "  Arguments:\n",
            "    seq: Sequence instance.\n",
            "\n",
            "  Yields:\n",
            "    Batches of data from the Sequence.\n",
            "  \"\"\"\n",
            "  while True:\n",
            "    for item in seq:\n",
            "      yield item\n",
            "\n",
            "\n",
            "# Global variables to be shared across processes\n",
            "_SHARED_SEQUENCES = {}\n",
            "# We use a Value to provide unique id to different processes.\n",
            "_SEQUENCE_COUNTER = None\n",
            "\n",
            "\n",
            "# Because multiprocessing pools are inherently unsafe, starting from a clean\n",
            "# state can be essential to avoiding deadlocks. In order to accomplish this, we\n",
            "# need to be able to check on the status of Pools that we create.\n",
            "_DATA_POOLS = weakref.WeakSet()\n",
            "_WORKER_ID_QUEUE = None  # Only created if needed.\n",
            "_WORKER_IDS = set()\n",
            "\n",
            "\n",
            "def get_worker_id_queue():\n",
            "  \"\"\"Lazily create the queue to track worker ids.\"\"\"\n",
            "  global _WORKER_ID_QUEUE\n",
            "  if _WORKER_ID_QUEUE is None:\n",
            "    _WORKER_ID_QUEUE = multiprocessing.Queue()\n",
            "  return _WORKER_ID_QUEUE\n",
            "\n",
            "\n",
            "def init_pool(seqs):\n",
            "  global _SHARED_SEQUENCES\n",
            "  _SHARED_SEQUENCES = seqs\n",
            "\n",
            "\n",
            "@keras_export('keras.experimental.terminate_keras_multiprocessing_pools')\n",
            "def terminate_keras_multiprocessing_pools(grace_period=0.1, use_sigkill=False):\n",
            "  \"\"\"Destroy Keras' multiprocessing pools to prevent deadlocks.\n",
            "\n",
            "  In general multiprocessing.Pool can interact quite badly with other, seemingly\n",
            "  unrelated, parts of a codebase due to Pool's reliance on fork. This method\n",
            "  cleans up all pools which are known to belong to Keras (and thus can be safely\n",
            "  terminated).\n",
            "\n",
            "  Args:\n",
            "    grace_period: Time (in seconds) to wait for process cleanup to propagate.\n",
            "    use_sigkill: Boolean of whether or not to perform a cleanup pass using\n",
            "      SIGKILL.\n",
            "\n",
            "  Returns:\n",
            "    A list of human readable strings describing all issues encountered. It is up\n",
            "    to the caller to decide whether to treat this as an error condition.\n",
            "  \"\"\"\n",
            "  errors = []\n",
            "\n",
            "  # First cleanup the pools spawned by Keras. If we start killing workers and\n",
            "  # a parent pool is still alive it will just spawn replacements which we don't\n",
            "  # want.\n",
            "  gc.collect()\n",
            "  for pool in _DATA_POOLS:\n",
            "    pool.close()\n",
            "    pool.terminate()\n",
            "    # We do not join the pool, because that would wait forever if a worker\n",
            "    # refused to exit.\n",
            "\n",
            "    # Finally, delete our reference to the pool so that we do not block garbage\n",
            "    # collection.\n",
            "    del pool\n",
            "\n",
            "  # If there were any pools, sleep for a small grace period to allow everything\n",
            "  # to finalize.\n",
            "  if _DATA_POOLS:\n",
            "    time.sleep(grace_period)\n",
            "\n",
            "  # Now we kill any workers which are still alive. However we must compare\n",
            "  # the worker identifier to the set of identifiers which are known to have been\n",
            "  # spawned by pools belonging to Keras to avoid deleting unrelated workers.\n",
            "  # First we call the .terminate() method of a worker, and then if it still\n",
            "  # persists we directly send a signal to the process.  Certain worker tasks may\n",
            "  # be able to gracefully handle shutdown, so we send a SIGTERM and then\n",
            "  # optionally follow up with a SIGKILL.\n",
            "  visited_workers = set()\n",
            "  cleanup_passes = ['.terminate', 'SIGTERM']\n",
            "  if use_sigkill:\n",
            "    cleanup_passes.append('SIGKILL')\n",
            "  cleanup_passes.append('log')\n",
            "\n",
            "  for cleanup_pass in cleanup_passes:\n",
            "    while True:\n",
            "      # In rare cases, queue.qsize() overestimates the number of elements. This\n",
            "      # loop is designed to be more robust.\n",
            "      try:\n",
            "        _WORKER_IDS.add(get_worker_id_queue().get_nowait())\n",
            "      except queue.Empty:\n",
            "        break\n",
            "\n",
            "    gc.collect()\n",
            "    workers_terminated_this_pass = False\n",
            "    for worker in multiprocessing.active_children():\n",
            "      ident = worker.ident\n",
            "      if ident in _WORKER_IDS and worker.is_alive():\n",
            "        try:\n",
            "          if cleanup_pass == '.terminate':\n",
            "            # First we ask nicely.\n",
            "            worker.terminate()\n",
            "            worker.join(timeout=grace_period)\n",
            "            visited_workers.add(ident)\n",
            "            workers_terminated_this_pass = True\n",
            "          elif cleanup_pass in ('SIGTERM', 'SIGKILL'):\n",
            "            # Then we ask increasingly tersely.\n",
            "            os.kill(worker.pid, signal.SIGKILL if cleanup_pass == 'SIGKILL'\n",
            "                    else signal.SIGTERM)\n",
            "            workers_terminated_this_pass = True\n",
            "\n",
            "          elif cleanup_pass == 'log':\n",
            "            # And finally we give up and log the failure.\n",
            "            errors.append('worker still alive: {}, pid={}, hash={}'\n",
            "                          .format(worker.name, worker.pid, hash(worker)))\n",
            "\n",
            "        except OSError:\n",
            "          # Worker exited since the start of this loop.\n",
            "          pass\n",
            "\n",
            "    if workers_terminated_this_pass:\n",
            "      # There can be a small propagation delay between worker destruction and\n",
            "      # workers reporting False for is_alive and no longer appearing in the\n",
            "      # list of active children. Once again, we sleep for a small grace period.\n",
            "      # This prevents false positives from workers which are simply still in the\n",
            "      # process of spinning down.\n",
            "      time.sleep(grace_period)\n",
            "\n",
            "  # Finally we remove the visited worker ids to handle the edge case that a\n",
            "  # pid is reused.\n",
            "  _WORKER_IDS.difference_update(visited_workers)\n",
            "\n",
            "  gc.collect()\n",
            "  for pool in _DATA_POOLS:\n",
            "    errors.append('pool still exists: {}, hash={}'.format(pool, hash(pool)))\n",
            "\n",
            "  return errors\n",
            "\n",
            "\n",
            "def get_index(uid, i):\n",
            "  \"\"\"Get the value from the Sequence `uid` at index `i`.\n",
            "\n",
            "  To allow multiple Sequences to be used at the same time, we use `uid` to\n",
            "  get a specific one. A single Sequence would cause the validation to\n",
            "  overwrite the training Sequence.\n",
            "\n",
            "  Arguments:\n",
            "      uid: int, Sequence identifier\n",
            "      i: index\n",
            "\n",
            "  Returns:\n",
            "      The value at index `i`.\n",
            "  \"\"\"\n",
            "  return _SHARED_SEQUENCES[uid][i]\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.SequenceEnqueuer')\n",
            "class SequenceEnqueuer(object):\n",
            "  \"\"\"Base class to enqueue inputs.\n",
            "\n",
            "  The task of an Enqueuer is to use parallelism to speed up preprocessing.\n",
            "  This is done with processes or threads.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "      enqueuer = SequenceEnqueuer(...)\n",
            "      enqueuer.start()\n",
            "      datas = enqueuer.get()\n",
            "      for data in datas:\n",
            "          # Use the inputs; training, evaluating, predicting.\n",
            "          # ... stop sometime.\n",
            "      enqueuer.close()\n",
            "  ```\n",
            "\n",
            "  The `enqueuer.get()` should be an infinite stream of datas.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, sequence,\n",
            "               use_multiprocessing=False):\n",
            "    self.sequence = sequence\n",
            "    self.use_multiprocessing = use_multiprocessing\n",
            "\n",
            "    global _SEQUENCE_COUNTER\n",
            "    if _SEQUENCE_COUNTER is None:\n",
            "      try:\n",
            "        _SEQUENCE_COUNTER = multiprocessing.Value('i', 0)\n",
            "      except OSError:\n",
            "        # In this case the OS does not allow us to use\n",
            "        # multiprocessing. We resort to an int\n",
            "        # for enqueuer indexing.\n",
            "        _SEQUENCE_COUNTER = 0\n",
            "\n",
            "    if isinstance(_SEQUENCE_COUNTER, int):\n",
            "      self.uid = _SEQUENCE_COUNTER\n",
            "      _SEQUENCE_COUNTER += 1\n",
            "    else:\n",
            "      # Doing Multiprocessing.Value += x is not process-safe.\n",
            "      with _SEQUENCE_COUNTER.get_lock():\n",
            "        self.uid = _SEQUENCE_COUNTER.value\n",
            "        _SEQUENCE_COUNTER.value += 1\n",
            "\n",
            "    self.workers = 0\n",
            "    self.executor_fn = None\n",
            "    self.queue = None\n",
            "    self.run_thread = None\n",
            "    self.stop_signal = None\n",
            "\n",
            "  def is_running(self):\n",
            "    return self.stop_signal is not None and not self.stop_signal.is_set()\n",
            "\n",
            "  def start(self, workers=1, max_queue_size=10):\n",
            "    \"\"\"Starts the handler's workers.\n",
            "\n",
            "    Arguments:\n",
            "        workers: Number of workers.\n",
            "        max_queue_size: queue size\n",
            "            (when full, workers could block on `put()`)\n",
            "    \"\"\"\n",
            "    if self.use_multiprocessing:\n",
            "      self.executor_fn = self._get_executor_init(workers)\n",
            "    else:\n",
            "      # We do not need the init since it's threads.\n",
            "      self.executor_fn = lambda _: ThreadPool(workers)\n",
            "    self.workers = workers\n",
            "    self.queue = queue.Queue(max_queue_size)\n",
            "    self.stop_signal = threading.Event()\n",
            "    self.run_thread = threading.Thread(target=self._run)\n",
            "    self.run_thread.daemon = True\n",
            "    self.run_thread.start()\n",
            "\n",
            "  def _send_sequence(self):\n",
            "    \"\"\"Sends current Iterable to all workers.\"\"\"\n",
            "    # For new processes that may spawn\n",
            "    _SHARED_SEQUENCES[self.uid] = self.sequence\n",
            "\n",
            "  def stop(self, timeout=None):\n",
            "    \"\"\"Stops running threads and wait for them to exit, if necessary.\n",
            "\n",
            "    Should be called by the same thread which called `start()`.\n",
            "\n",
            "    Arguments:\n",
            "        timeout: maximum time to wait on `thread.join()`\n",
            "    \"\"\"\n",
            "    self.stop_signal.set()\n",
            "    with self.queue.mutex:\n",
            "      self.queue.queue.clear()\n",
            "      self.queue.unfinished_tasks = 0\n",
            "      self.queue.not_full.notify()\n",
            "    self.run_thread.join(timeout)\n",
            "    _SHARED_SEQUENCES[self.uid] = None\n",
            "\n",
            "  @abstractmethod\n",
            "  def _run(self):\n",
            "    \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n",
            "    raise NotImplementedError\n",
            "\n",
            "  @abstractmethod\n",
            "  def _get_executor_init(self, workers):\n",
            "    \"\"\"Gets the Pool initializer for multiprocessing.\n",
            "\n",
            "    Arguments:\n",
            "        workers: Number of workers.\n",
            "\n",
            "    Returns:\n",
            "        Function, a Function to initialize the pool\n",
            "    \"\"\"\n",
            "    raise NotImplementedError\n",
            "\n",
            "  @abstractmethod\n",
            "  def get(self):\n",
            "    \"\"\"Creates a generator to extract data from the queue.\n",
            "\n",
            "    Skip the data if it is `None`.\n",
            "    # Returns\n",
            "        Generator yielding tuples `(inputs, targets)`\n",
            "            or `(inputs, targets, sample_weights)`.\n",
            "    \"\"\"\n",
            "    raise NotImplementedError\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.OrderedEnqueuer')\n",
            "class OrderedEnqueuer(SequenceEnqueuer):\n",
            "  \"\"\"Builds a Enqueuer from a Sequence.\n",
            "\n",
            "  Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n",
            "\n",
            "  Arguments:\n",
            "      sequence: A `tf.keras.utils.data_utils.Sequence` object.\n",
            "      use_multiprocessing: use multiprocessing if True, otherwise threading\n",
            "      shuffle: whether to shuffle the data at the beginning of each epoch\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, sequence, use_multiprocessing=False, shuffle=False):\n",
            "    super(OrderedEnqueuer, self).__init__(sequence, use_multiprocessing)\n",
            "    self.shuffle = shuffle\n",
            "\n",
            "  def _get_executor_init(self, workers):\n",
            "    \"\"\"Gets the Pool initializer for multiprocessing.\n",
            "\n",
            "    Arguments:\n",
            "        workers: Number of workers.\n",
            "\n",
            "    Returns:\n",
            "        Function, a Function to initialize the pool\n",
            "    \"\"\"\n",
            "    def pool_fn(seqs):\n",
            "      pool = multiprocessing.Pool(\n",
            "          workers, initializer=init_pool_generator,\n",
            "          initargs=(seqs, None, get_worker_id_queue()))\n",
            "      _DATA_POOLS.add(pool)\n",
            "      return pool\n",
            "\n",
            "    return pool_fn\n",
            "\n",
            "  def _wait_queue(self):\n",
            "    \"\"\"Wait for the queue to be empty.\"\"\"\n",
            "    while True:\n",
            "      time.sleep(0.1)\n",
            "      if self.queue.unfinished_tasks == 0 or self.stop_signal.is_set():\n",
            "        return\n",
            "\n",
            "  def _run(self):\n",
            "    \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n",
            "    sequence = list(range(len(self.sequence)))\n",
            "    self._send_sequence()  # Share the initial sequence\n",
            "    while True:\n",
            "      if self.shuffle:\n",
            "        random.shuffle(sequence)\n",
            "\n",
            "      with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
            "        for i in sequence:\n",
            "          if self.stop_signal.is_set():\n",
            "            return\n",
            "\n",
            "          self.queue.put(\n",
            "              executor.apply_async(get_index, (self.uid, i)), block=True)\n",
            "\n",
            "        # Done with the current epoch, waiting for the final batches\n",
            "        self._wait_queue()\n",
            "\n",
            "        if self.stop_signal.is_set():\n",
            "          # We're done\n",
            "          return\n",
            "\n",
            "      # Call the internal on epoch end.\n",
            "      self.sequence.on_epoch_end()\n",
            "      self._send_sequence()  # Update the pool\n",
            "\n",
            "  def get(self):\n",
            "    \"\"\"Creates a generator to extract data from the queue.\n",
            "\n",
            "    Skip the data if it is `None`.\n",
            "\n",
            "    Yields:\n",
            "        The next element in the queue, i.e. a tuple\n",
            "        `(inputs, targets)` or\n",
            "        `(inputs, targets, sample_weights)`.\n",
            "    \"\"\"\n",
            "    try:\n",
            "      while self.is_running():\n",
            "        inputs = self.queue.get(block=True).get()\n",
            "        self.queue.task_done()\n",
            "        if inputs is not None:\n",
            "          yield inputs\n",
            "    except Exception:  # pylint: disable=broad-except\n",
            "      self.stop()\n",
            "      six.reraise(*sys.exc_info())\n",
            "\n",
            "\n",
            "def init_pool_generator(gens, random_seed=None, id_queue=None):\n",
            "  \"\"\"Initializer function for pool workers.\n",
            "\n",
            "  Args:\n",
            "    gens: State which should be made available to worker processes.\n",
            "    random_seed: An optional value with which to seed child processes.\n",
            "    id_queue: A multiprocessing Queue of worker ids. This is used to indicate\n",
            "      that a worker process was created by Keras and can be terminated using\n",
            "      the cleanup_all_keras_forkpools utility.\n",
            "  \"\"\"\n",
            "  global _SHARED_SEQUENCES\n",
            "  _SHARED_SEQUENCES = gens\n",
            "\n",
            "  worker_proc = multiprocessing.current_process()\n",
            "\n",
            "  # name isn't used for anything, but setting a more descriptive name is helpful\n",
            "  # when diagnosing orphaned processes.\n",
            "  worker_proc.name = 'Keras_worker_{}'.format(worker_proc.name)\n",
            "\n",
            "  if random_seed is not None:\n",
            "    np.random.seed(random_seed + worker_proc.ident)\n",
            "\n",
            "  if id_queue is not None:\n",
            "    # If a worker dies during init, the pool will just create a replacement.\n",
            "    id_queue.put(worker_proc.ident, block=True, timeout=0.1)\n",
            "\n",
            "\n",
            "def next_sample(uid):\n",
            "  \"\"\"Gets the next value from the generator `uid`.\n",
            "\n",
            "  To allow multiple generators to be used at the same time, we use `uid` to\n",
            "  get a specific one. A single generator would cause the validation to\n",
            "  overwrite the training generator.\n",
            "\n",
            "  Arguments:\n",
            "      uid: int, generator identifier\n",
            "\n",
            "  Returns:\n",
            "      The next value of generator `uid`.\n",
            "  \"\"\"\n",
            "  return six.next(_SHARED_SEQUENCES[uid])\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.GeneratorEnqueuer')\n",
            "class GeneratorEnqueuer(SequenceEnqueuer):\n",
            "  \"\"\"Builds a queue out of a data generator.\n",
            "\n",
            "  The provided generator can be finite in which case the class will throw\n",
            "  a `StopIteration` exception.\n",
            "\n",
            "  Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n",
            "\n",
            "  Arguments:\n",
            "      generator: a generator function which yields data\n",
            "      use_multiprocessing: use multiprocessing if True, otherwise threading\n",
            "      wait_time: time to sleep in-between calls to `put()`\n",
            "      random_seed: Initial seed for workers,\n",
            "          will be incremented by one for each worker.\n",
            "  \"\"\"\n",
            "\n",
            "  def __init__(self, sequence,\n",
            "               use_multiprocessing=False,\n",
            "               random_seed=None):\n",
            "    super(GeneratorEnqueuer, self).__init__(sequence, use_multiprocessing)\n",
            "    self.random_seed = random_seed\n",
            "\n",
            "  def _get_executor_init(self, workers):\n",
            "    \"\"\"Gets the Pool initializer for multiprocessing.\n",
            "\n",
            "    Arguments:\n",
            "      workers: Number of works.\n",
            "\n",
            "    Returns:\n",
            "        A Function to initialize the pool\n",
            "    \"\"\"\n",
            "    def pool_fn(seqs):\n",
            "      pool = multiprocessing.Pool(\n",
            "          workers, initializer=init_pool_generator,\n",
            "          initargs=(seqs, self.random_seed, get_worker_id_queue()))\n",
            "      _DATA_POOLS.add(pool)\n",
            "      return pool\n",
            "    return pool_fn\n",
            "\n",
            "  def _run(self):\n",
            "    \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n",
            "    self._send_sequence()  # Share the initial generator\n",
            "    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n",
            "      while True:\n",
            "        if self.stop_signal.is_set():\n",
            "          return\n",
            "\n",
            "        self.queue.put(\n",
            "            executor.apply_async(next_sample, (self.uid,)), block=True)\n",
            "\n",
            "  def get(self):\n",
            "    \"\"\"Creates a generator to extract data from the queue.\n",
            "\n",
            "    Skip the data if it is `None`.\n",
            "\n",
            "    Yields:\n",
            "        The next element in the queue, i.e. a tuple\n",
            "        `(inputs, targets)` or\n",
            "        `(inputs, targets, sample_weights)`.\n",
            "    \"\"\"\n",
            "    try:\n",
            "      while self.is_running():\n",
            "        inputs = self.queue.get(block=True).get()\n",
            "        self.queue.task_done()\n",
            "        if inputs is not None:\n",
            "          yield inputs\n",
            "    except StopIteration:\n",
            "      # Special case for finite generators\n",
            "      last_ones = []\n",
            "      while self.queue.qsize() > 0:\n",
            "        last_ones.append(self.queue.get(block=True))\n",
            "      # Wait for them to complete\n",
            "      for f in last_ones:\n",
            "        f.wait()\n",
            "      # Keep the good ones\n",
            "      last_ones = [future.get() for future in last_ones if future.successful()]\n",
            "      for inputs in last_ones:\n",
            "        if inputs is not None:\n",
            "          yield inputs\n",
            "    except Exception as e:  # pylint: disable=broad-except\n",
            "      self.stop()\n",
            "      if 'generator already executing' in str(e):\n",
            "        raise RuntimeError(\n",
            "            'Your generator is NOT thread-safe. '\n",
            "            'Keras requires a thread-safe generator when '\n",
            "            '`use_multiprocessing=False, workers > 1`. ')\n",
            "      six.reraise(*sys.exc_info())\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "\"\"\"Utils related to keras metrics.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import functools\n",
            "import weakref\n",
            "\n",
            "from enum import Enum\n",
            "\n",
            "from tensorflow.python.distribute import distribution_strategy_context\n",
            "from tensorflow.python.framework import dtypes\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.keras.utils import tf_utils\n",
            "from tensorflow.python.keras.utils.generic_utils import to_list\n",
            "from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import check_ops\n",
            "from tensorflow.python.ops import control_flow_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import nn_ops\n",
            "from tensorflow.python.ops import weights_broadcast_ops\n",
            "from tensorflow.python.util import tf_decorator\n",
            "\n",
            "NEG_INF = -1e10\n",
            "\n",
            "\n",
            "class Reduction(Enum):\n",
            "  \"\"\"Types of metrics reduction.\n",
            "\n",
            "  Contains the following values:\n",
            "\n",
            "  * `SUM`: Scalar sum of weighted values.\n",
            "  * `SUM_OVER_BATCH_SIZE`: Scalar sum of weighted values divided by\n",
            "        number of elements.\n",
            "  * `WEIGHTED_MEAN`: Scalar sum of weighted values divided by sum of weights.\n",
            "  \"\"\"\n",
            "  SUM = 'sum'\n",
            "  SUM_OVER_BATCH_SIZE = 'sum_over_batch_size'\n",
            "  WEIGHTED_MEAN = 'weighted_mean'\n",
            "\n",
            "\n",
            "def update_state_wrapper(update_state_fn):\n",
            "  \"\"\"Decorator to wrap metric `update_state()` with `add_update()`.\n",
            "\n",
            "  Args:\n",
            "    update_state_fn: function that accumulates metric statistics.\n",
            "\n",
            "  Returns:\n",
            "    Decorated function that wraps `update_state_fn()` with `add_update()`.\n",
            "  \"\"\"\n",
            "\n",
            "  def decorated(metric_obj, *args, **kwargs):\n",
            "    \"\"\"Decorated function with `add_update()`.\"\"\"\n",
            "\n",
            "    with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\n",
            "      update_op = update_state_fn(*args, **kwargs)\n",
            "    if update_op is not None:  # update_op will be None in eager execution.\n",
            "      metric_obj.add_update(update_op, inputs=True)\n",
            "    return update_op\n",
            "\n",
            "  return tf_decorator.make_decorator(update_state_fn, decorated)\n",
            "\n",
            "\n",
            "def result_wrapper(result_fn):\n",
            "  \"\"\"Decorator to wrap metric `result()` function in `merge_call()`.\n",
            "\n",
            "  Result computation is an idempotent operation that simply calculates the\n",
            "  metric value using the state variables.\n",
            "\n",
            "  If metric state variables are distributed across replicas/devices and\n",
            "  `result()` is requested from the context of one device - This function wraps\n",
            "  `result()` in a distribution strategy `merge_call()`. With this,\n",
            "  the metric state variables will be aggregated across devices.\n",
            "\n",
            "  Args:\n",
            "    result_fn: function that computes the metric result.\n",
            "\n",
            "  Returns:\n",
            "    Decorated function that wraps `result_fn()` in distribution strategy\n",
            "    `merge_call()`.\n",
            "  \"\"\"\n",
            "\n",
            "  def decorated(_, *args):\n",
            "    \"\"\"Decorated function with merge_call.\"\"\"\n",
            "    replica_context = distribution_strategy_context.get_replica_context()\n",
            "    if replica_context is None:  # if in cross replica context already\n",
            "      result_t = array_ops.identity(result_fn(*args))\n",
            "    else:\n",
            "      # TODO(psv): Test distribution of metrics using different distribution\n",
            "      # strategies.\n",
            "\n",
            "      # Creating a wrapper for merge_fn. merge_call invokes the given merge_fn\n",
            "      # with distribution object as the first parameter. We create a wrapper\n",
            "      # here so that the result function need not have that parameter.\n",
            "      def merge_fn_wrapper(distribution, merge_fn, *args):\n",
            "        # We will get `PerReplica` merge function. Taking the first one as all\n",
            "        # are identical copies of the function that we had passed below.\n",
            "        merged_result_fn = (\n",
            "            distribution.experimental_local_results(merge_fn)[0](*args))\n",
            "\n",
            "        # Wrapping result in identity so that control dependency between\n",
            "        # update_op from `update_state` and result works in case result returns\n",
            "        # a tensor.\n",
            "        return array_ops.identity(merged_result_fn)\n",
            "\n",
            "      # Wrapping result in merge_call. merge_call is used when we want to leave\n",
            "      # replica mode and compute a value in cross replica mode.\n",
            "      result_t = replica_context.merge_call(\n",
            "          merge_fn_wrapper, args=(result_fn,) + args)\n",
            "    return result_t\n",
            "\n",
            "  return tf_decorator.make_decorator(result_fn, decorated)\n",
            "\n",
            "\n",
            "def weakmethod(method):\n",
            "  \"\"\"Creates a weak reference to the bound method.\"\"\"\n",
            "\n",
            "  cls = method.im_class\n",
            "  func = method.im_func\n",
            "  instance_ref = weakref.ref(method.im_self)\n",
            "\n",
            "  @functools.wraps(method)\n",
            "  def inner(*args, **kwargs):\n",
            "    return func.__get__(instance_ref(), cls)(*args, **kwargs)\n",
            "\n",
            "  del method\n",
            "  return inner\n",
            "\n",
            "\n",
            "def assert_thresholds_range(thresholds):\n",
            "  if thresholds is not None:\n",
            "    invalid_thresholds = [t for t in thresholds if t is None or t < 0 or t > 1]\n",
            "    if invalid_thresholds:\n",
            "      raise ValueError(\n",
            "          'Threshold values must be in [0, 1]. Invalid values: {}'.format(\n",
            "              invalid_thresholds))\n",
            "\n",
            "\n",
            "def parse_init_thresholds(thresholds, default_threshold=0.5):\n",
            "  if thresholds is not None:\n",
            "    assert_thresholds_range(to_list(thresholds))\n",
            "  thresholds = to_list(default_threshold if thresholds is None else thresholds)\n",
            "  return thresholds\n",
            "\n",
            "\n",
            "class ConfusionMatrix(Enum):\n",
            "  TRUE_POSITIVES = 'tp'\n",
            "  FALSE_POSITIVES = 'fp'\n",
            "  TRUE_NEGATIVES = 'tn'\n",
            "  FALSE_NEGATIVES = 'fn'\n",
            "\n",
            "\n",
            "class AUCCurve(Enum):\n",
            "  \"\"\"Type of AUC Curve (ROC or PR).\"\"\"\n",
            "  ROC = 'ROC'\n",
            "  PR = 'PR'\n",
            "\n",
            "  @staticmethod\n",
            "  def from_str(key):\n",
            "    if key in ('pr', 'PR'):\n",
            "      return AUCCurve.PR\n",
            "    elif key in ('roc', 'ROC'):\n",
            "      return AUCCurve.ROC\n",
            "    else:\n",
            "      raise ValueError('Invalid AUC curve value \"%s\".' % key)\n",
            "\n",
            "\n",
            "class AUCSummationMethod(Enum):\n",
            "  \"\"\"Type of AUC summation method.\n",
            "\n",
            "  https://en.wikipedia.org/wiki/Riemann_sum)\n",
            "\n",
            "  Contains the following values:\n",
            "  * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For\n",
            "    `PR` curve, interpolates (true/false) positives but not the ratio that is\n",
            "    precision (see Davis & Goadrich 2006 for details).\n",
            "  * 'minoring': Applies left summation for increasing intervals and right\n",
            "    summation for decreasing intervals.\n",
            "  * 'majoring': Applies right summation for increasing intervals and left\n",
            "    summation for decreasing intervals.\n",
            "  \"\"\"\n",
            "  INTERPOLATION = 'interpolation'\n",
            "  MAJORING = 'majoring'\n",
            "  MINORING = 'minoring'\n",
            "\n",
            "  @staticmethod\n",
            "  def from_str(key):\n",
            "    if key in ('interpolation', 'Interpolation'):\n",
            "      return AUCSummationMethod.INTERPOLATION\n",
            "    elif key in ('majoring', 'Majoring'):\n",
            "      return AUCSummationMethod.MAJORING\n",
            "    elif key in ('minoring', 'Minoring'):\n",
            "      return AUCSummationMethod.MINORING\n",
            "    else:\n",
            "      raise ValueError('Invalid AUC summation method value \"%s\".' % key)\n",
            "\n",
            "\n",
            "def update_confusion_matrix_variables(variables_to_update,\n",
            "                                      y_true,\n",
            "                                      y_pred,\n",
            "                                      thresholds,\n",
            "                                      top_k=None,\n",
            "                                      class_id=None,\n",
            "                                      sample_weight=None):\n",
            "  \"\"\"Returns op to update the given confusion matrix variables.\n",
            "\n",
            "  For every pair of values in y_true and y_pred:\n",
            "\n",
            "  true_positive: y_true == True and y_pred > thresholds\n",
            "  false_negatives: y_true == True and y_pred <= thresholds\n",
            "  true_negatives: y_true == False and y_pred <= thresholds\n",
            "  false_positive: y_true == False and y_pred > thresholds\n",
            "\n",
            "  The results will be weighted and added together. When multiple thresholds are\n",
            "  provided, we will repeat the same for every threshold.\n",
            "\n",
            "  For estimation of these metrics over a stream of data, the function creates an\n",
            "  `update_op` operation that updates the given variables.\n",
            "\n",
            "  If `sample_weight` is `None`, weights default to 1.\n",
            "  Use weights of 0 to mask values.\n",
            "\n",
            "  Args:\n",
            "    variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n",
            "      and corresponding variables to update as values.\n",
            "    y_true: A `Tensor` whose shape matches `y_pred`. Will be cast to `bool`.\n",
            "    y_pred: A floating point `Tensor` of arbitrary shape and whose values are in\n",
            "      the range `[0, 1]`.\n",
            "    thresholds: A float value or a python list or tuple of float thresholds in\n",
            "      `[0, 1]`, or NEG_INF (used when top_k is set).\n",
            "    top_k: Optional int, indicates that the positive labels should be limited to\n",
            "      the top k predictions.\n",
            "    class_id: Optional int, limits the prediction and labels to the class\n",
            "      specified by this argument.\n",
            "    sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n",
            "      `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions must\n",
            "      be either `1`, or the same as the corresponding `y_true` dimension).\n",
            "\n",
            "  Returns:\n",
            "    Update op.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: If `y_pred` and `y_true` have mismatched shapes, or if\n",
            "      `sample_weight` is not `None` and its shape doesn't match `y_pred`, or if\n",
            "      `variables_to_update` contains invalid keys.\n",
            "  \"\"\"\n",
            "  if variables_to_update is None:\n",
            "    return\n",
            "  y_true = math_ops.cast(y_true, dtype=dtypes.float32)\n",
            "  y_pred = math_ops.cast(y_pred, dtype=dtypes.float32)\n",
            "  y_pred.shape.assert_is_compatible_with(y_true.shape)\n",
            "\n",
            "  if not any(\n",
            "      key for key in variables_to_update if key in list(ConfusionMatrix)):\n",
            "    raise ValueError(\n",
            "        'Please provide at least one valid confusion matrix '\n",
            "        'variable to update. Valid variable key options are: \"{}\". '\n",
            "        'Received: \"{}\"'.format(\n",
            "            list(ConfusionMatrix), variables_to_update.keys()))\n",
            "\n",
            "  invalid_keys = [\n",
            "      key for key in variables_to_update if key not in list(ConfusionMatrix)\n",
            "  ]\n",
            "  if invalid_keys:\n",
            "    raise ValueError(\n",
            "        'Invalid keys: {}. Valid variable key options are: \"{}\"'.format(\n",
            "            invalid_keys, list(ConfusionMatrix)))\n",
            "\n",
            "  with ops.control_dependencies([\n",
            "      check_ops.assert_greater_equal(\n",
            "          y_pred,\n",
            "          math_ops.cast(0.0, dtype=y_pred.dtype),\n",
            "          message='predictions must be >= 0'),\n",
            "      check_ops.assert_less_equal(\n",
            "          y_pred,\n",
            "          math_ops.cast(1.0, dtype=y_pred.dtype),\n",
            "          message='predictions must be <= 1')\n",
            "  ]):\n",
            "    y_pred, y_true, sample_weight = squeeze_or_expand_dimensions(\n",
            "        y_pred, y_true, sample_weight)\n",
            "\n",
            "  if top_k is not None:\n",
            "    y_pred = _filter_top_k(y_pred, top_k)\n",
            "  if class_id is not None:\n",
            "    y_true = y_true[..., class_id]\n",
            "    y_pred = y_pred[..., class_id]\n",
            "\n",
            "  thresholds = to_list(thresholds)\n",
            "  num_thresholds = len(thresholds)\n",
            "  num_predictions = array_ops.size(y_pred)\n",
            "\n",
            "  # Reshape predictions and labels.\n",
            "  predictions_2d = array_ops.reshape(y_pred, [1, -1])\n",
            "  labels_2d = array_ops.reshape(\n",
            "      math_ops.cast(y_true, dtype=dtypes.bool), [1, -1])\n",
            "\n",
            "  # Tile the thresholds for every prediction.\n",
            "  thresh_tiled = array_ops.tile(\n",
            "      array_ops.expand_dims(array_ops.constant(thresholds), 1),\n",
            "      array_ops.stack([1, num_predictions]))\n",
            "\n",
            "  # Tile the predictions for every threshold.\n",
            "  preds_tiled = array_ops.tile(predictions_2d, [num_thresholds, 1])\n",
            "\n",
            "  # Compare predictions and threshold.\n",
            "  pred_is_pos = math_ops.greater(preds_tiled, thresh_tiled)\n",
            "\n",
            "  # Tile labels by number of thresholds\n",
            "  label_is_pos = array_ops.tile(labels_2d, [num_thresholds, 1])\n",
            "\n",
            "  if sample_weight is not None:\n",
            "    weights = weights_broadcast_ops.broadcast_weights(\n",
            "        math_ops.cast(sample_weight, dtype=dtypes.float32), y_pred)\n",
            "    weights_tiled = array_ops.tile(\n",
            "        array_ops.reshape(weights, [1, -1]), [num_thresholds, 1])\n",
            "  else:\n",
            "    weights_tiled = None\n",
            "\n",
            "  update_ops = []\n",
            "\n",
            "  def weighted_assign_add(label, pred, weights, var):\n",
            "    label_and_pred = math_ops.cast(\n",
            "        math_ops.logical_and(label, pred), dtype=dtypes.float32)\n",
            "    if weights is not None:\n",
            "      label_and_pred *= weights\n",
            "    return var.assign_add(math_ops.reduce_sum(label_and_pred, 1))\n",
            "\n",
            "  loop_vars = {\n",
            "      ConfusionMatrix.TRUE_POSITIVES: (label_is_pos, pred_is_pos),\n",
            "  }\n",
            "  update_tn = ConfusionMatrix.TRUE_NEGATIVES in variables_to_update\n",
            "  update_fp = ConfusionMatrix.FALSE_POSITIVES in variables_to_update\n",
            "  update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update\n",
            "\n",
            "  if update_fn or update_tn:\n",
            "    pred_is_neg = math_ops.logical_not(pred_is_pos)\n",
            "    loop_vars[ConfusionMatrix.FALSE_NEGATIVES] = (label_is_pos, pred_is_neg)\n",
            "\n",
            "  if update_fp or update_tn:\n",
            "    label_is_neg = math_ops.logical_not(label_is_pos)\n",
            "    loop_vars[ConfusionMatrix.FALSE_POSITIVES] = (label_is_neg, pred_is_pos)\n",
            "    if update_tn:\n",
            "      loop_vars[ConfusionMatrix.TRUE_NEGATIVES] = (label_is_neg, pred_is_neg)\n",
            "\n",
            "  for matrix_cond, (label, pred) in loop_vars.items():\n",
            "    if matrix_cond in variables_to_update:\n",
            "      update_ops.append(\n",
            "          weighted_assign_add(label, pred, weights_tiled,\n",
            "                              variables_to_update[matrix_cond]))\n",
            "  return control_flow_ops.group(update_ops)\n",
            "\n",
            "\n",
            "def _filter_top_k(x, k):\n",
            "  \"\"\"Filters top-k values in the last dim of x and set the rest to NEG_INF.\n",
            "\n",
            "  Used for computing top-k prediction values in dense labels (which has the same\n",
            "  shape as predictions) for recall and precision top-k metrics.\n",
            "\n",
            "  Args:\n",
            "    x: tensor with any dimensions.\n",
            "    k: the number of values to keep.\n",
            "\n",
            "  Returns:\n",
            "    tensor with same shape and dtype as x.\n",
            "  \"\"\"\n",
            "  _, top_k_idx = nn_ops.top_k(x, k, sorted=False)\n",
            "  top_k_mask = math_ops.reduce_sum(\n",
            "      array_ops.one_hot(top_k_idx, x.shape[-1], axis=-1), axis=-2)\n",
            "  return x * top_k_mask + NEG_INF * (1 - top_k_mask)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities for multi-gpu training.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.keras.engine.training import Model\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "def _get_available_devices():\n",
            "  return [x.name for x in K.get_session().list_devices()]\n",
            "\n",
            "\n",
            "def _normalize_device_name(name):\n",
            "  name = '/' + name.lower().split('device:')[1]\n",
            "  return name\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.multi_gpu_model')\n",
            "def multi_gpu_model(model, gpus, cpu_merge=True, cpu_relocation=False):\n",
            "  \"\"\"Replicates a model on different GPUs.\n",
            "\n",
            "  Specifically, this function implements single-machine\n",
            "  multi-GPU data parallelism. It works in the following way:\n",
            "\n",
            "  - Divide the model's input(s) into multiple sub-batches.\n",
            "  - Apply a model copy on each sub-batch. Every model copy\n",
            "      is executed on a dedicated GPU.\n",
            "  - Concatenate the results (on CPU) into one big batch.\n",
            "\n",
            "  E.g. if your `batch_size` is 64 and you use `gpus=2`,\n",
            "  then we will divide the input into 2 sub-batches of 32 samples,\n",
            "  process each sub-batch on one GPU, then return the full\n",
            "  batch of 64 processed samples.\n",
            "\n",
            "  This induces quasi-linear speedup on up to 8 GPUs.\n",
            "\n",
            "  This function is only available with the TensorFlow backend\n",
            "  for the time being.\n",
            "\n",
            "  Arguments:\n",
            "      model: A Keras model instance. To avoid OOM errors,\n",
            "          this model could have been built on CPU, for instance\n",
            "          (see usage example below).\n",
            "      gpus: Integer >= 2, number of on GPUs on which to create\n",
            "          model replicas.\n",
            "      cpu_merge: A boolean value to identify whether to force\n",
            "          merging model weights under the scope of the CPU or not.\n",
            "      cpu_relocation: A boolean value to identify whether to\n",
            "          create the model's weights under the scope of the CPU.\n",
            "          If the model is not defined under any preceding device\n",
            "          scope, you can still rescue it by activating this option.\n",
            "\n",
            "  Returns:\n",
            "      A Keras `Model` instance which can be used just like the initial\n",
            "      `model` argument, but which distributes its workload on multiple GPUs.\n",
            "\n",
            "  Example 1: Training models with weights merge on CPU\n",
            "\n",
            "  ```python\n",
            "      import tensorflow as tf\n",
            "      from keras.applications import Xception\n",
            "      from keras.utils import multi_gpu_model\n",
            "      import numpy as np\n",
            "\n",
            "      num_samples = 1000\n",
            "      height = 224\n",
            "      width = 224\n",
            "      num_classes = 1000\n",
            "\n",
            "      # Instantiate the base model (or \"template\" model).\n",
            "      # We recommend doing this with under a CPU device scope,\n",
            "      # so that the model's weights are hosted on CPU memory.\n",
            "      # Otherwise they may end up hosted on a GPU, which would\n",
            "      # complicate weight sharing.\n",
            "      with tf.device('/cpu:0'):\n",
            "          model = Xception(weights=None,\n",
            "                           input_shape=(height, width, 3),\n",
            "                           classes=num_classes)\n",
            "\n",
            "      # Replicates the model on 8 GPUs.\n",
            "      # This assumes that your machine has 8 available GPUs.\n",
            "      parallel_model = multi_gpu_model(model, gpus=8)\n",
            "      parallel_model.compile(loss='categorical_crossentropy',\n",
            "                             optimizer='rmsprop')\n",
            "\n",
            "      # Generate dummy data.\n",
            "      x = np.random.random((num_samples, height, width, 3))\n",
            "      y = np.random.random((num_samples, num_classes))\n",
            "\n",
            "      # This `fit` call will be distributed on 8 GPUs.\n",
            "      # Since the batch size is 256, each GPU will process 32 samples.\n",
            "      parallel_model.fit(x, y, epochs=20, batch_size=256)\n",
            "\n",
            "      # Save model via the template model (which shares the same weights):\n",
            "      model.save('my_model.h5')\n",
            "  ```\n",
            "\n",
            "  Example 2: Training models with weights merge on CPU using cpu_relocation\n",
            "\n",
            "  ```python\n",
            "       ..\n",
            "       # Not needed to change the device scope for model definition:\n",
            "       model = Xception(weights=None, ..)\n",
            "\n",
            "       try:\n",
            "           model = multi_gpu_model(model, cpu_relocation=True)\n",
            "           print(\"Training using multiple GPUs..\")\n",
            "       except:\n",
            "           print(\"Training using single GPU or CPU..\")\n",
            "\n",
            "       model.compile(..)\n",
            "       ..\n",
            "  ```\n",
            "\n",
            "  Example 3: Training models with weights merge on GPU (recommended for NV-link)\n",
            "\n",
            "  ```python\n",
            "       ..\n",
            "       # Not needed to change the device scope for model definition:\n",
            "       model = Xception(weights=None, ..)\n",
            "\n",
            "       try:\n",
            "           model = multi_gpu_model(model, cpu_merge=False)\n",
            "           print(\"Training using multiple GPUs..\")\n",
            "       except:\n",
            "           print(\"Training using single GPU or CPU..\")\n",
            "       model.compile(..)\n",
            "       ..\n",
            "  ```\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if the `gpus` argument does not match available devices.\n",
            "  \"\"\"\n",
            "  # pylint: disable=g-import-not-at-top\n",
            "  from tensorflow.python.keras.layers.core import Lambda\n",
            "  from tensorflow.python.keras.layers.merge import concatenate\n",
            "\n",
            "  if isinstance(gpus, (list, tuple)):\n",
            "    if len(gpus) <= 1:\n",
            "      raise ValueError('For multi-gpu usage to be effective, '\n",
            "                       'call `multi_gpu_model` with `len(gpus) >= 2`. '\n",
            "                       'Received: `gpus=%s`' % gpus)\n",
            "    num_gpus = len(gpus)\n",
            "    target_gpu_ids = gpus\n",
            "  else:\n",
            "    if gpus <= 1:\n",
            "      raise ValueError('For multi-gpu usage to be effective, '\n",
            "                       'call `multi_gpu_model` with `gpus >= 2`. '\n",
            "                       'Received: `gpus=%s`' % gpus)\n",
            "    num_gpus = gpus\n",
            "    target_gpu_ids = range(num_gpus)\n",
            "\n",
            "  target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in target_gpu_ids]\n",
            "  available_devices = _get_available_devices()\n",
            "  available_devices = [\n",
            "      _normalize_device_name(name) for name in available_devices\n",
            "  ]\n",
            "  for device in target_devices:\n",
            "    if device not in available_devices:\n",
            "      raise ValueError('To call `multi_gpu_model` with `gpus=%s`, '\n",
            "                       'we expect the following devices to be available: %s. '\n",
            "                       'However this machine only has: %s. '\n",
            "                       'Try reducing `gpus`.' % (gpus, target_devices,\n",
            "                                                 available_devices))\n",
            "\n",
            "  def get_slice(data, i, parts):\n",
            "    \"\"\"Slice an array into `parts` slices and return slice `i`.\n",
            "\n",
            "    Arguments:\n",
            "      data: array to slice.\n",
            "      i: index of slice to return.\n",
            "      parts: number of slices to make.\n",
            "\n",
            "    Returns:\n",
            "      Slice `i` of `data`.\n",
            "    \"\"\"\n",
            "    shape = array_ops.shape(data)\n",
            "    batch_size = shape[:1]\n",
            "    input_shape = shape[1:]\n",
            "    step = batch_size // parts\n",
            "    if i == parts - 1:\n",
            "      size = batch_size - step * i\n",
            "    else:\n",
            "      size = step\n",
            "    size = array_ops.concat([size, input_shape], axis=0)\n",
            "    stride = array_ops.concat([step, input_shape * 0], axis=0)\n",
            "    start = stride * i\n",
            "    return array_ops.slice(data, start, size)\n",
            "\n",
            "  # Relocate the model definition under CPU device scope if needed\n",
            "  if cpu_relocation:\n",
            "    from tensorflow.python.keras.models import clone_model  # pylint: disable=g-import-not-at-top\n",
            "    with ops.device('/cpu:0'):\n",
            "      model = clone_model(model)\n",
            "\n",
            "  all_outputs = []\n",
            "  for i in range(len(model.outputs)):\n",
            "    all_outputs.append([])\n",
            "\n",
            "  # Place a copy of the model on each GPU,\n",
            "  # each getting a slice of the inputs.\n",
            "  for i, gpu_id in enumerate(target_gpu_ids):\n",
            "    with ops.device('/gpu:%d' % gpu_id):\n",
            "      with K.name_scope('replica_%d' % gpu_id):\n",
            "        inputs = []\n",
            "        # Retrieve a slice of the input.\n",
            "        for x in model.inputs:\n",
            "          input_shape = tuple(x.shape.as_list())[1:]\n",
            "          slice_i = Lambda(\n",
            "              get_slice,\n",
            "              output_shape=input_shape,\n",
            "              arguments={\n",
            "                  'i': i,\n",
            "                  'parts': num_gpus\n",
            "              })(\n",
            "                  x)\n",
            "          inputs.append(slice_i)\n",
            "\n",
            "        # Apply model on slice\n",
            "        # (creating a model replica on the target device).\n",
            "        outputs = model(inputs)\n",
            "        if not isinstance(outputs, list):\n",
            "          outputs = [outputs]\n",
            "\n",
            "        # Save the outputs for merging back together later.\n",
            "        for o in range(len(outputs)):\n",
            "          all_outputs[o].append(outputs[o])\n",
            "\n",
            "  # Deduplicate output names to handle Siamese networks.\n",
            "  occurrences = {}\n",
            "  for n in model.output_names:\n",
            "    if n not in occurrences:\n",
            "      occurrences[n] = 1\n",
            "    else:\n",
            "      occurrences[n] += 1\n",
            "  conflict_counter = {n: 0 for n, count in occurrences.items() if count > 1}\n",
            "  output_names = []\n",
            "  for n in model.output_names:\n",
            "    if n in conflict_counter:\n",
            "      conflict_counter[n] += 1\n",
            "      n += '_%d' % conflict_counter[n]\n",
            "    output_names.append(n)\n",
            "\n",
            "  # Merge outputs under expected scope.\n",
            "  with ops.device('/cpu:0' if cpu_merge else '/gpu:%d' % target_gpu_ids[0]):\n",
            "    merged = []\n",
            "    for name, outputs in zip(output_names, all_outputs):\n",
            "      merged.append(concatenate(outputs, axis=0, name=name))\n",
            "    return Model(model.inputs, merged)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=protected-access\n",
            "\"\"\"Utilities related to loss functions.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.distribute import distribution_strategy_context\n",
            "from tensorflow.python.framework import dtypes\n",
            "from tensorflow.python.framework import ops\n",
            "from tensorflow.python.keras import backend as K\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import confusion_matrix\n",
            "from tensorflow.python.ops import control_flow_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "from tensorflow.python.ops import weights_broadcast_ops\n",
            "from tensorflow.python.ops.losses import loss_reduction\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "# TODO(joshl/psv): Update references to ReductionV2 to point to its\n",
            "# new location.\n",
            "ReductionV2 = keras_export(  # pylint: disable=invalid-name\n",
            "    'keras.losses.Reduction', v1=[])(loss_reduction.ReductionV2)\n",
            "\n",
            "\n",
            "def squeeze_or_expand_dimensions(y_pred, y_true, sample_weight):\n",
            "  \"\"\"Squeeze or expand last dimension if needed.\n",
            "\n",
            "  1. Squeezes last dim of `y_pred` or `y_true` if their rank differs by 1\n",
            "  (using `confusion_matrix.remove_squeezable_dimensions`).\n",
            "  2. Squeezes or expands last dim of `sample_weight` if its rank differs by 1\n",
            "  from the new rank of `y_pred`.\n",
            "  If `sample_weight` is scalar, it is kept scalar.\n",
            "\n",
            "  This will use static shape if available. Otherwise, it will add graph\n",
            "  operations, which could result in a performance hit.\n",
            "\n",
            "  Args:\n",
            "    y_pred: Predicted values, a `Tensor` of arbitrary dimensions.\n",
            "    y_true: Optional label `Tensor` whose dimensions match `y_pred`.\n",
            "    sample_weight: Optional weight scalar or `Tensor` whose dimensions match\n",
            "      `y_pred`.\n",
            "\n",
            "  Returns:\n",
            "    Tuple of `y_pred`, `y_true` and `sample_weight`. Each of them possibly has\n",
            "    the last dimension squeezed,\n",
            "    `sample_weight` could be extended by one dimension.\n",
            "  \"\"\"\n",
            "  y_pred_shape = y_pred.shape\n",
            "  y_pred_rank = y_pred_shape.ndims\n",
            "  if y_true is not None:\n",
            "\n",
            "    # If sparse matrix is provided as `y_true`, the last dimension in `y_pred`\n",
            "    # may be > 1. Eg: y_true = [0, 1, 2] (shape=(3,)),\n",
            "    # y_pred = [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]] (shape=(3, 3))\n",
            "    # In this case, we should not try to remove squeezable dimension.\n",
            "    y_true_shape = y_true.shape\n",
            "    y_true_rank = y_true_shape.ndims\n",
            "    if (y_true_rank is not None) and (y_pred_rank is not None):\n",
            "      # Use static rank for `y_true` and `y_pred`.\n",
            "      if (y_pred_rank - y_true_rank != 1) or y_pred_shape[-1] == 1:\n",
            "        y_true, y_pred = confusion_matrix.remove_squeezable_dimensions(\n",
            "            y_true, y_pred)\n",
            "    else:\n",
            "      # Use dynamic rank.\n",
            "      rank_diff = array_ops.rank(y_pred) - array_ops.rank(y_true)\n",
            "      squeeze_dims = lambda: confusion_matrix.remove_squeezable_dimensions(  # pylint: disable=g-long-lambda\n",
            "          y_true, y_pred)\n",
            "      is_last_dim_1 = math_ops.equal(1, array_ops.shape(y_pred)[-1])\n",
            "      maybe_squeeze_dims = lambda: control_flow_ops.cond(  # pylint: disable=g-long-lambda\n",
            "          is_last_dim_1, squeeze_dims, lambda: (y_true, y_pred))\n",
            "      y_true, y_pred = control_flow_ops.cond(\n",
            "          math_ops.equal(1, rank_diff), maybe_squeeze_dims, squeeze_dims)\n",
            "\n",
            "  if sample_weight is None:\n",
            "    return y_pred, y_true, None\n",
            "\n",
            "  sample_weight = ops.convert_to_tensor(sample_weight)\n",
            "  weights_shape = sample_weight.shape\n",
            "  weights_rank = weights_shape.ndims\n",
            "  if weights_rank == 0:  # If weights is scalar, do nothing.\n",
            "    return y_pred, y_true, sample_weight\n",
            "\n",
            "  if (y_pred_rank is not None) and (weights_rank is not None):\n",
            "    # Use static rank.\n",
            "    if weights_rank - y_pred_rank == 1:\n",
            "      sample_weight = array_ops.squeeze(sample_weight, [-1])\n",
            "    elif y_pred_rank - weights_rank == 1:\n",
            "      sample_weight = array_ops.expand_dims(sample_weight, [-1])\n",
            "    return y_pred, y_true, sample_weight\n",
            "\n",
            "  # Use dynamic rank.\n",
            "  weights_rank_tensor = array_ops.rank(sample_weight)\n",
            "  rank_diff = weights_rank_tensor - array_ops.rank(y_pred)\n",
            "  maybe_squeeze_weights = lambda: array_ops.squeeze(sample_weight, [-1])\n",
            "\n",
            "  def _maybe_expand_weights():\n",
            "    return control_flow_ops.cond(\n",
            "        math_ops.equal(rank_diff,\n",
            "                       -1), lambda: array_ops.expand_dims(sample_weight, [-1]),\n",
            "        lambda: sample_weight)\n",
            "\n",
            "  def _maybe_adjust_weights():\n",
            "    return control_flow_ops.cond(\n",
            "        math_ops.equal(rank_diff, 1), maybe_squeeze_weights,\n",
            "        _maybe_expand_weights)\n",
            "\n",
            "  # squeeze or expand last dim of `sample_weight` if its rank differs by 1\n",
            "  # from the new rank of `y_pred`.\n",
            "  sample_weight = control_flow_ops.cond(\n",
            "      math_ops.equal(weights_rank_tensor, 0), lambda: sample_weight,\n",
            "      _maybe_adjust_weights)\n",
            "  return y_pred, y_true, sample_weight\n",
            "\n",
            "\n",
            "def _safe_mean(losses, num_present):\n",
            "  \"\"\"Computes a safe mean of the losses.\n",
            "\n",
            "  Args:\n",
            "    losses: `Tensor` whose elements contain individual loss measurements.\n",
            "    num_present: The number of measurable elements in `losses`.\n",
            "\n",
            "  Returns:\n",
            "    A scalar representing the mean of `losses`. If `num_present` is zero,\n",
            "      then zero is returned.\n",
            "  \"\"\"\n",
            "  total_loss = math_ops.reduce_sum(losses)\n",
            "  return math_ops.div_no_nan(total_loss, num_present, name='value')\n",
            "\n",
            "\n",
            "def _num_elements(losses):\n",
            "  \"\"\"Computes the number of elements in `losses` tensor.\"\"\"\n",
            "  with K.name_scope('num_elements') as scope:\n",
            "    return math_ops.cast(array_ops.size(losses, name=scope), dtype=losses.dtype)\n",
            "\n",
            "\n",
            "def reduce_weighted_loss(weighted_losses,\n",
            "                         reduction=ReductionV2.SUM_OVER_BATCH_SIZE):\n",
            "  \"\"\"Reduces the individual weighted loss measurements.\"\"\"\n",
            "  if reduction == ReductionV2.NONE:\n",
            "    loss = weighted_losses\n",
            "  else:\n",
            "    loss = math_ops.reduce_sum(weighted_losses)\n",
            "    if reduction == ReductionV2.SUM_OVER_BATCH_SIZE:\n",
            "      loss = _safe_mean(loss, _num_elements(weighted_losses))\n",
            "  return loss\n",
            "\n",
            "\n",
            "def compute_weighted_loss(losses,\n",
            "                          sample_weight=None,\n",
            "                          reduction=ReductionV2.SUM_OVER_BATCH_SIZE,\n",
            "                          name=None):\n",
            "  \"\"\"Computes the weighted loss.\n",
            "\n",
            "  Args:\n",
            "    losses: `Tensor` of shape `[batch_size, d1, ... dN]`.\n",
            "    sample_weight: Optional `Tensor` whose rank is either 0, or the same rank as\n",
            "      `losses`, or be broadcastable to `losses`.\n",
            "    reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to loss.\n",
            "      Default value is `SUM_OVER_BATCH_SIZE`.\n",
            "    name: Optional name for the op.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: If the shape of `sample_weight` is not compatible with `losses`.\n",
            "\n",
            "  Returns:\n",
            "    Weighted loss `Tensor` of the same type as `losses`. If `reduction` is\n",
            "    `NONE`, this has the same shape as `losses`; otherwise, it is scalar.\n",
            "  \"\"\"\n",
            "  ReductionV2.validate(reduction)\n",
            "\n",
            "  # If this function is called directly, then we just default 'AUTO' to\n",
            "  # 'SUM_OVER_BATCH_SIZE'. Eg. Canned estimator use cases.\n",
            "  if reduction == ReductionV2.AUTO:\n",
            "    reduction = ReductionV2.SUM_OVER_BATCH_SIZE\n",
            "  if sample_weight is None:\n",
            "    sample_weight = 1.0\n",
            "  with K.name_scope(name or 'weighted_loss'):\n",
            "    # Save the `reduction` argument for loss normalization when distributing\n",
            "    # to multiple replicas. Used only for estimator + v1 optimizer flow.\n",
            "    ops.get_default_graph()._last_loss_reduction = reduction  # pylint: disable=protected-access\n",
            "\n",
            "    # Update dimensions of `sample_weight` to match with `losses` if possible.\n",
            "    losses, _, sample_weight = squeeze_or_expand_dimensions(\n",
            "        losses, None, sample_weight)\n",
            "    losses = ops.convert_to_tensor(losses)\n",
            "    input_dtype = losses.dtype\n",
            "    losses = math_ops.cast(losses, dtypes.float32)\n",
            "    sample_weight = math_ops.cast(sample_weight, dtypes.float32)\n",
            "\n",
            "    try:\n",
            "      # Broadcast weights if possible.\n",
            "      sample_weight = weights_broadcast_ops.broadcast_weights(\n",
            "          sample_weight, losses)\n",
            "    except ValueError:\n",
            "      # Reduce values to same ndim as weight array.\n",
            "      ndim = K.ndim(losses)\n",
            "      weight_ndim = K.ndim(sample_weight)\n",
            "      losses = K.mean(losses, axis=list(range(weight_ndim, ndim)))\n",
            "\n",
            "    sample_weight.shape.assert_is_compatible_with(losses.shape)\n",
            "    weighted_losses = math_ops.multiply(losses, sample_weight)\n",
            "    # Apply reduction function to the individual weighted losses.\n",
            "    loss = reduce_weighted_loss(weighted_losses, reduction)\n",
            "    # Convert the result back to the input type.\n",
            "    loss = math_ops.cast(loss, input_dtype)\n",
            "    return loss\n",
            "\n",
            "\n",
            "def scale_loss_for_distribution(loss_value):\n",
            "  \"\"\"Scales and returns the given loss value by the number of replicas.\"\"\"\n",
            "  num_replicas = (\n",
            "      distribution_strategy_context.get_strategy().num_replicas_in_sync)\n",
            "  if num_replicas > 1:\n",
            "    loss_value *= (1. / num_replicas)\n",
            "  return loss_value\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utilities used by convolution layers.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import itertools\n",
            "import numpy as np\n",
            "from six.moves import range  # pylint: disable=redefined-builtin\n",
            "\n",
            "from tensorflow.python.keras import backend\n",
            "\n",
            "\n",
            "def convert_data_format(data_format, ndim):\n",
            "  if data_format == 'channels_last':\n",
            "    if ndim == 3:\n",
            "      return 'NWC'\n",
            "    elif ndim == 4:\n",
            "      return 'NHWC'\n",
            "    elif ndim == 5:\n",
            "      return 'NDHWC'\n",
            "    else:\n",
            "      raise ValueError('Input rank not supported:', ndim)\n",
            "  elif data_format == 'channels_first':\n",
            "    if ndim == 3:\n",
            "      return 'NCW'\n",
            "    elif ndim == 4:\n",
            "      return 'NCHW'\n",
            "    elif ndim == 5:\n",
            "      return 'NCDHW'\n",
            "    else:\n",
            "      raise ValueError('Input rank not supported:', ndim)\n",
            "  else:\n",
            "    raise ValueError('Invalid data_format:', data_format)\n",
            "\n",
            "\n",
            "def normalize_tuple(value, n, name):\n",
            "  \"\"\"Transforms a single integer or iterable of integers into an integer tuple.\n",
            "\n",
            "  Arguments:\n",
            "    value: The value to validate and convert. Could an int, or any iterable\n",
            "      of ints.\n",
            "    n: The size of the tuple to be returned.\n",
            "    name: The name of the argument being validated, e.g. \"strides\" or\n",
            "      \"kernel_size\". This is only used to format error messages.\n",
            "\n",
            "  Returns:\n",
            "    A tuple of n integers.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: If something else than an int/long or iterable thereof was\n",
            "      passed.\n",
            "  \"\"\"\n",
            "  if isinstance(value, int):\n",
            "    return (value,) * n\n",
            "  else:\n",
            "    try:\n",
            "      value_tuple = tuple(value)\n",
            "    except TypeError:\n",
            "      raise ValueError('The `' + name + '` argument must be a tuple of ' +\n",
            "                       str(n) + ' integers. Received: ' + str(value))\n",
            "    if len(value_tuple) != n:\n",
            "      raise ValueError('The `' + name + '` argument must be a tuple of ' +\n",
            "                       str(n) + ' integers. Received: ' + str(value))\n",
            "    for single_value in value_tuple:\n",
            "      try:\n",
            "        int(single_value)\n",
            "      except (ValueError, TypeError):\n",
            "        raise ValueError('The `' + name + '` argument must be a tuple of ' +\n",
            "                         str(n) + ' integers. Received: ' + str(value) + ' '\n",
            "                         'including element ' + str(single_value) + ' of type' +\n",
            "                         ' ' + str(type(single_value)))\n",
            "    return value_tuple\n",
            "\n",
            "\n",
            "def conv_output_length(input_length, filter_size, padding, stride, dilation=1):\n",
            "  \"\"\"Determines output length of a convolution given input length.\n",
            "\n",
            "  Arguments:\n",
            "      input_length: integer.\n",
            "      filter_size: integer.\n",
            "      padding: one of \"same\", \"valid\", \"full\", \"causal\"\n",
            "      stride: integer.\n",
            "      dilation: dilation rate, integer.\n",
            "\n",
            "  Returns:\n",
            "      The output length (integer).\n",
            "  \"\"\"\n",
            "  if input_length is None:\n",
            "    return None\n",
            "  assert padding in {'same', 'valid', 'full', 'causal'}\n",
            "  dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
            "  if padding in ['same', 'causal']:\n",
            "    output_length = input_length\n",
            "  elif padding == 'valid':\n",
            "    output_length = input_length - dilated_filter_size + 1\n",
            "  elif padding == 'full':\n",
            "    output_length = input_length + dilated_filter_size - 1\n",
            "  return (output_length + stride - 1) // stride\n",
            "\n",
            "\n",
            "def conv_input_length(output_length, filter_size, padding, stride):\n",
            "  \"\"\"Determines input length of a convolution given output length.\n",
            "\n",
            "  Arguments:\n",
            "      output_length: integer.\n",
            "      filter_size: integer.\n",
            "      padding: one of \"same\", \"valid\", \"full\".\n",
            "      stride: integer.\n",
            "\n",
            "  Returns:\n",
            "      The input length (integer).\n",
            "  \"\"\"\n",
            "  if output_length is None:\n",
            "    return None\n",
            "  assert padding in {'same', 'valid', 'full'}\n",
            "  if padding == 'same':\n",
            "    pad = filter_size // 2\n",
            "  elif padding == 'valid':\n",
            "    pad = 0\n",
            "  elif padding == 'full':\n",
            "    pad = filter_size - 1\n",
            "  return (output_length - 1) * stride - 2 * pad + filter_size\n",
            "\n",
            "\n",
            "def deconv_output_length(input_length, filter_size, padding,\n",
            "                         output_padding=None, stride=0, dilation=1):\n",
            "  \"\"\"Determines output length of a transposed convolution given input length.\n",
            "\n",
            "  Arguments:\n",
            "      input_length: Integer.\n",
            "      filter_size: Integer.\n",
            "      padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n",
            "      output_padding: Integer, amount of padding along the output dimension.\n",
            "          Can be set to `None` in which case the output length is inferred.\n",
            "      stride: Integer.\n",
            "      dilation: Integer.\n",
            "\n",
            "  Returns:\n",
            "      The output length (integer).\n",
            "  \"\"\"\n",
            "  assert padding in {'same', 'valid', 'full'}\n",
            "  if input_length is None:\n",
            "    return None\n",
            "\n",
            "  # Get the dilated kernel size\n",
            "  filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
            "\n",
            "  # Infer length if output padding is None, else compute the exact length\n",
            "  if output_padding is None:\n",
            "    if padding == 'valid':\n",
            "      length = input_length * stride + max(filter_size - stride, 0)\n",
            "    elif padding == 'full':\n",
            "      length = input_length * stride - (stride + filter_size - 2)\n",
            "    elif padding == 'same':\n",
            "      length = input_length * stride\n",
            "\n",
            "  else:\n",
            "    if padding == 'same':\n",
            "      pad = filter_size // 2\n",
            "    elif padding == 'valid':\n",
            "      pad = 0\n",
            "    elif padding == 'full':\n",
            "      pad = filter_size - 1\n",
            "\n",
            "    length = ((input_length - 1) * stride + filter_size - 2 * pad +\n",
            "              output_padding)\n",
            "  return length\n",
            "\n",
            "\n",
            "def normalize_data_format(value):\n",
            "  if value is None:\n",
            "    value = backend.image_data_format()\n",
            "  data_format = value.lower()\n",
            "  if data_format not in {'channels_first', 'channels_last'}:\n",
            "    raise ValueError('The `data_format` argument must be one of '\n",
            "                     '\"channels_first\", \"channels_last\". Received: ' +\n",
            "                     str(value))\n",
            "  return data_format\n",
            "\n",
            "\n",
            "def normalize_padding(value):\n",
            "  if isinstance(value, (list, tuple)):\n",
            "    return value\n",
            "  padding = value.lower()\n",
            "  if padding not in {'valid', 'same', 'causal'}:\n",
            "    raise ValueError('The `padding` argument must be a list/tuple or one of '\n",
            "                     '\"valid\", \"same\" (or \"causal\", only for `Conv1D). '\n",
            "                     'Received: ' + str(padding))\n",
            "  return padding\n",
            "\n",
            "\n",
            "def convert_kernel(kernel):\n",
            "  \"\"\"Converts a Numpy kernel matrix from Theano format to TensorFlow format.\n",
            "\n",
            "  Also works reciprocally, since the transformation is its own inverse.\n",
            "\n",
            "  Arguments:\n",
            "      kernel: Numpy array (3D, 4D or 5D).\n",
            "\n",
            "  Returns:\n",
            "      The converted kernel.\n",
            "\n",
            "  Raises:\n",
            "      ValueError: in case of invalid kernel shape or invalid data_format.\n",
            "  \"\"\"\n",
            "  kernel = np.asarray(kernel)\n",
            "  if not 3 <= kernel.ndim <= 5:\n",
            "    raise ValueError('Invalid kernel shape:', kernel.shape)\n",
            "  slices = [slice(None, None, -1) for _ in range(kernel.ndim)]\n",
            "  no_flip = (slice(None, None), slice(None, None))\n",
            "  slices[-2:] = no_flip\n",
            "  return np.copy(kernel[slices])\n",
            "\n",
            "\n",
            "def conv_kernel_mask(input_shape, kernel_shape, strides, padding):\n",
            "  \"\"\"Compute a mask representing the connectivity of a convolution operation.\n",
            "\n",
            "  Assume a convolution with given parameters is applied to an input having N\n",
            "  spatial dimensions with `input_shape = (d_in1, ..., d_inN)` to produce an\n",
            "  output with shape `(d_out1, ..., d_outN)`. This method returns a boolean array\n",
            "  of shape `(d_in1, ..., d_inN, d_out1, ..., d_outN)` with `True` entries\n",
            "  indicating pairs of input and output locations that are connected by a weight.\n",
            "\n",
            "  Example:\n",
            "    ```python\n",
            "        >>> input_shape = (4,)\n",
            "        >>> kernel_shape = (2,)\n",
            "        >>> strides = (1,)\n",
            "        >>> padding = \"valid\"\n",
            "        >>> conv_kernel_mask(input_shape, kernel_shape, strides, padding)\n",
            "        array([[ True, False, False],\n",
            "               [ True,  True, False],\n",
            "               [False,  True,  True],\n",
            "               [False, False,  True]], dtype=bool)\n",
            "    ```\n",
            "    where rows and columns correspond to inputs and outputs respectively.\n",
            "\n",
            "\n",
            "  Args:\n",
            "    input_shape: tuple of size N: `(d_in1, ..., d_inN)`,\n",
            "                 spatial shape of the input.\n",
            "    kernel_shape: tuple of size N, spatial shape of the convolutional kernel\n",
            "                  / receptive field.\n",
            "    strides: tuple of size N, strides along each spatial dimension.\n",
            "    padding: type of padding, string `\"same\"` or `\"valid\"`.\n",
            "\n",
            "  Returns:\n",
            "    A boolean 2N-D `np.ndarray` of shape\n",
            "    `(d_in1, ..., d_inN, d_out1, ..., d_outN)`, where `(d_out1, ..., d_outN)`\n",
            "    is the spatial shape of the output. `True` entries in the mask represent\n",
            "    pairs of input-output locations that are connected by a weight.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if `input_shape`, `kernel_shape` and `strides` don't have the\n",
            "        same number of dimensions.\n",
            "    NotImplementedError: if `padding` is not in {`\"same\"`, `\"valid\"`}.\n",
            "  \"\"\"\n",
            "  if padding not in {'same', 'valid'}:\n",
            "    raise NotImplementedError('Padding type %s not supported. '\n",
            "                              'Only \"valid\" and \"same\" '\n",
            "                              'are implemented.' % padding)\n",
            "\n",
            "  in_dims = len(input_shape)\n",
            "  if isinstance(kernel_shape, int):\n",
            "    kernel_shape = (kernel_shape,) * in_dims\n",
            "  if isinstance(strides, int):\n",
            "    strides = (strides,) * in_dims\n",
            "\n",
            "  kernel_dims = len(kernel_shape)\n",
            "  stride_dims = len(strides)\n",
            "  if kernel_dims != in_dims or stride_dims != in_dims:\n",
            "    raise ValueError('Number of strides, input and kernel dimensions must all '\n",
            "                     'match. Received: %d, %d, %d.' %\n",
            "                     (stride_dims, in_dims, kernel_dims))\n",
            "\n",
            "  output_shape = conv_output_shape(input_shape, kernel_shape, strides, padding)\n",
            "\n",
            "  mask_shape = input_shape + output_shape\n",
            "  mask = np.zeros(mask_shape, np.bool)\n",
            "\n",
            "  output_axes_ticks = [range(dim) for dim in output_shape]\n",
            "  for output_position in itertools.product(*output_axes_ticks):\n",
            "    input_axes_ticks = conv_connected_inputs(input_shape,\n",
            "                                             kernel_shape,\n",
            "                                             output_position,\n",
            "                                             strides,\n",
            "                                             padding)\n",
            "    for input_position in itertools.product(*input_axes_ticks):\n",
            "      mask[input_position + output_position] = True\n",
            "\n",
            "  return mask\n",
            "\n",
            "\n",
            "def conv_connected_inputs(input_shape,\n",
            "                          kernel_shape,\n",
            "                          output_position,\n",
            "                          strides,\n",
            "                          padding):\n",
            "  \"\"\"Return locations of the input connected to an output position.\n",
            "\n",
            "  Assume a convolution with given parameters is applied to an input having N\n",
            "  spatial dimensions with `input_shape = (d_in1, ..., d_inN)`. This method\n",
            "  returns N ranges specifying the input region that was convolved with the\n",
            "  kernel to produce the output at position\n",
            "  `output_position = (p_out1, ..., p_outN)`.\n",
            "\n",
            "  Example:\n",
            "    ```python\n",
            "        >>> input_shape = (4, 4)\n",
            "        >>> kernel_shape = (2, 1)\n",
            "        >>> output_position = (1, 1)\n",
            "        >>> strides = (1, 1)\n",
            "        >>> padding = \"valid\"\n",
            "        >>> conv_connected_inputs(input_shape, kernel_shape, output_position,\n",
            "        >>>                       strides, padding)\n",
            "        [xrange(1, 3), xrange(1, 2)]\n",
            "    ```\n",
            "  Args:\n",
            "    input_shape: tuple of size N: `(d_in1, ..., d_inN)`,\n",
            "                 spatial shape of the input.\n",
            "    kernel_shape: tuple of size N, spatial shape of the convolutional kernel\n",
            "                  / receptive field.\n",
            "    output_position: tuple of size N: `(p_out1, ..., p_outN)`,\n",
            "                     a single position in the output of the convolution.\n",
            "    strides: tuple of size N, strides along each spatial dimension.\n",
            "    padding: type of padding, string `\"same\"` or `\"valid\"`.\n",
            "\n",
            "  Returns:\n",
            "    N ranges `[[p_in_left1, ..., p_in_right1], ...,\n",
            "              [p_in_leftN, ..., p_in_rightN]]` specifying the region in the\n",
            "    input connected to output_position.\n",
            "  \"\"\"\n",
            "  ranges = []\n",
            "\n",
            "  ndims = len(input_shape)\n",
            "  for d in range(ndims):\n",
            "    left_shift = int(kernel_shape[d] / 2)\n",
            "    right_shift = kernel_shape[d] - left_shift\n",
            "\n",
            "    center = output_position[d] * strides[d]\n",
            "\n",
            "    if padding == 'valid':\n",
            "      center += left_shift\n",
            "\n",
            "    start = max(0, center - left_shift)\n",
            "    end = min(input_shape[d], center + right_shift)\n",
            "\n",
            "    ranges.append(range(start, end))\n",
            "\n",
            "  return ranges\n",
            "\n",
            "\n",
            "def conv_output_shape(input_shape, kernel_shape, strides, padding):\n",
            "  \"\"\"Return the output shape of an N-D convolution.\n",
            "\n",
            "  Forces dimensions where input is empty (size 0) to remain empty.\n",
            "\n",
            "  Args:\n",
            "    input_shape: tuple of size N: `(d_in1, ..., d_inN)`,\n",
            "                 spatial shape of the input.\n",
            "    kernel_shape: tuple of size N, spatial shape of the convolutional kernel\n",
            "                  / receptive field.\n",
            "    strides: tuple of size N, strides along each spatial dimension.\n",
            "    padding: type of padding, string `\"same\"` or `\"valid\"`.\n",
            "\n",
            "  Returns:\n",
            "    tuple of size N: `(d_out1, ..., d_outN)`, spatial shape of the output.\n",
            "  \"\"\"\n",
            "  dims = range(len(kernel_shape))\n",
            "  output_shape = [conv_output_length(input_shape[d],\n",
            "                                     kernel_shape[d],\n",
            "                                     padding,\n",
            "                                     strides[d])\n",
            "                  for d in dims]\n",
            "  output_shape = tuple([0 if input_shape[d] == 0 else output_shape[d]\n",
            "                        for d in dims])\n",
            "  return output_shape\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=g-import-not-at-top\n",
            "\"\"\"Utilities related to disk I/O.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import collections\n",
            "\n",
            "import numpy as np\n",
            "import six\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "try:\n",
            "  import h5py\n",
            "except ImportError:\n",
            "  h5py = None\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.HDF5Matrix')\n",
            "class HDF5Matrix(object):\n",
            "  \"\"\"Representation of HDF5 dataset to be used instead of a Numpy array.\n",
            "\n",
            "  Example:\n",
            "\n",
            "  ```python\n",
            "      x_data = HDF5Matrix('input/file.hdf5', 'data')\n",
            "      model.predict(x_data)\n",
            "  ```\n",
            "\n",
            "  Providing `start` and `end` allows use of a slice of the dataset.\n",
            "\n",
            "  Optionally, a normalizer function (or lambda) can be given. This will\n",
            "  be called on every slice of data retrieved.\n",
            "\n",
            "  Arguments:\n",
            "      datapath: string, path to a HDF5 file\n",
            "      dataset: string, name of the HDF5 dataset in the file specified\n",
            "          in datapath\n",
            "      start: int, start of desired slice of the specified dataset\n",
            "      end: int, end of desired slice of the specified dataset\n",
            "      normalizer: function to be called on data when retrieved\n",
            "\n",
            "  Returns:\n",
            "      An array-like HDF5 dataset.\n",
            "  \"\"\"\n",
            "  refs = collections.defaultdict(int)\n",
            "\n",
            "  def __init__(self, datapath, dataset, start=0, end=None, normalizer=None):\n",
            "    if h5py is None:\n",
            "      raise ImportError('The use of HDF5Matrix requires '\n",
            "                        'HDF5 and h5py installed.')\n",
            "\n",
            "    if datapath not in list(self.refs.keys()):\n",
            "      f = h5py.File(datapath)\n",
            "      self.refs[datapath] = f\n",
            "    else:\n",
            "      f = self.refs[datapath]\n",
            "    self.data = f[dataset]\n",
            "    self.start = start\n",
            "    if end is None:\n",
            "      self.end = self.data.shape[0]\n",
            "    else:\n",
            "      self.end = end\n",
            "    self.normalizer = normalizer\n",
            "\n",
            "  def __len__(self):\n",
            "    return self.end - self.start\n",
            "\n",
            "  def __getitem__(self, key):\n",
            "    if isinstance(key, slice):\n",
            "      start, stop = key.start, key.stop\n",
            "      if start is None:\n",
            "        start = 0\n",
            "      if stop is None:\n",
            "        stop = self.shape[0]\n",
            "      if stop + self.start <= self.end:\n",
            "        idx = slice(start + self.start, stop + self.start)\n",
            "      else:\n",
            "        raise IndexError\n",
            "    elif isinstance(key, (int, np.integer)):\n",
            "      if key + self.start < self.end:\n",
            "        idx = key + self.start\n",
            "      else:\n",
            "        raise IndexError\n",
            "    elif isinstance(key, np.ndarray):\n",
            "      if np.max(key) + self.start < self.end:\n",
            "        idx = (self.start + key).tolist()\n",
            "      else:\n",
            "        raise IndexError\n",
            "    else:\n",
            "      # Assume list/iterable\n",
            "      if max(key) + self.start < self.end:\n",
            "        idx = [x + self.start for x in key]\n",
            "      else:\n",
            "        raise IndexError\n",
            "    if self.normalizer is not None:\n",
            "      return self.normalizer(self.data[idx])\n",
            "    else:\n",
            "      return self.data[idx]\n",
            "\n",
            "  @property\n",
            "  def shape(self):\n",
            "    \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n",
            "\n",
            "    Returns:\n",
            "        A numpy-style shape tuple.\n",
            "    \"\"\"\n",
            "    return (self.end - self.start,) + self.data.shape[1:]\n",
            "\n",
            "  @property\n",
            "  def dtype(self):\n",
            "    \"\"\"Gets the datatype of the dataset.\n",
            "\n",
            "    Returns:\n",
            "        A numpy dtype string.\n",
            "    \"\"\"\n",
            "    return self.data.dtype\n",
            "\n",
            "  @property\n",
            "  def ndim(self):\n",
            "    \"\"\"Gets the number of dimensions (rank) of the dataset.\n",
            "\n",
            "    Returns:\n",
            "        An integer denoting the number of dimensions (rank) of the dataset.\n",
            "    \"\"\"\n",
            "    return self.data.ndim\n",
            "\n",
            "  @property\n",
            "  def size(self):\n",
            "    \"\"\"Gets the total dataset size (number of elements).\n",
            "\n",
            "    Returns:\n",
            "        An integer denoting the number of elements in the dataset.\n",
            "    \"\"\"\n",
            "    return np.prod(self.shape)\n",
            "\n",
            "\n",
            "def ask_to_proceed_with_overwrite(filepath):\n",
            "  \"\"\"Produces a prompt asking about overwriting a file.\n",
            "\n",
            "  Arguments:\n",
            "      filepath: the path to the file to be overwritten.\n",
            "\n",
            "  Returns:\n",
            "      True if we can proceed with overwrite, False otherwise.\n",
            "  \"\"\"\n",
            "  overwrite = six.moves.input('[WARNING] %s already exists - overwrite? '\n",
            "                              '[y/n]' % (filepath)).strip().lower()\n",
            "  while overwrite not in ('y', 'n'):\n",
            "    overwrite = six.moves.input('Enter \"y\" (overwrite) or \"n\" '\n",
            "                                '(cancel).').strip().lower()\n",
            "  if overwrite == 'n':\n",
            "    return False\n",
            "  print('[TIP] Next time specify overwrite=True!')\n",
            "  return True\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Numpy-related utilities.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import numpy as np\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.to_categorical')\n",
            "def to_categorical(y, num_classes=None, dtype='float32'):\n",
            "  \"\"\"Converts a class vector (integers) to binary class matrix.\n",
            "\n",
            "  E.g. for use with categorical_crossentropy.\n",
            "\n",
            "  Arguments:\n",
            "      y: class vector to be converted into a matrix\n",
            "          (integers from 0 to num_classes).\n",
            "      num_classes: total number of classes.\n",
            "      dtype: The data type expected by the input. Default: `'float32'`.\n",
            "\n",
            "  Returns:\n",
            "      A binary matrix representation of the input. The classes axis is placed\n",
            "      last.\n",
            "  \"\"\"\n",
            "  y = np.array(y, dtype='int')\n",
            "  input_shape = y.shape\n",
            "  if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
            "    input_shape = tuple(input_shape[:-1])\n",
            "  y = y.ravel()\n",
            "  if not num_classes:\n",
            "    num_classes = np.max(y) + 1\n",
            "  n = y.shape[0]\n",
            "  categorical = np.zeros((n, num_classes), dtype=dtype)\n",
            "  categorical[np.arange(n), y] = 1\n",
            "  output_shape = input_shape + (num_classes,)\n",
            "  categorical = np.reshape(categorical, output_shape)\n",
            "  return categorical\n",
            "\n",
            "\n",
            "@keras_export('keras.utils.normalize')\n",
            "def normalize(x, axis=-1, order=2):\n",
            "  \"\"\"Normalizes a Numpy array.\n",
            "\n",
            "  Arguments:\n",
            "      x: Numpy array to normalize.\n",
            "      axis: axis along which to normalize.\n",
            "      order: Normalization order (e.g. 2 for L2 norm).\n",
            "\n",
            "  Returns:\n",
            "      A normalized copy of the array.\n",
            "  \"\"\"\n",
            "  l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n",
            "  l2[l2 == 0] = 1\n",
            "  return x / np.expand_dims(l2, axis)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Utility methods related to kernelized layers.\"\"\"\n",
            "\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from tensorflow.python.ops import array_ops\n",
            "from tensorflow.python.ops import math_ops\n",
            "\n",
            "\n",
            "def _to_matrix(u):\n",
            "  \"\"\"If input tensor is a vector (i.e., has rank 1), converts it to matrix.\"\"\"\n",
            "  u_rank = len(u.shape)\n",
            "  if u_rank not in [1, 2]:\n",
            "    raise ValueError('The input tensor should have rank 1 or 2. Given rank: {}'\n",
            "                     .format(u_rank))\n",
            "  if u_rank == 1:\n",
            "    return array_ops.expand_dims(u, 0)\n",
            "  return u\n",
            "\n",
            "\n",
            "def _align_matrices(x, y):\n",
            "  \"\"\"Aligns x and y tensors to allow computations over pairs of their rows.\"\"\"\n",
            "  x_matrix = _to_matrix(x)\n",
            "  y_matrix = _to_matrix(y)\n",
            "  x_shape = x_matrix.shape\n",
            "  y_shape = y_matrix.shape\n",
            "  if y_shape[1] != x_shape[1]:  # dimensions do not match.\n",
            "    raise ValueError(\n",
            "        'The outermost dimensions of the input tensors should match. Given: {} '\n",
            "        'vs {}.'.format(y_shape[1], x_shape[1]))\n",
            "\n",
            "  x_tile = array_ops.tile(\n",
            "      array_ops.expand_dims(x_matrix, 1), [1, y_shape[0], 1])\n",
            "  y_tile = array_ops.tile(\n",
            "      array_ops.expand_dims(y_matrix, 0), [x_shape[0], 1, 1])\n",
            "  return x_tile, y_tile\n",
            "\n",
            "\n",
            "def inner_product(u, v):\n",
            "  u = _to_matrix(u)\n",
            "  v = _to_matrix(v)\n",
            "  return math_ops.matmul(u, v, transpose_b=True)\n",
            "\n",
            "\n",
            "def exact_gaussian_kernel(x, y, stddev):\n",
            "  r\"\"\"Computes exact Gaussian kernel value(s) for tensors x and y and stddev.\n",
            "\n",
            "  The Gaussian kernel for vectors u, v is defined as follows:\n",
            "       K(u, v) = exp(-||u-v||^2 / (2* stddev^2))\n",
            "  where the norm is the l2-norm. x, y can be either vectors or matrices. If they\n",
            "  are vectors, they must have the same dimension. If they are matrices, they\n",
            "  must have the same number of columns. In the latter case, the method returns\n",
            "  (as a matrix) K(u, v) values for all pairs (u, v) where u is a row from x and\n",
            "  v is a row from y.\n",
            "\n",
            "  Args:\n",
            "    x: a tensor of rank 1 or 2. It's shape should be either [dim] or [m, dim].\n",
            "    y: a tensor of rank 1 or 2. It's shape should be either [dim] or [n, dim].\n",
            "    stddev: The width of the Gaussian kernel.\n",
            "\n",
            "  Returns:\n",
            "    A single value (scalar) with shape (1, 1) (if x, y are vectors) or a matrix\n",
            "      of shape (m, n) with entries K(u, v) (where K is the Gaussian kernel) for\n",
            "      all (u,v) pairs where u, v are rows from x and y respectively.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if the shapes of x, y are not compatible.\n",
            "  \"\"\"\n",
            "  x_aligned, y_aligned = _align_matrices(x, y)\n",
            "  diff_squared_l2_norm = math_ops.reduce_sum(\n",
            "      math_ops.squared_difference(x_aligned, y_aligned), 2)\n",
            "  return math_ops.exp(-diff_squared_l2_norm / (2 * stddev * stddev))\n",
            "\n",
            "\n",
            "def exact_laplacian_kernel(x, y, stddev):\n",
            "  r\"\"\"Computes exact Laplacian kernel value(s) for tensors x and y using stddev.\n",
            "\n",
            "  The Laplacian kernel for vectors u, v is defined as follows:\n",
            "       K(u, v) = exp(-||u-v|| / stddev)\n",
            "  where the norm is the l1-norm. x, y can be either vectors or matrices. If they\n",
            "  are vectors, they must have the same dimension. If they are matrices, they\n",
            "  must have the same number of columns. In the latter case, the method returns\n",
            "  (as a matrix) K(u, v) values for all pairs (u, v) where u is a row from x and\n",
            "  v is a row from y.\n",
            "\n",
            "  Args:\n",
            "    x: a tensor of rank 1 or 2. It's shape should be either [dim] or [m, dim].\n",
            "    y: a tensor of rank 1 or 2. It's shape should be either [dim] or [n, dim].\n",
            "    stddev: The width of the Gaussian kernel.\n",
            "\n",
            "  Returns:\n",
            "    A single value (scalar) with shape (1, 1)  if x, y are vectors or a matrix\n",
            "    of shape (m, n) with entries K(u, v) (where K is the Laplacian kernel) for\n",
            "    all (u,v) pairs where u, v are rows from x and y respectively.\n",
            "\n",
            "  Raises:\n",
            "    ValueError: if the shapes of x, y are not compatible.\n",
            "  \"\"\"\n",
            "  x_aligned, y_aligned = _align_matrices(x, y)\n",
            "  diff_l1_norm = math_ops.reduce_sum(\n",
            "      math_ops.abs(math_ops.subtract(x_aligned, y_aligned)), 2)\n",
            "  return math_ops.exp(-diff_l1_norm / stddev)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras model mode constants.\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "# pylint: disable=unused-import\n",
            "from tensorflow.python.saved_model.model_utils.mode_keys import KerasModeKeys as ModeKeys\n",
            "# pylint: enable=unused-import\n",
            "\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/__pycache__\n",
            "Directories []\n",
            "Files ['mode_keys.cpython-36.pyc', 'losses_utils.cpython-36.pyc', 'np_utils.cpython-36.pyc', 'metrics_utils.cpython-36.pyc', 'layer_utils.cpython-36.pyc', 'data_utils.cpython-36.pyc', 'multi_gpu_utils.cpython-36.pyc', 'kernelized_utils.cpython-36.pyc', 'vis_utils.cpython-36.pyc', 'tf_utils.cpython-36.pyc', 'io_utils.cpython-36.pyc', 'conv_utils.cpython-36.pyc', '__init__.cpython-36.pyc', 'generic_utils.cpython-36.pyc']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xda in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xa0 in position 8: invalid start byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xf4 in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xf0 in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe0 in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xf3 in position 8: invalid continuation byte\n",
            "Found files.\n",
            "In try\n",
            "'utf-8' codec can't decode byte 0xe3 in position 12: invalid continuation byte\n",
            "Path /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications\n",
            "Directories ['__pycache__']\n",
            "Files ['densenet.py', 'mobilenet.py', '__init__.py', 'imagenet_utils.py', 'vgg16.py', 'mobilenet_v2.py', 'resnet50.py', 'nasnet.py', 'xception.py', 'vgg19.py', 'inception_v3.py', 'inception_resnet_v2.py']\n",
            "Incrementing count..\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=invalid-name\n",
            "\"\"\"DenseNet models for Keras.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from keras_applications import densenet\n",
            "\n",
            "from tensorflow.python.keras.applications import keras_modules_injection\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.densenet.DenseNet121',\n",
            "              'keras.applications.DenseNet121')\n",
            "@keras_modules_injection\n",
            "def DenseNet121(*args, **kwargs):\n",
            "  return densenet.DenseNet121(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.densenet.DenseNet169',\n",
            "              'keras.applications.DenseNet169')\n",
            "@keras_modules_injection\n",
            "def DenseNet169(*args, **kwargs):\n",
            "  return densenet.DenseNet169(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.densenet.DenseNet201',\n",
            "              'keras.applications.DenseNet201')\n",
            "@keras_modules_injection\n",
            "def DenseNet201(*args, **kwargs):\n",
            "  return densenet.DenseNet201(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.densenet.decode_predictions')\n",
            "@keras_modules_injection\n",
            "def decode_predictions(*args, **kwargs):\n",
            "  return densenet.decode_predictions(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.densenet.preprocess_input')\n",
            "@keras_modules_injection\n",
            "def preprocess_input(*args, **kwargs):\n",
            "  return densenet.preprocess_input(*args, **kwargs)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "# pylint: disable=invalid-name\n",
            "\"\"\"MobileNet v1 models for Keras.\n",
            "\"\"\"\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "from keras_applications import mobilenet\n",
            "\n",
            "from tensorflow.python.keras.applications import keras_modules_injection\n",
            "from tensorflow.python.util.tf_export import keras_export\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.mobilenet.MobileNet',\n",
            "              'keras.applications.MobileNet')\n",
            "@keras_modules_injection\n",
            "def MobileNet(*args, **kwargs):\n",
            "  return mobilenet.MobileNet(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.mobilenet.decode_predictions')\n",
            "@keras_modules_injection\n",
            "def decode_predictions(*args, **kwargs):\n",
            "  return mobilenet.decode_predictions(*args, **kwargs)\n",
            "\n",
            "\n",
            "@keras_export('keras.applications.mobilenet.preprocess_input')\n",
            "@keras_modules_injection\n",
            "def preprocess_input(*args, **kwargs):\n",
            "  return mobilenet.preprocess_input(*args, **kwargs)\n",
            "\n",
            "Found files.\n",
            "In try\n",
            "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "# ==============================================================================\n",
            "\"\"\"Keras Applications are canned architectures with pre-trained weights.\"\"\"\n",
            "# pylint: disable=g-import-not-at-top\n",
            "# pylint: disable=g-bad-import-order\n",
            "from __future__ import absolute_import\n",
            "from __future__ import division\n",
            "from __future__ import print_function\n",
            "\n",
            "import keras_applications\n",
            "\n",
            "from tensorflow.python.keras import backend\n",
            "from tensorflow.python.keras import engine\n",
            "from tensorflow.python.keras import layers\n",
            "from tensorflow.python.keras import models\n",
            "from tensorflow.python.keras import utils\n",
            "from tensorflow.python.util import tf_inspect\n",
            "\n",
            "# `get_submodules_from_kwargs` has been introduced in 1.0.5, but we would\n",
            "# like to be able to handle prior versions. Note that prior to 1.0.5,\n",
            "# `keras_applications` did not expose a `__version__` attribute.\n",
            "if not hasattr(keras_applications, 'get_submodules_from_kwargs'):\n",
            "\n",
            "  if 'engine' in tf_inspect.getfullargspec(\n",
            "      keras_applications.set_keras_submodules)[0]:\n",
            "    keras_applications.set_keras_submodules(\n",
            "        backend=backend,\n",
            "        layers=layers,\n",
            "        models=models,\n",
            "        utils=utils,\n",
            "        engine=engine)\n",
            "  else:\n",
            "    keras_applications.set_keras_submodules(\n",
            "        backend=backend,\n",
            "        layers=layers,\n",
            "        models=models,\n",
            "        utils=utils)\n",
            "\n",
            "\n",
            "def keras_modules_injection(base_fun):\n",
            "  \"\"\"Decorator injecting tf.keras replacements for Keras modules.\n",
            "\n",
            "  Arguments:\n",
            "      base_fun: Application function to decorate (e.g. `MobileNet`).\n",
            "\n",
            "  Returns:\n",
            "      Decorated function that injects keyword argument for the tf.keras\n",
            "      modules required by the Applications.\n",
            "  \"\"\"\n",
            "\n",
            "  def wrapper(*args, **kwargs):\n",
            "    if hasattr(keras_applications, 'get_submodules_from_kwargs'):\n",
            "      kwargs['backend'] = backend\n",
            "      if 'layers' not in kwargs:\n",
            "        kwargs['layers'] = layers\n",
            "      kwargs['models'] = models\n",
            "      kwargs['utils'] = utils\n",
            "    return base_fun(*args, **kwargs)\n",
            "  return wrapper\n",
            "\n",
            "\n",
            "from tensorflow.python.keras.applications.densenet import DenseNet121\n",
            "from tensorflow.python.keras.applications.densenet import DenseNet169\n",
            "from tensorflow.python.keras.applications.densenet import DenseNet201\n",
            "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
            "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
            "from tensorflow.python.keras.applications.mobilenet import MobileNet\n",
            "from tensorflow.python.keras.applications.mobilenet_v2 import MobileNetV2\n",
            "from tensorflow.python.keras.applications.nasnet import NASNetLarge\n",
            "from tensorflow.python.keras.applications.nasnet import NASNetMobile\n",
            "from tensorflow.python.keras.applications.resnet50 import ResNet50\n",
            "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
            "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
            "from tensorflow.python.keras.applications.xception import Xception\n",
            "\n",
            "del absolute_import\n",
            "del division\n",
            "del print_function\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh46XGZH53hC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd1ee63e-cf72-4e0f-aa35-c6682b1c6828"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input.txt  sample_data\tunconventional-neural-networks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLe2FJWb6hcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "6f5e781c-1b87-48b5-dbb3-32b6a7a9eef2"
      },
      "source": [
        "!git clone https://github.com/sourcecode369/unconventional-neural-networks"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unconventional-neural-networks'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/29)\u001b[K\rremote: Counting objects:   6% (2/29)\u001b[K\rremote: Counting objects:  10% (3/29)\u001b[K\rremote: Counting objects:  13% (4/29)\u001b[K\rremote: Counting objects:  17% (5/29)\u001b[K\rremote: Counting objects:  20% (6/29)\u001b[K\rremote: Counting objects:  24% (7/29)\u001b[K\rremote: Counting objects:  27% (8/29)\u001b[K\rremote: Counting objects:  31% (9/29)\u001b[K\rremote: Counting objects:  34% (10/29)\u001b[K\rremote: Counting objects:  37% (11/29)\u001b[K\rremote: Counting objects:  41% (12/29)\u001b[K\rremote: Counting objects:  44% (13/29)\u001b[K\rremote: Counting objects:  48% (14/29)\u001b[K\rremote: Counting objects:  51% (15/29)\u001b[K\rremote: Counting objects:  55% (16/29)\u001b[K\rremote: Counting objects:  58% (17/29)\u001b[K\rremote: Counting objects:  62% (18/29)\u001b[K\rremote: Counting objects:  65% (19/29)\u001b[K\rremote: Counting objects:  68% (20/29)\u001b[K\rremote: Counting objects:  72% (21/29)\u001b[K\rremote: Counting objects:  75% (22/29)\u001b[K\rremote: Counting objects:  79% (23/29)\u001b[K\rremote: Counting objects:  82% (24/29)\u001b[K\rremote: Counting objects:  86% (25/29)\u001b[K\rremote: Counting objects:  89% (26/29)\u001b[K\rremote: Counting objects:  93% (27/29)\u001b[K\rremote: Counting objects:  96% (28/29)\u001b[K\rremote: Counting objects: 100% (29/29)\u001b[K\rremote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/26)\u001b[K\rremote: Compressing objects:   7% (2/26)\u001b[K\rremote: Compressing objects:  11% (3/26)\u001b[K\rremote: Compressing objects:  15% (4/26)\u001b[K\rremote: Compressing objects:  19% (5/26)\u001b[K\rremote: Compressing objects:  23% (6/26)\u001b[K\rremote: Compressing objects:  26% (7/26)\u001b[K\rremote: Compressing objects:  30% (8/26)\u001b[K\rremote: Compressing objects:  34% (9/26)\u001b[K\rremote: Compressing objects:  38% (10/26)\u001b[K\rremote: Compressing objects:  42% (11/26)\u001b[K\rremote: Compressing objects:  46% (12/26)\u001b[K\rremote: Compressing objects:  50% (13/26)\u001b[K\rremote: Compressing objects:  53% (14/26)\u001b[K\rremote: Compressing objects:  57% (15/26)\u001b[K\rremote: Compressing objects:  61% (16/26)\u001b[K\rremote: Compressing objects:  65% (17/26)\u001b[K\rremote: Compressing objects:  69% (18/26)\u001b[K\rremote: Compressing objects:  73% (19/26)\u001b[K\rremote: Compressing objects:  76% (20/26)\u001b[K\rremote: Compressing objects:  80% (21/26)\u001b[K\rremote: Compressing objects:  84% (22/26)\u001b[K\rremote: Compressing objects:  88% (23/26)\u001b[K\rremote: Compressing objects:  92% (24/26)\u001b[K\rremote: Compressing objects:  96% (25/26)\u001b[K\rremote: Compressing objects: 100% (26/26)\u001b[K\rremote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "Unpacking objects:   3% (1/29)   \rUnpacking objects:   6% (2/29)   \rUnpacking objects:  10% (3/29)   \rUnpacking objects:  13% (4/29)   \rUnpacking objects:  17% (5/29)   \rUnpacking objects:  20% (6/29)   \rUnpacking objects:  24% (7/29)   \rUnpacking objects:  27% (8/29)   \rUnpacking objects:  31% (9/29)   \rUnpacking objects:  34% (10/29)   \rUnpacking objects:  37% (11/29)   \rUnpacking objects:  41% (12/29)   \rUnpacking objects:  44% (13/29)   \rUnpacking objects:  48% (14/29)   \rUnpacking objects:  51% (15/29)   \rUnpacking objects:  55% (16/29)   \rremote: Total 29 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  58% (17/29)   \rUnpacking objects:  62% (18/29)   \rUnpacking objects:  65% (19/29)   \rUnpacking objects:  68% (20/29)   \rUnpacking objects:  72% (21/29)   \rUnpacking objects:  75% (22/29)   \rUnpacking objects:  79% (23/29)   \rUnpacking objects:  82% (24/29)   \rUnpacking objects:  86% (25/29)   \rUnpacking objects:  89% (26/29)   \rUnpacking objects:  93% (27/29)   \rUnpacking objects:  96% (28/29)   \rUnpacking objects: 100% (29/29)   \rUnpacking objects: 100% (29/29), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxG0oPJG6oQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebce49d3-2be7-48e9-984c-defaed4c1524"
      },
      "source": [
        "input."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input.txt  sample_data\tunconventional-neural-networks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQIaq6v67ltu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "739ba09d-3f92-4fd8-c251-c2cffc24d091"
      },
      "source": [
        "os.listdir('unconventional-neural-networks/char-rnn-tensorflow/')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LICENSE.md',\n",
              " 'utils.py',\n",
              " 'sample.py',\n",
              " 'train.py',\n",
              " 'README.md',\n",
              " 'data',\n",
              " 'model.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglrrvA37sGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir unconventional-neural-networks/char-rnn-tensorflow/data/code/\n",
        "!mv input.txt unconventional-neural-networks/char-rnn-tensorflow/data/code/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpTvDMv08K1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "047e385a-dabb-4fa1-d63d-b9384f89c75e"
      },
      "source": [
        "!python unconventional-neural-networks/char-rnn-tensorflow/train.py --data_dir=unconventional-neural-networks/char-rnn-tensorflow/data/code/ --num_epochs=5000"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading preprocessed files\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:57: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/train.py:99: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-09-13 19:25:52.262798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-13 19:25:52.279938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.280655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-13 19:25:52.280938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:25:52.282324: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-13 19:25:52.283669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-13 19:25:52.284024: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-13 19:25:52.298038: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-13 19:25:52.306677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-13 19:25:52.326024: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-13 19:25:52.326161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.326979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.327662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-13 19:25:52.337015: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-13 19:25:52.337254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e4ddc0 executing computations on platform Host. Devices:\n",
            "2019-09-13 19:25:52.337289: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-13 19:25:52.395846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.396660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e4e840 executing computations on platform CUDA. Devices:\n",
            "2019-09-13 19:25:52.396689: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-13 19:25:52.396902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.397579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-13 19:25:52.397650: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:25:52.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-13 19:25:52.397701: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-13 19:25:52.397723: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-13 19:25:52.397746: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-13 19:25:52.397769: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-13 19:25:52.397793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-13 19:25:52.397923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.398643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.399307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-13 19:25:52.399367: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:25:52.400814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-13 19:25:52.400844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-13 19:25:52.400871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-13 19:25:52.401010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.401740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:25:52.402421: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-13 19:25:52.402469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/train.py:101: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/train.py:102: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/train.py:107: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2019-09-13 19:25:54.222537: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "0/7395000 (epoch 0), train_loss = 5.766, time/batch = 1.107\n",
            "model saved to save/model.ckpt\n",
            "1/7395000 (epoch 0), train_loss = 5.736, time/batch = 0.080\n",
            "2/7395000 (epoch 0), train_loss = 5.672, time/batch = 0.079\n",
            "3/7395000 (epoch 0), train_loss = 5.462, time/batch = 0.077\n",
            "4/7395000 (epoch 0), train_loss = 5.032, time/batch = 0.074\n",
            "5/7395000 (epoch 0), train_loss = 4.609, time/batch = 0.069\n",
            "6/7395000 (epoch 0), train_loss = 4.276, time/batch = 0.065\n",
            "7/7395000 (epoch 0), train_loss = 4.148, time/batch = 0.065\n",
            "8/7395000 (epoch 0), train_loss = 3.826, time/batch = 0.062\n",
            "9/7395000 (epoch 0), train_loss = 3.761, time/batch = 0.062\n",
            "10/7395000 (epoch 0), train_loss = 3.715, time/batch = 0.060\n",
            "11/7395000 (epoch 0), train_loss = 3.534, time/batch = 0.060\n",
            "12/7395000 (epoch 0), train_loss = 3.409, time/batch = 0.058\n",
            "13/7395000 (epoch 0), train_loss = 3.384, time/batch = 0.058\n",
            "14/7395000 (epoch 0), train_loss = 3.336, time/batch = 0.059\n",
            "15/7395000 (epoch 0), train_loss = 3.417, time/batch = 0.055\n",
            "16/7395000 (epoch 0), train_loss = 3.375, time/batch = 0.055\n",
            "17/7395000 (epoch 0), train_loss = 3.407, time/batch = 0.056\n",
            "18/7395000 (epoch 0), train_loss = 3.448, time/batch = 0.053\n",
            "19/7395000 (epoch 0), train_loss = 3.454, time/batch = 0.052\n",
            "20/7395000 (epoch 0), train_loss = 3.429, time/batch = 0.058\n",
            "21/7395000 (epoch 0), train_loss = 3.465, time/batch = 0.055\n",
            "22/7395000 (epoch 0), train_loss = 3.387, time/batch = 0.052\n",
            "23/7395000 (epoch 0), train_loss = 3.447, time/batch = 0.054\n",
            "24/7395000 (epoch 0), train_loss = 3.384, time/batch = 0.053\n",
            "25/7395000 (epoch 0), train_loss = 3.433, time/batch = 0.053\n",
            "26/7395000 (epoch 0), train_loss = 3.383, time/batch = 0.054\n",
            "27/7395000 (epoch 0), train_loss = 3.415, time/batch = 0.052\n",
            "28/7395000 (epoch 0), train_loss = 3.290, time/batch = 0.058\n",
            "29/7395000 (epoch 0), train_loss = 3.332, time/batch = 0.054\n",
            "30/7395000 (epoch 0), train_loss = 3.366, time/batch = 0.054\n",
            "31/7395000 (epoch 0), train_loss = 3.322, time/batch = 0.053\n",
            "32/7395000 (epoch 0), train_loss = 3.368, time/batch = 0.055\n",
            "33/7395000 (epoch 0), train_loss = 3.362, time/batch = 0.057\n",
            "34/7395000 (epoch 0), train_loss = 3.355, time/batch = 0.053\n",
            "35/7395000 (epoch 0), train_loss = 3.357, time/batch = 0.052\n",
            "36/7395000 (epoch 0), train_loss = 3.440, time/batch = 0.052\n",
            "37/7395000 (epoch 0), train_loss = 3.444, time/batch = 0.057\n",
            "38/7395000 (epoch 0), train_loss = 3.454, time/batch = 0.051\n",
            "39/7395000 (epoch 0), train_loss = 3.362, time/batch = 0.052\n",
            "40/7395000 (epoch 0), train_loss = 3.289, time/batch = 0.053\n",
            "41/7395000 (epoch 0), train_loss = 3.244, time/batch = 0.055\n",
            "42/7395000 (epoch 0), train_loss = 3.351, time/batch = 0.052\n",
            "43/7395000 (epoch 0), train_loss = 3.320, time/batch = 0.053\n",
            "44/7395000 (epoch 0), train_loss = 3.370, time/batch = 0.052\n",
            "45/7395000 (epoch 0), train_loss = 3.369, time/batch = 0.057\n",
            "46/7395000 (epoch 0), train_loss = 3.270, time/batch = 0.052\n",
            "47/7395000 (epoch 0), train_loss = 3.379, time/batch = 0.057\n",
            "48/7395000 (epoch 0), train_loss = 3.301, time/batch = 0.050\n",
            "49/7395000 (epoch 0), train_loss = 3.341, time/batch = 0.053\n",
            "50/7395000 (epoch 0), train_loss = 3.203, time/batch = 0.052\n",
            "51/7395000 (epoch 0), train_loss = 3.304, time/batch = 0.059\n",
            "52/7395000 (epoch 0), train_loss = 3.352, time/batch = 0.051\n",
            "53/7395000 (epoch 0), train_loss = 3.339, time/batch = 0.052\n",
            "54/7395000 (epoch 0), train_loss = 3.369, time/batch = 0.047\n",
            "55/7395000 (epoch 0), train_loss = 3.305, time/batch = 0.051\n",
            "56/7395000 (epoch 0), train_loss = 3.255, time/batch = 0.050\n",
            "57/7395000 (epoch 0), train_loss = 3.156, time/batch = 0.052\n",
            "58/7395000 (epoch 0), train_loss = 3.265, time/batch = 0.051\n",
            "59/7395000 (epoch 0), train_loss = 3.210, time/batch = 0.053\n",
            "60/7395000 (epoch 0), train_loss = 3.216, time/batch = 0.050\n",
            "61/7395000 (epoch 0), train_loss = 3.275, time/batch = 0.058\n",
            "62/7395000 (epoch 0), train_loss = 3.254, time/batch = 0.053\n",
            "63/7395000 (epoch 0), train_loss = 3.362, time/batch = 0.052\n",
            "64/7395000 (epoch 0), train_loss = 3.188, time/batch = 0.059\n",
            "65/7395000 (epoch 0), train_loss = 3.279, time/batch = 0.052\n",
            "66/7395000 (epoch 0), train_loss = 3.300, time/batch = 0.056\n",
            "67/7395000 (epoch 0), train_loss = 3.285, time/batch = 0.051\n",
            "68/7395000 (epoch 0), train_loss = 3.178, time/batch = 0.050\n",
            "69/7395000 (epoch 0), train_loss = 3.220, time/batch = 0.051\n",
            "70/7395000 (epoch 0), train_loss = 3.245, time/batch = 0.059\n",
            "71/7395000 (epoch 0), train_loss = 3.331, time/batch = 0.046\n",
            "72/7395000 (epoch 0), train_loss = 3.270, time/batch = 0.050\n",
            "73/7395000 (epoch 0), train_loss = 3.212, time/batch = 0.050\n",
            "74/7395000 (epoch 0), train_loss = 3.290, time/batch = 0.054\n",
            "75/7395000 (epoch 0), train_loss = 3.228, time/batch = 0.049\n",
            "76/7395000 (epoch 0), train_loss = 3.316, time/batch = 0.054\n",
            "77/7395000 (epoch 0), train_loss = 3.172, time/batch = 0.049\n",
            "78/7395000 (epoch 0), train_loss = 3.223, time/batch = 0.048\n",
            "79/7395000 (epoch 0), train_loss = 3.276, time/batch = 0.051\n",
            "80/7395000 (epoch 0), train_loss = 3.337, time/batch = 0.048\n",
            "81/7395000 (epoch 0), train_loss = 3.243, time/batch = 0.050\n",
            "82/7395000 (epoch 0), train_loss = 3.222, time/batch = 0.052\n",
            "83/7395000 (epoch 0), train_loss = 3.401, time/batch = 0.051\n",
            "84/7395000 (epoch 0), train_loss = 3.249, time/batch = 0.057\n",
            "85/7395000 (epoch 0), train_loss = 3.281, time/batch = 0.050\n",
            "86/7395000 (epoch 0), train_loss = 3.329, time/batch = 0.055\n",
            "87/7395000 (epoch 0), train_loss = 3.306, time/batch = 0.051\n",
            "88/7395000 (epoch 0), train_loss = 3.279, time/batch = 0.049\n",
            "89/7395000 (epoch 0), train_loss = 3.296, time/batch = 0.053\n",
            "90/7395000 (epoch 0), train_loss = 3.323, time/batch = 0.052\n",
            "91/7395000 (epoch 0), train_loss = 3.296, time/batch = 0.049\n",
            "92/7395000 (epoch 0), train_loss = 3.182, time/batch = 0.049\n",
            "93/7395000 (epoch 0), train_loss = 3.285, time/batch = 0.053\n",
            "94/7395000 (epoch 0), train_loss = 3.289, time/batch = 0.051\n",
            "95/7395000 (epoch 0), train_loss = 3.288, time/batch = 0.052\n",
            "96/7395000 (epoch 0), train_loss = 3.285, time/batch = 0.060\n",
            "97/7395000 (epoch 0), train_loss = 3.244, time/batch = 0.059\n",
            "98/7395000 (epoch 0), train_loss = 3.210, time/batch = 0.052\n",
            "99/7395000 (epoch 0), train_loss = 3.238, time/batch = 0.052\n",
            "100/7395000 (epoch 0), train_loss = 3.254, time/batch = 0.051\n",
            "101/7395000 (epoch 0), train_loss = 3.176, time/batch = 0.052\n",
            "102/7395000 (epoch 0), train_loss = 3.198, time/batch = 0.051\n",
            "103/7395000 (epoch 0), train_loss = 3.318, time/batch = 0.053\n",
            "104/7395000 (epoch 0), train_loss = 3.275, time/batch = 0.051\n",
            "105/7395000 (epoch 0), train_loss = 3.283, time/batch = 0.051\n",
            "106/7395000 (epoch 0), train_loss = 3.283, time/batch = 0.052\n",
            "107/7395000 (epoch 0), train_loss = 3.258, time/batch = 0.054\n",
            "108/7395000 (epoch 0), train_loss = 3.262, time/batch = 0.053\n",
            "109/7395000 (epoch 0), train_loss = 3.142, time/batch = 0.051\n",
            "110/7395000 (epoch 0), train_loss = 3.264, time/batch = 0.057\n",
            "111/7395000 (epoch 0), train_loss = 3.267, time/batch = 0.051\n",
            "112/7395000 (epoch 0), train_loss = 3.301, time/batch = 0.053\n",
            "113/7395000 (epoch 0), train_loss = 3.205, time/batch = 0.051\n",
            "114/7395000 (epoch 0), train_loss = 3.199, time/batch = 0.056\n",
            "115/7395000 (epoch 0), train_loss = 3.159, time/batch = 0.056\n",
            "116/7395000 (epoch 0), train_loss = 3.148, time/batch = 0.055\n",
            "117/7395000 (epoch 0), train_loss = 3.180, time/batch = 0.053\n",
            "118/7395000 (epoch 0), train_loss = 3.107, time/batch = 0.054\n",
            "119/7395000 (epoch 0), train_loss = 3.195, time/batch = 0.053\n",
            "120/7395000 (epoch 0), train_loss = 3.148, time/batch = 0.054\n",
            "121/7395000 (epoch 0), train_loss = 3.029, time/batch = 0.054\n",
            "122/7395000 (epoch 0), train_loss = 3.053, time/batch = 0.053\n",
            "123/7395000 (epoch 0), train_loss = 3.185, time/batch = 0.054\n",
            "124/7395000 (epoch 0), train_loss = 3.091, time/batch = 0.053\n",
            "125/7395000 (epoch 0), train_loss = 3.164, time/batch = 0.055\n",
            "126/7395000 (epoch 0), train_loss = 3.095, time/batch = 0.051\n",
            "127/7395000 (epoch 0), train_loss = 3.278, time/batch = 0.053\n",
            "128/7395000 (epoch 0), train_loss = 3.089, time/batch = 0.049\n",
            "129/7395000 (epoch 0), train_loss = 3.102, time/batch = 0.068\n",
            "130/7395000 (epoch 0), train_loss = 3.121, time/batch = 0.053\n",
            "131/7395000 (epoch 0), train_loss = 3.015, time/batch = 0.054\n",
            "132/7395000 (epoch 0), train_loss = 3.096, time/batch = 0.053\n",
            "133/7395000 (epoch 0), train_loss = 3.218, time/batch = 0.058\n",
            "134/7395000 (epoch 0), train_loss = 3.150, time/batch = 0.058\n",
            "135/7395000 (epoch 0), train_loss = 3.082, time/batch = 0.053\n",
            "136/7395000 (epoch 0), train_loss = 3.113, time/batch = 0.053\n",
            "137/7395000 (epoch 0), train_loss = 3.299, time/batch = 0.053\n",
            "138/7395000 (epoch 0), train_loss = 3.224, time/batch = 0.053\n",
            "139/7395000 (epoch 0), train_loss = 2.959, time/batch = 0.052\n",
            "140/7395000 (epoch 0), train_loss = 3.182, time/batch = 0.056\n",
            "141/7395000 (epoch 0), train_loss = 3.093, time/batch = 0.054\n",
            "142/7395000 (epoch 0), train_loss = 3.096, time/batch = 0.053\n",
            "143/7395000 (epoch 0), train_loss = 3.041, time/batch = 0.055\n",
            "144/7395000 (epoch 0), train_loss = 3.111, time/batch = 0.053\n",
            "145/7395000 (epoch 0), train_loss = 3.093, time/batch = 0.054\n",
            "146/7395000 (epoch 0), train_loss = 3.112, time/batch = 0.052\n",
            "147/7395000 (epoch 0), train_loss = 3.057, time/batch = 0.057\n",
            "148/7395000 (epoch 0), train_loss = 3.047, time/batch = 0.058\n",
            "149/7395000 (epoch 0), train_loss = 2.983, time/batch = 0.055\n",
            "150/7395000 (epoch 0), train_loss = 3.000, time/batch = 0.053\n",
            "151/7395000 (epoch 0), train_loss = 3.081, time/batch = 0.054\n",
            "152/7395000 (epoch 0), train_loss = 3.073, time/batch = 0.051\n",
            "153/7395000 (epoch 0), train_loss = 3.036, time/batch = 0.054\n",
            "154/7395000 (epoch 0), train_loss = 2.982, time/batch = 0.054\n",
            "155/7395000 (epoch 0), train_loss = 2.925, time/batch = 0.057\n",
            "156/7395000 (epoch 0), train_loss = 2.969, time/batch = 0.053\n",
            "157/7395000 (epoch 0), train_loss = 3.038, time/batch = 0.053\n",
            "158/7395000 (epoch 0), train_loss = 2.963, time/batch = 0.051\n",
            "159/7395000 (epoch 0), train_loss = 2.992, time/batch = 0.054\n",
            "160/7395000 (epoch 0), train_loss = 3.063, time/batch = 0.057\n",
            "161/7395000 (epoch 0), train_loss = 3.015, time/batch = 0.052\n",
            "162/7395000 (epoch 0), train_loss = 2.990, time/batch = 0.054\n",
            "163/7395000 (epoch 0), train_loss = 3.024, time/batch = 0.054\n",
            "164/7395000 (epoch 0), train_loss = 2.918, time/batch = 0.052\n",
            "165/7395000 (epoch 0), train_loss = 2.952, time/batch = 0.054\n",
            "166/7395000 (epoch 0), train_loss = 2.892, time/batch = 0.051\n",
            "167/7395000 (epoch 0), train_loss = 2.969, time/batch = 0.059\n",
            "168/7395000 (epoch 0), train_loss = 2.955, time/batch = 0.053\n",
            "169/7395000 (epoch 0), train_loss = 2.989, time/batch = 0.052\n",
            "170/7395000 (epoch 0), train_loss = 2.891, time/batch = 0.056\n",
            "171/7395000 (epoch 0), train_loss = 2.992, time/batch = 0.054\n",
            "172/7395000 (epoch 0), train_loss = 2.991, time/batch = 0.052\n",
            "173/7395000 (epoch 0), train_loss = 2.942, time/batch = 0.057\n",
            "174/7395000 (epoch 0), train_loss = 2.985, time/batch = 0.056\n",
            "175/7395000 (epoch 0), train_loss = 3.004, time/batch = 0.054\n",
            "176/7395000 (epoch 0), train_loss = 2.886, time/batch = 0.055\n",
            "177/7395000 (epoch 0), train_loss = 2.919, time/batch = 0.055\n",
            "178/7395000 (epoch 0), train_loss = 2.848, time/batch = 0.059\n",
            "179/7395000 (epoch 0), train_loss = 2.813, time/batch = 0.062\n",
            "180/7395000 (epoch 0), train_loss = 2.876, time/batch = 0.054\n",
            "181/7395000 (epoch 0), train_loss = 2.793, time/batch = 0.059\n",
            "182/7395000 (epoch 0), train_loss = 2.806, time/batch = 0.054\n",
            "183/7395000 (epoch 0), train_loss = 2.989, time/batch = 0.054\n",
            "184/7395000 (epoch 0), train_loss = 3.013, time/batch = 0.054\n",
            "185/7395000 (epoch 0), train_loss = 2.873, time/batch = 0.058\n",
            "186/7395000 (epoch 0), train_loss = 3.011, time/batch = 0.053\n",
            "187/7395000 (epoch 0), train_loss = 2.958, time/batch = 0.057\n",
            "188/7395000 (epoch 0), train_loss = 3.087, time/batch = 0.055\n",
            "189/7395000 (epoch 0), train_loss = 3.008, time/batch = 0.060\n",
            "190/7395000 (epoch 0), train_loss = 3.012, time/batch = 0.055\n",
            "191/7395000 (epoch 0), train_loss = 2.915, time/batch = 0.055\n",
            "192/7395000 (epoch 0), train_loss = 2.973, time/batch = 0.057\n",
            "193/7395000 (epoch 0), train_loss = 2.928, time/batch = 0.056\n",
            "194/7395000 (epoch 0), train_loss = 2.978, time/batch = 0.052\n",
            "195/7395000 (epoch 0), train_loss = 3.046, time/batch = 0.060\n",
            "196/7395000 (epoch 0), train_loss = 3.113, time/batch = 0.055\n",
            "197/7395000 (epoch 0), train_loss = 3.058, time/batch = 0.059\n",
            "198/7395000 (epoch 0), train_loss = 3.019, time/batch = 0.055\n",
            "199/7395000 (epoch 0), train_loss = 2.966, time/batch = 0.054\n",
            "200/7395000 (epoch 0), train_loss = 3.048, time/batch = 0.053\n",
            "201/7395000 (epoch 0), train_loss = 2.969, time/batch = 0.056\n",
            "202/7395000 (epoch 0), train_loss = 2.874, time/batch = 0.054\n",
            "203/7395000 (epoch 0), train_loss = 3.064, time/batch = 0.059\n",
            "204/7395000 (epoch 0), train_loss = 2.965, time/batch = 0.058\n",
            "205/7395000 (epoch 0), train_loss = 2.992, time/batch = 0.053\n",
            "206/7395000 (epoch 0), train_loss = 3.028, time/batch = 0.055\n",
            "207/7395000 (epoch 0), train_loss = 3.071, time/batch = 0.056\n",
            "208/7395000 (epoch 0), train_loss = 2.973, time/batch = 0.054\n",
            "209/7395000 (epoch 0), train_loss = 3.058, time/batch = 0.061\n",
            "210/7395000 (epoch 0), train_loss = 2.945, time/batch = 0.055\n",
            "211/7395000 (epoch 0), train_loss = 3.084, time/batch = 0.055\n",
            "212/7395000 (epoch 0), train_loss = 3.030, time/batch = 0.053\n",
            "213/7395000 (epoch 0), train_loss = 2.963, time/batch = 0.055\n",
            "214/7395000 (epoch 0), train_loss = 2.865, time/batch = 0.051\n",
            "215/7395000 (epoch 0), train_loss = 2.968, time/batch = 0.058\n",
            "216/7395000 (epoch 0), train_loss = 2.873, time/batch = 0.054\n",
            "217/7395000 (epoch 0), train_loss = 3.001, time/batch = 0.055\n",
            "218/7395000 (epoch 0), train_loss = 2.998, time/batch = 0.050\n",
            "219/7395000 (epoch 0), train_loss = 2.785, time/batch = 0.054\n",
            "220/7395000 (epoch 0), train_loss = 2.984, time/batch = 0.049\n",
            "221/7395000 (epoch 0), train_loss = 3.008, time/batch = 0.049\n",
            "222/7395000 (epoch 0), train_loss = 2.971, time/batch = 0.069\n",
            "223/7395000 (epoch 0), train_loss = 2.963, time/batch = 0.051\n",
            "224/7395000 (epoch 0), train_loss = 2.885, time/batch = 0.051\n",
            "225/7395000 (epoch 0), train_loss = 2.976, time/batch = 0.054\n",
            "226/7395000 (epoch 0), train_loss = 2.923, time/batch = 0.050\n",
            "227/7395000 (epoch 0), train_loss = 2.977, time/batch = 0.051\n",
            "228/7395000 (epoch 0), train_loss = 2.908, time/batch = 0.050\n",
            "229/7395000 (epoch 0), train_loss = 2.825, time/batch = 0.059\n",
            "230/7395000 (epoch 0), train_loss = 2.990, time/batch = 0.052\n",
            "231/7395000 (epoch 0), train_loss = 2.988, time/batch = 0.051\n",
            "232/7395000 (epoch 0), train_loss = 2.940, time/batch = 0.052\n",
            "233/7395000 (epoch 0), train_loss = 2.857, time/batch = 0.051\n",
            "234/7395000 (epoch 0), train_loss = 3.046, time/batch = 0.057\n",
            "235/7395000 (epoch 0), train_loss = 2.988, time/batch = 0.051\n",
            "236/7395000 (epoch 0), train_loss = 2.953, time/batch = 0.053\n",
            "237/7395000 (epoch 0), train_loss = 2.941, time/batch = 0.058\n",
            "238/7395000 (epoch 0), train_loss = 2.910, time/batch = 0.055\n",
            "239/7395000 (epoch 0), train_loss = 3.007, time/batch = 0.055\n",
            "240/7395000 (epoch 0), train_loss = 2.970, time/batch = 0.056\n",
            "241/7395000 (epoch 0), train_loss = 2.889, time/batch = 0.065\n",
            "242/7395000 (epoch 0), train_loss = 2.924, time/batch = 0.057\n",
            "243/7395000 (epoch 0), train_loss = 2.916, time/batch = 0.058\n",
            "244/7395000 (epoch 0), train_loss = 2.962, time/batch = 0.056\n",
            "245/7395000 (epoch 0), train_loss = 2.933, time/batch = 0.058\n",
            "246/7395000 (epoch 0), train_loss = 2.876, time/batch = 0.055\n",
            "247/7395000 (epoch 0), train_loss = 2.909, time/batch = 0.057\n",
            "248/7395000 (epoch 0), train_loss = 2.950, time/batch = 0.056\n",
            "249/7395000 (epoch 0), train_loss = 2.934, time/batch = 0.056\n",
            "250/7395000 (epoch 0), train_loss = 2.909, time/batch = 0.063\n",
            "251/7395000 (epoch 0), train_loss = 2.895, time/batch = 0.057\n",
            "252/7395000 (epoch 0), train_loss = 2.936, time/batch = 0.057\n",
            "253/7395000 (epoch 0), train_loss = 2.961, time/batch = 0.058\n",
            "254/7395000 (epoch 0), train_loss = 2.902, time/batch = 0.056\n",
            "255/7395000 (epoch 0), train_loss = 2.901, time/batch = 0.057\n",
            "256/7395000 (epoch 0), train_loss = 2.916, time/batch = 0.060\n",
            "257/7395000 (epoch 0), train_loss = 2.913, time/batch = 0.057\n",
            "258/7395000 (epoch 0), train_loss = 2.910, time/batch = 0.057\n",
            "259/7395000 (epoch 0), train_loss = 2.929, time/batch = 0.055\n",
            "260/7395000 (epoch 0), train_loss = 2.882, time/batch = 0.057\n",
            "261/7395000 (epoch 0), train_loss = 2.963, time/batch = 0.054\n",
            "262/7395000 (epoch 0), train_loss = 2.958, time/batch = 0.058\n",
            "263/7395000 (epoch 0), train_loss = 2.870, time/batch = 0.055\n",
            "264/7395000 (epoch 0), train_loss = 2.936, time/batch = 0.056\n",
            "265/7395000 (epoch 0), train_loss = 2.897, time/batch = 0.056\n",
            "266/7395000 (epoch 0), train_loss = 2.808, time/batch = 0.056\n",
            "267/7395000 (epoch 0), train_loss = 2.878, time/batch = 0.056\n",
            "268/7395000 (epoch 0), train_loss = 2.926, time/batch = 0.055\n",
            "269/7395000 (epoch 0), train_loss = 2.804, time/batch = 0.056\n",
            "270/7395000 (epoch 0), train_loss = 2.821, time/batch = 0.056\n",
            "271/7395000 (epoch 0), train_loss = 2.731, time/batch = 0.058\n",
            "272/7395000 (epoch 0), train_loss = 2.776, time/batch = 0.057\n",
            "273/7395000 (epoch 0), train_loss = 2.722, time/batch = 0.056\n",
            "274/7395000 (epoch 0), train_loss = 2.766, time/batch = 0.055\n",
            "275/7395000 (epoch 0), train_loss = 2.896, time/batch = 0.059\n",
            "276/7395000 (epoch 0), train_loss = 2.879, time/batch = 0.058\n",
            "277/7395000 (epoch 0), train_loss = 2.872, time/batch = 0.067\n",
            "278/7395000 (epoch 0), train_loss = 2.800, time/batch = 0.058\n",
            "279/7395000 (epoch 0), train_loss = 2.816, time/batch = 0.058\n",
            "280/7395000 (epoch 0), train_loss = 2.821, time/batch = 0.055\n",
            "281/7395000 (epoch 0), train_loss = 2.778, time/batch = 0.056\n",
            "282/7395000 (epoch 0), train_loss = 2.675, time/batch = 0.062\n",
            "283/7395000 (epoch 0), train_loss = 2.794, time/batch = 0.057\n",
            "284/7395000 (epoch 0), train_loss = 2.719, time/batch = 0.059\n",
            "285/7395000 (epoch 0), train_loss = 2.751, time/batch = 0.058\n",
            "286/7395000 (epoch 0), train_loss = 2.700, time/batch = 0.060\n",
            "287/7395000 (epoch 0), train_loss = 2.715, time/batch = 0.059\n",
            "288/7395000 (epoch 0), train_loss = 2.663, time/batch = 0.058\n",
            "289/7395000 (epoch 0), train_loss = 2.774, time/batch = 0.058\n",
            "290/7395000 (epoch 0), train_loss = 2.847, time/batch = 0.052\n",
            "291/7395000 (epoch 0), train_loss = 2.701, time/batch = 0.053\n",
            "292/7395000 (epoch 0), train_loss = 2.728, time/batch = 0.052\n",
            "293/7395000 (epoch 0), train_loss = 2.821, time/batch = 0.056\n",
            "294/7395000 (epoch 0), train_loss = 2.720, time/batch = 0.056\n",
            "295/7395000 (epoch 0), train_loss = 2.795, time/batch = 0.065\n",
            "296/7395000 (epoch 0), train_loss = 2.787, time/batch = 0.055\n",
            "297/7395000 (epoch 0), train_loss = 2.652, time/batch = 0.052\n",
            "298/7395000 (epoch 0), train_loss = 2.733, time/batch = 0.055\n",
            "299/7395000 (epoch 0), train_loss = 2.720, time/batch = 0.052\n",
            "300/7395000 (epoch 0), train_loss = 2.647, time/batch = 0.053\n",
            "301/7395000 (epoch 0), train_loss = 2.815, time/batch = 0.052\n",
            "302/7395000 (epoch 0), train_loss = 2.687, time/batch = 0.058\n",
            "303/7395000 (epoch 0), train_loss = 2.786, time/batch = 0.057\n",
            "304/7395000 (epoch 0), train_loss = 2.647, time/batch = 0.056\n",
            "305/7395000 (epoch 0), train_loss = 2.734, time/batch = 0.058\n",
            "306/7395000 (epoch 0), train_loss = 2.824, time/batch = 0.057\n",
            "307/7395000 (epoch 0), train_loss = 2.759, time/batch = 0.063\n",
            "308/7395000 (epoch 0), train_loss = 2.712, time/batch = 0.060\n",
            "309/7395000 (epoch 0), train_loss = 2.727, time/batch = 0.056\n",
            "310/7395000 (epoch 0), train_loss = 2.731, time/batch = 0.058\n",
            "311/7395000 (epoch 0), train_loss = 2.743, time/batch = 0.060\n",
            "312/7395000 (epoch 0), train_loss = 2.681, time/batch = 0.068\n",
            "313/7395000 (epoch 0), train_loss = 2.719, time/batch = 0.058\n",
            "314/7395000 (epoch 0), train_loss = 2.789, time/batch = 0.061\n",
            "315/7395000 (epoch 0), train_loss = 2.746, time/batch = 0.061\n",
            "316/7395000 (epoch 0), train_loss = 2.641, time/batch = 0.059\n",
            "317/7395000 (epoch 0), train_loss = 2.798, time/batch = 0.059\n",
            "318/7395000 (epoch 0), train_loss = 2.719, time/batch = 0.059\n",
            "319/7395000 (epoch 0), train_loss = 2.695, time/batch = 0.059\n",
            "320/7395000 (epoch 0), train_loss = 2.731, time/batch = 0.064\n",
            "321/7395000 (epoch 0), train_loss = 2.795, time/batch = 0.059\n",
            "322/7395000 (epoch 0), train_loss = 2.586, time/batch = 0.059\n",
            "323/7395000 (epoch 0), train_loss = 2.748, time/batch = 0.060\n",
            "324/7395000 (epoch 0), train_loss = 2.671, time/batch = 0.057\n",
            "325/7395000 (epoch 0), train_loss = 2.659, time/batch = 0.059\n",
            "326/7395000 (epoch 0), train_loss = 2.585, time/batch = 0.064\n",
            "327/7395000 (epoch 0), train_loss = 2.710, time/batch = 0.059\n",
            "328/7395000 (epoch 0), train_loss = 2.504, time/batch = 0.058\n",
            "329/7395000 (epoch 0), train_loss = 2.534, time/batch = 0.063\n",
            "330/7395000 (epoch 0), train_loss = 2.638, time/batch = 0.058\n",
            "331/7395000 (epoch 0), train_loss = 2.464, time/batch = 0.058\n",
            "332/7395000 (epoch 0), train_loss = 2.588, time/batch = 0.059\n",
            "333/7395000 (epoch 0), train_loss = 2.607, time/batch = 0.059\n",
            "334/7395000 (epoch 0), train_loss = 2.553, time/batch = 0.061\n",
            "335/7395000 (epoch 0), train_loss = 2.598, time/batch = 0.059\n",
            "336/7395000 (epoch 0), train_loss = 2.651, time/batch = 0.069\n",
            "337/7395000 (epoch 0), train_loss = 2.576, time/batch = 0.058\n",
            "338/7395000 (epoch 0), train_loss = 2.670, time/batch = 0.069\n",
            "339/7395000 (epoch 0), train_loss = 2.710, time/batch = 0.063\n",
            "340/7395000 (epoch 0), train_loss = 2.510, time/batch = 0.056\n",
            "341/7395000 (epoch 0), train_loss = 2.498, time/batch = 0.059\n",
            "342/7395000 (epoch 0), train_loss = 2.559, time/batch = 0.070\n",
            "343/7395000 (epoch 0), train_loss = 2.508, time/batch = 0.058\n",
            "344/7395000 (epoch 0), train_loss = 2.474, time/batch = 0.058\n",
            "345/7395000 (epoch 0), train_loss = 2.547, time/batch = 0.060\n",
            "346/7395000 (epoch 0), train_loss = 2.692, time/batch = 0.070\n",
            "347/7395000 (epoch 0), train_loss = 2.555, time/batch = 0.060\n",
            "348/7395000 (epoch 0), train_loss = 2.510, time/batch = 0.068\n",
            "349/7395000 (epoch 0), train_loss = 2.568, time/batch = 0.060\n",
            "350/7395000 (epoch 0), train_loss = 2.649, time/batch = 0.059\n",
            "351/7395000 (epoch 0), train_loss = 2.708, time/batch = 0.060\n",
            "352/7395000 (epoch 0), train_loss = 2.577, time/batch = 0.060\n",
            "353/7395000 (epoch 0), train_loss = 2.618, time/batch = 0.060\n",
            "354/7395000 (epoch 0), train_loss = 2.640, time/batch = 0.060\n",
            "355/7395000 (epoch 0), train_loss = 2.629, time/batch = 0.058\n",
            "356/7395000 (epoch 0), train_loss = 2.521, time/batch = 0.060\n",
            "357/7395000 (epoch 0), train_loss = 2.546, time/batch = 0.060\n",
            "358/7395000 (epoch 0), train_loss = 2.600, time/batch = 0.060\n",
            "359/7395000 (epoch 0), train_loss = 2.644, time/batch = 0.058\n",
            "360/7395000 (epoch 0), train_loss = 2.665, time/batch = 0.059\n",
            "361/7395000 (epoch 0), train_loss = 2.682, time/batch = 0.058\n",
            "362/7395000 (epoch 0), train_loss = 2.453, time/batch = 0.061\n",
            "363/7395000 (epoch 0), train_loss = 2.626, time/batch = 0.074\n",
            "364/7395000 (epoch 0), train_loss = 2.697, time/batch = 0.059\n",
            "365/7395000 (epoch 0), train_loss = 2.607, time/batch = 0.060\n",
            "366/7395000 (epoch 0), train_loss = 2.678, time/batch = 0.060\n",
            "367/7395000 (epoch 0), train_loss = 2.623, time/batch = 0.057\n",
            "368/7395000 (epoch 0), train_loss = 2.491, time/batch = 0.056\n",
            "369/7395000 (epoch 0), train_loss = 2.651, time/batch = 0.058\n",
            "370/7395000 (epoch 0), train_loss = 2.688, time/batch = 0.058\n",
            "371/7395000 (epoch 0), train_loss = 2.684, time/batch = 0.059\n",
            "372/7395000 (epoch 0), train_loss = 2.631, time/batch = 0.059\n",
            "373/7395000 (epoch 0), train_loss = 2.632, time/batch = 0.059\n",
            "374/7395000 (epoch 0), train_loss = 2.579, time/batch = 0.056\n",
            "375/7395000 (epoch 0), train_loss = 2.564, time/batch = 0.058\n",
            "376/7395000 (epoch 0), train_loss = 2.603, time/batch = 0.058\n",
            "377/7395000 (epoch 0), train_loss = 2.621, time/batch = 0.059\n",
            "378/7395000 (epoch 0), train_loss = 2.501, time/batch = 0.058\n",
            "379/7395000 (epoch 0), train_loss = 2.544, time/batch = 0.059\n",
            "380/7395000 (epoch 0), train_loss = 2.586, time/batch = 0.060\n",
            "381/7395000 (epoch 0), train_loss = 2.620, time/batch = 0.055\n",
            "382/7395000 (epoch 0), train_loss = 2.603, time/batch = 0.059\n",
            "383/7395000 (epoch 0), train_loss = 2.594, time/batch = 0.060\n",
            "384/7395000 (epoch 0), train_loss = 2.546, time/batch = 0.060\n",
            "385/7395000 (epoch 0), train_loss = 2.536, time/batch = 0.056\n",
            "386/7395000 (epoch 0), train_loss = 2.682, time/batch = 0.058\n",
            "387/7395000 (epoch 0), train_loss = 2.485, time/batch = 0.058\n",
            "388/7395000 (epoch 0), train_loss = 2.555, time/batch = 0.064\n",
            "389/7395000 (epoch 0), train_loss = 2.535, time/batch = 0.061\n",
            "390/7395000 (epoch 0), train_loss = 2.553, time/batch = 0.058\n",
            "391/7395000 (epoch 0), train_loss = 2.601, time/batch = 0.060\n",
            "392/7395000 (epoch 0), train_loss = 2.573, time/batch = 0.057\n",
            "393/7395000 (epoch 0), train_loss = 2.625, time/batch = 0.057\n",
            "394/7395000 (epoch 0), train_loss = 2.543, time/batch = 0.057\n",
            "395/7395000 (epoch 0), train_loss = 2.494, time/batch = 0.058\n",
            "396/7395000 (epoch 0), train_loss = 2.566, time/batch = 0.058\n",
            "397/7395000 (epoch 0), train_loss = 2.564, time/batch = 0.058\n",
            "398/7395000 (epoch 0), train_loss = 2.554, time/batch = 0.074\n",
            "399/7395000 (epoch 0), train_loss = 2.493, time/batch = 0.059\n",
            "400/7395000 (epoch 0), train_loss = 2.511, time/batch = 0.057\n",
            "401/7395000 (epoch 0), train_loss = 2.567, time/batch = 0.060\n",
            "402/7395000 (epoch 0), train_loss = 2.475, time/batch = 0.058\n",
            "403/7395000 (epoch 0), train_loss = 2.550, time/batch = 0.057\n",
            "404/7395000 (epoch 0), train_loss = 2.522, time/batch = 0.057\n",
            "405/7395000 (epoch 0), train_loss = 2.654, time/batch = 0.060\n",
            "406/7395000 (epoch 0), train_loss = 2.541, time/batch = 0.052\n",
            "407/7395000 (epoch 0), train_loss = 2.528, time/batch = 0.058\n",
            "408/7395000 (epoch 0), train_loss = 2.561, time/batch = 0.064\n",
            "409/7395000 (epoch 0), train_loss = 2.491, time/batch = 0.060\n",
            "410/7395000 (epoch 0), train_loss = 2.578, time/batch = 0.056\n",
            "411/7395000 (epoch 0), train_loss = 2.582, time/batch = 0.058\n",
            "412/7395000 (epoch 0), train_loss = 2.380, time/batch = 0.057\n",
            "413/7395000 (epoch 0), train_loss = 2.419, time/batch = 0.066\n",
            "414/7395000 (epoch 0), train_loss = 2.473, time/batch = 0.058\n",
            "415/7395000 (epoch 0), train_loss = 2.441, time/batch = 0.066\n",
            "416/7395000 (epoch 0), train_loss = 2.611, time/batch = 0.061\n",
            "417/7395000 (epoch 0), train_loss = 2.472, time/batch = 0.058\n",
            "418/7395000 (epoch 0), train_loss = 2.528, time/batch = 0.060\n",
            "419/7395000 (epoch 0), train_loss = 2.551, time/batch = 0.060\n",
            "420/7395000 (epoch 0), train_loss = 2.584, time/batch = 0.057\n",
            "421/7395000 (epoch 0), train_loss = 2.488, time/batch = 0.063\n",
            "422/7395000 (epoch 0), train_loss = 2.462, time/batch = 0.059\n",
            "423/7395000 (epoch 0), train_loss = 2.540, time/batch = 0.060\n",
            "424/7395000 (epoch 0), train_loss = 2.450, time/batch = 0.057\n",
            "425/7395000 (epoch 0), train_loss = 2.557, time/batch = 0.066\n",
            "426/7395000 (epoch 0), train_loss = 2.471, time/batch = 0.061\n",
            "427/7395000 (epoch 0), train_loss = 2.596, time/batch = 0.060\n",
            "428/7395000 (epoch 0), train_loss = 2.440, time/batch = 0.057\n",
            "429/7395000 (epoch 0), train_loss = 2.429, time/batch = 0.059\n",
            "430/7395000 (epoch 0), train_loss = 2.452, time/batch = 0.058\n",
            "431/7395000 (epoch 0), train_loss = 2.409, time/batch = 0.058\n",
            "432/7395000 (epoch 0), train_loss = 2.613, time/batch = 0.062\n",
            "433/7395000 (epoch 0), train_loss = 2.620, time/batch = 0.059\n",
            "434/7395000 (epoch 0), train_loss = 2.598, time/batch = 0.058\n",
            "435/7395000 (epoch 0), train_loss = 2.594, time/batch = 0.058\n",
            "436/7395000 (epoch 0), train_loss = 2.591, time/batch = 0.059\n",
            "437/7395000 (epoch 0), train_loss = 2.516, time/batch = 0.058\n",
            "438/7395000 (epoch 0), train_loss = 2.558, time/batch = 0.060\n",
            "439/7395000 (epoch 0), train_loss = 2.579, time/batch = 0.059\n",
            "440/7395000 (epoch 0), train_loss = 2.563, time/batch = 0.058\n",
            "441/7395000 (epoch 0), train_loss = 2.585, time/batch = 0.058\n",
            "442/7395000 (epoch 0), train_loss = 2.527, time/batch = 0.059\n",
            "443/7395000 (epoch 0), train_loss = 2.522, time/batch = 0.057\n",
            "444/7395000 (epoch 0), train_loss = 2.540, time/batch = 0.057\n",
            "445/7395000 (epoch 0), train_loss = 2.466, time/batch = 0.059\n",
            "446/7395000 (epoch 0), train_loss = 2.598, time/batch = 0.060\n",
            "447/7395000 (epoch 0), train_loss = 2.527, time/batch = 0.059\n",
            "448/7395000 (epoch 0), train_loss = 2.471, time/batch = 0.059\n",
            "449/7395000 (epoch 0), train_loss = 2.524, time/batch = 0.060\n",
            "450/7395000 (epoch 0), train_loss = 2.613, time/batch = 0.055\n",
            "451/7395000 (epoch 0), train_loss = 2.419, time/batch = 0.058\n",
            "452/7395000 (epoch 0), train_loss = 2.496, time/batch = 0.065\n",
            "453/7395000 (epoch 0), train_loss = 2.528, time/batch = 0.059\n",
            "454/7395000 (epoch 0), train_loss = 2.545, time/batch = 0.058\n",
            "455/7395000 (epoch 0), train_loss = 2.567, time/batch = 0.058\n",
            "456/7395000 (epoch 0), train_loss = 2.510, time/batch = 0.060\n",
            "457/7395000 (epoch 0), train_loss = 2.543, time/batch = 0.064\n",
            "458/7395000 (epoch 0), train_loss = 2.422, time/batch = 0.057\n",
            "459/7395000 (epoch 0), train_loss = 2.497, time/batch = 0.060\n",
            "460/7395000 (epoch 0), train_loss = 2.633, time/batch = 0.058\n",
            "461/7395000 (epoch 0), train_loss = 2.553, time/batch = 0.059\n",
            "462/7395000 (epoch 0), train_loss = 2.486, time/batch = 0.057\n",
            "463/7395000 (epoch 0), train_loss = 2.455, time/batch = 0.059\n",
            "464/7395000 (epoch 0), train_loss = 2.523, time/batch = 0.059\n",
            "465/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.058\n",
            "466/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.059\n",
            "467/7395000 (epoch 0), train_loss = 2.446, time/batch = 0.066\n",
            "468/7395000 (epoch 0), train_loss = 2.457, time/batch = 0.057\n",
            "469/7395000 (epoch 0), train_loss = 2.487, time/batch = 0.059\n",
            "470/7395000 (epoch 0), train_loss = 2.474, time/batch = 0.057\n",
            "471/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.058\n",
            "472/7395000 (epoch 0), train_loss = 2.528, time/batch = 0.060\n",
            "473/7395000 (epoch 0), train_loss = 2.472, time/batch = 0.058\n",
            "474/7395000 (epoch 0), train_loss = 2.443, time/batch = 0.060\n",
            "475/7395000 (epoch 0), train_loss = 2.591, time/batch = 0.058\n",
            "476/7395000 (epoch 0), train_loss = 2.512, time/batch = 0.058\n",
            "477/7395000 (epoch 0), train_loss = 2.549, time/batch = 0.058\n",
            "478/7395000 (epoch 0), train_loss = 2.521, time/batch = 0.062\n",
            "479/7395000 (epoch 0), train_loss = 2.515, time/batch = 0.058\n",
            "480/7395000 (epoch 0), train_loss = 2.515, time/batch = 0.059\n",
            "481/7395000 (epoch 0), train_loss = 2.526, time/batch = 0.063\n",
            "482/7395000 (epoch 0), train_loss = 2.557, time/batch = 0.058\n",
            "483/7395000 (epoch 0), train_loss = 2.532, time/batch = 0.053\n",
            "484/7395000 (epoch 0), train_loss = 2.553, time/batch = 0.061\n",
            "485/7395000 (epoch 0), train_loss = 2.418, time/batch = 0.053\n",
            "486/7395000 (epoch 0), train_loss = 2.500, time/batch = 0.068\n",
            "487/7395000 (epoch 0), train_loss = 2.438, time/batch = 0.052\n",
            "488/7395000 (epoch 0), train_loss = 2.518, time/batch = 0.052\n",
            "489/7395000 (epoch 0), train_loss = 2.451, time/batch = 0.058\n",
            "490/7395000 (epoch 0), train_loss = 2.514, time/batch = 0.059\n",
            "491/7395000 (epoch 0), train_loss = 2.478, time/batch = 0.059\n",
            "492/7395000 (epoch 0), train_loss = 2.526, time/batch = 0.058\n",
            "493/7395000 (epoch 0), train_loss = 2.400, time/batch = 0.059\n",
            "494/7395000 (epoch 0), train_loss = 2.500, time/batch = 0.059\n",
            "495/7395000 (epoch 0), train_loss = 2.518, time/batch = 0.062\n",
            "496/7395000 (epoch 0), train_loss = 2.483, time/batch = 0.057\n",
            "497/7395000 (epoch 0), train_loss = 2.502, time/batch = 0.057\n",
            "498/7395000 (epoch 0), train_loss = 2.577, time/batch = 0.058\n",
            "499/7395000 (epoch 0), train_loss = 2.488, time/batch = 0.058\n",
            "500/7395000 (epoch 0), train_loss = 2.513, time/batch = 0.062\n",
            "501/7395000 (epoch 0), train_loss = 2.529, time/batch = 0.061\n",
            "502/7395000 (epoch 0), train_loss = 2.438, time/batch = 0.057\n",
            "503/7395000 (epoch 0), train_loss = 2.452, time/batch = 0.062\n",
            "504/7395000 (epoch 0), train_loss = 2.464, time/batch = 0.058\n",
            "505/7395000 (epoch 0), train_loss = 2.433, time/batch = 0.059\n",
            "506/7395000 (epoch 0), train_loss = 2.419, time/batch = 0.059\n",
            "507/7395000 (epoch 0), train_loss = 2.426, time/batch = 0.060\n",
            "508/7395000 (epoch 0), train_loss = 2.511, time/batch = 0.060\n",
            "509/7395000 (epoch 0), train_loss = 2.582, time/batch = 0.061\n",
            "510/7395000 (epoch 0), train_loss = 2.494, time/batch = 0.055\n",
            "511/7395000 (epoch 0), train_loss = 2.456, time/batch = 0.058\n",
            "512/7395000 (epoch 0), train_loss = 2.594, time/batch = 0.057\n",
            "513/7395000 (epoch 0), train_loss = 2.455, time/batch = 0.058\n",
            "514/7395000 (epoch 0), train_loss = 2.554, time/batch = 0.059\n",
            "515/7395000 (epoch 0), train_loss = 2.433, time/batch = 0.059\n",
            "516/7395000 (epoch 0), train_loss = 2.513, time/batch = 0.060\n",
            "517/7395000 (epoch 0), train_loss = 2.487, time/batch = 0.059\n",
            "518/7395000 (epoch 0), train_loss = 2.509, time/batch = 0.059\n",
            "519/7395000 (epoch 0), train_loss = 2.509, time/batch = 0.061\n",
            "520/7395000 (epoch 0), train_loss = 2.498, time/batch = 0.058\n",
            "521/7395000 (epoch 0), train_loss = 2.484, time/batch = 0.057\n",
            "522/7395000 (epoch 0), train_loss = 2.464, time/batch = 0.058\n",
            "523/7395000 (epoch 0), train_loss = 2.478, time/batch = 0.059\n",
            "524/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.060\n",
            "525/7395000 (epoch 0), train_loss = 2.434, time/batch = 0.058\n",
            "526/7395000 (epoch 0), train_loss = 2.403, time/batch = 0.056\n",
            "527/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.059\n",
            "528/7395000 (epoch 0), train_loss = 2.510, time/batch = 0.063\n",
            "529/7395000 (epoch 0), train_loss = 2.514, time/batch = 0.059\n",
            "530/7395000 (epoch 0), train_loss = 2.402, time/batch = 0.057\n",
            "531/7395000 (epoch 0), train_loss = 2.490, time/batch = 0.058\n",
            "532/7395000 (epoch 0), train_loss = 2.325, time/batch = 0.057\n",
            "533/7395000 (epoch 0), train_loss = 2.486, time/batch = 0.058\n",
            "534/7395000 (epoch 0), train_loss = 2.472, time/batch = 0.061\n",
            "535/7395000 (epoch 0), train_loss = 2.436, time/batch = 0.061\n",
            "536/7395000 (epoch 0), train_loss = 2.519, time/batch = 0.059\n",
            "537/7395000 (epoch 0), train_loss = 2.415, time/batch = 0.062\n",
            "538/7395000 (epoch 0), train_loss = 2.437, time/batch = 0.058\n",
            "539/7395000 (epoch 0), train_loss = 2.370, time/batch = 0.062\n",
            "540/7395000 (epoch 0), train_loss = 2.521, time/batch = 0.060\n",
            "541/7395000 (epoch 0), train_loss = 2.392, time/batch = 0.057\n",
            "542/7395000 (epoch 0), train_loss = 2.449, time/batch = 0.060\n",
            "543/7395000 (epoch 0), train_loss = 2.447, time/batch = 0.058\n",
            "544/7395000 (epoch 0), train_loss = 2.323, time/batch = 0.061\n",
            "545/7395000 (epoch 0), train_loss = 2.424, time/batch = 0.060\n",
            "546/7395000 (epoch 0), train_loss = 2.330, time/batch = 0.057\n",
            "547/7395000 (epoch 0), train_loss = 2.419, time/batch = 0.059\n",
            "548/7395000 (epoch 0), train_loss = 2.442, time/batch = 0.058\n",
            "549/7395000 (epoch 0), train_loss = 2.480, time/batch = 0.058\n",
            "550/7395000 (epoch 0), train_loss = 2.351, time/batch = 0.062\n",
            "551/7395000 (epoch 0), train_loss = 2.415, time/batch = 0.058\n",
            "552/7395000 (epoch 0), train_loss = 2.366, time/batch = 0.059\n",
            "553/7395000 (epoch 0), train_loss = 2.438, time/batch = 0.062\n",
            "554/7395000 (epoch 0), train_loss = 2.335, time/batch = 0.055\n",
            "555/7395000 (epoch 0), train_loss = 2.312, time/batch = 0.056\n",
            "556/7395000 (epoch 0), train_loss = 2.328, time/batch = 0.058\n",
            "557/7395000 (epoch 0), train_loss = 2.335, time/batch = 0.060\n",
            "558/7395000 (epoch 0), train_loss = 2.462, time/batch = 0.062\n",
            "559/7395000 (epoch 0), train_loss = 2.411, time/batch = 0.060\n",
            "560/7395000 (epoch 0), train_loss = 2.288, time/batch = 0.058\n",
            "561/7395000 (epoch 0), train_loss = 2.347, time/batch = 0.058\n",
            "562/7395000 (epoch 0), train_loss = 2.314, time/batch = 0.057\n",
            "563/7395000 (epoch 0), train_loss = 2.444, time/batch = 0.059\n",
            "564/7395000 (epoch 0), train_loss = 2.437, time/batch = 0.060\n",
            "565/7395000 (epoch 0), train_loss = 2.419, time/batch = 0.068\n",
            "566/7395000 (epoch 0), train_loss = 2.353, time/batch = 0.060\n",
            "567/7395000 (epoch 0), train_loss = 2.370, time/batch = 0.059\n",
            "568/7395000 (epoch 0), train_loss = 2.342, time/batch = 0.058\n",
            "569/7395000 (epoch 0), train_loss = 2.417, time/batch = 0.059\n",
            "570/7395000 (epoch 0), train_loss = 2.317, time/batch = 0.058\n",
            "571/7395000 (epoch 0), train_loss = 2.347, time/batch = 0.061\n",
            "572/7395000 (epoch 0), train_loss = 2.324, time/batch = 0.058\n",
            "573/7395000 (epoch 0), train_loss = 2.467, time/batch = 0.062\n",
            "574/7395000 (epoch 0), train_loss = 2.408, time/batch = 0.057\n",
            "575/7395000 (epoch 0), train_loss = 2.413, time/batch = 0.061\n",
            "576/7395000 (epoch 0), train_loss = 2.291, time/batch = 0.060\n",
            "577/7395000 (epoch 0), train_loss = 2.412, time/batch = 0.055\n",
            "578/7395000 (epoch 0), train_loss = 2.409, time/batch = 0.059\n",
            "579/7395000 (epoch 0), train_loss = 2.368, time/batch = 0.060\n",
            "580/7395000 (epoch 0), train_loss = 2.427, time/batch = 0.058\n",
            "581/7395000 (epoch 0), train_loss = 2.380, time/batch = 0.059\n",
            "582/7395000 (epoch 0), train_loss = 2.371, time/batch = 0.059\n",
            "583/7395000 (epoch 0), train_loss = 2.358, time/batch = 0.060\n",
            "584/7395000 (epoch 0), train_loss = 2.405, time/batch = 0.054\n",
            "585/7395000 (epoch 0), train_loss = 2.301, time/batch = 0.060\n",
            "586/7395000 (epoch 0), train_loss = 2.428, time/batch = 0.057\n",
            "587/7395000 (epoch 0), train_loss = 2.453, time/batch = 0.060\n",
            "588/7395000 (epoch 0), train_loss = 2.380, time/batch = 0.065\n",
            "589/7395000 (epoch 0), train_loss = 2.318, time/batch = 0.058\n",
            "590/7395000 (epoch 0), train_loss = 2.341, time/batch = 0.060\n",
            "591/7395000 (epoch 0), train_loss = 2.362, time/batch = 0.058\n",
            "592/7395000 (epoch 0), train_loss = 2.287, time/batch = 0.067\n",
            "593/7395000 (epoch 0), train_loss = 2.371, time/batch = 0.058\n",
            "594/7395000 (epoch 0), train_loss = 2.223, time/batch = 0.060\n",
            "595/7395000 (epoch 0), train_loss = 2.401, time/batch = 0.060\n",
            "596/7395000 (epoch 0), train_loss = 2.353, time/batch = 0.059\n",
            "597/7395000 (epoch 0), train_loss = 2.379, time/batch = 0.063\n",
            "598/7395000 (epoch 0), train_loss = 2.437, time/batch = 0.057\n",
            "599/7395000 (epoch 0), train_loss = 2.395, time/batch = 0.057\n",
            "600/7395000 (epoch 0), train_loss = 2.397, time/batch = 0.061\n",
            "601/7395000 (epoch 0), train_loss = 2.330, time/batch = 0.059\n",
            "602/7395000 (epoch 0), train_loss = 2.402, time/batch = 0.059\n",
            "603/7395000 (epoch 0), train_loss = 2.328, time/batch = 0.062\n",
            "604/7395000 (epoch 0), train_loss = 2.450, time/batch = 0.059\n",
            "605/7395000 (epoch 0), train_loss = 2.469, time/batch = 0.066\n",
            "606/7395000 (epoch 0), train_loss = 2.499, time/batch = 0.059\n",
            "607/7395000 (epoch 0), train_loss = 2.371, time/batch = 0.060\n",
            "608/7395000 (epoch 0), train_loss = 2.343, time/batch = 0.058\n",
            "609/7395000 (epoch 0), train_loss = 2.395, time/batch = 0.062\n",
            "610/7395000 (epoch 0), train_loss = 2.415, time/batch = 0.055\n",
            "611/7395000 (epoch 0), train_loss = 2.391, time/batch = 0.053\n",
            "612/7395000 (epoch 0), train_loss = 2.307, time/batch = 0.054\n",
            "613/7395000 (epoch 0), train_loss = 2.415, time/batch = 0.053\n",
            "614/7395000 (epoch 0), train_loss = 2.294, time/batch = 0.054\n",
            "615/7395000 (epoch 0), train_loss = 2.453, time/batch = 0.054\n",
            "616/7395000 (epoch 0), train_loss = 2.340, time/batch = 0.061\n",
            "617/7395000 (epoch 0), train_loss = 2.364, time/batch = 0.052\n",
            "618/7395000 (epoch 0), train_loss = 2.284, time/batch = 0.053\n",
            "619/7395000 (epoch 0), train_loss = 2.270, time/batch = 0.053\n",
            "620/7395000 (epoch 0), train_loss = 2.266, time/batch = 0.056\n",
            "621/7395000 (epoch 0), train_loss = 2.228, time/batch = 0.052\n",
            "622/7395000 (epoch 0), train_loss = 2.254, time/batch = 0.054\n",
            "623/7395000 (epoch 0), train_loss = 2.281, time/batch = 0.058\n",
            "624/7395000 (epoch 0), train_loss = 2.335, time/batch = 0.056\n",
            "625/7395000 (epoch 0), train_loss = 2.271, time/batch = 0.066\n",
            "626/7395000 (epoch 0), train_loss = 2.239, time/batch = 0.057\n",
            "627/7395000 (epoch 0), train_loss = 2.283, time/batch = 0.059\n",
            "628/7395000 (epoch 0), train_loss = 2.257, time/batch = 0.063\n",
            "629/7395000 (epoch 0), train_loss = 2.277, time/batch = 0.064\n",
            "630/7395000 (epoch 0), train_loss = 2.361, time/batch = 0.059\n",
            "631/7395000 (epoch 0), train_loss = 2.364, time/batch = 0.061\n",
            "632/7395000 (epoch 0), train_loss = 2.356, time/batch = 0.058\n",
            "633/7395000 (epoch 0), train_loss = 2.248, time/batch = 0.061\n",
            "634/7395000 (epoch 0), train_loss = 2.343, time/batch = 0.059\n",
            "635/7395000 (epoch 0), train_loss = 2.232, time/batch = 0.060\n",
            "636/7395000 (epoch 0), train_loss = 2.363, time/batch = 0.058\n",
            "637/7395000 (epoch 0), train_loss = 2.209, time/batch = 0.059\n",
            "638/7395000 (epoch 0), train_loss = 2.313, time/batch = 0.057\n",
            "639/7395000 (epoch 0), train_loss = 2.309, time/batch = 0.061\n",
            "640/7395000 (epoch 0), train_loss = 2.283, time/batch = 0.060\n",
            "641/7395000 (epoch 0), train_loss = 2.357, time/batch = 0.059\n",
            "642/7395000 (epoch 0), train_loss = 2.226, time/batch = 0.059\n",
            "643/7395000 (epoch 0), train_loss = 2.289, time/batch = 0.061\n",
            "644/7395000 (epoch 0), train_loss = 2.371, time/batch = 0.055\n",
            "645/7395000 (epoch 0), train_loss = 2.327, time/batch = 0.062\n",
            "646/7395000 (epoch 0), train_loss = 2.293, time/batch = 0.060\n",
            "647/7395000 (epoch 0), train_loss = 2.171, time/batch = 0.065\n",
            "648/7395000 (epoch 0), train_loss = 2.216, time/batch = 0.059\n",
            "649/7395000 (epoch 0), train_loss = 2.304, time/batch = 0.058\n",
            "650/7395000 (epoch 0), train_loss = 2.228, time/batch = 0.063\n",
            "651/7395000 (epoch 0), train_loss = 2.252, time/batch = 0.059\n",
            "652/7395000 (epoch 0), train_loss = 2.151, time/batch = 0.058\n",
            "653/7395000 (epoch 0), train_loss = 2.266, time/batch = 0.061\n",
            "654/7395000 (epoch 0), train_loss = 2.302, time/batch = 0.058\n",
            "655/7395000 (epoch 0), train_loss = 2.389, time/batch = 0.059\n",
            "656/7395000 (epoch 0), train_loss = 2.362, time/batch = 0.059\n",
            "657/7395000 (epoch 0), train_loss = 2.374, time/batch = 0.065\n",
            "658/7395000 (epoch 0), train_loss = 2.325, time/batch = 0.056\n",
            "659/7395000 (epoch 0), train_loss = 2.285, time/batch = 0.057\n",
            "660/7395000 (epoch 0), train_loss = 2.261, time/batch = 0.058\n",
            "661/7395000 (epoch 0), train_loss = 2.238, time/batch = 0.059\n",
            "662/7395000 (epoch 0), train_loss = 2.368, time/batch = 0.061\n",
            "663/7395000 (epoch 0), train_loss = 2.257, time/batch = 0.060\n",
            "664/7395000 (epoch 0), train_loss = 2.280, time/batch = 0.059\n",
            "665/7395000 (epoch 0), train_loss = 2.282, time/batch = 0.056\n",
            "666/7395000 (epoch 0), train_loss = 2.201, time/batch = 0.058\n",
            "667/7395000 (epoch 0), train_loss = 2.312, time/batch = 0.060\n",
            "668/7395000 (epoch 0), train_loss = 2.266, time/batch = 0.057\n",
            "669/7395000 (epoch 0), train_loss = 2.190, time/batch = 0.061\n",
            "670/7395000 (epoch 0), train_loss = 2.303, time/batch = 0.057\n",
            "671/7395000 (epoch 0), train_loss = 2.229, time/batch = 0.058\n",
            "672/7395000 (epoch 0), train_loss = 2.291, time/batch = 0.058\n",
            "673/7395000 (epoch 0), train_loss = 2.183, time/batch = 0.058\n",
            "674/7395000 (epoch 0), train_loss = 2.236, time/batch = 0.059\n",
            "675/7395000 (epoch 0), train_loss = 2.167, time/batch = 0.063\n",
            "676/7395000 (epoch 0), train_loss = 2.222, time/batch = 0.059\n",
            "677/7395000 (epoch 0), train_loss = 2.238, time/batch = 0.064\n",
            "678/7395000 (epoch 0), train_loss = 2.260, time/batch = 0.053\n",
            "679/7395000 (epoch 0), train_loss = 2.282, time/batch = 0.058\n",
            "680/7395000 (epoch 0), train_loss = 2.312, time/batch = 0.060\n",
            "681/7395000 (epoch 0), train_loss = 2.277, time/batch = 0.058\n",
            "682/7395000 (epoch 0), train_loss = 2.229, time/batch = 0.057\n",
            "683/7395000 (epoch 0), train_loss = 2.243, time/batch = 0.066\n",
            "684/7395000 (epoch 0), train_loss = 2.225, time/batch = 0.058\n",
            "685/7395000 (epoch 0), train_loss = 2.113, time/batch = 0.059\n",
            "686/7395000 (epoch 0), train_loss = 2.260, time/batch = 0.059\n",
            "687/7395000 (epoch 0), train_loss = 2.237, time/batch = 0.059\n",
            "688/7395000 (epoch 0), train_loss = 2.253, time/batch = 0.062\n",
            "689/7395000 (epoch 0), train_loss = 2.212, time/batch = 0.059\n",
            "690/7395000 (epoch 0), train_loss = 2.228, time/batch = 0.057\n",
            "691/7395000 (epoch 0), train_loss = 2.210, time/batch = 0.056\n",
            "692/7395000 (epoch 0), train_loss = 2.209, time/batch = 0.067\n",
            "693/7395000 (epoch 0), train_loss = 2.181, time/batch = 0.061\n",
            "694/7395000 (epoch 0), train_loss = 2.302, time/batch = 0.059\n",
            "695/7395000 (epoch 0), train_loss = 2.170, time/batch = 0.057\n",
            "696/7395000 (epoch 0), train_loss = 2.233, time/batch = 0.063\n",
            "697/7395000 (epoch 0), train_loss = 2.203, time/batch = 0.059\n",
            "698/7395000 (epoch 0), train_loss = 2.256, time/batch = 0.059\n",
            "699/7395000 (epoch 0), train_loss = 2.201, time/batch = 0.060\n",
            "700/7395000 (epoch 0), train_loss = 2.201, time/batch = 0.059\n",
            "701/7395000 (epoch 0), train_loss = 2.208, time/batch = 0.059\n",
            "702/7395000 (epoch 0), train_loss = 2.129, time/batch = 0.068\n",
            "703/7395000 (epoch 0), train_loss = 2.168, time/batch = 0.064\n",
            "704/7395000 (epoch 0), train_loss = 2.196, time/batch = 0.057\n",
            "705/7395000 (epoch 0), train_loss = 2.187, time/batch = 0.058\n",
            "706/7395000 (epoch 0), train_loss = 2.154, time/batch = 0.058\n",
            "707/7395000 (epoch 0), train_loss = 2.138, time/batch = 0.057\n",
            "708/7395000 (epoch 0), train_loss = 2.070, time/batch = 0.062\n",
            "709/7395000 (epoch 0), train_loss = 2.046, time/batch = 0.069\n",
            "710/7395000 (epoch 0), train_loss = 2.098, time/batch = 0.056\n",
            "711/7395000 (epoch 0), train_loss = 2.112, time/batch = 0.061\n",
            "712/7395000 (epoch 0), train_loss = 2.083, time/batch = 0.058\n",
            "713/7395000 (epoch 0), train_loss = 2.067, time/batch = 0.064\n",
            "714/7395000 (epoch 0), train_loss = 2.081, time/batch = 0.060\n",
            "715/7395000 (epoch 0), train_loss = 2.107, time/batch = 0.058\n",
            "716/7395000 (epoch 0), train_loss = 2.075, time/batch = 0.063\n",
            "717/7395000 (epoch 0), train_loss = 2.205, time/batch = 0.061\n",
            "718/7395000 (epoch 0), train_loss = 2.161, time/batch = 0.056\n",
            "719/7395000 (epoch 0), train_loss = 2.091, time/batch = 0.061\n",
            "720/7395000 (epoch 0), train_loss = 2.199, time/batch = 0.057\n",
            "721/7395000 (epoch 0), train_loss = 2.096, time/batch = 0.062\n",
            "722/7395000 (epoch 0), train_loss = 2.184, time/batch = 0.058\n",
            "723/7395000 (epoch 0), train_loss = 2.179, time/batch = 0.064\n",
            "724/7395000 (epoch 0), train_loss = 2.189, time/batch = 0.060\n",
            "725/7395000 (epoch 0), train_loss = 2.202, time/batch = 0.060\n",
            "726/7395000 (epoch 0), train_loss = 2.285, time/batch = 0.066\n",
            "727/7395000 (epoch 0), train_loss = 2.165, time/batch = 0.059\n",
            "728/7395000 (epoch 0), train_loss = 2.207, time/batch = 0.061\n",
            "729/7395000 (epoch 0), train_loss = 2.186, time/batch = 0.058\n",
            "730/7395000 (epoch 0), train_loss = 2.167, time/batch = 0.057\n",
            "731/7395000 (epoch 0), train_loss = 2.085, time/batch = 0.062\n",
            "732/7395000 (epoch 0), train_loss = 2.132, time/batch = 0.057\n",
            "733/7395000 (epoch 0), train_loss = 2.150, time/batch = 0.061\n",
            "734/7395000 (epoch 0), train_loss = 2.225, time/batch = 0.058\n",
            "735/7395000 (epoch 0), train_loss = 2.201, time/batch = 0.061\n",
            "736/7395000 (epoch 0), train_loss = 2.159, time/batch = 0.061\n",
            "737/7395000 (epoch 0), train_loss = 2.136, time/batch = 0.057\n",
            "738/7395000 (epoch 0), train_loss = 2.100, time/batch = 0.060\n",
            "739/7395000 (epoch 0), train_loss = 2.111, time/batch = 0.059\n",
            "740/7395000 (epoch 0), train_loss = 2.131, time/batch = 0.058\n",
            "741/7395000 (epoch 0), train_loss = 2.052, time/batch = 0.061\n",
            "742/7395000 (epoch 0), train_loss = 2.014, time/batch = 0.071\n",
            "743/7395000 (epoch 0), train_loss = 2.110, time/batch = 0.066\n",
            "744/7395000 (epoch 0), train_loss = 2.051, time/batch = 0.058\n",
            "745/7395000 (epoch 0), train_loss = 2.165, time/batch = 0.057\n",
            "746/7395000 (epoch 0), train_loss = 2.110, time/batch = 0.061\n",
            "747/7395000 (epoch 0), train_loss = 2.076, time/batch = 0.062\n",
            "748/7395000 (epoch 0), train_loss = 2.160, time/batch = 0.062\n",
            "749/7395000 (epoch 0), train_loss = 2.164, time/batch = 0.059\n",
            "750/7395000 (epoch 0), train_loss = 2.140, time/batch = 0.056\n",
            "751/7395000 (epoch 0), train_loss = 2.185, time/batch = 0.060\n",
            "752/7395000 (epoch 0), train_loss = 2.097, time/batch = 0.059\n",
            "753/7395000 (epoch 0), train_loss = 2.139, time/batch = 0.067\n",
            "754/7395000 (epoch 0), train_loss = 2.163, time/batch = 0.060\n",
            "755/7395000 (epoch 0), train_loss = 2.140, time/batch = 0.056\n",
            "756/7395000 (epoch 0), train_loss = 2.141, time/batch = 0.064\n",
            "757/7395000 (epoch 0), train_loss = 2.097, time/batch = 0.061\n",
            "758/7395000 (epoch 0), train_loss = 2.106, time/batch = 0.058\n",
            "759/7395000 (epoch 0), train_loss = 2.178, time/batch = 0.065\n",
            "760/7395000 (epoch 0), train_loss = 2.073, time/batch = 0.062\n",
            "761/7395000 (epoch 0), train_loss = 2.065, time/batch = 0.060\n",
            "762/7395000 (epoch 0), train_loss = 2.187, time/batch = 0.060\n",
            "763/7395000 (epoch 0), train_loss = 2.148, time/batch = 0.062\n",
            "764/7395000 (epoch 0), train_loss = 2.087, time/batch = 0.058\n",
            "765/7395000 (epoch 0), train_loss = 2.064, time/batch = 0.061\n",
            "766/7395000 (epoch 0), train_loss = 2.145, time/batch = 0.060\n",
            "767/7395000 (epoch 0), train_loss = 2.109, time/batch = 0.061\n",
            "768/7395000 (epoch 0), train_loss = 2.133, time/batch = 0.062\n",
            "769/7395000 (epoch 0), train_loss = 2.186, time/batch = 0.060\n",
            "770/7395000 (epoch 0), train_loss = 2.078, time/batch = 0.057\n",
            "771/7395000 (epoch 0), train_loss = 2.187, time/batch = 0.067\n",
            "772/7395000 (epoch 0), train_loss = 2.175, time/batch = 0.057\n",
            "773/7395000 (epoch 0), train_loss = 2.131, time/batch = 0.058\n",
            "774/7395000 (epoch 0), train_loss = 2.113, time/batch = 0.059\n",
            "775/7395000 (epoch 0), train_loss = 2.158, time/batch = 0.059\n",
            "776/7395000 (epoch 0), train_loss = 2.076, time/batch = 0.063\n",
            "777/7395000 (epoch 0), train_loss = 2.170, time/batch = 0.069\n",
            "778/7395000 (epoch 0), train_loss = 2.154, time/batch = 0.060\n",
            "779/7395000 (epoch 0), train_loss = 2.089, time/batch = 0.061\n",
            "780/7395000 (epoch 0), train_loss = 2.046, time/batch = 0.059\n",
            "781/7395000 (epoch 0), train_loss = 2.103, time/batch = 0.062\n",
            "782/7395000 (epoch 0), train_loss = 2.157, time/batch = 0.059\n",
            "783/7395000 (epoch 0), train_loss = 2.110, time/batch = 0.059\n",
            "784/7395000 (epoch 0), train_loss = 2.229, time/batch = 0.058\n",
            "785/7395000 (epoch 0), train_loss = 2.118, time/batch = 0.053\n",
            "786/7395000 (epoch 0), train_loss = 2.208, time/batch = 0.060\n",
            "787/7395000 (epoch 0), train_loss = 2.126, time/batch = 0.060\n",
            "788/7395000 (epoch 0), train_loss = 2.117, time/batch = 0.062\n",
            "789/7395000 (epoch 0), train_loss = 2.028, time/batch = 0.058\n",
            "790/7395000 (epoch 0), train_loss = 2.114, time/batch = 0.057\n",
            "791/7395000 (epoch 0), train_loss = 2.204, time/batch = 0.060\n",
            "792/7395000 (epoch 0), train_loss = 2.095, time/batch = 0.057\n",
            "793/7395000 (epoch 0), train_loss = 2.078, time/batch = 0.061\n",
            "794/7395000 (epoch 0), train_loss = 2.134, time/batch = 0.063\n",
            "795/7395000 (epoch 0), train_loss = 2.060, time/batch = 0.058\n",
            "796/7395000 (epoch 0), train_loss = 2.165, time/batch = 0.058\n",
            "797/7395000 (epoch 0), train_loss = 2.114, time/batch = 0.062\n",
            "798/7395000 (epoch 0), train_loss = 2.172, time/batch = 0.058\n",
            "799/7395000 (epoch 0), train_loss = 2.090, time/batch = 0.060\n",
            "800/7395000 (epoch 0), train_loss = 2.148, time/batch = 0.060\n",
            "801/7395000 (epoch 0), train_loss = 2.114, time/batch = 0.059\n",
            "802/7395000 (epoch 0), train_loss = 2.040, time/batch = 0.059\n",
            "803/7395000 (epoch 0), train_loss = 2.109, time/batch = 0.063\n",
            "804/7395000 (epoch 0), train_loss = 2.166, time/batch = 0.059\n",
            "805/7395000 (epoch 0), train_loss = 2.193, time/batch = 0.061\n",
            "806/7395000 (epoch 0), train_loss = 2.035, time/batch = 0.058\n",
            "807/7395000 (epoch 0), train_loss = 2.133, time/batch = 0.058\n",
            "808/7395000 (epoch 0), train_loss = 2.060, time/batch = 0.058\n",
            "809/7395000 (epoch 0), train_loss = 2.075, time/batch = 0.061\n",
            "810/7395000 (epoch 0), train_loss = 2.123, time/batch = 0.061\n",
            "811/7395000 (epoch 0), train_loss = 2.187, time/batch = 0.068\n",
            "812/7395000 (epoch 0), train_loss = 2.176, time/batch = 0.059\n",
            "813/7395000 (epoch 0), train_loss = 2.119, time/batch = 0.062\n",
            "814/7395000 (epoch 0), train_loss = 2.092, time/batch = 0.062\n",
            "815/7395000 (epoch 0), train_loss = 2.195, time/batch = 0.057\n",
            "816/7395000 (epoch 0), train_loss = 2.114, time/batch = 0.059\n",
            "817/7395000 (epoch 0), train_loss = 2.160, time/batch = 0.058\n",
            "818/7395000 (epoch 0), train_loss = 2.144, time/batch = 0.059\n",
            "819/7395000 (epoch 0), train_loss = 2.212, time/batch = 0.061\n",
            "820/7395000 (epoch 0), train_loss = 2.002, time/batch = 0.060\n",
            "821/7395000 (epoch 0), train_loss = 2.100, time/batch = 0.059\n",
            "822/7395000 (epoch 0), train_loss = 2.186, time/batch = 0.059\n",
            "823/7395000 (epoch 0), train_loss = 2.025, time/batch = 0.060\n",
            "824/7395000 (epoch 0), train_loss = 2.094, time/batch = 0.060\n",
            "825/7395000 (epoch 0), train_loss = 2.121, time/batch = 0.061\n",
            "826/7395000 (epoch 0), train_loss = 2.054, time/batch = 0.067\n",
            "827/7395000 (epoch 0), train_loss = 2.163, time/batch = 0.057\n",
            "828/7395000 (epoch 0), train_loss = 2.148, time/batch = 0.071\n",
            "829/7395000 (epoch 0), train_loss = 2.194, time/batch = 0.060\n",
            "830/7395000 (epoch 0), train_loss = 2.062, time/batch = 0.059\n",
            "831/7395000 (epoch 0), train_loss = 2.159, time/batch = 0.058\n",
            "832/7395000 (epoch 0), train_loss = 2.122, time/batch = 0.054\n",
            "833/7395000 (epoch 0), train_loss = 2.195, time/batch = 0.059\n",
            "834/7395000 (epoch 0), train_loss = 2.141, time/batch = 0.060\n",
            "835/7395000 (epoch 0), train_loss = 2.213, time/batch = 0.058\n",
            "836/7395000 (epoch 0), train_loss = 2.173, time/batch = 0.058\n",
            "837/7395000 (epoch 0), train_loss = 2.152, time/batch = 0.062\n",
            "838/7395000 (epoch 0), train_loss = 2.122, time/batch = 0.057\n",
            "839/7395000 (epoch 0), train_loss = 2.130, time/batch = 0.060\n",
            "840/7395000 (epoch 0), train_loss = 2.125, time/batch = 0.058\n",
            "841/7395000 (epoch 0), train_loss = 2.132, time/batch = 0.060\n",
            "842/7395000 (epoch 0), train_loss = 2.154, time/batch = 0.062\n",
            "843/7395000 (epoch 0), train_loss = 2.204, time/batch = 0.059\n",
            "844/7395000 (epoch 0), train_loss = 2.083, time/batch = 0.057\n",
            "845/7395000 (epoch 0), train_loss = 2.251, time/batch = 0.060\n",
            "846/7395000 (epoch 0), train_loss = 2.183, time/batch = 0.059\n",
            "847/7395000 (epoch 0), train_loss = 2.113, time/batch = 0.059\n",
            "848/7395000 (epoch 0), train_loss = 2.179, time/batch = 0.060\n",
            "849/7395000 (epoch 0), train_loss = 2.062, time/batch = 0.058\n",
            "850/7395000 (epoch 0), train_loss = 2.149, time/batch = 0.058\n",
            "851/7395000 (epoch 0), train_loss = 2.199, time/batch = 0.065\n",
            "852/7395000 (epoch 0), train_loss = 2.138, time/batch = 0.059\n",
            "853/7395000 (epoch 0), train_loss = 2.101, time/batch = 0.060\n",
            "854/7395000 (epoch 0), train_loss = 2.053, time/batch = 0.059\n",
            "855/7395000 (epoch 0), train_loss = 2.034, time/batch = 0.060\n",
            "856/7395000 (epoch 0), train_loss = 2.043, time/batch = 0.058\n",
            "857/7395000 (epoch 0), train_loss = 2.072, time/batch = 0.062\n",
            "858/7395000 (epoch 0), train_loss = 2.101, time/batch = 0.059\n",
            "859/7395000 (epoch 0), train_loss = 2.121, time/batch = 0.060\n",
            "860/7395000 (epoch 0), train_loss = 2.078, time/batch = 0.059\n",
            "861/7395000 (epoch 0), train_loss = 2.085, time/batch = 0.059\n",
            "862/7395000 (epoch 0), train_loss = 2.111, time/batch = 0.066\n",
            "863/7395000 (epoch 0), train_loss = 2.151, time/batch = 0.061\n",
            "864/7395000 (epoch 0), train_loss = 2.085, time/batch = 0.058\n",
            "865/7395000 (epoch 0), train_loss = 2.101, time/batch = 0.059\n",
            "866/7395000 (epoch 0), train_loss = 2.105, time/batch = 0.058\n",
            "867/7395000 (epoch 0), train_loss = 2.111, time/batch = 0.060\n",
            "868/7395000 (epoch 0), train_loss = 2.001, time/batch = 0.060\n",
            "869/7395000 (epoch 0), train_loss = 2.092, time/batch = 0.059\n",
            "870/7395000 (epoch 0), train_loss = 2.121, time/batch = 0.058\n",
            "871/7395000 (epoch 0), train_loss = 2.134, time/batch = 0.060\n",
            "872/7395000 (epoch 0), train_loss = 2.032, time/batch = 0.062\n",
            "873/7395000 (epoch 0), train_loss = 2.126, time/batch = 0.058\n",
            "874/7395000 (epoch 0), train_loss = 2.061, time/batch = 0.062\n",
            "875/7395000 (epoch 0), train_loss = 2.044, time/batch = 0.056\n",
            "876/7395000 (epoch 0), train_loss = 2.073, time/batch = 0.060\n",
            "877/7395000 (epoch 0), train_loss = 2.070, time/batch = 0.062\n",
            "878/7395000 (epoch 0), train_loss = 2.070, time/batch = 0.058\n",
            "879/7395000 (epoch 0), train_loss = 2.094, time/batch = 0.062\n",
            "880/7395000 (epoch 0), train_loss = 2.090, time/batch = 0.060\n",
            "881/7395000 (epoch 0), train_loss = 2.104, time/batch = 0.060\n",
            "882/7395000 (epoch 0), train_loss = 2.131, time/batch = 0.061\n",
            "883/7395000 (epoch 0), train_loss = 2.046, time/batch = 0.061\n",
            "884/7395000 (epoch 0), train_loss = 2.031, time/batch = 0.060\n",
            "885/7395000 (epoch 0), train_loss = 2.110, time/batch = 0.061\n",
            "886/7395000 (epoch 0), train_loss = 2.093, time/batch = 0.072\n",
            "887/7395000 (epoch 0), train_loss = 1.960, time/batch = 0.057\n",
            "888/7395000 (epoch 0), train_loss = 2.046, time/batch = 0.060\n",
            "889/7395000 (epoch 0), train_loss = 2.069, time/batch = 0.059\n",
            "890/7395000 (epoch 0), train_loss = 2.092, time/batch = 0.059\n",
            "891/7395000 (epoch 0), train_loss = 1.981, time/batch = 0.056\n",
            "892/7395000 (epoch 0), train_loss = 1.928, time/batch = 0.058\n",
            "893/7395000 (epoch 0), train_loss = 1.974, time/batch = 0.059\n",
            "894/7395000 (epoch 0), train_loss = 1.971, time/batch = 0.063\n",
            "895/7395000 (epoch 0), train_loss = 1.938, time/batch = 0.059\n",
            "896/7395000 (epoch 0), train_loss = 2.024, time/batch = 0.070\n",
            "897/7395000 (epoch 0), train_loss = 1.997, time/batch = 0.060\n",
            "898/7395000 (epoch 0), train_loss = 1.910, time/batch = 0.058\n",
            "899/7395000 (epoch 0), train_loss = 2.067, time/batch = 0.060\n",
            "900/7395000 (epoch 0), train_loss = 2.066, time/batch = 0.058\n",
            "901/7395000 (epoch 0), train_loss = 2.082, time/batch = 0.062\n",
            "902/7395000 (epoch 0), train_loss = 2.095, time/batch = 0.061\n",
            "903/7395000 (epoch 0), train_loss = 2.064, time/batch = 0.059\n",
            "904/7395000 (epoch 0), train_loss = 2.091, time/batch = 0.059\n",
            "905/7395000 (epoch 0), train_loss = 2.062, time/batch = 0.060\n",
            "906/7395000 (epoch 0), train_loss = 2.107, time/batch = 0.058\n",
            "907/7395000 (epoch 0), train_loss = 2.133, time/batch = 0.065\n",
            "908/7395000 (epoch 0), train_loss = 2.120, time/batch = 0.060\n",
            "909/7395000 (epoch 0), train_loss = 2.057, time/batch = 0.059\n",
            "910/7395000 (epoch 0), train_loss = 2.028, time/batch = 0.058\n",
            "911/7395000 (epoch 0), train_loss = 2.036, time/batch = 0.060\n",
            "912/7395000 (epoch 0), train_loss = 1.947, time/batch = 0.060\n",
            "913/7395000 (epoch 0), train_loss = 1.961, time/batch = 0.069\n",
            "914/7395000 (epoch 0), train_loss = 2.061, time/batch = 0.064\n",
            "915/7395000 (epoch 0), train_loss = 2.132, time/batch = 0.058\n",
            "916/7395000 (epoch 0), train_loss = 2.073, time/batch = 0.059\n",
            "917/7395000 (epoch 0), train_loss = 2.027, time/batch = 0.063\n",
            "918/7395000 (epoch 0), train_loss = 2.017, time/batch = 0.059\n",
            "919/7395000 (epoch 0), train_loss = 2.024, time/batch = 0.058\n",
            "920/7395000 (epoch 0), train_loss = 2.010, time/batch = 0.058\n",
            "921/7395000 (epoch 0), train_loss = 2.013, time/batch = 0.053\n",
            "922/7395000 (epoch 0), train_loss = 2.046, time/batch = 0.060\n",
            "923/7395000 (epoch 0), train_loss = 1.949, time/batch = 0.060\n",
            "924/7395000 (epoch 0), train_loss = 1.993, time/batch = 0.059\n",
            "925/7395000 (epoch 0), train_loss = 2.009, time/batch = 0.062\n",
            "926/7395000 (epoch 0), train_loss = 2.009, time/batch = 0.060\n",
            "927/7395000 (epoch 0), train_loss = 2.003, time/batch = 0.060\n",
            "928/7395000 (epoch 0), train_loss = 2.037, time/batch = 0.059\n",
            "929/7395000 (epoch 0), train_loss = 2.026, time/batch = 0.060\n",
            "930/7395000 (epoch 0), train_loss = 2.084, time/batch = 0.064\n",
            "931/7395000 (epoch 0), train_loss = 1.948, time/batch = 0.059\n",
            "932/7395000 (epoch 0), train_loss = 1.988, time/batch = 0.057\n",
            "933/7395000 (epoch 0), train_loss = 2.122, time/batch = 0.061\n",
            "934/7395000 (epoch 0), train_loss = 2.042, time/batch = 0.064\n",
            "935/7395000 (epoch 0), train_loss = 1.961, time/batch = 0.059\n",
            "936/7395000 (epoch 0), train_loss = 2.034, time/batch = 0.060\n",
            "937/7395000 (epoch 0), train_loss = 1.914, time/batch = 0.061\n",
            "938/7395000 (epoch 0), train_loss = 1.960, time/batch = 0.059\n",
            "939/7395000 (epoch 0), train_loss = 1.911, time/batch = 0.060\n",
            "940/7395000 (epoch 0), train_loss = 2.054, time/batch = 0.058\n",
            "941/7395000 (epoch 0), train_loss = 1.839, time/batch = 0.061\n",
            "942/7395000 (epoch 0), train_loss = 1.956, time/batch = 0.059\n",
            "943/7395000 (epoch 0), train_loss = 1.902, time/batch = 0.060\n",
            "944/7395000 (epoch 0), train_loss = 1.998, time/batch = 0.059\n",
            "945/7395000 (epoch 0), train_loss = 1.916, time/batch = 0.065\n",
            "946/7395000 (epoch 0), train_loss = 1.999, time/batch = 0.059\n",
            "947/7395000 (epoch 0), train_loss = 1.979, time/batch = 0.068\n",
            "948/7395000 (epoch 0), train_loss = 1.893, time/batch = 0.059\n",
            "949/7395000 (epoch 0), train_loss = 1.971, time/batch = 0.061\n",
            "950/7395000 (epoch 0), train_loss = 1.887, time/batch = 0.057\n",
            "951/7395000 (epoch 0), train_loss = 1.827, time/batch = 0.062\n",
            "952/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.055\n",
            "953/7395000 (epoch 0), train_loss = 1.891, time/batch = 0.058\n",
            "954/7395000 (epoch 0), train_loss = 1.937, time/batch = 0.062\n",
            "955/7395000 (epoch 0), train_loss = 1.925, time/batch = 0.062\n",
            "956/7395000 (epoch 0), train_loss = 1.902, time/batch = 0.065\n",
            "957/7395000 (epoch 0), train_loss = 1.984, time/batch = 0.060\n",
            "958/7395000 (epoch 0), train_loss = 1.949, time/batch = 0.060\n",
            "959/7395000 (epoch 0), train_loss = 1.905, time/batch = 0.062\n",
            "960/7395000 (epoch 0), train_loss = 2.021, time/batch = 0.060\n",
            "961/7395000 (epoch 0), train_loss = 2.006, time/batch = 0.062\n",
            "962/7395000 (epoch 0), train_loss = 1.993, time/batch = 0.062\n",
            "963/7395000 (epoch 0), train_loss = 1.948, time/batch = 0.061\n",
            "964/7395000 (epoch 0), train_loss = 1.981, time/batch = 0.062\n",
            "965/7395000 (epoch 0), train_loss = 1.968, time/batch = 0.063\n",
            "966/7395000 (epoch 0), train_loss = 1.941, time/batch = 0.061\n",
            "967/7395000 (epoch 0), train_loss = 1.918, time/batch = 0.062\n",
            "968/7395000 (epoch 0), train_loss = 1.961, time/batch = 0.061\n",
            "969/7395000 (epoch 0), train_loss = 1.975, time/batch = 0.060\n",
            "970/7395000 (epoch 0), train_loss = 1.988, time/batch = 0.061\n",
            "971/7395000 (epoch 0), train_loss = 1.998, time/batch = 0.059\n",
            "972/7395000 (epoch 0), train_loss = 1.920, time/batch = 0.062\n",
            "973/7395000 (epoch 0), train_loss = 1.942, time/batch = 0.062\n",
            "974/7395000 (epoch 0), train_loss = 1.889, time/batch = 0.061\n",
            "975/7395000 (epoch 0), train_loss = 1.875, time/batch = 0.063\n",
            "976/7395000 (epoch 0), train_loss = 1.996, time/batch = 0.060\n",
            "977/7395000 (epoch 0), train_loss = 1.965, time/batch = 0.066\n",
            "978/7395000 (epoch 0), train_loss = 1.942, time/batch = 0.059\n",
            "979/7395000 (epoch 0), train_loss = 1.954, time/batch = 0.061\n",
            "980/7395000 (epoch 0), train_loss = 1.921, time/batch = 0.062\n",
            "981/7395000 (epoch 0), train_loss = 1.948, time/batch = 0.058\n",
            "982/7395000 (epoch 0), train_loss = 1.987, time/batch = 0.061\n",
            "983/7395000 (epoch 0), train_loss = 2.000, time/batch = 0.063\n",
            "984/7395000 (epoch 0), train_loss = 1.972, time/batch = 0.060\n",
            "985/7395000 (epoch 0), train_loss = 1.970, time/batch = 0.060\n",
            "986/7395000 (epoch 0), train_loss = 2.001, time/batch = 0.056\n",
            "987/7395000 (epoch 0), train_loss = 1.978, time/batch = 0.063\n",
            "988/7395000 (epoch 0), train_loss = 2.001, time/batch = 0.060\n",
            "989/7395000 (epoch 0), train_loss = 1.987, time/batch = 0.062\n",
            "990/7395000 (epoch 0), train_loss = 1.908, time/batch = 0.062\n",
            "991/7395000 (epoch 0), train_loss = 2.029, time/batch = 0.060\n",
            "992/7395000 (epoch 0), train_loss = 1.968, time/batch = 0.061\n",
            "993/7395000 (epoch 0), train_loss = 1.915, time/batch = 0.060\n",
            "994/7395000 (epoch 0), train_loss = 1.965, time/batch = 0.060\n",
            "995/7395000 (epoch 0), train_loss = 1.908, time/batch = 0.062\n",
            "996/7395000 (epoch 0), train_loss = 2.042, time/batch = 0.062\n",
            "997/7395000 (epoch 0), train_loss = 1.956, time/batch = 0.067\n",
            "998/7395000 (epoch 0), train_loss = 2.126, time/batch = 0.061\n",
            "999/7395000 (epoch 0), train_loss = 1.961, time/batch = 0.062\n",
            "1000/7395000 (epoch 0), train_loss = 1.994, time/batch = 0.063\n",
            "model saved to save/model.ckpt\n",
            "1001/7395000 (epoch 0), train_loss = 2.008, time/batch = 0.062\n",
            "1002/7395000 (epoch 0), train_loss = 1.979, time/batch = 0.060\n",
            "1003/7395000 (epoch 0), train_loss = 1.895, time/batch = 0.061\n",
            "1004/7395000 (epoch 0), train_loss = 1.943, time/batch = 0.062\n",
            "1005/7395000 (epoch 0), train_loss = 2.004, time/batch = 0.066\n",
            "1006/7395000 (epoch 0), train_loss = 2.004, time/batch = 0.061\n",
            "1007/7395000 (epoch 0), train_loss = 1.902, time/batch = 0.063\n",
            "1008/7395000 (epoch 0), train_loss = 2.019, time/batch = 0.061\n",
            "1009/7395000 (epoch 0), train_loss = 1.913, time/batch = 0.068\n",
            "1010/7395000 (epoch 0), train_loss = 1.931, time/batch = 0.056\n",
            "1011/7395000 (epoch 0), train_loss = 1.890, time/batch = 0.058\n",
            "1012/7395000 (epoch 0), train_loss = 1.911, time/batch = 0.060\n",
            "1013/7395000 (epoch 0), train_loss = 2.048, time/batch = 0.061\n",
            "1014/7395000 (epoch 0), train_loss = 1.961, time/batch = 0.066\n",
            "1015/7395000 (epoch 0), train_loss = 2.012, time/batch = 0.063\n",
            "1016/7395000 (epoch 0), train_loss = 1.935, time/batch = 0.067\n",
            "1017/7395000 (epoch 0), train_loss = 2.004, time/batch = 0.062\n",
            "1018/7395000 (epoch 0), train_loss = 2.112, time/batch = 0.063\n",
            "1019/7395000 (epoch 0), train_loss = 1.938, time/batch = 0.056\n",
            "1020/7395000 (epoch 0), train_loss = 2.041, time/batch = 0.062\n",
            "1021/7395000 (epoch 0), train_loss = 1.912, time/batch = 0.062\n",
            "1022/7395000 (epoch 0), train_loss = 1.979, time/batch = 0.068\n",
            "1023/7395000 (epoch 0), train_loss = 1.908, time/batch = 0.061\n",
            "1024/7395000 (epoch 0), train_loss = 2.023, time/batch = 0.058\n",
            "1025/7395000 (epoch 0), train_loss = 1.924, time/batch = 0.066\n",
            "1026/7395000 (epoch 0), train_loss = 1.939, time/batch = 0.065\n",
            "1027/7395000 (epoch 0), train_loss = 1.975, time/batch = 0.062\n",
            "1028/7395000 (epoch 0), train_loss = 1.944, time/batch = 0.064\n",
            "1029/7395000 (epoch 0), train_loss = 1.954, time/batch = 0.061\n",
            "1030/7395000 (epoch 0), train_loss = 1.901, time/batch = 0.063\n",
            "1031/7395000 (epoch 0), train_loss = 1.948, time/batch = 0.061\n",
            "1032/7395000 (epoch 0), train_loss = 1.944, time/batch = 0.056\n",
            "1033/7395000 (epoch 0), train_loss = 1.983, time/batch = 0.062\n",
            "1034/7395000 (epoch 0), train_loss = 1.923, time/batch = 0.057\n",
            "1035/7395000 (epoch 0), train_loss = 1.925, time/batch = 0.055\n",
            "1036/7395000 (epoch 0), train_loss = 1.879, time/batch = 0.060\n",
            "1037/7395000 (epoch 0), train_loss = 1.906, time/batch = 0.063\n",
            "1038/7395000 (epoch 0), train_loss = 1.877, time/batch = 0.060\n",
            "1039/7395000 (epoch 0), train_loss = 1.956, time/batch = 0.061\n",
            "1040/7395000 (epoch 0), train_loss = 1.915, time/batch = 0.063\n",
            "1041/7395000 (epoch 0), train_loss = 1.936, time/batch = 0.060\n",
            "1042/7395000 (epoch 0), train_loss = 2.003, time/batch = 0.059\n",
            "1043/7395000 (epoch 0), train_loss = 1.953, time/batch = 0.066\n",
            "1044/7395000 (epoch 0), train_loss = 1.865, time/batch = 0.060\n",
            "1045/7395000 (epoch 0), train_loss = 1.933, time/batch = 0.073\n",
            "1046/7395000 (epoch 0), train_loss = 1.960, time/batch = 0.061\n",
            "1047/7395000 (epoch 0), train_loss = 1.973, time/batch = 0.061\n",
            "1048/7395000 (epoch 0), train_loss = 2.108, time/batch = 0.061\n",
            "1049/7395000 (epoch 0), train_loss = 1.955, time/batch = 0.061\n",
            "1050/7395000 (epoch 0), train_loss = 2.107, time/batch = 0.059\n",
            "1051/7395000 (epoch 0), train_loss = 1.919, time/batch = 0.064\n",
            "1052/7395000 (epoch 0), train_loss = 1.936, time/batch = 0.060\n",
            "1053/7395000 (epoch 0), train_loss = 1.994, time/batch = 0.062\n",
            "1054/7395000 (epoch 0), train_loss = 1.940, time/batch = 0.060\n",
            "1055/7395000 (epoch 0), train_loss = 1.980, time/batch = 0.069\n",
            "1056/7395000 (epoch 0), train_loss = 2.050, time/batch = 0.058\n",
            "1057/7395000 (epoch 0), train_loss = 2.027, time/batch = 0.063\n",
            "1058/7395000 (epoch 0), train_loss = 1.952, time/batch = 0.062\n",
            "1059/7395000 (epoch 0), train_loss = 1.929, time/batch = 0.066\n",
            "1060/7395000 (epoch 0), train_loss = 2.007, time/batch = 0.062\n",
            "1061/7395000 (epoch 0), train_loss = 1.994, time/batch = 0.063\n",
            "1062/7395000 (epoch 0), train_loss = 2.074, time/batch = 0.063\n",
            "1063/7395000 (epoch 0), train_loss = 2.052, time/batch = 0.060\n",
            "1064/7395000 (epoch 0), train_loss = 1.998, time/batch = 0.064\n",
            "1065/7395000 (epoch 0), train_loss = 1.888, time/batch = 0.062\n",
            "1066/7395000 (epoch 0), train_loss = 1.952, time/batch = 0.062\n",
            "1067/7395000 (epoch 0), train_loss = 1.905, time/batch = 0.061\n",
            "1068/7395000 (epoch 0), train_loss = 1.994, time/batch = 0.057\n",
            "1069/7395000 (epoch 0), train_loss = 1.847, time/batch = 0.062\n",
            "1070/7395000 (epoch 0), train_loss = 1.792, time/batch = 0.061\n",
            "1071/7395000 (epoch 0), train_loss = 1.900, time/batch = 0.063\n",
            "1072/7395000 (epoch 0), train_loss = 1.901, time/batch = 0.060\n",
            "1073/7395000 (epoch 0), train_loss = 1.819, time/batch = 0.055\n",
            "1074/7395000 (epoch 0), train_loss = 1.831, time/batch = 0.065\n",
            "1075/7395000 (epoch 0), train_loss = 1.808, time/batch = 0.062\n",
            "1076/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.069\n",
            "1077/7395000 (epoch 0), train_loss = 1.827, time/batch = 0.064\n",
            "1078/7395000 (epoch 0), train_loss = 1.865, time/batch = 0.060\n",
            "1079/7395000 (epoch 0), train_loss = 1.779, time/batch = 0.062\n",
            "1080/7395000 (epoch 0), train_loss = 1.750, time/batch = 0.063\n",
            "1081/7395000 (epoch 0), train_loss = 1.906, time/batch = 0.058\n",
            "1082/7395000 (epoch 0), train_loss = 1.977, time/batch = 0.063\n",
            "1083/7395000 (epoch 0), train_loss = 1.885, time/batch = 0.062\n",
            "1084/7395000 (epoch 0), train_loss = 1.849, time/batch = 0.061\n",
            "1085/7395000 (epoch 0), train_loss = 1.926, time/batch = 0.060\n",
            "1086/7395000 (epoch 0), train_loss = 1.999, time/batch = 0.060\n",
            "1087/7395000 (epoch 0), train_loss = 1.906, time/batch = 0.065\n",
            "1088/7395000 (epoch 0), train_loss = 1.913, time/batch = 0.069\n",
            "1089/7395000 (epoch 0), train_loss = 1.951, time/batch = 0.059\n",
            "1090/7395000 (epoch 0), train_loss = 1.858, time/batch = 0.060\n",
            "1091/7395000 (epoch 0), train_loss = 1.888, time/batch = 0.062\n",
            "1092/7395000 (epoch 0), train_loss = 1.940, time/batch = 0.068\n",
            "1093/7395000 (epoch 0), train_loss = 1.896, time/batch = 0.058\n",
            "1094/7395000 (epoch 0), train_loss = 1.950, time/batch = 0.061\n",
            "1095/7395000 (epoch 0), train_loss = 1.842, time/batch = 0.065\n",
            "1096/7395000 (epoch 0), train_loss = 1.944, time/batch = 0.061\n",
            "1097/7395000 (epoch 0), train_loss = 1.858, time/batch = 0.059\n",
            "1098/7395000 (epoch 0), train_loss = 1.845, time/batch = 0.061\n",
            "1099/7395000 (epoch 0), train_loss = 1.942, time/batch = 0.056\n",
            "1100/7395000 (epoch 0), train_loss = 1.885, time/batch = 0.062\n",
            "1101/7395000 (epoch 0), train_loss = 1.837, time/batch = 0.062\n",
            "1102/7395000 (epoch 0), train_loss = 1.910, time/batch = 0.065\n",
            "1103/7395000 (epoch 0), train_loss = 1.755, time/batch = 0.062\n",
            "1104/7395000 (epoch 0), train_loss = 1.829, time/batch = 0.061\n",
            "1105/7395000 (epoch 0), train_loss = 1.827, time/batch = 0.060\n",
            "1106/7395000 (epoch 0), train_loss = 1.973, time/batch = 0.062\n",
            "1107/7395000 (epoch 0), train_loss = 1.927, time/batch = 0.062\n",
            "1108/7395000 (epoch 0), train_loss = 1.892, time/batch = 0.060\n",
            "1109/7395000 (epoch 0), train_loss = 1.912, time/batch = 0.071\n",
            "1110/7395000 (epoch 0), train_loss = 1.968, time/batch = 0.059\n",
            "1111/7395000 (epoch 0), train_loss = 1.879, time/batch = 0.070\n",
            "1112/7395000 (epoch 0), train_loss = 1.894, time/batch = 0.061\n",
            "1113/7395000 (epoch 0), train_loss = 1.827, time/batch = 0.060\n",
            "1114/7395000 (epoch 0), train_loss = 1.818, time/batch = 0.062\n",
            "1115/7395000 (epoch 0), train_loss = 1.854, time/batch = 0.062\n",
            "1116/7395000 (epoch 0), train_loss = 1.897, time/batch = 0.063\n",
            "1117/7395000 (epoch 0), train_loss = 2.009, time/batch = 0.058\n",
            "1118/7395000 (epoch 0), train_loss = 1.876, time/batch = 0.059\n",
            "1119/7395000 (epoch 0), train_loss = 1.830, time/batch = 0.062\n",
            "1120/7395000 (epoch 0), train_loss = 1.823, time/batch = 0.063\n",
            "1121/7395000 (epoch 0), train_loss = 1.780, time/batch = 0.063\n",
            "1122/7395000 (epoch 0), train_loss = 1.876, time/batch = 0.061\n",
            "1123/7395000 (epoch 0), train_loss = 1.832, time/batch = 0.060\n",
            "1124/7395000 (epoch 0), train_loss = 1.834, time/batch = 0.060\n",
            "1125/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.061\n",
            "1126/7395000 (epoch 0), train_loss = 1.818, time/batch = 0.059\n",
            "1127/7395000 (epoch 0), train_loss = 1.799, time/batch = 0.061\n",
            "1128/7395000 (epoch 0), train_loss = 1.913, time/batch = 0.065\n",
            "1129/7395000 (epoch 0), train_loss = 1.897, time/batch = 0.062\n",
            "1130/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.060\n",
            "1131/7395000 (epoch 0), train_loss = 1.889, time/batch = 0.060\n",
            "1132/7395000 (epoch 0), train_loss = 1.877, time/batch = 0.059\n",
            "1133/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.064\n",
            "1134/7395000 (epoch 0), train_loss = 1.857, time/batch = 0.061\n",
            "1135/7395000 (epoch 0), train_loss = 1.815, time/batch = 0.061\n",
            "1136/7395000 (epoch 0), train_loss = 1.856, time/batch = 0.058\n",
            "1137/7395000 (epoch 0), train_loss = 1.938, time/batch = 0.060\n",
            "1138/7395000 (epoch 0), train_loss = 2.021, time/batch = 0.064\n",
            "1139/7395000 (epoch 0), train_loss = 1.777, time/batch = 0.058\n",
            "1140/7395000 (epoch 0), train_loss = 1.908, time/batch = 0.059\n",
            "1141/7395000 (epoch 0), train_loss = 1.825, time/batch = 0.060\n",
            "1142/7395000 (epoch 0), train_loss = 1.887, time/batch = 0.061\n",
            "1143/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.059\n",
            "1144/7395000 (epoch 0), train_loss = 1.838, time/batch = 0.062\n",
            "1145/7395000 (epoch 0), train_loss = 1.902, time/batch = 0.059\n",
            "1146/7395000 (epoch 0), train_loss = 1.830, time/batch = 0.061\n",
            "1147/7395000 (epoch 0), train_loss = 1.896, time/batch = 0.054\n",
            "1148/7395000 (epoch 0), train_loss = 1.788, time/batch = 0.059\n",
            "1149/7395000 (epoch 0), train_loss = 1.815, time/batch = 0.059\n",
            "1150/7395000 (epoch 0), train_loss = 1.854, time/batch = 0.061\n",
            "1151/7395000 (epoch 0), train_loss = 1.877, time/batch = 0.067\n",
            "1152/7395000 (epoch 0), train_loss = 1.811, time/batch = 0.058\n",
            "1153/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.062\n",
            "1154/7395000 (epoch 0), train_loss = 1.847, time/batch = 0.059\n",
            "1155/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.060\n",
            "1156/7395000 (epoch 0), train_loss = 1.858, time/batch = 0.060\n",
            "1157/7395000 (epoch 0), train_loss = 1.871, time/batch = 0.062\n",
            "1158/7395000 (epoch 0), train_loss = 1.835, time/batch = 0.065\n",
            "1159/7395000 (epoch 0), train_loss = 1.830, time/batch = 0.066\n",
            "1160/7395000 (epoch 0), train_loss = 1.779, time/batch = 0.061\n",
            "1161/7395000 (epoch 0), train_loss = 1.840, time/batch = 0.064\n",
            "1162/7395000 (epoch 0), train_loss = 1.812, time/batch = 0.061\n",
            "1163/7395000 (epoch 0), train_loss = 1.782, time/batch = 0.061\n",
            "1164/7395000 (epoch 0), train_loss = 1.861, time/batch = 0.064\n",
            "1165/7395000 (epoch 0), train_loss = 1.862, time/batch = 0.061\n",
            "1166/7395000 (epoch 0), train_loss = 1.776, time/batch = 0.062\n",
            "1167/7395000 (epoch 0), train_loss = 1.826, time/batch = 0.060\n",
            "1168/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.061\n",
            "1169/7395000 (epoch 0), train_loss = 1.873, time/batch = 0.065\n",
            "1170/7395000 (epoch 0), train_loss = 1.988, time/batch = 0.064\n",
            "1171/7395000 (epoch 0), train_loss = 1.893, time/batch = 0.061\n",
            "1172/7395000 (epoch 0), train_loss = 1.839, time/batch = 0.060\n",
            "1173/7395000 (epoch 0), train_loss = 1.826, time/batch = 0.055\n",
            "1174/7395000 (epoch 0), train_loss = 1.822, time/batch = 0.059\n",
            "1175/7395000 (epoch 0), train_loss = 1.901, time/batch = 0.060\n",
            "1176/7395000 (epoch 0), train_loss = 1.859, time/batch = 0.067\n",
            "1177/7395000 (epoch 0), train_loss = 1.816, time/batch = 0.061\n",
            "1178/7395000 (epoch 0), train_loss = 1.756, time/batch = 0.054\n",
            "1179/7395000 (epoch 0), train_loss = 1.803, time/batch = 0.063\n",
            "1180/7395000 (epoch 0), train_loss = 1.935, time/batch = 0.059\n",
            "1181/7395000 (epoch 0), train_loss = 1.921, time/batch = 0.060\n",
            "1182/7395000 (epoch 0), train_loss = 1.912, time/batch = 0.062\n",
            "1183/7395000 (epoch 0), train_loss = 1.900, time/batch = 0.062\n",
            "1184/7395000 (epoch 0), train_loss = 1.869, time/batch = 0.060\n",
            "1185/7395000 (epoch 0), train_loss = 1.875, time/batch = 0.062\n",
            "1186/7395000 (epoch 0), train_loss = 1.860, time/batch = 0.063\n",
            "1187/7395000 (epoch 0), train_loss = 1.842, time/batch = 0.059\n",
            "1188/7395000 (epoch 0), train_loss = 1.876, time/batch = 0.059\n",
            "1189/7395000 (epoch 0), train_loss = 1.898, time/batch = 0.061\n",
            "1190/7395000 (epoch 0), train_loss = 1.894, time/batch = 0.059\n",
            "1191/7395000 (epoch 0), train_loss = 1.825, time/batch = 0.064\n",
            "1192/7395000 (epoch 0), train_loss = 1.863, time/batch = 0.059\n",
            "1193/7395000 (epoch 0), train_loss = 1.856, time/batch = 0.057\n",
            "1194/7395000 (epoch 0), train_loss = 1.868, time/batch = 0.063\n",
            "1195/7395000 (epoch 0), train_loss = 1.906, time/batch = 0.059\n",
            "1196/7395000 (epoch 0), train_loss = 1.828, time/batch = 0.063\n",
            "1197/7395000 (epoch 0), train_loss = 1.858, time/batch = 0.061\n",
            "1198/7395000 (epoch 0), train_loss = 1.792, time/batch = 0.060\n",
            "1199/7395000 (epoch 0), train_loss = 1.964, time/batch = 0.062\n",
            "1200/7395000 (epoch 0), train_loss = 1.812, time/batch = 0.063\n",
            "1201/7395000 (epoch 0), train_loss = 1.864, time/batch = 0.059\n",
            "1202/7395000 (epoch 0), train_loss = 1.798, time/batch = 0.059\n",
            "1203/7395000 (epoch 0), train_loss = 1.823, time/batch = 0.062\n",
            "1204/7395000 (epoch 0), train_loss = 1.717, time/batch = 0.060\n",
            "1205/7395000 (epoch 0), train_loss = 1.742, time/batch = 0.061\n",
            "1206/7395000 (epoch 0), train_loss = 1.760, time/batch = 0.063\n",
            "1207/7395000 (epoch 0), train_loss = 1.765, time/batch = 0.063\n",
            "1208/7395000 (epoch 0), train_loss = 1.671, time/batch = 0.059\n",
            "1209/7395000 (epoch 0), train_loss = 1.889, time/batch = 0.067\n",
            "1210/7395000 (epoch 0), train_loss = 1.758, time/batch = 0.061\n",
            "1211/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.057\n",
            "1212/7395000 (epoch 0), train_loss = 1.887, time/batch = 0.064\n",
            "1213/7395000 (epoch 0), train_loss = 1.740, time/batch = 0.061\n",
            "1214/7395000 (epoch 0), train_loss = 1.770, time/batch = 0.065\n",
            "1215/7395000 (epoch 0), train_loss = 1.703, time/batch = 0.060\n",
            "1216/7395000 (epoch 0), train_loss = 1.814, time/batch = 0.060\n",
            "1217/7395000 (epoch 0), train_loss = 1.750, time/batch = 0.062\n",
            "1218/7395000 (epoch 0), train_loss = 1.748, time/batch = 0.062\n",
            "1219/7395000 (epoch 0), train_loss = 1.776, time/batch = 0.063\n",
            "1220/7395000 (epoch 0), train_loss = 1.766, time/batch = 0.063\n",
            "1221/7395000 (epoch 0), train_loss = 1.772, time/batch = 0.060\n",
            "1222/7395000 (epoch 0), train_loss = 1.749, time/batch = 0.060\n",
            "1223/7395000 (epoch 0), train_loss = 1.894, time/batch = 0.061\n",
            "1224/7395000 (epoch 0), train_loss = 1.823, time/batch = 0.064\n",
            "1225/7395000 (epoch 0), train_loss = 1.872, time/batch = 0.064\n",
            "1226/7395000 (epoch 0), train_loss = 1.795, time/batch = 0.063\n",
            "1227/7395000 (epoch 0), train_loss = 1.729, time/batch = 0.063\n",
            "1228/7395000 (epoch 0), train_loss = 1.762, time/batch = 0.056\n",
            "1229/7395000 (epoch 0), train_loss = 1.837, time/batch = 0.060\n",
            "1230/7395000 (epoch 0), train_loss = 1.837, time/batch = 0.063\n",
            "1231/7395000 (epoch 0), train_loss = 1.911, time/batch = 0.062\n",
            "1232/7395000 (epoch 0), train_loss = 1.887, time/batch = 0.059\n",
            "1233/7395000 (epoch 0), train_loss = 1.775, time/batch = 0.061\n",
            "1234/7395000 (epoch 0), train_loss = 1.846, time/batch = 0.060\n",
            "1235/7395000 (epoch 0), train_loss = 1.784, time/batch = 0.060\n",
            "1236/7395000 (epoch 0), train_loss = 1.812, time/batch = 0.055\n",
            "1237/7395000 (epoch 0), train_loss = 1.739, time/batch = 0.060\n",
            "1238/7395000 (epoch 0), train_loss = 1.766, time/batch = 0.062\n",
            "1239/7395000 (epoch 0), train_loss = 1.864, time/batch = 0.061\n",
            "1240/7395000 (epoch 0), train_loss = 1.758, time/batch = 0.060\n",
            "1241/7395000 (epoch 0), train_loss = 1.813, time/batch = 0.064\n",
            "1242/7395000 (epoch 0), train_loss = 1.874, time/batch = 0.066\n",
            "1243/7395000 (epoch 0), train_loss = 1.847, time/batch = 0.059\n",
            "1244/7395000 (epoch 0), train_loss = 1.780, time/batch = 0.060\n",
            "1245/7395000 (epoch 0), train_loss = 1.678, time/batch = 0.062\n",
            "1246/7395000 (epoch 0), train_loss = 1.756, time/batch = 0.059\n",
            "1247/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.061\n",
            "1248/7395000 (epoch 0), train_loss = 1.720, time/batch = 0.060\n",
            "1249/7395000 (epoch 0), train_loss = 1.680, time/batch = 0.061\n",
            "1250/7395000 (epoch 0), train_loss = 1.631, time/batch = 0.061\n",
            "1251/7395000 (epoch 0), train_loss = 1.766, time/batch = 0.061\n",
            "1252/7395000 (epoch 0), train_loss = 1.828, time/batch = 0.062\n",
            "1253/7395000 (epoch 0), train_loss = 1.742, time/batch = 0.070\n",
            "1254/7395000 (epoch 0), train_loss = 1.825, time/batch = 0.060\n",
            "1255/7395000 (epoch 0), train_loss = 1.912, time/batch = 0.062\n",
            "1256/7395000 (epoch 0), train_loss = 1.780, time/batch = 0.061\n",
            "1257/7395000 (epoch 0), train_loss = 1.754, time/batch = 0.060\n",
            "1258/7395000 (epoch 0), train_loss = 1.719, time/batch = 0.061\n",
            "1259/7395000 (epoch 0), train_loss = 1.854, time/batch = 0.062\n",
            "1260/7395000 (epoch 0), train_loss = 1.772, time/batch = 0.057\n",
            "1261/7395000 (epoch 0), train_loss = 1.724, time/batch = 0.061\n",
            "1262/7395000 (epoch 0), train_loss = 1.798, time/batch = 0.055\n",
            "1263/7395000 (epoch 0), train_loss = 1.787, time/batch = 0.060\n",
            "1264/7395000 (epoch 0), train_loss = 1.810, time/batch = 0.059\n",
            "1265/7395000 (epoch 0), train_loss = 1.737, time/batch = 0.059\n",
            "1266/7395000 (epoch 0), train_loss = 1.838, time/batch = 0.058\n",
            "1267/7395000 (epoch 0), train_loss = 1.643, time/batch = 0.053\n",
            "1268/7395000 (epoch 0), train_loss = 1.728, time/batch = 0.053\n",
            "1269/7395000 (epoch 0), train_loss = 1.707, time/batch = 0.053\n",
            "1270/7395000 (epoch 0), train_loss = 1.679, time/batch = 0.070\n",
            "1271/7395000 (epoch 0), train_loss = 1.721, time/batch = 0.054\n",
            "1272/7395000 (epoch 0), train_loss = 1.864, time/batch = 0.055\n",
            "1273/7395000 (epoch 0), train_loss = 1.790, time/batch = 0.054\n",
            "1274/7395000 (epoch 0), train_loss = 1.776, time/batch = 0.053\n",
            "1275/7395000 (epoch 0), train_loss = 1.752, time/batch = 0.057\n",
            "1276/7395000 (epoch 0), train_loss = 1.698, time/batch = 0.057\n",
            "1277/7395000 (epoch 0), train_loss = 1.681, time/batch = 0.055\n",
            "1278/7395000 (epoch 0), train_loss = 1.706, time/batch = 0.054\n",
            "1279/7395000 (epoch 0), train_loss = 1.764, time/batch = 0.062\n",
            "1280/7395000 (epoch 0), train_loss = 1.708, time/batch = 0.059\n",
            "1281/7395000 (epoch 0), train_loss = 1.660, time/batch = 0.061\n",
            "1282/7395000 (epoch 0), train_loss = 1.782, time/batch = 0.061\n",
            "1283/7395000 (epoch 0), train_loss = 1.771, time/batch = 0.063\n",
            "1284/7395000 (epoch 0), train_loss = 1.807, time/batch = 0.060\n",
            "1285/7395000 (epoch 0), train_loss = 1.796, time/batch = 0.060\n",
            "1286/7395000 (epoch 0), train_loss = 1.791, time/batch = 0.063\n",
            "1287/7395000 (epoch 0), train_loss = 1.741, time/batch = 0.060\n",
            "1288/7395000 (epoch 0), train_loss = 1.747, time/batch = 0.059\n",
            "1289/7395000 (epoch 0), train_loss = 1.788, time/batch = 0.066\n",
            "1290/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.061\n",
            "1291/7395000 (epoch 0), train_loss = 1.738, time/batch = 0.061\n",
            "1292/7395000 (epoch 0), train_loss = 1.713, time/batch = 0.061\n",
            "1293/7395000 (epoch 0), train_loss = 1.783, time/batch = 0.061\n",
            "1294/7395000 (epoch 0), train_loss = 1.706, time/batch = 0.066\n",
            "1295/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.061\n",
            "1296/7395000 (epoch 0), train_loss = 1.665, time/batch = 0.060\n",
            "1297/7395000 (epoch 0), train_loss = 1.709, time/batch = 0.063\n",
            "1298/7395000 (epoch 0), train_loss = 1.714, time/batch = 0.060\n",
            "1299/7395000 (epoch 0), train_loss = 1.618, time/batch = 0.060\n",
            "1300/7395000 (epoch 0), train_loss = 1.681, time/batch = 0.061\n",
            "1301/7395000 (epoch 0), train_loss = 1.719, time/batch = 0.060\n",
            "1302/7395000 (epoch 0), train_loss = 1.720, time/batch = 0.062\n",
            "1303/7395000 (epoch 0), train_loss = 1.711, time/batch = 0.061\n",
            "1304/7395000 (epoch 0), train_loss = 1.702, time/batch = 0.059\n",
            "1305/7395000 (epoch 0), train_loss = 1.698, time/batch = 0.060\n",
            "1306/7395000 (epoch 0), train_loss = 1.779, time/batch = 0.060\n",
            "1307/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.062\n",
            "1308/7395000 (epoch 0), train_loss = 1.658, time/batch = 0.060\n",
            "1309/7395000 (epoch 0), train_loss = 1.770, time/batch = 0.062\n",
            "1310/7395000 (epoch 0), train_loss = 1.674, time/batch = 0.059\n",
            "1311/7395000 (epoch 0), train_loss = 1.666, time/batch = 0.063\n",
            "1312/7395000 (epoch 0), train_loss = 1.747, time/batch = 0.060\n",
            "1313/7395000 (epoch 0), train_loss = 1.614, time/batch = 0.060\n",
            "1314/7395000 (epoch 0), train_loss = 1.782, time/batch = 0.059\n",
            "1315/7395000 (epoch 0), train_loss = 1.805, time/batch = 0.059\n",
            "1316/7395000 (epoch 0), train_loss = 1.958, time/batch = 0.058\n",
            "1317/7395000 (epoch 0), train_loss = 1.846, time/batch = 0.054\n",
            "1318/7395000 (epoch 0), train_loss = 1.827, time/batch = 0.058\n",
            "1319/7395000 (epoch 0), train_loss = 1.723, time/batch = 0.058\n",
            "1320/7395000 (epoch 0), train_loss = 1.729, time/batch = 0.053\n",
            "1321/7395000 (epoch 0), train_loss = 1.737, time/batch = 0.054\n",
            "1322/7395000 (epoch 0), train_loss = 1.744, time/batch = 0.053\n",
            "1323/7395000 (epoch 0), train_loss = 1.817, time/batch = 0.057\n",
            "1324/7395000 (epoch 0), train_loss = 1.758, time/batch = 0.053\n",
            "1325/7395000 (epoch 0), train_loss = 1.815, time/batch = 0.055\n",
            "1326/7395000 (epoch 0), train_loss = 1.750, time/batch = 0.062\n",
            "1327/7395000 (epoch 0), train_loss = 1.828, time/batch = 0.052\n",
            "1328/7395000 (epoch 0), train_loss = 1.828, time/batch = 0.056\n",
            "1329/7395000 (epoch 0), train_loss = 1.808, time/batch = 0.061\n",
            "1330/7395000 (epoch 0), train_loss = 1.861, time/batch = 0.059\n",
            "1331/7395000 (epoch 0), train_loss = 1.795, time/batch = 0.071\n",
            "1332/7395000 (epoch 0), train_loss = 1.755, time/batch = 0.059\n",
            "1333/7395000 (epoch 0), train_loss = 1.864, time/batch = 0.061\n",
            "1334/7395000 (epoch 0), train_loss = 1.804, time/batch = 0.058\n",
            "1335/7395000 (epoch 0), train_loss = 1.885, time/batch = 0.069\n",
            "1336/7395000 (epoch 0), train_loss = 1.801, time/batch = 0.060\n",
            "1337/7395000 (epoch 0), train_loss = 1.891, time/batch = 0.065\n",
            "1338/7395000 (epoch 0), train_loss = 1.775, time/batch = 0.059\n",
            "1339/7395000 (epoch 0), train_loss = 1.767, time/batch = 0.060\n",
            "1340/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.058\n",
            "1341/7395000 (epoch 0), train_loss = 1.721, time/batch = 0.055\n",
            "1342/7395000 (epoch 0), train_loss = 1.789, time/batch = 0.060\n",
            "1343/7395000 (epoch 0), train_loss = 1.806, time/batch = 0.060\n",
            "1344/7395000 (epoch 0), train_loss = 1.762, time/batch = 0.058\n",
            "1345/7395000 (epoch 0), train_loss = 1.720, time/batch = 0.066\n",
            "1346/7395000 (epoch 0), train_loss = 1.857, time/batch = 0.061\n",
            "1347/7395000 (epoch 0), train_loss = 1.799, time/batch = 0.059\n",
            "1348/7395000 (epoch 0), train_loss = 1.781, time/batch = 0.060\n",
            "1349/7395000 (epoch 0), train_loss = 1.868, time/batch = 0.059\n",
            "1350/7395000 (epoch 0), train_loss = 1.748, time/batch = 0.060\n",
            "1351/7395000 (epoch 0), train_loss = 1.814, time/batch = 0.060\n",
            "1352/7395000 (epoch 0), train_loss = 1.725, time/batch = 0.057\n",
            "1353/7395000 (epoch 0), train_loss = 1.733, time/batch = 0.063\n",
            "1354/7395000 (epoch 0), train_loss = 1.688, time/batch = 0.059\n",
            "1355/7395000 (epoch 0), train_loss = 1.789, time/batch = 0.061\n",
            "1356/7395000 (epoch 0), train_loss = 1.644, time/batch = 0.059\n",
            "1357/7395000 (epoch 0), train_loss = 1.721, time/batch = 0.060\n",
            "1358/7395000 (epoch 0), train_loss = 1.816, time/batch = 0.056\n",
            "1359/7395000 (epoch 0), train_loss = 1.675, time/batch = 0.059\n",
            "1360/7395000 (epoch 0), train_loss = 1.679, time/batch = 0.058\n",
            "1361/7395000 (epoch 0), train_loss = 1.842, time/batch = 0.058\n",
            "1362/7395000 (epoch 0), train_loss = 1.709, time/batch = 0.061\n",
            "1363/7395000 (epoch 0), train_loss = 1.822, time/batch = 0.062\n",
            "1364/7395000 (epoch 0), train_loss = 1.805, time/batch = 0.060\n",
            "1365/7395000 (epoch 0), train_loss = 1.772, time/batch = 0.061\n",
            "1366/7395000 (epoch 0), train_loss = 1.795, time/batch = 0.058\n",
            "1367/7395000 (epoch 0), train_loss = 1.704, time/batch = 0.062\n",
            "1368/7395000 (epoch 0), train_loss = 1.795, time/batch = 0.059\n",
            "1369/7395000 (epoch 0), train_loss = 1.664, time/batch = 0.058\n",
            "1370/7395000 (epoch 0), train_loss = 1.784, time/batch = 0.069\n",
            "1371/7395000 (epoch 0), train_loss = 1.768, time/batch = 0.060\n",
            "1372/7395000 (epoch 0), train_loss = 1.740, time/batch = 0.060\n",
            "1373/7395000 (epoch 0), train_loss = 1.702, time/batch = 0.063\n",
            "1374/7395000 (epoch 0), train_loss = 1.739, time/batch = 0.060\n",
            "1375/7395000 (epoch 0), train_loss = 1.634, time/batch = 0.059\n",
            "1376/7395000 (epoch 0), train_loss = 1.773, time/batch = 0.061\n",
            "1377/7395000 (epoch 0), train_loss = 1.849, time/batch = 0.061\n",
            "1378/7395000 (epoch 0), train_loss = 1.717, time/batch = 0.064\n",
            "1379/7395000 (epoch 0), train_loss = 1.834, time/batch = 0.060\n",
            "1380/7395000 (epoch 0), train_loss = 1.850, time/batch = 0.063\n",
            "1381/7395000 (epoch 0), train_loss = 1.737, time/batch = 0.061\n",
            "1382/7395000 (epoch 0), train_loss = 1.797, time/batch = 0.060\n",
            "1383/7395000 (epoch 0), train_loss = 1.696, time/batch = 0.061\n",
            "1384/7395000 (epoch 0), train_loss = 1.785, time/batch = 0.060\n",
            "1385/7395000 (epoch 0), train_loss = 1.872, time/batch = 0.061\n",
            "1386/7395000 (epoch 0), train_loss = 1.761, time/batch = 0.061\n",
            "1387/7395000 (epoch 0), train_loss = 1.793, time/batch = 0.056\n",
            "1388/7395000 (epoch 0), train_loss = 1.879, time/batch = 0.059\n",
            "1389/7395000 (epoch 0), train_loss = 1.772, time/batch = 0.060\n",
            "1390/7395000 (epoch 0), train_loss = 1.639, time/batch = 0.059\n",
            "1391/7395000 (epoch 0), train_loss = 1.769, time/batch = 0.059\n",
            "1392/7395000 (epoch 0), train_loss = 1.870, time/batch = 0.059\n",
            "1393/7395000 (epoch 0), train_loss = 1.857, time/batch = 0.065\n",
            "1394/7395000 (epoch 0), train_loss = 1.836, time/batch = 0.059\n",
            "1395/7395000 (epoch 0), train_loss = 1.829, time/batch = 0.061\n",
            "1396/7395000 (epoch 0), train_loss = 1.826, time/batch = 0.059\n",
            "1397/7395000 (epoch 0), train_loss = 1.830, time/batch = 0.066\n",
            "1398/7395000 (epoch 0), train_loss = 1.752, time/batch = 0.061\n",
            "1399/7395000 (epoch 0), train_loss = 1.874, time/batch = 0.066\n",
            "1400/7395000 (epoch 0), train_loss = 1.795, time/batch = 0.060\n",
            "1401/7395000 (epoch 0), train_loss = 1.776, time/batch = 0.061\n",
            "1402/7395000 (epoch 0), train_loss = 1.809, time/batch = 0.060\n",
            "1403/7395000 (epoch 0), train_loss = 1.824, time/batch = 0.069\n",
            "1404/7395000 (epoch 0), train_loss = 1.841, time/batch = 0.062\n",
            "1405/7395000 (epoch 0), train_loss = 1.856, time/batch = 0.059\n",
            "1406/7395000 (epoch 0), train_loss = 1.829, time/batch = 0.060\n",
            "1407/7395000 (epoch 0), train_loss = 1.785, time/batch = 0.059\n",
            "1408/7395000 (epoch 0), train_loss = 1.813, time/batch = 0.061\n",
            "1409/7395000 (epoch 0), train_loss = 1.721, time/batch = 0.058\n",
            "1410/7395000 (epoch 0), train_loss = 1.761, time/batch = 0.060\n",
            "1411/7395000 (epoch 0), train_loss = 1.696, time/batch = 0.066\n",
            "1412/7395000 (epoch 0), train_loss = 1.765, time/batch = 0.061\n",
            "1413/7395000 (epoch 0), train_loss = 1.798, time/batch = 0.067\n",
            "1414/7395000 (epoch 0), train_loss = 1.746, time/batch = 0.054\n",
            "1415/7395000 (epoch 0), train_loss = 1.740, time/batch = 0.061\n",
            "1416/7395000 (epoch 0), train_loss = 1.749, time/batch = 0.059\n",
            "1417/7395000 (epoch 0), train_loss = 1.688, time/batch = 0.061\n",
            "1418/7395000 (epoch 0), train_loss = 1.759, time/batch = 0.063\n",
            "1419/7395000 (epoch 0), train_loss = 1.779, time/batch = 0.059\n",
            "1420/7395000 (epoch 0), train_loss = 1.798, time/batch = 0.059\n",
            "1421/7395000 (epoch 0), train_loss = 1.845, time/batch = 0.062\n",
            "1422/7395000 (epoch 0), train_loss = 1.779, time/batch = 0.063\n",
            "1423/7395000 (epoch 0), train_loss = 1.800, time/batch = 0.061\n",
            "1424/7395000 (epoch 0), train_loss = 1.720, time/batch = 0.059\n",
            "1425/7395000 (epoch 0), train_loss = 1.600, time/batch = 0.060\n",
            "1426/7395000 (epoch 0), train_loss = 1.738, time/batch = 0.060\n",
            "1427/7395000 (epoch 0), train_loss = 1.769, time/batch = 0.062\n",
            "1428/7395000 (epoch 0), train_loss = 1.661, time/batch = 0.063\n",
            "1429/7395000 (epoch 0), train_loss = 1.774, time/batch = 0.059\n",
            "1430/7395000 (epoch 0), train_loss = 1.799, time/batch = 0.063\n",
            "1431/7395000 (epoch 0), train_loss = 1.713, time/batch = 0.061\n",
            "1432/7395000 (epoch 0), train_loss = 1.807, time/batch = 0.059\n",
            "1433/7395000 (epoch 0), train_loss = 1.741, time/batch = 0.061\n",
            "1434/7395000 (epoch 0), train_loss = 1.611, time/batch = 0.058\n",
            "1435/7395000 (epoch 0), train_loss = 1.692, time/batch = 0.061\n",
            "1436/7395000 (epoch 0), train_loss = 1.636, time/batch = 0.059\n",
            "1437/7395000 (epoch 0), train_loss = 1.777, time/batch = 0.059\n",
            "1438/7395000 (epoch 0), train_loss = 1.715, time/batch = 0.060\n",
            "1439/7395000 (epoch 0), train_loss = 1.760, time/batch = 0.060\n",
            "1440/7395000 (epoch 0), train_loss = 1.778, time/batch = 0.060\n",
            "1441/7395000 (epoch 0), train_loss = 1.871, time/batch = 0.060\n",
            "1442/7395000 (epoch 0), train_loss = 1.885, time/batch = 0.062\n",
            "1443/7395000 (epoch 0), train_loss = 1.843, time/batch = 0.062\n",
            "1444/7395000 (epoch 0), train_loss = 1.825, time/batch = 0.060\n",
            "1445/7395000 (epoch 0), train_loss = 1.773, time/batch = 0.060\n",
            "1446/7395000 (epoch 0), train_loss = 1.832, time/batch = 0.059\n",
            "1447/7395000 (epoch 0), train_loss = 1.802, time/batch = 0.064\n",
            "1448/7395000 (epoch 0), train_loss = 1.782, time/batch = 0.063\n",
            "1449/7395000 (epoch 0), train_loss = 1.788, time/batch = 0.060\n",
            "1450/7395000 (epoch 0), train_loss = 1.758, time/batch = 0.059\n",
            "1451/7395000 (epoch 0), train_loss = 1.684, time/batch = 0.062\n",
            "1452/7395000 (epoch 0), train_loss = 1.681, time/batch = 0.059\n",
            "1453/7395000 (epoch 0), train_loss = 1.748, time/batch = 0.061\n",
            "1454/7395000 (epoch 0), train_loss = 1.824, time/batch = 0.059\n",
            "1455/7395000 (epoch 0), train_loss = 1.678, time/batch = 0.059\n",
            "1456/7395000 (epoch 0), train_loss = 1.709, time/batch = 0.060\n",
            "1457/7395000 (epoch 0), train_loss = 1.738, time/batch = 0.061\n",
            "1458/7395000 (epoch 0), train_loss = 1.785, time/batch = 0.061\n",
            "1459/7395000 (epoch 0), train_loss = 1.669, time/batch = 0.060\n",
            "1460/7395000 (epoch 0), train_loss = 1.655, time/batch = 0.054\n",
            "1461/7395000 (epoch 0), train_loss = 1.674, time/batch = 0.061\n",
            "1462/7395000 (epoch 0), train_loss = 1.616, time/batch = 0.062\n",
            "1463/7395000 (epoch 0), train_loss = 1.669, time/batch = 0.060\n",
            "1464/7395000 (epoch 0), train_loss = 1.759, time/batch = 0.065\n",
            "1465/7395000 (epoch 0), train_loss = 1.633, time/batch = 0.059\n",
            "1466/7395000 (epoch 0), train_loss = 1.580, time/batch = 0.059\n",
            "1467/7395000 (epoch 0), train_loss = 1.685, time/batch = 0.061\n",
            "1468/7395000 (epoch 0), train_loss = 1.695, time/batch = 0.068\n",
            "1469/7395000 (epoch 0), train_loss = 1.574, time/batch = 0.058\n",
            "1470/7395000 (epoch 0), train_loss = 1.614, time/batch = 0.057\n",
            "1471/7395000 (epoch 0), train_loss = 1.672, time/batch = 0.064\n",
            "1472/7395000 (epoch 0), train_loss = 1.718, time/batch = 0.060\n",
            "1473/7395000 (epoch 0), train_loss = 1.529, time/batch = 0.060\n",
            "1474/7395000 (epoch 0), train_loss = 1.742, time/batch = 0.066\n",
            "1475/7395000 (epoch 0), train_loss = 1.690, time/batch = 0.061\n",
            "1476/7395000 (epoch 0), train_loss = 1.632, time/batch = 0.064\n",
            "1477/7395000 (epoch 0), train_loss = 1.651, time/batch = 0.062\n",
            "1478/7395000 (epoch 0), train_loss = 1.682, time/batch = 0.060\n",
            "1479/7395000 (epoch 1), train_loss = 1.984, time/batch = 0.058\n",
            "1480/7395000 (epoch 1), train_loss = 1.727, time/batch = 0.059\n",
            "1481/7395000 (epoch 1), train_loss = 1.699, time/batch = 0.060\n",
            "1482/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.061\n",
            "1483/7395000 (epoch 1), train_loss = 1.672, time/batch = 0.061\n",
            "1484/7395000 (epoch 1), train_loss = 1.631, time/batch = 0.065\n",
            "1485/7395000 (epoch 1), train_loss = 1.623, time/batch = 0.060\n",
            "1486/7395000 (epoch 1), train_loss = 1.715, time/batch = 0.061\n",
            "1487/7395000 (epoch 1), train_loss = 1.608, time/batch = 0.062\n",
            "1488/7395000 (epoch 1), train_loss = 1.633, time/batch = 0.060\n",
            "1489/7395000 (epoch 1), train_loss = 1.750, time/batch = 0.060\n",
            "1490/7395000 (epoch 1), train_loss = 1.708, time/batch = 0.058\n",
            "1491/7395000 (epoch 1), train_loss = 1.675, time/batch = 0.061\n",
            "1492/7395000 (epoch 1), train_loss = 1.729, time/batch = 0.057\n",
            "1493/7395000 (epoch 1), train_loss = 1.574, time/batch = 0.060\n",
            "1494/7395000 (epoch 1), train_loss = 1.736, time/batch = 0.061\n",
            "1495/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.068\n",
            "1496/7395000 (epoch 1), train_loss = 1.690, time/batch = 0.064\n",
            "1497/7395000 (epoch 1), train_loss = 1.686, time/batch = 0.064\n",
            "1498/7395000 (epoch 1), train_loss = 1.768, time/batch = 0.058\n",
            "1499/7395000 (epoch 1), train_loss = 1.682, time/batch = 0.061\n",
            "1500/7395000 (epoch 1), train_loss = 1.828, time/batch = 0.058\n",
            "1501/7395000 (epoch 1), train_loss = 1.704, time/batch = 0.061\n",
            "1502/7395000 (epoch 1), train_loss = 1.755, time/batch = 0.062\n",
            "1503/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.060\n",
            "1504/7395000 (epoch 1), train_loss = 1.764, time/batch = 0.058\n",
            "1505/7395000 (epoch 1), train_loss = 1.722, time/batch = 0.060\n",
            "1506/7395000 (epoch 1), train_loss = 1.692, time/batch = 0.061\n",
            "1507/7395000 (epoch 1), train_loss = 1.593, time/batch = 0.062\n",
            "1508/7395000 (epoch 1), train_loss = 1.578, time/batch = 0.059\n",
            "1509/7395000 (epoch 1), train_loss = 1.680, time/batch = 0.061\n",
            "1510/7395000 (epoch 1), train_loss = 1.713, time/batch = 0.059\n",
            "1511/7395000 (epoch 1), train_loss = 1.680, time/batch = 0.060\n",
            "1512/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.063\n",
            "1513/7395000 (epoch 1), train_loss = 1.832, time/batch = 0.059\n",
            "1514/7395000 (epoch 1), train_loss = 1.741, time/batch = 0.060\n",
            "1515/7395000 (epoch 1), train_loss = 1.826, time/batch = 0.060\n",
            "1516/7395000 (epoch 1), train_loss = 1.820, time/batch = 0.060\n",
            "1517/7395000 (epoch 1), train_loss = 1.790, time/batch = 0.062\n",
            "1518/7395000 (epoch 1), train_loss = 1.813, time/batch = 0.064\n",
            "1519/7395000 (epoch 1), train_loss = 1.743, time/batch = 0.059\n",
            "1520/7395000 (epoch 1), train_loss = 1.616, time/batch = 0.058\n",
            "1521/7395000 (epoch 1), train_loss = 1.679, time/batch = 0.059\n",
            "1522/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.063\n",
            "1523/7395000 (epoch 1), train_loss = 1.716, time/batch = 0.062\n",
            "1524/7395000 (epoch 1), train_loss = 1.721, time/batch = 0.060\n",
            "1525/7395000 (epoch 1), train_loss = 1.667, time/batch = 0.062\n",
            "1526/7395000 (epoch 1), train_loss = 1.631, time/batch = 0.060\n",
            "1527/7395000 (epoch 1), train_loss = 1.634, time/batch = 0.062\n",
            "1528/7395000 (epoch 1), train_loss = 1.674, time/batch = 0.058\n",
            "1529/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.059\n",
            "1530/7395000 (epoch 1), train_loss = 1.661, time/batch = 0.062\n",
            "1531/7395000 (epoch 1), train_loss = 1.712, time/batch = 0.060\n",
            "1532/7395000 (epoch 1), train_loss = 1.730, time/batch = 0.062\n",
            "1533/7395000 (epoch 1), train_loss = 1.800, time/batch = 0.061\n",
            "1534/7395000 (epoch 1), train_loss = 1.589, time/batch = 0.059\n",
            "1535/7395000 (epoch 1), train_loss = 1.668, time/batch = 0.059\n",
            "1536/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.058\n",
            "1537/7395000 (epoch 1), train_loss = 1.682, time/batch = 0.062\n",
            "1538/7395000 (epoch 1), train_loss = 1.685, time/batch = 0.060\n",
            "1539/7395000 (epoch 1), train_loss = 1.634, time/batch = 0.060\n",
            "1540/7395000 (epoch 1), train_loss = 1.680, time/batch = 0.060\n",
            "1541/7395000 (epoch 1), train_loss = 1.668, time/batch = 0.061\n",
            "1542/7395000 (epoch 1), train_loss = 1.735, time/batch = 0.060\n",
            "1543/7395000 (epoch 1), train_loss = 1.618, time/batch = 0.060\n",
            "1544/7395000 (epoch 1), train_loss = 1.682, time/batch = 0.060\n",
            "1545/7395000 (epoch 1), train_loss = 1.710, time/batch = 0.060\n",
            "1546/7395000 (epoch 1), train_loss = 1.655, time/batch = 0.060\n",
            "1547/7395000 (epoch 1), train_loss = 1.745, time/batch = 0.072\n",
            "1548/7395000 (epoch 1), train_loss = 1.624, time/batch = 0.060\n",
            "1549/7395000 (epoch 1), train_loss = 1.668, time/batch = 0.058\n",
            "1550/7395000 (epoch 1), train_loss = 1.717, time/batch = 0.060\n",
            "1551/7395000 (epoch 1), train_loss = 1.704, time/batch = 0.060\n",
            "1552/7395000 (epoch 1), train_loss = 1.632, time/batch = 0.060\n",
            "1553/7395000 (epoch 1), train_loss = 1.765, time/batch = 0.061\n",
            "1554/7395000 (epoch 1), train_loss = 1.686, time/batch = 0.059\n",
            "1555/7395000 (epoch 1), train_loss = 1.712, time/batch = 0.061\n",
            "1556/7395000 (epoch 1), train_loss = 1.635, time/batch = 0.058\n",
            "1557/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.064\n",
            "1558/7395000 (epoch 1), train_loss = 1.675, time/batch = 0.060\n",
            "1559/7395000 (epoch 1), train_loss = 1.662, time/batch = 0.059\n",
            "1560/7395000 (epoch 1), train_loss = 1.588, time/batch = 0.060\n",
            "1561/7395000 (epoch 1), train_loss = 1.642, time/batch = 0.059\n",
            "1562/7395000 (epoch 1), train_loss = 1.675, time/batch = 0.060\n",
            "1563/7395000 (epoch 1), train_loss = 1.679, time/batch = 0.061\n",
            "1564/7395000 (epoch 1), train_loss = 1.663, time/batch = 0.062\n",
            "1565/7395000 (epoch 1), train_loss = 1.707, time/batch = 0.062\n",
            "1566/7395000 (epoch 1), train_loss = 1.713, time/batch = 0.058\n",
            "1567/7395000 (epoch 1), train_loss = 1.718, time/batch = 0.070\n",
            "1568/7395000 (epoch 1), train_loss = 1.699, time/batch = 0.058\n",
            "1569/7395000 (epoch 1), train_loss = 1.801, time/batch = 0.059\n",
            "1570/7395000 (epoch 1), train_loss = 1.739, time/batch = 0.060\n",
            "1571/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.060\n",
            "1572/7395000 (epoch 1), train_loss = 1.681, time/batch = 0.073\n",
            "1573/7395000 (epoch 1), train_loss = 1.659, time/batch = 0.061\n",
            "1574/7395000 (epoch 1), train_loss = 1.626, time/batch = 0.059\n",
            "1575/7395000 (epoch 1), train_loss = 1.698, time/batch = 0.058\n",
            "1576/7395000 (epoch 1), train_loss = 1.685, time/batch = 0.058\n",
            "1577/7395000 (epoch 1), train_loss = 1.729, time/batch = 0.064\n",
            "1578/7395000 (epoch 1), train_loss = 1.683, time/batch = 0.058\n",
            "1579/7395000 (epoch 1), train_loss = 1.702, time/batch = 0.059\n",
            "1580/7395000 (epoch 1), train_loss = 1.645, time/batch = 0.059\n",
            "1581/7395000 (epoch 1), train_loss = 1.673, time/batch = 0.064\n",
            "1582/7395000 (epoch 1), train_loss = 1.713, time/batch = 0.058\n",
            "1583/7395000 (epoch 1), train_loss = 1.642, time/batch = 0.060\n",
            "1584/7395000 (epoch 1), train_loss = 1.655, time/batch = 0.059\n",
            "1585/7395000 (epoch 1), train_loss = 1.800, time/batch = 0.065\n",
            "1586/7395000 (epoch 1), train_loss = 1.798, time/batch = 0.060\n",
            "1587/7395000 (epoch 1), train_loss = 1.795, time/batch = 0.060\n",
            "1588/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.055\n",
            "1589/7395000 (epoch 1), train_loss = 1.719, time/batch = 0.061\n",
            "1590/7395000 (epoch 1), train_loss = 1.806, time/batch = 0.061\n",
            "1591/7395000 (epoch 1), train_loss = 1.913, time/batch = 0.059\n",
            "1592/7395000 (epoch 1), train_loss = 1.777, time/batch = 0.058\n",
            "1593/7395000 (epoch 1), train_loss = 1.768, time/batch = 0.059\n",
            "1594/7395000 (epoch 1), train_loss = 1.741, time/batch = 0.056\n",
            "1595/7395000 (epoch 1), train_loss = 1.650, time/batch = 0.067\n",
            "1596/7395000 (epoch 1), train_loss = 1.769, time/batch = 0.058\n",
            "1597/7395000 (epoch 1), train_loss = 1.608, time/batch = 0.061\n",
            "1598/7395000 (epoch 1), train_loss = 1.708, time/batch = 0.060\n",
            "1599/7395000 (epoch 1), train_loss = 1.626, time/batch = 0.059\n",
            "1600/7395000 (epoch 1), train_loss = 1.648, time/batch = 0.059\n",
            "1601/7395000 (epoch 1), train_loss = 1.623, time/batch = 0.066\n",
            "1602/7395000 (epoch 1), train_loss = 1.672, time/batch = 0.060\n",
            "1603/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.061\n",
            "1604/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.059\n",
            "1605/7395000 (epoch 1), train_loss = 1.558, time/batch = 0.061\n",
            "1606/7395000 (epoch 1), train_loss = 1.687, time/batch = 0.060\n",
            "1607/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.061\n",
            "1608/7395000 (epoch 1), train_loss = 1.649, time/batch = 0.059\n",
            "1609/7395000 (epoch 1), train_loss = 1.697, time/batch = 0.059\n",
            "1610/7395000 (epoch 1), train_loss = 1.652, time/batch = 0.063\n",
            "1611/7395000 (epoch 1), train_loss = 1.592, time/batch = 0.061\n",
            "1612/7395000 (epoch 1), train_loss = 1.671, time/batch = 0.060\n",
            "1613/7395000 (epoch 1), train_loss = 1.646, time/batch = 0.062\n",
            "1614/7395000 (epoch 1), train_loss = 1.685, time/batch = 0.061\n",
            "1615/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.066\n",
            "1616/7395000 (epoch 1), train_loss = 1.609, time/batch = 0.060\n",
            "1617/7395000 (epoch 1), train_loss = 1.730, time/batch = 0.060\n",
            "1618/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.059\n",
            "1619/7395000 (epoch 1), train_loss = 1.782, time/batch = 0.059\n",
            "1620/7395000 (epoch 1), train_loss = 1.695, time/batch = 0.062\n",
            "1621/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.059\n",
            "1622/7395000 (epoch 1), train_loss = 1.648, time/batch = 0.058\n",
            "1623/7395000 (epoch 1), train_loss = 1.677, time/batch = 0.059\n",
            "1624/7395000 (epoch 1), train_loss = 1.648, time/batch = 0.059\n",
            "1625/7395000 (epoch 1), train_loss = 1.771, time/batch = 0.062\n",
            "1626/7395000 (epoch 1), train_loss = 1.637, time/batch = 0.060\n",
            "1627/7395000 (epoch 1), train_loss = 1.716, time/batch = 0.063\n",
            "1628/7395000 (epoch 1), train_loss = 1.664, time/batch = 0.057\n",
            "1629/7395000 (epoch 1), train_loss = 1.564, time/batch = 0.061\n",
            "1630/7395000 (epoch 1), train_loss = 1.707, time/batch = 0.062\n",
            "1631/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.062\n",
            "1632/7395000 (epoch 1), train_loss = 1.568, time/batch = 0.057\n",
            "1633/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.061\n",
            "1634/7395000 (epoch 1), train_loss = 1.512, time/batch = 0.061\n",
            "1635/7395000 (epoch 1), train_loss = 1.685, time/batch = 0.060\n",
            "1636/7395000 (epoch 1), train_loss = 1.614, time/batch = 0.058\n",
            "1637/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.060\n",
            "1638/7395000 (epoch 1), train_loss = 1.584, time/batch = 0.060\n",
            "1639/7395000 (epoch 1), train_loss = 1.676, time/batch = 0.065\n",
            "1640/7395000 (epoch 1), train_loss = 1.661, time/batch = 0.060\n",
            "1641/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.059\n",
            "1642/7395000 (epoch 1), train_loss = 1.599, time/batch = 0.059\n",
            "1643/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.060\n",
            "1644/7395000 (epoch 1), train_loss = 1.617, time/batch = 0.059\n",
            "1645/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.059\n",
            "1646/7395000 (epoch 1), train_loss = 1.695, time/batch = 0.057\n",
            "1647/7395000 (epoch 1), train_loss = 1.562, time/batch = 0.063\n",
            "1648/7395000 (epoch 1), train_loss = 1.643, time/batch = 0.060\n",
            "1649/7395000 (epoch 1), train_loss = 1.578, time/batch = 0.064\n",
            "1650/7395000 (epoch 1), train_loss = 1.607, time/batch = 0.062\n",
            "1651/7395000 (epoch 1), train_loss = 1.620, time/batch = 0.062\n",
            "1652/7395000 (epoch 1), train_loss = 1.600, time/batch = 0.064\n",
            "1653/7395000 (epoch 1), train_loss = 1.673, time/batch = 0.061\n",
            "1654/7395000 (epoch 1), train_loss = 1.652, time/batch = 0.058\n",
            "1655/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.061\n",
            "1656/7395000 (epoch 1), train_loss = 1.614, time/batch = 0.058\n",
            "1657/7395000 (epoch 1), train_loss = 1.531, time/batch = 0.059\n",
            "1658/7395000 (epoch 1), train_loss = 1.609, time/batch = 0.058\n",
            "1659/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.061\n",
            "1660/7395000 (epoch 1), train_loss = 1.530, time/batch = 0.058\n",
            "1661/7395000 (epoch 1), train_loss = 1.499, time/batch = 0.060\n",
            "1662/7395000 (epoch 1), train_loss = 1.657, time/batch = 0.063\n",
            "1663/7395000 (epoch 1), train_loss = 1.612, time/batch = 0.062\n",
            "1664/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.059\n",
            "1665/7395000 (epoch 1), train_loss = 1.657, time/batch = 0.061\n",
            "1666/7395000 (epoch 1), train_loss = 1.622, time/batch = 0.057\n",
            "1667/7395000 (epoch 1), train_loss = 1.662, time/batch = 0.059\n",
            "1668/7395000 (epoch 1), train_loss = 1.647, time/batch = 0.061\n",
            "1669/7395000 (epoch 1), train_loss = 1.694, time/batch = 0.068\n",
            "1670/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.060\n",
            "1671/7395000 (epoch 1), train_loss = 1.658, time/batch = 0.061\n",
            "1672/7395000 (epoch 1), train_loss = 1.614, time/batch = 0.064\n",
            "1673/7395000 (epoch 1), train_loss = 1.683, time/batch = 0.059\n",
            "1674/7395000 (epoch 1), train_loss = 1.661, time/batch = 0.060\n",
            "1675/7395000 (epoch 1), train_loss = 1.744, time/batch = 0.059\n",
            "1676/7395000 (epoch 1), train_loss = 1.731, time/batch = 0.060\n",
            "1677/7395000 (epoch 1), train_loss = 1.656, time/batch = 0.065\n",
            "1678/7395000 (epoch 1), train_loss = 1.664, time/batch = 0.058\n",
            "1679/7395000 (epoch 1), train_loss = 1.680, time/batch = 0.060\n",
            "1680/7395000 (epoch 1), train_loss = 1.621, time/batch = 0.058\n",
            "1681/7395000 (epoch 1), train_loss = 1.571, time/batch = 0.061\n",
            "1682/7395000 (epoch 1), train_loss = 1.625, time/batch = 0.057\n",
            "1683/7395000 (epoch 1), train_loss = 1.685, time/batch = 0.060\n",
            "1684/7395000 (epoch 1), train_loss = 1.632, time/batch = 0.058\n",
            "1685/7395000 (epoch 1), train_loss = 1.741, time/batch = 0.065\n",
            "1686/7395000 (epoch 1), train_loss = 1.727, time/batch = 0.058\n",
            "1687/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.062\n",
            "1688/7395000 (epoch 1), train_loss = 1.708, time/batch = 0.058\n",
            "1689/7395000 (epoch 1), train_loss = 1.679, time/batch = 0.059\n",
            "1690/7395000 (epoch 1), train_loss = 1.719, time/batch = 0.059\n",
            "1691/7395000 (epoch 1), train_loss = 1.734, time/batch = 0.062\n",
            "1692/7395000 (epoch 1), train_loss = 1.636, time/batch = 0.061\n",
            "1693/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.060\n",
            "1694/7395000 (epoch 1), train_loss = 1.628, time/batch = 0.060\n",
            "1695/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.061\n",
            "1696/7395000 (epoch 1), train_loss = 1.661, time/batch = 0.061\n",
            "1697/7395000 (epoch 1), train_loss = 1.665, time/batch = 0.062\n",
            "1698/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.059\n",
            "1699/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.061\n",
            "1700/7395000 (epoch 1), train_loss = 1.742, time/batch = 0.058\n",
            "1701/7395000 (epoch 1), train_loss = 1.656, time/batch = 0.053\n",
            "1702/7395000 (epoch 1), train_loss = 1.692, time/batch = 0.053\n",
            "1703/7395000 (epoch 1), train_loss = 1.589, time/batch = 0.053\n",
            "1704/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.057\n",
            "1705/7395000 (epoch 1), train_loss = 1.584, time/batch = 0.054\n",
            "1706/7395000 (epoch 1), train_loss = 1.584, time/batch = 0.059\n",
            "1707/7395000 (epoch 1), train_loss = 1.590, time/batch = 0.061\n",
            "1708/7395000 (epoch 1), train_loss = 1.587, time/batch = 0.063\n",
            "1709/7395000 (epoch 1), train_loss = 1.699, time/batch = 0.063\n",
            "1710/7395000 (epoch 1), train_loss = 1.688, time/batch = 0.058\n",
            "1711/7395000 (epoch 1), train_loss = 1.628, time/batch = 0.061\n",
            "1712/7395000 (epoch 1), train_loss = 1.563, time/batch = 0.060\n",
            "1713/7395000 (epoch 1), train_loss = 1.736, time/batch = 0.057\n",
            "1714/7395000 (epoch 1), train_loss = 1.624, time/batch = 0.062\n",
            "1715/7395000 (epoch 1), train_loss = 1.561, time/batch = 0.061\n",
            "1716/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.059\n",
            "1717/7395000 (epoch 1), train_loss = 1.527, time/batch = 0.063\n",
            "1718/7395000 (epoch 1), train_loss = 1.663, time/batch = 0.056\n",
            "1719/7395000 (epoch 1), train_loss = 1.664, time/batch = 0.062\n",
            "1720/7395000 (epoch 1), train_loss = 1.565, time/batch = 0.060\n",
            "1721/7395000 (epoch 1), train_loss = 1.621, time/batch = 0.059\n",
            "1722/7395000 (epoch 1), train_loss = 1.636, time/batch = 0.060\n",
            "1723/7395000 (epoch 1), train_loss = 1.678, time/batch = 0.058\n",
            "1724/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.059\n",
            "1725/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.059\n",
            "1726/7395000 (epoch 1), train_loss = 1.596, time/batch = 0.062\n",
            "1727/7395000 (epoch 1), train_loss = 1.656, time/batch = 0.061\n",
            "1728/7395000 (epoch 1), train_loss = 1.731, time/batch = 0.060\n",
            "1729/7395000 (epoch 1), train_loss = 1.723, time/batch = 0.060\n",
            "1730/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.059\n",
            "1731/7395000 (epoch 1), train_loss = 1.681, time/batch = 0.063\n",
            "1732/7395000 (epoch 1), train_loss = 1.668, time/batch = 0.058\n",
            "1733/7395000 (epoch 1), train_loss = 1.650, time/batch = 0.062\n",
            "1734/7395000 (epoch 1), train_loss = 1.696, time/batch = 0.066\n",
            "1735/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.061\n",
            "1736/7395000 (epoch 1), train_loss = 1.593, time/batch = 0.063\n",
            "1737/7395000 (epoch 1), train_loss = 1.780, time/batch = 0.061\n",
            "1738/7395000 (epoch 1), train_loss = 1.724, time/batch = 0.060\n",
            "1739/7395000 (epoch 1), train_loss = 1.710, time/batch = 0.060\n",
            "1740/7395000 (epoch 1), train_loss = 1.727, time/batch = 0.058\n",
            "1741/7395000 (epoch 1), train_loss = 1.710, time/batch = 0.062\n",
            "1742/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.058\n",
            "1743/7395000 (epoch 1), train_loss = 1.726, time/batch = 0.061\n",
            "1744/7395000 (epoch 1), train_loss = 1.620, time/batch = 0.059\n",
            "1745/7395000 (epoch 1), train_loss = 1.635, time/batch = 0.059\n",
            "1746/7395000 (epoch 1), train_loss = 1.649, time/batch = 0.060\n",
            "1747/7395000 (epoch 1), train_loss = 1.772, time/batch = 0.061\n",
            "1748/7395000 (epoch 1), train_loss = 1.727, time/batch = 0.061\n",
            "1749/7395000 (epoch 1), train_loss = 1.666, time/batch = 0.060\n",
            "1750/7395000 (epoch 1), train_loss = 1.608, time/batch = 0.059\n",
            "1751/7395000 (epoch 1), train_loss = 1.595, time/batch = 0.070\n",
            "1752/7395000 (epoch 1), train_loss = 1.601, time/batch = 0.052\n",
            "1753/7395000 (epoch 1), train_loss = 1.644, time/batch = 0.056\n",
            "1754/7395000 (epoch 1), train_loss = 1.662, time/batch = 0.053\n",
            "1755/7395000 (epoch 1), train_loss = 1.708, time/batch = 0.064\n",
            "1756/7395000 (epoch 1), train_loss = 1.733, time/batch = 0.061\n",
            "1757/7395000 (epoch 1), train_loss = 1.755, time/batch = 0.066\n",
            "1758/7395000 (epoch 1), train_loss = 1.744, time/batch = 0.061\n",
            "1759/7395000 (epoch 1), train_loss = 1.746, time/batch = 0.058\n",
            "1760/7395000 (epoch 1), train_loss = 1.741, time/batch = 0.058\n",
            "1761/7395000 (epoch 1), train_loss = 1.623, time/batch = 0.060\n",
            "1762/7395000 (epoch 1), train_loss = 1.693, time/batch = 0.059\n",
            "1763/7395000 (epoch 1), train_loss = 1.497, time/batch = 0.061\n",
            "1764/7395000 (epoch 1), train_loss = 1.616, time/batch = 0.060\n",
            "1765/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.055\n",
            "1766/7395000 (epoch 1), train_loss = 1.603, time/batch = 0.060\n",
            "1767/7395000 (epoch 1), train_loss = 1.606, time/batch = 0.061\n",
            "1768/7395000 (epoch 1), train_loss = 1.662, time/batch = 0.064\n",
            "1769/7395000 (epoch 1), train_loss = 1.759, time/batch = 0.061\n",
            "1770/7395000 (epoch 1), train_loss = 1.697, time/batch = 0.054\n",
            "1771/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.061\n",
            "1772/7395000 (epoch 1), train_loss = 1.723, time/batch = 0.058\n",
            "1773/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.059\n",
            "1774/7395000 (epoch 1), train_loss = 1.671, time/batch = 0.059\n",
            "1775/7395000 (epoch 1), train_loss = 1.620, time/batch = 0.059\n",
            "1776/7395000 (epoch 1), train_loss = 1.604, time/batch = 0.058\n",
            "1777/7395000 (epoch 1), train_loss = 1.687, time/batch = 0.060\n",
            "1778/7395000 (epoch 1), train_loss = 1.688, time/batch = 0.059\n",
            "1779/7395000 (epoch 1), train_loss = 1.619, time/batch = 0.061\n",
            "1780/7395000 (epoch 1), train_loss = 1.702, time/batch = 0.065\n",
            "1781/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.061\n",
            "1782/7395000 (epoch 1), train_loss = 1.678, time/batch = 0.060\n",
            "1783/7395000 (epoch 1), train_loss = 1.568, time/batch = 0.057\n",
            "1784/7395000 (epoch 1), train_loss = 1.636, time/batch = 0.059\n",
            "1785/7395000 (epoch 1), train_loss = 1.691, time/batch = 0.063\n",
            "1786/7395000 (epoch 1), train_loss = 1.674, time/batch = 0.060\n",
            "1787/7395000 (epoch 1), train_loss = 1.642, time/batch = 0.060\n",
            "1788/7395000 (epoch 1), train_loss = 1.629, time/batch = 0.061\n",
            "1789/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.061\n",
            "1790/7395000 (epoch 1), train_loss = 1.711, time/batch = 0.063\n",
            "1791/7395000 (epoch 1), train_loss = 1.636, time/batch = 0.061\n",
            "1792/7395000 (epoch 1), train_loss = 1.608, time/batch = 0.059\n",
            "1793/7395000 (epoch 1), train_loss = 1.590, time/batch = 0.061\n",
            "1794/7395000 (epoch 1), train_loss = 1.746, time/batch = 0.059\n",
            "1795/7395000 (epoch 1), train_loss = 1.632, time/batch = 0.062\n",
            "1796/7395000 (epoch 1), train_loss = 1.741, time/batch = 0.060\n",
            "1797/7395000 (epoch 1), train_loss = 1.672, time/batch = 0.061\n",
            "1798/7395000 (epoch 1), train_loss = 1.639, time/batch = 0.060\n",
            "1799/7395000 (epoch 1), train_loss = 1.703, time/batch = 0.058\n",
            "1800/7395000 (epoch 1), train_loss = 1.715, time/batch = 0.060\n",
            "1801/7395000 (epoch 1), train_loss = 1.639, time/batch = 0.064\n",
            "1802/7395000 (epoch 1), train_loss = 1.697, time/batch = 0.063\n",
            "1803/7395000 (epoch 1), train_loss = 1.628, time/batch = 0.061\n",
            "1804/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.059\n",
            "1805/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.060\n",
            "1806/7395000 (epoch 1), train_loss = 1.612, time/batch = 0.058\n",
            "1807/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.064\n",
            "1808/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.058\n",
            "1809/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.054\n",
            "1810/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.055\n",
            "1811/7395000 (epoch 1), train_loss = 1.609, time/batch = 0.053\n",
            "1812/7395000 (epoch 1), train_loss = 1.599, time/batch = 0.053\n",
            "1813/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.054\n",
            "1814/7395000 (epoch 1), train_loss = 1.545, time/batch = 0.055\n",
            "1815/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.053\n",
            "1816/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.056\n",
            "1817/7395000 (epoch 1), train_loss = 1.553, time/batch = 0.053\n",
            "1818/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.053\n",
            "1819/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.053\n",
            "1820/7395000 (epoch 1), train_loss = 1.546, time/batch = 0.071\n",
            "1821/7395000 (epoch 1), train_loss = 1.535, time/batch = 0.058\n",
            "1822/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.053\n",
            "1823/7395000 (epoch 1), train_loss = 1.518, time/batch = 0.054\n",
            "1824/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.054\n",
            "1825/7395000 (epoch 1), train_loss = 1.673, time/batch = 0.061\n",
            "1826/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.072\n",
            "1827/7395000 (epoch 1), train_loss = 1.516, time/batch = 0.061\n",
            "1828/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.060\n",
            "1829/7395000 (epoch 1), train_loss = 1.544, time/batch = 0.064\n",
            "1830/7395000 (epoch 1), train_loss = 1.642, time/batch = 0.057\n",
            "1831/7395000 (epoch 1), train_loss = 1.584, time/batch = 0.060\n",
            "1832/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.063\n",
            "1833/7395000 (epoch 1), train_loss = 1.646, time/batch = 0.061\n",
            "1834/7395000 (epoch 1), train_loss = 1.649, time/batch = 0.063\n",
            "1835/7395000 (epoch 1), train_loss = 1.532, time/batch = 0.059\n",
            "1836/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.060\n",
            "1837/7395000 (epoch 1), train_loss = 1.539, time/batch = 0.063\n",
            "1838/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.054\n",
            "1839/7395000 (epoch 1), train_loss = 1.616, time/batch = 0.062\n",
            "1840/7395000 (epoch 1), train_loss = 1.588, time/batch = 0.065\n",
            "1841/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.058\n",
            "1842/7395000 (epoch 1), train_loss = 1.563, time/batch = 0.059\n",
            "1843/7395000 (epoch 1), train_loss = 1.622, time/batch = 0.058\n",
            "1844/7395000 (epoch 1), train_loss = 1.571, time/batch = 0.060\n",
            "1845/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.061\n",
            "1846/7395000 (epoch 1), train_loss = 1.633, time/batch = 0.062\n",
            "1847/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.058\n",
            "1848/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.057\n",
            "1849/7395000 (epoch 1), train_loss = 1.642, time/batch = 0.060\n",
            "1850/7395000 (epoch 1), train_loss = 1.643, time/batch = 0.060\n",
            "1851/7395000 (epoch 1), train_loss = 1.590, time/batch = 0.065\n",
            "1852/7395000 (epoch 1), train_loss = 1.571, time/batch = 0.063\n",
            "1853/7395000 (epoch 1), train_loss = 1.575, time/batch = 0.062\n",
            "1854/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.062\n",
            "1855/7395000 (epoch 1), train_loss = 1.506, time/batch = 0.059\n",
            "1856/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.059\n",
            "1857/7395000 (epoch 1), train_loss = 1.502, time/batch = 0.057\n",
            "1858/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.058\n",
            "1859/7395000 (epoch 1), train_loss = 1.538, time/batch = 0.063\n",
            "1860/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.060\n",
            "1861/7395000 (epoch 1), train_loss = 1.627, time/batch = 0.061\n",
            "1862/7395000 (epoch 1), train_loss = 1.503, time/batch = 0.060\n",
            "1863/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.062\n",
            "1864/7395000 (epoch 1), train_loss = 1.536, time/batch = 0.061\n",
            "1865/7395000 (epoch 1), train_loss = 1.718, time/batch = 0.060\n",
            "1866/7395000 (epoch 1), train_loss = 1.522, time/batch = 0.060\n",
            "1867/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.058\n",
            "1868/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.060\n",
            "1869/7395000 (epoch 1), train_loss = 1.597, time/batch = 0.061\n",
            "1870/7395000 (epoch 1), train_loss = 1.593, time/batch = 0.060\n",
            "1871/7395000 (epoch 1), train_loss = 1.512, time/batch = 0.070\n",
            "1872/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.061\n",
            "1873/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.056\n",
            "1874/7395000 (epoch 1), train_loss = 1.572, time/batch = 0.059\n",
            "1875/7395000 (epoch 1), train_loss = 1.582, time/batch = 0.060\n",
            "1876/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.059\n",
            "1877/7395000 (epoch 1), train_loss = 1.580, time/batch = 0.061\n",
            "1878/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.060\n",
            "1879/7395000 (epoch 1), train_loss = 1.582, time/batch = 0.060\n",
            "1880/7395000 (epoch 1), train_loss = 1.621, time/batch = 0.057\n",
            "1881/7395000 (epoch 1), train_loss = 1.587, time/batch = 0.060\n",
            "1882/7395000 (epoch 1), train_loss = 1.657, time/batch = 0.059\n",
            "1883/7395000 (epoch 1), train_loss = 1.618, time/batch = 0.061\n",
            "1884/7395000 (epoch 1), train_loss = 1.729, time/batch = 0.065\n",
            "1885/7395000 (epoch 1), train_loss = 1.592, time/batch = 0.060\n",
            "1886/7395000 (epoch 1), train_loss = 1.559, time/batch = 0.059\n",
            "1887/7395000 (epoch 1), train_loss = 1.555, time/batch = 0.059\n",
            "1888/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.065\n",
            "1889/7395000 (epoch 1), train_loss = 1.670, time/batch = 0.062\n",
            "1890/7395000 (epoch 1), train_loss = 1.597, time/batch = 0.061\n",
            "1891/7395000 (epoch 1), train_loss = 1.475, time/batch = 0.062\n",
            "1892/7395000 (epoch 1), train_loss = 1.514, time/batch = 0.060\n",
            "1893/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.058\n",
            "1894/7395000 (epoch 1), train_loss = 1.448, time/batch = 0.058\n",
            "1895/7395000 (epoch 1), train_loss = 1.618, time/batch = 0.060\n",
            "1896/7395000 (epoch 1), train_loss = 1.482, time/batch = 0.070\n",
            "1897/7395000 (epoch 1), train_loss = 1.582, time/batch = 0.061\n",
            "1898/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.059\n",
            "1899/7395000 (epoch 1), train_loss = 1.622, time/batch = 0.059\n",
            "1900/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.058\n",
            "1901/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.060\n",
            "1902/7395000 (epoch 1), train_loss = 1.580, time/batch = 0.062\n",
            "1903/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.058\n",
            "1904/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.061\n",
            "1905/7395000 (epoch 1), train_loss = 1.508, time/batch = 0.070\n",
            "1906/7395000 (epoch 1), train_loss = 1.546, time/batch = 0.059\n",
            "1907/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.063\n",
            "1908/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.057\n",
            "1909/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.062\n",
            "1910/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.059\n",
            "1911/7395000 (epoch 1), train_loss = 1.664, time/batch = 0.062\n",
            "1912/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.060\n",
            "1913/7395000 (epoch 1), train_loss = 1.620, time/batch = 0.061\n",
            "1914/7395000 (epoch 1), train_loss = 1.616, time/batch = 0.057\n",
            "1915/7395000 (epoch 1), train_loss = 1.660, time/batch = 0.061\n",
            "1916/7395000 (epoch 1), train_loss = 1.557, time/batch = 0.064\n",
            "1917/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.063\n",
            "1918/7395000 (epoch 1), train_loss = 1.574, time/batch = 0.058\n",
            "1919/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.059\n",
            "1920/7395000 (epoch 1), train_loss = 1.570, time/batch = 0.058\n",
            "1921/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.061\n",
            "1922/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.061\n",
            "1923/7395000 (epoch 1), train_loss = 1.572, time/batch = 0.061\n",
            "1924/7395000 (epoch 1), train_loss = 1.450, time/batch = 0.060\n",
            "1925/7395000 (epoch 1), train_loss = 1.589, time/batch = 0.059\n",
            "1926/7395000 (epoch 1), train_loss = 1.463, time/batch = 0.059\n",
            "1927/7395000 (epoch 1), train_loss = 1.467, time/batch = 0.059\n",
            "1928/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.057\n",
            "1929/7395000 (epoch 1), train_loss = 1.565, time/batch = 0.060\n",
            "1930/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.058\n",
            "1931/7395000 (epoch 1), train_loss = 1.500, time/batch = 0.064\n",
            "1932/7395000 (epoch 1), train_loss = 1.533, time/batch = 0.058\n",
            "1933/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.059\n",
            "1934/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.058\n",
            "1935/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.070\n",
            "1936/7395000 (epoch 1), train_loss = 1.573, time/batch = 0.060\n",
            "1937/7395000 (epoch 1), train_loss = 1.533, time/batch = 0.062\n",
            "1938/7395000 (epoch 1), train_loss = 1.547, time/batch = 0.062\n",
            "1939/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.072\n",
            "1940/7395000 (epoch 1), train_loss = 1.547, time/batch = 0.060\n",
            "1941/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.061\n",
            "1942/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.062\n",
            "1943/7395000 (epoch 1), train_loss = 1.485, time/batch = 0.060\n",
            "1944/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.060\n",
            "1945/7395000 (epoch 1), train_loss = 1.612, time/batch = 0.062\n",
            "1946/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.057\n",
            "1947/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.064\n",
            "1948/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.058\n",
            "1949/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.065\n",
            "1950/7395000 (epoch 1), train_loss = 1.557, time/batch = 0.060\n",
            "1951/7395000 (epoch 1), train_loss = 1.615, time/batch = 0.060\n",
            "1952/7395000 (epoch 1), train_loss = 1.597, time/batch = 0.059\n",
            "1953/7395000 (epoch 1), train_loss = 1.539, time/batch = 0.060\n",
            "1954/7395000 (epoch 1), train_loss = 1.655, time/batch = 0.054\n",
            "1955/7395000 (epoch 1), train_loss = 1.624, time/batch = 0.065\n",
            "1956/7395000 (epoch 1), train_loss = 1.643, time/batch = 0.057\n",
            "1957/7395000 (epoch 1), train_loss = 1.563, time/batch = 0.061\n",
            "1958/7395000 (epoch 1), train_loss = 1.553, time/batch = 0.058\n",
            "1959/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.061\n",
            "1960/7395000 (epoch 1), train_loss = 1.633, time/batch = 0.060\n",
            "1961/7395000 (epoch 1), train_loss = 1.691, time/batch = 0.062\n",
            "1962/7395000 (epoch 1), train_loss = 1.657, time/batch = 0.059\n",
            "1963/7395000 (epoch 1), train_loss = 1.603, time/batch = 0.059\n",
            "1964/7395000 (epoch 1), train_loss = 1.553, time/batch = 0.063\n",
            "1965/7395000 (epoch 1), train_loss = 1.635, time/batch = 0.062\n",
            "1966/7395000 (epoch 1), train_loss = 1.580, time/batch = 0.059\n",
            "1967/7395000 (epoch 1), train_loss = 1.657, time/batch = 0.061\n",
            "1968/7395000 (epoch 1), train_loss = 1.608, time/batch = 0.060\n",
            "1969/7395000 (epoch 1), train_loss = 1.556, time/batch = 0.062\n",
            "1970/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.067\n",
            "1971/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.059\n",
            "1972/7395000 (epoch 1), train_loss = 1.573, time/batch = 0.060\n",
            "1973/7395000 (epoch 1), train_loss = 1.671, time/batch = 0.057\n",
            "1974/7395000 (epoch 1), train_loss = 1.708, time/batch = 0.061\n",
            "1975/7395000 (epoch 1), train_loss = 1.687, time/batch = 0.062\n",
            "1976/7395000 (epoch 1), train_loss = 1.643, time/batch = 0.059\n",
            "1977/7395000 (epoch 1), train_loss = 1.792, time/batch = 0.062\n",
            "1978/7395000 (epoch 1), train_loss = 1.643, time/batch = 0.064\n",
            "1979/7395000 (epoch 1), train_loss = 1.599, time/batch = 0.060\n",
            "1980/7395000 (epoch 1), train_loss = 1.714, time/batch = 0.060\n",
            "1981/7395000 (epoch 1), train_loss = 1.619, time/batch = 0.058\n",
            "1982/7395000 (epoch 1), train_loss = 1.606, time/batch = 0.063\n",
            "1983/7395000 (epoch 1), train_loss = 1.540, time/batch = 0.060\n",
            "1984/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.058\n",
            "1985/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.059\n",
            "1986/7395000 (epoch 1), train_loss = 1.568, time/batch = 0.060\n",
            "1987/7395000 (epoch 1), train_loss = 1.612, time/batch = 0.062\n",
            "1988/7395000 (epoch 1), train_loss = 1.704, time/batch = 0.058\n",
            "1989/7395000 (epoch 1), train_loss = 1.618, time/batch = 0.064\n",
            "1990/7395000 (epoch 1), train_loss = 1.665, time/batch = 0.058\n",
            "1991/7395000 (epoch 1), train_loss = 1.762, time/batch = 0.059\n",
            "1992/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.062\n",
            "1993/7395000 (epoch 1), train_loss = 1.707, time/batch = 0.062\n",
            "1994/7395000 (epoch 1), train_loss = 1.585, time/batch = 0.057\n",
            "1995/7395000 (epoch 1), train_loss = 1.636, time/batch = 0.059\n",
            "1996/7395000 (epoch 1), train_loss = 1.711, time/batch = 0.058\n",
            "1997/7395000 (epoch 1), train_loss = 1.635, time/batch = 0.060\n",
            "1998/7395000 (epoch 1), train_loss = 1.641, time/batch = 0.058\n",
            "1999/7395000 (epoch 1), train_loss = 1.655, time/batch = 0.060\n",
            "2000/7395000 (epoch 1), train_loss = 1.654, time/batch = 0.059\n",
            "model saved to save/model.ckpt\n",
            "2001/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.060\n",
            "2002/7395000 (epoch 1), train_loss = 1.623, time/batch = 0.063\n",
            "2003/7395000 (epoch 1), train_loss = 1.600, time/batch = 0.058\n",
            "2004/7395000 (epoch 1), train_loss = 1.531, time/batch = 0.058\n",
            "2005/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.057\n",
            "2006/7395000 (epoch 1), train_loss = 1.652, time/batch = 0.061\n",
            "2007/7395000 (epoch 1), train_loss = 1.648, time/batch = 0.064\n",
            "2008/7395000 (epoch 1), train_loss = 1.628, time/batch = 0.062\n",
            "2009/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.058\n",
            "2010/7395000 (epoch 1), train_loss = 1.631, time/batch = 0.059\n",
            "2011/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.061\n",
            "2012/7395000 (epoch 1), train_loss = 1.635, time/batch = 0.060\n",
            "2013/7395000 (epoch 1), train_loss = 1.632, time/batch = 0.061\n",
            "2014/7395000 (epoch 1), train_loss = 1.602, time/batch = 0.059\n",
            "2015/7395000 (epoch 1), train_loss = 1.624, time/batch = 0.061\n",
            "2016/7395000 (epoch 1), train_loss = 1.611, time/batch = 0.060\n",
            "2017/7395000 (epoch 1), train_loss = 1.581, time/batch = 0.062\n",
            "2018/7395000 (epoch 1), train_loss = 1.467, time/batch = 0.065\n",
            "2019/7395000 (epoch 1), train_loss = 1.653, time/batch = 0.065\n",
            "2020/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.060\n",
            "2021/7395000 (epoch 1), train_loss = 1.559, time/batch = 0.060\n",
            "2022/7395000 (epoch 1), train_loss = 1.580, time/batch = 0.059\n",
            "2023/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.059\n",
            "2024/7395000 (epoch 1), train_loss = 1.514, time/batch = 0.058\n",
            "2025/7395000 (epoch 1), train_loss = 1.461, time/batch = 0.061\n",
            "2026/7395000 (epoch 1), train_loss = 1.630, time/batch = 0.058\n",
            "2027/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.061\n",
            "2028/7395000 (epoch 1), train_loss = 1.621, time/batch = 0.060\n",
            "2029/7395000 (epoch 1), train_loss = 1.497, time/batch = 0.061\n",
            "2030/7395000 (epoch 1), train_loss = 1.538, time/batch = 0.060\n",
            "2031/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.059\n",
            "2032/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.061\n",
            "2033/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.060\n",
            "2034/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.060\n",
            "2035/7395000 (epoch 1), train_loss = 1.495, time/batch = 0.060\n",
            "2036/7395000 (epoch 1), train_loss = 1.491, time/batch = 0.063\n",
            "2037/7395000 (epoch 1), train_loss = 1.585, time/batch = 0.054\n",
            "2038/7395000 (epoch 1), train_loss = 1.592, time/batch = 0.059\n",
            "2039/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.056\n",
            "2040/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.054\n",
            "2041/7395000 (epoch 1), train_loss = 1.430, time/batch = 0.052\n",
            "2042/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.053\n",
            "2043/7395000 (epoch 1), train_loss = 1.506, time/batch = 0.053\n",
            "2044/7395000 (epoch 1), train_loss = 1.535, time/batch = 0.056\n",
            "2045/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.061\n",
            "2046/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.053\n",
            "2047/7395000 (epoch 1), train_loss = 1.430, time/batch = 0.054\n",
            "2048/7395000 (epoch 1), train_loss = 1.501, time/batch = 0.053\n",
            "2049/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.057\n",
            "2050/7395000 (epoch 1), train_loss = 1.571, time/batch = 0.053\n",
            "2051/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.054\n",
            "2052/7395000 (epoch 1), train_loss = 1.552, time/batch = 0.058\n",
            "2053/7395000 (epoch 1), train_loss = 1.570, time/batch = 0.059\n",
            "2054/7395000 (epoch 1), train_loss = 1.663, time/batch = 0.058\n",
            "2055/7395000 (epoch 1), train_loss = 1.449, time/batch = 0.060\n",
            "2056/7395000 (epoch 1), train_loss = 1.581, time/batch = 0.059\n",
            "2057/7395000 (epoch 1), train_loss = 1.594, time/batch = 0.059\n",
            "2058/7395000 (epoch 1), train_loss = 1.503, time/batch = 0.062\n",
            "2059/7395000 (epoch 1), train_loss = 1.555, time/batch = 0.059\n",
            "2060/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.055\n",
            "2061/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.061\n",
            "2062/7395000 (epoch 1), train_loss = 1.514, time/batch = 0.059\n",
            "2063/7395000 (epoch 1), train_loss = 1.538, time/batch = 0.061\n",
            "2064/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.060\n",
            "2065/7395000 (epoch 1), train_loss = 1.572, time/batch = 0.060\n",
            "2066/7395000 (epoch 1), train_loss = 1.648, time/batch = 0.061\n",
            "2067/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.061\n",
            "2068/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.060\n",
            "2069/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.061\n",
            "2070/7395000 (epoch 1), train_loss = 1.570, time/batch = 0.055\n",
            "2071/7395000 (epoch 1), train_loss = 1.538, time/batch = 0.060\n",
            "2072/7395000 (epoch 1), train_loss = 1.522, time/batch = 0.060\n",
            "2073/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.061\n",
            "2074/7395000 (epoch 1), train_loss = 1.566, time/batch = 0.059\n",
            "2075/7395000 (epoch 1), train_loss = 1.581, time/batch = 0.061\n",
            "2076/7395000 (epoch 1), train_loss = 1.603, time/batch = 0.057\n",
            "2077/7395000 (epoch 1), train_loss = 1.621, time/batch = 0.060\n",
            "2078/7395000 (epoch 1), train_loss = 1.537, time/batch = 0.062\n",
            "2079/7395000 (epoch 1), train_loss = 1.598, time/batch = 0.061\n",
            "2080/7395000 (epoch 1), train_loss = 1.533, time/batch = 0.062\n",
            "2081/7395000 (epoch 1), train_loss = 1.589, time/batch = 0.060\n",
            "2082/7395000 (epoch 1), train_loss = 1.547, time/batch = 0.058\n",
            "2083/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.062\n",
            "2084/7395000 (epoch 1), train_loss = 1.617, time/batch = 0.062\n",
            "2085/7395000 (epoch 1), train_loss = 1.669, time/batch = 0.060\n",
            "2086/7395000 (epoch 1), train_loss = 1.596, time/batch = 0.060\n",
            "2087/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.060\n",
            "2088/7395000 (epoch 1), train_loss = 1.623, time/batch = 0.060\n",
            "2089/7395000 (epoch 1), train_loss = 1.626, time/batch = 0.066\n",
            "2090/7395000 (epoch 1), train_loss = 1.553, time/batch = 0.061\n",
            "2091/7395000 (epoch 1), train_loss = 1.467, time/batch = 0.063\n",
            "2092/7395000 (epoch 1), train_loss = 1.609, time/batch = 0.059\n",
            "2093/7395000 (epoch 1), train_loss = 1.501, time/batch = 0.062\n",
            "2094/7395000 (epoch 1), train_loss = 1.624, time/batch = 0.057\n",
            "2095/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.060\n",
            "2096/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.066\n",
            "2097/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.059\n",
            "2098/7395000 (epoch 1), train_loss = 1.483, time/batch = 0.058\n",
            "2099/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.059\n",
            "2100/7395000 (epoch 1), train_loss = 1.421, time/batch = 0.057\n",
            "2101/7395000 (epoch 1), train_loss = 1.483, time/batch = 0.059\n",
            "2102/7395000 (epoch 1), train_loss = 1.424, time/batch = 0.058\n",
            "2103/7395000 (epoch 1), train_loss = 1.554, time/batch = 0.059\n",
            "2104/7395000 (epoch 1), train_loss = 1.559, time/batch = 0.057\n",
            "2105/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.063\n",
            "2106/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.067\n",
            "2107/7395000 (epoch 1), train_loss = 1.516, time/batch = 0.062\n",
            "2108/7395000 (epoch 1), train_loss = 1.546, time/batch = 0.059\n",
            "2109/7395000 (epoch 1), train_loss = 1.592, time/batch = 0.067\n",
            "2110/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.057\n",
            "2111/7395000 (epoch 1), train_loss = 1.593, time/batch = 0.074\n",
            "2112/7395000 (epoch 1), train_loss = 1.472, time/batch = 0.057\n",
            "2113/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.065\n",
            "2114/7395000 (epoch 1), train_loss = 1.455, time/batch = 0.059\n",
            "2115/7395000 (epoch 1), train_loss = 1.560, time/batch = 0.060\n",
            "2116/7395000 (epoch 1), train_loss = 1.417, time/batch = 0.058\n",
            "2117/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.059\n",
            "2118/7395000 (epoch 1), train_loss = 1.522, time/batch = 0.066\n",
            "2119/7395000 (epoch 1), train_loss = 1.527, time/batch = 0.062\n",
            "2120/7395000 (epoch 1), train_loss = 1.589, time/batch = 0.060\n",
            "2121/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.059\n",
            "2122/7395000 (epoch 1), train_loss = 1.554, time/batch = 0.057\n",
            "2123/7395000 (epoch 1), train_loss = 1.601, time/batch = 0.060\n",
            "2124/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.065\n",
            "2125/7395000 (epoch 1), train_loss = 1.575, time/batch = 0.061\n",
            "2126/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.061\n",
            "2127/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.058\n",
            "2128/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.060\n",
            "2129/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.062\n",
            "2130/7395000 (epoch 1), train_loss = 1.428, time/batch = 0.071\n",
            "2131/7395000 (epoch 1), train_loss = 1.398, time/batch = 0.062\n",
            "2132/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.061\n",
            "2133/7395000 (epoch 1), train_loss = 1.539, time/batch = 0.061\n",
            "2134/7395000 (epoch 1), train_loss = 1.632, time/batch = 0.063\n",
            "2135/7395000 (epoch 1), train_loss = 1.593, time/batch = 0.061\n",
            "2136/7395000 (epoch 1), train_loss = 1.585, time/batch = 0.064\n",
            "2137/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.066\n",
            "2138/7395000 (epoch 1), train_loss = 1.561, time/batch = 0.061\n",
            "2139/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.070\n",
            "2140/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.061\n",
            "2141/7395000 (epoch 1), train_loss = 1.605, time/batch = 0.062\n",
            "2142/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.063\n",
            "2143/7395000 (epoch 1), train_loss = 1.562, time/batch = 0.062\n",
            "2144/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.061\n",
            "2145/7395000 (epoch 1), train_loss = 1.447, time/batch = 0.063\n",
            "2146/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.066\n",
            "2147/7395000 (epoch 1), train_loss = 1.472, time/batch = 0.062\n",
            "2148/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.059\n",
            "2149/7395000 (epoch 1), train_loss = 1.530, time/batch = 0.059\n",
            "2150/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.061\n",
            "2151/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.061\n",
            "2152/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.066\n",
            "2153/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.061\n",
            "2154/7395000 (epoch 1), train_loss = 1.386, time/batch = 0.059\n",
            "2155/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.061\n",
            "2156/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.058\n",
            "2157/7395000 (epoch 1), train_loss = 1.535, time/batch = 0.062\n",
            "2158/7395000 (epoch 1), train_loss = 1.568, time/batch = 0.061\n",
            "2159/7395000 (epoch 1), train_loss = 1.613, time/batch = 0.062\n",
            "2160/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.061\n",
            "2161/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.063\n",
            "2162/7395000 (epoch 1), train_loss = 1.531, time/batch = 0.060\n",
            "2163/7395000 (epoch 1), train_loss = 1.475, time/batch = 0.064\n",
            "2164/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.060\n",
            "2165/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.062\n",
            "2166/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.060\n",
            "2167/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.067\n",
            "2168/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.062\n",
            "2169/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.063\n",
            "2170/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.060\n",
            "2171/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.061\n",
            "2172/7395000 (epoch 1), train_loss = 1.465, time/batch = 0.067\n",
            "2173/7395000 (epoch 1), train_loss = 1.578, time/batch = 0.063\n",
            "2174/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.061\n",
            "2175/7395000 (epoch 1), train_loss = 1.510, time/batch = 0.060\n",
            "2176/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.065\n",
            "2177/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.063\n",
            "2178/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.062\n",
            "2179/7395000 (epoch 1), train_loss = 1.498, time/batch = 0.061\n",
            "2180/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.068\n",
            "2181/7395000 (epoch 1), train_loss = 1.445, time/batch = 0.067\n",
            "2182/7395000 (epoch 1), train_loss = 1.446, time/batch = 0.059\n",
            "2183/7395000 (epoch 1), train_loss = 1.465, time/batch = 0.059\n",
            "2184/7395000 (epoch 1), train_loss = 1.485, time/batch = 0.060\n",
            "2185/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.061\n",
            "2186/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.061\n",
            "2187/7395000 (epoch 1), train_loss = 1.381, time/batch = 0.060\n",
            "2188/7395000 (epoch 1), train_loss = 1.371, time/batch = 0.058\n",
            "2189/7395000 (epoch 1), train_loss = 1.399, time/batch = 0.062\n",
            "2190/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.059\n",
            "2191/7395000 (epoch 1), train_loss = 1.368, time/batch = 0.062\n",
            "2192/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.060\n",
            "2193/7395000 (epoch 1), train_loss = 1.394, time/batch = 0.062\n",
            "2194/7395000 (epoch 1), train_loss = 1.441, time/batch = 0.062\n",
            "2195/7395000 (epoch 1), train_loss = 1.417, time/batch = 0.063\n",
            "2196/7395000 (epoch 1), train_loss = 1.533, time/batch = 0.064\n",
            "2197/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.061\n",
            "2198/7395000 (epoch 1), train_loss = 1.402, time/batch = 0.059\n",
            "2199/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.062\n",
            "2200/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.062\n",
            "2201/7395000 (epoch 1), train_loss = 1.494, time/batch = 0.062\n",
            "2202/7395000 (epoch 1), train_loss = 1.465, time/batch = 0.060\n",
            "2203/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.062\n",
            "2204/7395000 (epoch 1), train_loss = 1.508, time/batch = 0.061\n",
            "2205/7395000 (epoch 1), train_loss = 1.565, time/batch = 0.068\n",
            "2206/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.058\n",
            "2207/7395000 (epoch 1), train_loss = 1.499, time/batch = 0.061\n",
            "2208/7395000 (epoch 1), train_loss = 1.496, time/batch = 0.060\n",
            "2209/7395000 (epoch 1), train_loss = 1.465, time/batch = 0.061\n",
            "2210/7395000 (epoch 1), train_loss = 1.429, time/batch = 0.061\n",
            "2211/7395000 (epoch 1), train_loss = 1.458, time/batch = 0.057\n",
            "2212/7395000 (epoch 1), train_loss = 1.453, time/batch = 0.062\n",
            "2213/7395000 (epoch 1), train_loss = 1.557, time/batch = 0.062\n",
            "2214/7395000 (epoch 1), train_loss = 1.509, time/batch = 0.062\n",
            "2215/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.062\n",
            "2216/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.058\n",
            "2217/7395000 (epoch 1), train_loss = 1.453, time/batch = 0.063\n",
            "2218/7395000 (epoch 1), train_loss = 1.451, time/batch = 0.060\n",
            "2219/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.061\n",
            "2220/7395000 (epoch 1), train_loss = 1.421, time/batch = 0.062\n",
            "2221/7395000 (epoch 1), train_loss = 1.347, time/batch = 0.059\n",
            "2222/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.067\n",
            "2223/7395000 (epoch 1), train_loss = 1.412, time/batch = 0.063\n",
            "2224/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.059\n",
            "2225/7395000 (epoch 1), train_loss = 1.428, time/batch = 0.062\n",
            "2226/7395000 (epoch 1), train_loss = 1.432, time/batch = 0.061\n",
            "2227/7395000 (epoch 1), train_loss = 1.500, time/batch = 0.062\n",
            "2228/7395000 (epoch 1), train_loss = 1.511, time/batch = 0.061\n",
            "2229/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.060\n",
            "2230/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.059\n",
            "2231/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.054\n",
            "2232/7395000 (epoch 1), train_loss = 1.500, time/batch = 0.059\n",
            "2233/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.063\n",
            "2234/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.059\n",
            "2235/7395000 (epoch 1), train_loss = 1.522, time/batch = 0.064\n",
            "2236/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.061\n",
            "2237/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.063\n",
            "2238/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.059\n",
            "2239/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.058\n",
            "2240/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.062\n",
            "2241/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.065\n",
            "2242/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.064\n",
            "2243/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.063\n",
            "2244/7395000 (epoch 1), train_loss = 1.456, time/batch = 0.059\n",
            "2245/7395000 (epoch 1), train_loss = 1.503, time/batch = 0.063\n",
            "2246/7395000 (epoch 1), train_loss = 1.445, time/batch = 0.063\n",
            "2247/7395000 (epoch 1), train_loss = 1.418, time/batch = 0.061\n",
            "2248/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.061\n",
            "2249/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.061\n",
            "2250/7395000 (epoch 1), train_loss = 1.545, time/batch = 0.054\n",
            "2251/7395000 (epoch 1), train_loss = 1.552, time/batch = 0.053\n",
            "2252/7395000 (epoch 1), train_loss = 1.491, time/batch = 0.058\n",
            "2253/7395000 (epoch 1), train_loss = 1.499, time/batch = 0.054\n",
            "2254/7395000 (epoch 1), train_loss = 1.511, time/batch = 0.064\n",
            "2255/7395000 (epoch 1), train_loss = 1.475, time/batch = 0.053\n",
            "2256/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.065\n",
            "2257/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.058\n",
            "2258/7395000 (epoch 1), train_loss = 1.412, time/batch = 0.054\n",
            "2259/7395000 (epoch 1), train_loss = 1.427, time/batch = 0.054\n",
            "2260/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.054\n",
            "2261/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.056\n",
            "2262/7395000 (epoch 1), train_loss = 1.467, time/batch = 0.056\n",
            "2263/7395000 (epoch 1), train_loss = 1.544, time/batch = 0.055\n",
            "2264/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.062\n",
            "2265/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.060\n",
            "2266/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.065\n",
            "2267/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.061\n",
            "2268/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.060\n",
            "2269/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.061\n",
            "2270/7395000 (epoch 1), train_loss = 1.545, time/batch = 0.059\n",
            "2271/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.066\n",
            "2272/7395000 (epoch 1), train_loss = 1.406, time/batch = 0.062\n",
            "2273/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.068\n",
            "2274/7395000 (epoch 1), train_loss = 1.393, time/batch = 0.060\n",
            "2275/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.061\n",
            "2276/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.057\n",
            "2277/7395000 (epoch 1), train_loss = 1.558, time/batch = 0.063\n",
            "2278/7395000 (epoch 1), train_loss = 1.467, time/batch = 0.062\n",
            "2279/7395000 (epoch 1), train_loss = 1.503, time/batch = 0.060\n",
            "2280/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.061\n",
            "2281/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.057\n",
            "2282/7395000 (epoch 1), train_loss = 1.536, time/batch = 0.060\n",
            "2283/7395000 (epoch 1), train_loss = 1.590, time/batch = 0.061\n",
            "2284/7395000 (epoch 1), train_loss = 1.585, time/batch = 0.061\n",
            "2285/7395000 (epoch 1), train_loss = 1.435, time/batch = 0.062\n",
            "2286/7395000 (epoch 1), train_loss = 1.508, time/batch = 0.069\n",
            "2287/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.062\n",
            "2288/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.060\n",
            "2289/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.068\n",
            "2290/7395000 (epoch 1), train_loss = 1.586, time/batch = 0.062\n",
            "2291/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.063\n",
            "2292/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.059\n",
            "2293/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.061\n",
            "2294/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.059\n",
            "2295/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.067\n",
            "2296/7395000 (epoch 1), train_loss = 1.559, time/batch = 0.062\n",
            "2297/7395000 (epoch 1), train_loss = 1.540, time/batch = 0.061\n",
            "2298/7395000 (epoch 1), train_loss = 1.606, time/batch = 0.061\n",
            "2299/7395000 (epoch 1), train_loss = 1.376, time/batch = 0.062\n",
            "2300/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.063\n",
            "2301/7395000 (epoch 1), train_loss = 1.599, time/batch = 0.062\n",
            "2302/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.058\n",
            "2303/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.064\n",
            "2304/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.061\n",
            "2305/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.060\n",
            "2306/7395000 (epoch 1), train_loss = 1.606, time/batch = 0.065\n",
            "2307/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.062\n",
            "2308/7395000 (epoch 1), train_loss = 1.595, time/batch = 0.060\n",
            "2309/7395000 (epoch 1), train_loss = 1.484, time/batch = 0.062\n",
            "2310/7395000 (epoch 1), train_loss = 1.536, time/batch = 0.066\n",
            "2311/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.062\n",
            "2312/7395000 (epoch 1), train_loss = 1.574, time/batch = 0.059\n",
            "2313/7395000 (epoch 1), train_loss = 1.531, time/batch = 0.062\n",
            "2314/7395000 (epoch 1), train_loss = 1.579, time/batch = 0.060\n",
            "2315/7395000 (epoch 1), train_loss = 1.572, time/batch = 0.069\n",
            "2316/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.062\n",
            "2317/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.063\n",
            "2318/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.065\n",
            "2319/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.063\n",
            "2320/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.061\n",
            "2321/7395000 (epoch 1), train_loss = 1.547, time/batch = 0.064\n",
            "2322/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.067\n",
            "2323/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.055\n",
            "2324/7395000 (epoch 1), train_loss = 1.637, time/batch = 0.061\n",
            "2325/7395000 (epoch 1), train_loss = 1.566, time/batch = 0.060\n",
            "2326/7395000 (epoch 1), train_loss = 1.510, time/batch = 0.073\n",
            "2327/7395000 (epoch 1), train_loss = 1.604, time/batch = 0.061\n",
            "2328/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.060\n",
            "2329/7395000 (epoch 1), train_loss = 1.561, time/batch = 0.069\n",
            "2330/7395000 (epoch 1), train_loss = 1.622, time/batch = 0.059\n",
            "2331/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.061\n",
            "2332/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.062\n",
            "2333/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.074\n",
            "2334/7395000 (epoch 1), train_loss = 1.425, time/batch = 0.064\n",
            "2335/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.061\n",
            "2336/7395000 (epoch 1), train_loss = 1.460, time/batch = 0.061\n",
            "2337/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.059\n",
            "2338/7395000 (epoch 1), train_loss = 1.563, time/batch = 0.061\n",
            "2339/7395000 (epoch 1), train_loss = 1.497, time/batch = 0.064\n",
            "2340/7395000 (epoch 1), train_loss = 1.518, time/batch = 0.059\n",
            "2341/7395000 (epoch 1), train_loss = 1.559, time/batch = 0.063\n",
            "2342/7395000 (epoch 1), train_loss = 1.581, time/batch = 0.059\n",
            "2343/7395000 (epoch 1), train_loss = 1.494, time/batch = 0.061\n",
            "2344/7395000 (epoch 1), train_loss = 1.511, time/batch = 0.062\n",
            "2345/7395000 (epoch 1), train_loss = 1.472, time/batch = 0.063\n",
            "2346/7395000 (epoch 1), train_loss = 1.570, time/batch = 0.060\n",
            "2347/7395000 (epoch 1), train_loss = 1.442, time/batch = 0.063\n",
            "2348/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.061\n",
            "2349/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.061\n",
            "2350/7395000 (epoch 1), train_loss = 1.587, time/batch = 0.061\n",
            "2351/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.063\n",
            "2352/7395000 (epoch 1), train_loss = 1.575, time/batch = 0.066\n",
            "2353/7395000 (epoch 1), train_loss = 1.458, time/batch = 0.063\n",
            "2354/7395000 (epoch 1), train_loss = 1.425, time/batch = 0.059\n",
            "2355/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.067\n",
            "2356/7395000 (epoch 1), train_loss = 1.537, time/batch = 0.056\n",
            "2357/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.063\n",
            "2358/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.060\n",
            "2359/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.060\n",
            "2360/7395000 (epoch 1), train_loss = 1.543, time/batch = 0.060\n",
            "2361/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.061\n",
            "2362/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.061\n",
            "2363/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.062\n",
            "2364/7395000 (epoch 1), train_loss = 1.567, time/batch = 0.062\n",
            "2365/7395000 (epoch 1), train_loss = 1.544, time/batch = 0.067\n",
            "2366/7395000 (epoch 1), train_loss = 1.385, time/batch = 0.060\n",
            "2367/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.064\n",
            "2368/7395000 (epoch 1), train_loss = 1.536, time/batch = 0.061\n",
            "2369/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.063\n",
            "2370/7395000 (epoch 1), train_loss = 1.425, time/batch = 0.064\n",
            "2371/7395000 (epoch 1), train_loss = 1.365, time/batch = 0.060\n",
            "2372/7395000 (epoch 1), train_loss = 1.453, time/batch = 0.063\n",
            "2373/7395000 (epoch 1), train_loss = 1.472, time/batch = 0.063\n",
            "2374/7395000 (epoch 1), train_loss = 1.388, time/batch = 0.061\n",
            "2375/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.058\n",
            "2376/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.064\n",
            "2377/7395000 (epoch 1), train_loss = 1.358, time/batch = 0.057\n",
            "2378/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.062\n",
            "2379/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.061\n",
            "2380/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.058\n",
            "2381/7395000 (epoch 1), train_loss = 1.528, time/batch = 0.061\n",
            "2382/7395000 (epoch 1), train_loss = 1.457, time/batch = 0.061\n",
            "2383/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.061\n",
            "2384/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.059\n",
            "2385/7395000 (epoch 1), train_loss = 1.545, time/batch = 0.056\n",
            "2386/7395000 (epoch 1), train_loss = 1.553, time/batch = 0.061\n",
            "2387/7395000 (epoch 1), train_loss = 1.557, time/batch = 0.061\n",
            "2388/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.060\n",
            "2389/7395000 (epoch 1), train_loss = 1.463, time/batch = 0.065\n",
            "2390/7395000 (epoch 1), train_loss = 1.434, time/batch = 0.059\n",
            "2391/7395000 (epoch 1), train_loss = 1.411, time/batch = 0.061\n",
            "2392/7395000 (epoch 1), train_loss = 1.391, time/batch = 0.059\n",
            "2393/7395000 (epoch 1), train_loss = 1.475, time/batch = 0.064\n",
            "2394/7395000 (epoch 1), train_loss = 1.532, time/batch = 0.059\n",
            "2395/7395000 (epoch 1), train_loss = 1.491, time/batch = 0.064\n",
            "2396/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.062\n",
            "2397/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.062\n",
            "2398/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.061\n",
            "2399/7395000 (epoch 1), train_loss = 1.463, time/batch = 0.065\n",
            "2400/7395000 (epoch 1), train_loss = 1.476, time/batch = 0.058\n",
            "2401/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.063\n",
            "2402/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.060\n",
            "2403/7395000 (epoch 1), train_loss = 1.501, time/batch = 0.062\n",
            "2404/7395000 (epoch 1), train_loss = 1.498, time/batch = 0.059\n",
            "2405/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.066\n",
            "2406/7395000 (epoch 1), train_loss = 1.498, time/batch = 0.067\n",
            "2407/7395000 (epoch 1), train_loss = 1.493, time/batch = 0.063\n",
            "2408/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.060\n",
            "2409/7395000 (epoch 1), train_loss = 1.506, time/batch = 0.064\n",
            "2410/7395000 (epoch 1), train_loss = 1.418, time/batch = 0.059\n",
            "2411/7395000 (epoch 1), train_loss = 1.496, time/batch = 0.065\n",
            "2412/7395000 (epoch 1), train_loss = 1.590, time/batch = 0.060\n",
            "2413/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.063\n",
            "2414/7395000 (epoch 1), train_loss = 1.446, time/batch = 0.063\n",
            "2415/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.062\n",
            "2416/7395000 (epoch 1), train_loss = 1.367, time/batch = 0.059\n",
            "2417/7395000 (epoch 1), train_loss = 1.404, time/batch = 0.072\n",
            "2418/7395000 (epoch 1), train_loss = 1.383, time/batch = 0.060\n",
            "2419/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.062\n",
            "2420/7395000 (epoch 1), train_loss = 1.340, time/batch = 0.060\n",
            "2421/7395000 (epoch 1), train_loss = 1.456, time/batch = 0.063\n",
            "2422/7395000 (epoch 1), train_loss = 1.430, time/batch = 0.070\n",
            "2423/7395000 (epoch 1), train_loss = 1.527, time/batch = 0.064\n",
            "2424/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.059\n",
            "2425/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.064\n",
            "2426/7395000 (epoch 1), train_loss = 1.455, time/batch = 0.059\n",
            "2427/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.062\n",
            "2428/7395000 (epoch 1), train_loss = 1.441, time/batch = 0.062\n",
            "2429/7395000 (epoch 1), train_loss = 1.402, time/batch = 0.062\n",
            "2430/7395000 (epoch 1), train_loss = 1.355, time/batch = 0.061\n",
            "2431/7395000 (epoch 1), train_loss = 1.363, time/batch = 0.062\n",
            "2432/7395000 (epoch 1), train_loss = 1.372, time/batch = 0.058\n",
            "2433/7395000 (epoch 1), train_loss = 1.447, time/batch = 0.063\n",
            "2434/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.061\n",
            "2435/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.067\n",
            "2436/7395000 (epoch 1), train_loss = 1.498, time/batch = 0.061\n",
            "2437/7395000 (epoch 1), train_loss = 1.445, time/batch = 0.061\n",
            "2438/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.067\n",
            "2439/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.059\n",
            "2440/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.061\n",
            "2441/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.061\n",
            "2442/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.059\n",
            "2443/7395000 (epoch 1), train_loss = 1.455, time/batch = 0.062\n",
            "2444/7395000 (epoch 1), train_loss = 1.514, time/batch = 0.062\n",
            "2445/7395000 (epoch 1), train_loss = 1.482, time/batch = 0.062\n",
            "2446/7395000 (epoch 1), train_loss = 1.451, time/batch = 0.060\n",
            "2447/7395000 (epoch 1), train_loss = 1.453, time/batch = 0.062\n",
            "2448/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.060\n",
            "2449/7395000 (epoch 1), train_loss = 1.482, time/batch = 0.061\n",
            "2450/7395000 (epoch 1), train_loss = 1.505, time/batch = 0.059\n",
            "2451/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.062\n",
            "2452/7395000 (epoch 1), train_loss = 1.458, time/batch = 0.059\n",
            "2453/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.060\n",
            "2454/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.059\n",
            "2455/7395000 (epoch 1), train_loss = 1.526, time/batch = 0.075\n",
            "2456/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.061\n",
            "2457/7395000 (epoch 1), train_loss = 1.456, time/batch = 0.060\n",
            "2458/7395000 (epoch 1), train_loss = 1.450, time/batch = 0.061\n",
            "2459/7395000 (epoch 1), train_loss = 1.450, time/batch = 0.061\n",
            "2460/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.059\n",
            "2461/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.069\n",
            "2462/7395000 (epoch 1), train_loss = 1.542, time/batch = 0.059\n",
            "2463/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.062\n",
            "2464/7395000 (epoch 1), train_loss = 1.496, time/batch = 0.063\n",
            "2465/7395000 (epoch 1), train_loss = 1.560, time/batch = 0.061\n",
            "2466/7395000 (epoch 1), train_loss = 1.530, time/batch = 0.060\n",
            "2467/7395000 (epoch 1), train_loss = 1.483, time/batch = 0.066\n",
            "2468/7395000 (epoch 1), train_loss = 1.527, time/batch = 0.061\n",
            "2469/7395000 (epoch 1), train_loss = 1.442, time/batch = 0.065\n",
            "2470/7395000 (epoch 1), train_loss = 1.569, time/batch = 0.059\n",
            "2471/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.061\n",
            "2472/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.062\n",
            "2473/7395000 (epoch 1), train_loss = 1.495, time/batch = 0.061\n",
            "2474/7395000 (epoch 1), train_loss = 1.413, time/batch = 0.061\n",
            "2475/7395000 (epoch 1), train_loss = 1.551, time/batch = 0.061\n",
            "2476/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.059\n",
            "2477/7395000 (epoch 1), train_loss = 1.630, time/batch = 0.064\n",
            "2478/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.062\n",
            "2479/7395000 (epoch 1), train_loss = 1.527, time/batch = 0.063\n",
            "2480/7395000 (epoch 1), train_loss = 1.555, time/batch = 0.062\n",
            "2481/7395000 (epoch 1), train_loss = 1.503, time/batch = 0.060\n",
            "2482/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.062\n",
            "2483/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.061\n",
            "2484/7395000 (epoch 1), train_loss = 1.542, time/batch = 0.061\n",
            "2485/7395000 (epoch 1), train_loss = 1.540, time/batch = 0.064\n",
            "2486/7395000 (epoch 1), train_loss = 1.432, time/batch = 0.058\n",
            "2487/7395000 (epoch 1), train_loss = 1.577, time/batch = 0.065\n",
            "2488/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.065\n",
            "2489/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.061\n",
            "2490/7395000 (epoch 1), train_loss = 1.398, time/batch = 0.061\n",
            "2491/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.065\n",
            "2492/7395000 (epoch 1), train_loss = 1.551, time/batch = 0.061\n",
            "2493/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.060\n",
            "2494/7395000 (epoch 1), train_loss = 1.550, time/batch = 0.060\n",
            "2495/7395000 (epoch 1), train_loss = 1.511, time/batch = 0.062\n",
            "2496/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.061\n",
            "2497/7395000 (epoch 1), train_loss = 1.653, time/batch = 0.061\n",
            "2498/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.061\n",
            "2499/7395000 (epoch 1), train_loss = 1.557, time/batch = 0.062\n",
            "2500/7395000 (epoch 1), train_loss = 1.475, time/batch = 0.059\n",
            "2501/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.062\n",
            "2502/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.062\n",
            "2503/7395000 (epoch 1), train_loss = 1.637, time/batch = 0.061\n",
            "2504/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.059\n",
            "2505/7395000 (epoch 1), train_loss = 1.446, time/batch = 0.067\n",
            "2506/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.066\n",
            "2507/7395000 (epoch 1), train_loss = 1.494, time/batch = 0.063\n",
            "2508/7395000 (epoch 1), train_loss = 1.456, time/batch = 0.061\n",
            "2509/7395000 (epoch 1), train_loss = 1.406, time/batch = 0.063\n",
            "2510/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.059\n",
            "2511/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.063\n",
            "2512/7395000 (epoch 1), train_loss = 1.532, time/batch = 0.061\n",
            "2513/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.061\n",
            "2514/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.059\n",
            "2515/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.061\n",
            "2516/7395000 (epoch 1), train_loss = 1.453, time/batch = 0.063\n",
            "2517/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.063\n",
            "2518/7395000 (epoch 1), train_loss = 1.498, time/batch = 0.060\n",
            "2519/7395000 (epoch 1), train_loss = 1.432, time/batch = 0.064\n",
            "2520/7395000 (epoch 1), train_loss = 1.485, time/batch = 0.061\n",
            "2521/7395000 (epoch 1), train_loss = 1.516, time/batch = 0.065\n",
            "2522/7395000 (epoch 1), train_loss = 1.482, time/batch = 0.059\n",
            "2523/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.060\n",
            "2524/7395000 (epoch 1), train_loss = 1.507, time/batch = 0.066\n",
            "2525/7395000 (epoch 1), train_loss = 1.513, time/batch = 0.063\n",
            "2526/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.060\n",
            "2527/7395000 (epoch 1), train_loss = 1.651, time/batch = 0.062\n",
            "2528/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.065\n",
            "2529/7395000 (epoch 1), train_loss = 1.638, time/batch = 0.063\n",
            "2530/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.061\n",
            "2531/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.062\n",
            "2532/7395000 (epoch 1), train_loss = 1.515, time/batch = 0.061\n",
            "2533/7395000 (epoch 1), train_loss = 1.460, time/batch = 0.062\n",
            "2534/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.064\n",
            "2535/7395000 (epoch 1), train_loss = 1.586, time/batch = 0.063\n",
            "2536/7395000 (epoch 1), train_loss = 1.535, time/batch = 0.060\n",
            "2537/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.061\n",
            "2538/7395000 (epoch 1), train_loss = 1.465, time/batch = 0.061\n",
            "2539/7395000 (epoch 1), train_loss = 1.575, time/batch = 0.064\n",
            "2540/7395000 (epoch 1), train_loss = 1.552, time/batch = 0.061\n",
            "2541/7395000 (epoch 1), train_loss = 1.582, time/batch = 0.059\n",
            "2542/7395000 (epoch 1), train_loss = 1.599, time/batch = 0.059\n",
            "2543/7395000 (epoch 1), train_loss = 1.570, time/batch = 0.058\n",
            "2544/7395000 (epoch 1), train_loss = 1.455, time/batch = 0.064\n",
            "2545/7395000 (epoch 1), train_loss = 1.483, time/batch = 0.061\n",
            "2546/7395000 (epoch 1), train_loss = 1.439, time/batch = 0.060\n",
            "2547/7395000 (epoch 1), train_loss = 1.512, time/batch = 0.065\n",
            "2548/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.064\n",
            "2549/7395000 (epoch 1), train_loss = 1.386, time/batch = 0.061\n",
            "2550/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.060\n",
            "2551/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.062\n",
            "2552/7395000 (epoch 1), train_loss = 1.345, time/batch = 0.062\n",
            "2553/7395000 (epoch 1), train_loss = 1.310, time/batch = 0.063\n",
            "2554/7395000 (epoch 1), train_loss = 1.338, time/batch = 0.067\n",
            "2555/7395000 (epoch 1), train_loss = 1.336, time/batch = 0.062\n",
            "2556/7395000 (epoch 1), train_loss = 1.335, time/batch = 0.062\n",
            "2557/7395000 (epoch 1), train_loss = 1.417, time/batch = 0.065\n",
            "2558/7395000 (epoch 1), train_loss = 1.316, time/batch = 0.059\n",
            "2559/7395000 (epoch 1), train_loss = 1.360, time/batch = 0.059\n",
            "2560/7395000 (epoch 1), train_loss = 1.451, time/batch = 0.068\n",
            "2561/7395000 (epoch 1), train_loss = 1.534, time/batch = 0.063\n",
            "2562/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.062\n",
            "2563/7395000 (epoch 1), train_loss = 1.401, time/batch = 0.061\n",
            "2564/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.062\n",
            "2565/7395000 (epoch 1), train_loss = 1.562, time/batch = 0.064\n",
            "2566/7395000 (epoch 1), train_loss = 1.408, time/batch = 0.054\n",
            "2567/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.053\n",
            "2568/7395000 (epoch 1), train_loss = 1.507, time/batch = 0.054\n",
            "2569/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.054\n",
            "2570/7395000 (epoch 1), train_loss = 1.472, time/batch = 0.055\n",
            "2571/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.060\n",
            "2572/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.059\n",
            "2573/7395000 (epoch 1), train_loss = 1.506, time/batch = 0.061\n",
            "2574/7395000 (epoch 1), train_loss = 1.402, time/batch = 0.062\n",
            "2575/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.061\n",
            "2576/7395000 (epoch 1), train_loss = 1.427, time/batch = 0.061\n",
            "2577/7395000 (epoch 1), train_loss = 1.425, time/batch = 0.060\n",
            "2578/7395000 (epoch 1), train_loss = 1.555, time/batch = 0.058\n",
            "2579/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.063\n",
            "2580/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.058\n",
            "2581/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.060\n",
            "2582/7395000 (epoch 1), train_loss = 1.319, time/batch = 0.062\n",
            "2583/7395000 (epoch 1), train_loss = 1.414, time/batch = 0.060\n",
            "2584/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.062\n",
            "2585/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.061\n",
            "2586/7395000 (epoch 1), train_loss = 1.523, time/batch = 0.065\n",
            "2587/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.063\n",
            "2588/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.063\n",
            "2589/7395000 (epoch 1), train_loss = 1.555, time/batch = 0.061\n",
            "2590/7395000 (epoch 1), train_loss = 1.429, time/batch = 0.063\n",
            "2591/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.061\n",
            "2592/7395000 (epoch 1), train_loss = 1.416, time/batch = 0.059\n",
            "2593/7395000 (epoch 1), train_loss = 1.397, time/batch = 0.062\n",
            "2594/7395000 (epoch 1), train_loss = 1.385, time/batch = 0.062\n",
            "2595/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.061\n",
            "2596/7395000 (epoch 1), train_loss = 1.597, time/batch = 0.059\n",
            "2597/7395000 (epoch 1), train_loss = 1.437, time/batch = 0.062\n",
            "2598/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.060\n",
            "2599/7395000 (epoch 1), train_loss = 1.380, time/batch = 0.061\n",
            "2600/7395000 (epoch 1), train_loss = 1.380, time/batch = 0.059\n",
            "2601/7395000 (epoch 1), train_loss = 1.439, time/batch = 0.062\n",
            "2602/7395000 (epoch 1), train_loss = 1.420, time/batch = 0.063\n",
            "2603/7395000 (epoch 1), train_loss = 1.390, time/batch = 0.063\n",
            "2604/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.059\n",
            "2605/7395000 (epoch 1), train_loss = 1.385, time/batch = 0.062\n",
            "2606/7395000 (epoch 1), train_loss = 1.418, time/batch = 0.062\n",
            "2607/7395000 (epoch 1), train_loss = 1.497, time/batch = 0.061\n",
            "2608/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.060\n",
            "2609/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.062\n",
            "2610/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.061\n",
            "2611/7395000 (epoch 1), train_loss = 1.460, time/batch = 0.062\n",
            "2612/7395000 (epoch 1), train_loss = 1.388, time/batch = 0.058\n",
            "2613/7395000 (epoch 1), train_loss = 1.445, time/batch = 0.059\n",
            "2614/7395000 (epoch 1), train_loss = 1.414, time/batch = 0.061\n",
            "2615/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.064\n",
            "2616/7395000 (epoch 1), train_loss = 1.512, time/batch = 0.060\n",
            "2617/7395000 (epoch 1), train_loss = 1.595, time/batch = 0.062\n",
            "2618/7395000 (epoch 1), train_loss = 1.366, time/batch = 0.062\n",
            "2619/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.061\n",
            "2620/7395000 (epoch 1), train_loss = 1.419, time/batch = 0.061\n",
            "2621/7395000 (epoch 1), train_loss = 1.511, time/batch = 0.063\n",
            "2622/7395000 (epoch 1), train_loss = 1.371, time/batch = 0.056\n",
            "2623/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.065\n",
            "2624/7395000 (epoch 1), train_loss = 1.445, time/batch = 0.060\n",
            "2625/7395000 (epoch 1), train_loss = 1.377, time/batch = 0.068\n",
            "2626/7395000 (epoch 1), train_loss = 1.461, time/batch = 0.058\n",
            "2627/7395000 (epoch 1), train_loss = 1.366, time/batch = 0.060\n",
            "2628/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.066\n",
            "2629/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.062\n",
            "2630/7395000 (epoch 1), train_loss = 1.439, time/batch = 0.060\n",
            "2631/7395000 (epoch 1), train_loss = 1.381, time/batch = 0.062\n",
            "2632/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.061\n",
            "2633/7395000 (epoch 1), train_loss = 1.448, time/batch = 0.060\n",
            "2634/7395000 (epoch 1), train_loss = 1.450, time/batch = 0.059\n",
            "2635/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.060\n",
            "2636/7395000 (epoch 1), train_loss = 1.448, time/batch = 0.064\n",
            "2637/7395000 (epoch 1), train_loss = 1.395, time/batch = 0.062\n",
            "2638/7395000 (epoch 1), train_loss = 1.442, time/batch = 0.065\n",
            "2639/7395000 (epoch 1), train_loss = 1.361, time/batch = 0.062\n",
            "2640/7395000 (epoch 1), train_loss = 1.412, time/batch = 0.054\n",
            "2641/7395000 (epoch 1), train_loss = 1.399, time/batch = 0.061\n",
            "2642/7395000 (epoch 1), train_loss = 1.389, time/batch = 0.060\n",
            "2643/7395000 (epoch 1), train_loss = 1.430, time/batch = 0.062\n",
            "2644/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.059\n",
            "2645/7395000 (epoch 1), train_loss = 1.347, time/batch = 0.063\n",
            "2646/7395000 (epoch 1), train_loss = 1.379, time/batch = 0.061\n",
            "2647/7395000 (epoch 1), train_loss = 1.369, time/batch = 0.061\n",
            "2648/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.061\n",
            "2649/7395000 (epoch 1), train_loss = 1.506, time/batch = 0.060\n",
            "2650/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.060\n",
            "2651/7395000 (epoch 1), train_loss = 1.397, time/batch = 0.063\n",
            "2652/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.060\n",
            "2653/7395000 (epoch 1), train_loss = 1.390, time/batch = 0.064\n",
            "2654/7395000 (epoch 1), train_loss = 1.489, time/batch = 0.060\n",
            "2655/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.069\n",
            "2656/7395000 (epoch 1), train_loss = 1.436, time/batch = 0.059\n",
            "2657/7395000 (epoch 1), train_loss = 1.323, time/batch = 0.062\n",
            "2658/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.095\n",
            "2659/7395000 (epoch 1), train_loss = 1.551, time/batch = 0.062\n",
            "2660/7395000 (epoch 1), train_loss = 1.537, time/batch = 0.059\n",
            "2661/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.061\n",
            "2662/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.065\n",
            "2663/7395000 (epoch 1), train_loss = 1.461, time/batch = 0.058\n",
            "2664/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.060\n",
            "2665/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.062\n",
            "2666/7395000 (epoch 1), train_loss = 1.441, time/batch = 0.061\n",
            "2667/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.065\n",
            "2668/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.058\n",
            "2669/7395000 (epoch 1), train_loss = 1.463, time/batch = 0.061\n",
            "2670/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.063\n",
            "2671/7395000 (epoch 1), train_loss = 1.460, time/batch = 0.069\n",
            "2672/7395000 (epoch 1), train_loss = 1.412, time/batch = 0.060\n",
            "2673/7395000 (epoch 1), train_loss = 1.507, time/batch = 0.062\n",
            "2674/7395000 (epoch 1), train_loss = 1.516, time/batch = 0.060\n",
            "2675/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.063\n",
            "2676/7395000 (epoch 1), train_loss = 1.448, time/batch = 0.057\n",
            "2677/7395000 (epoch 1), train_loss = 1.410, time/batch = 0.060\n",
            "2678/7395000 (epoch 1), train_loss = 1.563, time/batch = 0.061\n",
            "2679/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.062\n",
            "2680/7395000 (epoch 1), train_loss = 1.458, time/batch = 0.063\n",
            "2681/7395000 (epoch 1), train_loss = 1.391, time/batch = 0.061\n",
            "2682/7395000 (epoch 1), train_loss = 1.390, time/batch = 0.059\n",
            "2683/7395000 (epoch 1), train_loss = 1.302, time/batch = 0.062\n",
            "2684/7395000 (epoch 1), train_loss = 1.359, time/batch = 0.060\n",
            "2685/7395000 (epoch 1), train_loss = 1.367, time/batch = 0.065\n",
            "2686/7395000 (epoch 1), train_loss = 1.377, time/batch = 0.057\n",
            "2687/7395000 (epoch 1), train_loss = 1.291, time/batch = 0.060\n",
            "2688/7395000 (epoch 1), train_loss = 1.488, time/batch = 0.060\n",
            "2689/7395000 (epoch 1), train_loss = 1.344, time/batch = 0.057\n",
            "2690/7395000 (epoch 1), train_loss = 1.441, time/batch = 0.060\n",
            "2691/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.061\n",
            "2692/7395000 (epoch 1), train_loss = 1.402, time/batch = 0.058\n",
            "2693/7395000 (epoch 1), train_loss = 1.403, time/batch = 0.062\n",
            "2694/7395000 (epoch 1), train_loss = 1.329, time/batch = 0.060\n",
            "2695/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.065\n",
            "2696/7395000 (epoch 1), train_loss = 1.375, time/batch = 0.057\n",
            "2697/7395000 (epoch 1), train_loss = 1.381, time/batch = 0.060\n",
            "2698/7395000 (epoch 1), train_loss = 1.405, time/batch = 0.061\n",
            "2699/7395000 (epoch 1), train_loss = 1.387, time/batch = 0.060\n",
            "2700/7395000 (epoch 1), train_loss = 1.424, time/batch = 0.063\n",
            "2701/7395000 (epoch 1), train_loss = 1.357, time/batch = 0.063\n",
            "2702/7395000 (epoch 1), train_loss = 1.521, time/batch = 0.060\n",
            "2703/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.062\n",
            "2704/7395000 (epoch 1), train_loss = 1.435, time/batch = 0.065\n",
            "2705/7395000 (epoch 1), train_loss = 1.383, time/batch = 0.067\n",
            "2706/7395000 (epoch 1), train_loss = 1.324, time/batch = 0.053\n",
            "2707/7395000 (epoch 1), train_loss = 1.358, time/batch = 0.053\n",
            "2708/7395000 (epoch 1), train_loss = 1.416, time/batch = 0.054\n",
            "2709/7395000 (epoch 1), train_loss = 1.424, time/batch = 0.056\n",
            "2710/7395000 (epoch 1), train_loss = 1.520, time/batch = 0.054\n",
            "2711/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.054\n",
            "2712/7395000 (epoch 1), train_loss = 1.383, time/batch = 0.055\n",
            "2713/7395000 (epoch 1), train_loss = 1.429, time/batch = 0.053\n",
            "2714/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.060\n",
            "2715/7395000 (epoch 1), train_loss = 1.439, time/batch = 0.054\n",
            "2716/7395000 (epoch 1), train_loss = 1.377, time/batch = 0.054\n",
            "2717/7395000 (epoch 1), train_loss = 1.382, time/batch = 0.064\n",
            "2718/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.056\n",
            "2719/7395000 (epoch 1), train_loss = 1.374, time/batch = 0.066\n",
            "2720/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.057\n",
            "2721/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.060\n",
            "2722/7395000 (epoch 1), train_loss = 1.462, time/batch = 0.063\n",
            "2723/7395000 (epoch 1), train_loss = 1.389, time/batch = 0.064\n",
            "2724/7395000 (epoch 1), train_loss = 1.302, time/batch = 0.064\n",
            "2725/7395000 (epoch 1), train_loss = 1.401, time/batch = 0.060\n",
            "2726/7395000 (epoch 1), train_loss = 1.392, time/batch = 0.059\n",
            "2727/7395000 (epoch 1), train_loss = 1.366, time/batch = 0.066\n",
            "2728/7395000 (epoch 1), train_loss = 1.330, time/batch = 0.059\n",
            "2729/7395000 (epoch 1), train_loss = 1.263, time/batch = 0.062\n",
            "2730/7395000 (epoch 1), train_loss = 1.387, time/batch = 0.064\n",
            "2731/7395000 (epoch 1), train_loss = 1.456, time/batch = 0.060\n",
            "2732/7395000 (epoch 1), train_loss = 1.347, time/batch = 0.062\n",
            "2733/7395000 (epoch 1), train_loss = 1.481, time/batch = 0.061\n",
            "2734/7395000 (epoch 1), train_loss = 1.533, time/batch = 0.059\n",
            "2735/7395000 (epoch 1), train_loss = 1.382, time/batch = 0.061\n",
            "2736/7395000 (epoch 1), train_loss = 1.348, time/batch = 0.063\n",
            "2737/7395000 (epoch 1), train_loss = 1.340, time/batch = 0.062\n",
            "2738/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.060\n",
            "2739/7395000 (epoch 1), train_loss = 1.388, time/batch = 0.065\n",
            "2740/7395000 (epoch 1), train_loss = 1.341, time/batch = 0.060\n",
            "2741/7395000 (epoch 1), train_loss = 1.404, time/batch = 0.060\n",
            "2742/7395000 (epoch 1), train_loss = 1.401, time/batch = 0.062\n",
            "2743/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.059\n",
            "2744/7395000 (epoch 1), train_loss = 1.341, time/batch = 0.060\n",
            "2745/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.061\n",
            "2746/7395000 (epoch 1), train_loss = 1.257, time/batch = 0.059\n",
            "2747/7395000 (epoch 1), train_loss = 1.375, time/batch = 0.062\n",
            "2748/7395000 (epoch 1), train_loss = 1.328, time/batch = 0.061\n",
            "2749/7395000 (epoch 1), train_loss = 1.312, time/batch = 0.060\n",
            "2750/7395000 (epoch 1), train_loss = 1.365, time/batch = 0.062\n",
            "2751/7395000 (epoch 1), train_loss = 1.495, time/batch = 0.062\n",
            "2752/7395000 (epoch 1), train_loss = 1.421, time/batch = 0.061\n",
            "2753/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.062\n",
            "2754/7395000 (epoch 1), train_loss = 1.370, time/batch = 0.068\n",
            "2755/7395000 (epoch 1), train_loss = 1.321, time/batch = 0.065\n",
            "2756/7395000 (epoch 1), train_loss = 1.336, time/batch = 0.060\n",
            "2757/7395000 (epoch 1), train_loss = 1.365, time/batch = 0.060\n",
            "2758/7395000 (epoch 1), train_loss = 1.383, time/batch = 0.058\n",
            "2759/7395000 (epoch 1), train_loss = 1.338, time/batch = 0.058\n",
            "2760/7395000 (epoch 1), train_loss = 1.287, time/batch = 0.064\n",
            "2761/7395000 (epoch 1), train_loss = 1.455, time/batch = 0.062\n",
            "2762/7395000 (epoch 1), train_loss = 1.409, time/batch = 0.058\n",
            "2763/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.063\n",
            "2764/7395000 (epoch 1), train_loss = 1.436, time/batch = 0.061\n",
            "2765/7395000 (epoch 1), train_loss = 1.420, time/batch = 0.066\n",
            "2766/7395000 (epoch 1), train_loss = 1.387, time/batch = 0.059\n",
            "2767/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.060\n",
            "2768/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.059\n",
            "2769/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.060\n",
            "2770/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.065\n",
            "2771/7395000 (epoch 1), train_loss = 1.382, time/batch = 0.060\n",
            "2772/7395000 (epoch 1), train_loss = 1.383, time/batch = 0.067\n",
            "2773/7395000 (epoch 1), train_loss = 1.356, time/batch = 0.062\n",
            "2774/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.060\n",
            "2775/7395000 (epoch 1), train_loss = 1.310, time/batch = 0.062\n",
            "2776/7395000 (epoch 1), train_loss = 1.349, time/batch = 0.060\n",
            "2777/7395000 (epoch 1), train_loss = 1.376, time/batch = 0.060\n",
            "2778/7395000 (epoch 1), train_loss = 1.301, time/batch = 0.064\n",
            "2779/7395000 (epoch 1), train_loss = 1.310, time/batch = 0.062\n",
            "2780/7395000 (epoch 1), train_loss = 1.361, time/batch = 0.061\n",
            "2781/7395000 (epoch 1), train_loss = 1.353, time/batch = 0.059\n",
            "2782/7395000 (epoch 1), train_loss = 1.334, time/batch = 0.058\n",
            "2783/7395000 (epoch 1), train_loss = 1.316, time/batch = 0.063\n",
            "2784/7395000 (epoch 1), train_loss = 1.377, time/batch = 0.061\n",
            "2785/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.061\n",
            "2786/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.063\n",
            "2787/7395000 (epoch 1), train_loss = 1.319, time/batch = 0.060\n",
            "2788/7395000 (epoch 1), train_loss = 1.414, time/batch = 0.059\n",
            "2789/7395000 (epoch 1), train_loss = 1.304, time/batch = 0.057\n",
            "2790/7395000 (epoch 1), train_loss = 1.306, time/batch = 0.060\n",
            "2791/7395000 (epoch 1), train_loss = 1.376, time/batch = 0.059\n",
            "2792/7395000 (epoch 1), train_loss = 1.266, time/batch = 0.059\n",
            "2793/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.064\n",
            "2794/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.060\n",
            "2795/7395000 (epoch 1), train_loss = 1.631, time/batch = 0.060\n",
            "2796/7395000 (epoch 1), train_loss = 1.549, time/batch = 0.060\n",
            "2797/7395000 (epoch 1), train_loss = 1.482, time/batch = 0.062\n",
            "2798/7395000 (epoch 1), train_loss = 1.381, time/batch = 0.066\n",
            "2799/7395000 (epoch 1), train_loss = 1.379, time/batch = 0.061\n",
            "2800/7395000 (epoch 1), train_loss = 1.388, time/batch = 0.060\n",
            "2801/7395000 (epoch 1), train_loss = 1.384, time/batch = 0.062\n",
            "2802/7395000 (epoch 1), train_loss = 1.431, time/batch = 0.077\n",
            "2803/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.058\n",
            "2804/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.059\n",
            "2805/7395000 (epoch 1), train_loss = 1.409, time/batch = 0.064\n",
            "2806/7395000 (epoch 1), train_loss = 1.470, time/batch = 0.062\n",
            "2807/7395000 (epoch 1), train_loss = 1.497, time/batch = 0.062\n",
            "2808/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.059\n",
            "2809/7395000 (epoch 1), train_loss = 1.461, time/batch = 0.062\n",
            "2810/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.061\n",
            "2811/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.056\n",
            "2812/7395000 (epoch 1), train_loss = 1.502, time/batch = 0.060\n",
            "2813/7395000 (epoch 1), train_loss = 1.466, time/batch = 0.062\n",
            "2814/7395000 (epoch 1), train_loss = 1.478, time/batch = 0.059\n",
            "2815/7395000 (epoch 1), train_loss = 1.460, time/batch = 0.061\n",
            "2816/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.061\n",
            "2817/7395000 (epoch 1), train_loss = 1.393, time/batch = 0.066\n",
            "2818/7395000 (epoch 1), train_loss = 1.392, time/batch = 0.055\n",
            "2819/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.061\n",
            "2820/7395000 (epoch 1), train_loss = 1.365, time/batch = 0.060\n",
            "2821/7395000 (epoch 1), train_loss = 1.404, time/batch = 0.062\n",
            "2822/7395000 (epoch 1), train_loss = 1.397, time/batch = 0.068\n",
            "2823/7395000 (epoch 1), train_loss = 1.386, time/batch = 0.060\n",
            "2824/7395000 (epoch 1), train_loss = 1.319, time/batch = 0.063\n",
            "2825/7395000 (epoch 1), train_loss = 1.517, time/batch = 0.060\n",
            "2826/7395000 (epoch 1), train_loss = 1.405, time/batch = 0.061\n",
            "2827/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.061\n",
            "2828/7395000 (epoch 1), train_loss = 1.468, time/batch = 0.059\n",
            "2829/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.061\n",
            "2830/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.059\n",
            "2831/7395000 (epoch 1), train_loss = 1.369, time/batch = 0.063\n",
            "2832/7395000 (epoch 1), train_loss = 1.362, time/batch = 0.060\n",
            "2833/7395000 (epoch 1), train_loss = 1.330, time/batch = 0.061\n",
            "2834/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.058\n",
            "2835/7395000 (epoch 1), train_loss = 1.284, time/batch = 0.063\n",
            "2836/7395000 (epoch 1), train_loss = 1.344, time/batch = 0.065\n",
            "2837/7395000 (epoch 1), train_loss = 1.421, time/batch = 0.060\n",
            "2838/7395000 (epoch 1), train_loss = 1.311, time/batch = 0.060\n",
            "2839/7395000 (epoch 1), train_loss = 1.306, time/batch = 0.058\n",
            "2840/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.059\n",
            "2841/7395000 (epoch 1), train_loss = 1.368, time/batch = 0.066\n",
            "2842/7395000 (epoch 1), train_loss = 1.451, time/batch = 0.061\n",
            "2843/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.062\n",
            "2844/7395000 (epoch 1), train_loss = 1.454, time/batch = 0.060\n",
            "2845/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.061\n",
            "2846/7395000 (epoch 1), train_loss = 1.364, time/batch = 0.060\n",
            "2847/7395000 (epoch 1), train_loss = 1.443, time/batch = 0.062\n",
            "2848/7395000 (epoch 1), train_loss = 1.264, time/batch = 0.064\n",
            "2849/7395000 (epoch 1), train_loss = 1.424, time/batch = 0.062\n",
            "2850/7395000 (epoch 1), train_loss = 1.426, time/batch = 0.058\n",
            "2851/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.058\n",
            "2852/7395000 (epoch 1), train_loss = 1.343, time/batch = 0.062\n",
            "2853/7395000 (epoch 1), train_loss = 1.394, time/batch = 0.061\n",
            "2854/7395000 (epoch 1), train_loss = 1.285, time/batch = 0.060\n",
            "2855/7395000 (epoch 1), train_loss = 1.397, time/batch = 0.064\n",
            "2856/7395000 (epoch 1), train_loss = 1.480, time/batch = 0.057\n",
            "2857/7395000 (epoch 1), train_loss = 1.385, time/batch = 0.062\n",
            "2858/7395000 (epoch 1), train_loss = 1.496, time/batch = 0.060\n",
            "2859/7395000 (epoch 1), train_loss = 1.486, time/batch = 0.066\n",
            "2860/7395000 (epoch 1), train_loss = 1.379, time/batch = 0.059\n",
            "2861/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.058\n",
            "2862/7395000 (epoch 1), train_loss = 1.298, time/batch = 0.061\n",
            "2863/7395000 (epoch 1), train_loss = 1.429, time/batch = 0.059\n",
            "2864/7395000 (epoch 1), train_loss = 1.507, time/batch = 0.061\n",
            "2865/7395000 (epoch 1), train_loss = 1.388, time/batch = 0.062\n",
            "2866/7395000 (epoch 1), train_loss = 1.438, time/batch = 0.060\n",
            "2867/7395000 (epoch 1), train_loss = 1.477, time/batch = 0.061\n",
            "2868/7395000 (epoch 1), train_loss = 1.417, time/batch = 0.058\n",
            "2869/7395000 (epoch 1), train_loss = 1.320, time/batch = 0.064\n",
            "2870/7395000 (epoch 1), train_loss = 1.436, time/batch = 0.060\n",
            "2871/7395000 (epoch 1), train_loss = 1.500, time/batch = 0.060\n",
            "2872/7395000 (epoch 1), train_loss = 1.516, time/batch = 0.065\n",
            "2873/7395000 (epoch 1), train_loss = 1.504, time/batch = 0.062\n",
            "2874/7395000 (epoch 1), train_loss = 1.469, time/batch = 0.060\n",
            "2875/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.062\n",
            "2876/7395000 (epoch 1), train_loss = 1.508, time/batch = 0.060\n",
            "2877/7395000 (epoch 1), train_loss = 1.408, time/batch = 0.062\n",
            "2878/7395000 (epoch 1), train_loss = 1.549, time/batch = 0.059\n",
            "2879/7395000 (epoch 1), train_loss = 1.471, time/batch = 0.063\n",
            "2880/7395000 (epoch 1), train_loss = 1.479, time/batch = 0.060\n",
            "2881/7395000 (epoch 1), train_loss = 1.459, time/batch = 0.062\n",
            "2882/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.060\n",
            "2883/7395000 (epoch 1), train_loss = 1.525, time/batch = 0.062\n",
            "2884/7395000 (epoch 1), train_loss = 1.541, time/batch = 0.057\n",
            "2885/7395000 (epoch 1), train_loss = 1.524, time/batch = 0.060\n",
            "2886/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.057\n",
            "2887/7395000 (epoch 1), train_loss = 1.473, time/batch = 0.066\n",
            "2888/7395000 (epoch 1), train_loss = 1.429, time/batch = 0.060\n",
            "2889/7395000 (epoch 1), train_loss = 1.434, time/batch = 0.070\n",
            "2890/7395000 (epoch 1), train_loss = 1.392, time/batch = 0.062\n",
            "2891/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.063\n",
            "2892/7395000 (epoch 1), train_loss = 1.500, time/batch = 0.057\n",
            "2893/7395000 (epoch 1), train_loss = 1.404, time/batch = 0.060\n",
            "2894/7395000 (epoch 1), train_loss = 1.406, time/batch = 0.061\n",
            "2895/7395000 (epoch 1), train_loss = 1.405, time/batch = 0.063\n",
            "2896/7395000 (epoch 1), train_loss = 1.347, time/batch = 0.058\n",
            "2897/7395000 (epoch 1), train_loss = 1.433, time/batch = 0.063\n",
            "2898/7395000 (epoch 1), train_loss = 1.447, time/batch = 0.059\n",
            "2899/7395000 (epoch 1), train_loss = 1.487, time/batch = 0.062\n",
            "2900/7395000 (epoch 1), train_loss = 1.519, time/batch = 0.059\n",
            "2901/7395000 (epoch 1), train_loss = 1.444, time/batch = 0.054\n",
            "2902/7395000 (epoch 1), train_loss = 1.474, time/batch = 0.063\n",
            "2903/7395000 (epoch 1), train_loss = 1.408, time/batch = 0.061\n",
            "2904/7395000 (epoch 1), train_loss = 1.298, time/batch = 0.061\n",
            "2905/7395000 (epoch 1), train_loss = 1.423, time/batch = 0.061\n",
            "2906/7395000 (epoch 1), train_loss = 1.421, time/batch = 0.059\n",
            "2907/7395000 (epoch 1), train_loss = 1.357, time/batch = 0.062\n",
            "2908/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.058\n",
            "2909/7395000 (epoch 1), train_loss = 1.441, time/batch = 0.063\n",
            "2910/7395000 (epoch 1), train_loss = 1.408, time/batch = 0.061\n",
            "2911/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.062\n",
            "2912/7395000 (epoch 1), train_loss = 1.436, time/batch = 0.059\n",
            "2913/7395000 (epoch 1), train_loss = 1.294, time/batch = 0.054\n",
            "2914/7395000 (epoch 1), train_loss = 1.396, time/batch = 0.055\n",
            "2915/7395000 (epoch 1), train_loss = 1.317, time/batch = 0.062\n",
            "2916/7395000 (epoch 1), train_loss = 1.442, time/batch = 0.059\n",
            "2917/7395000 (epoch 1), train_loss = 1.382, time/batch = 0.060\n",
            "2918/7395000 (epoch 1), train_loss = 1.449, time/batch = 0.061\n",
            "2919/7395000 (epoch 1), train_loss = 1.440, time/batch = 0.059\n",
            "2920/7395000 (epoch 1), train_loss = 1.549, time/batch = 0.064\n",
            "2921/7395000 (epoch 1), train_loss = 1.565, time/batch = 0.058\n",
            "2922/7395000 (epoch 1), train_loss = 1.502, time/batch = 0.061\n",
            "2923/7395000 (epoch 1), train_loss = 1.510, time/batch = 0.064\n",
            "2924/7395000 (epoch 1), train_loss = 1.490, time/batch = 0.060\n",
            "2925/7395000 (epoch 1), train_loss = 1.529, time/batch = 0.061\n",
            "2926/7395000 (epoch 1), train_loss = 1.499, time/batch = 0.060\n",
            "2927/7395000 (epoch 1), train_loss = 1.464, time/batch = 0.059\n",
            "2928/7395000 (epoch 1), train_loss = 1.463, time/batch = 0.060\n",
            "2929/7395000 (epoch 1), train_loss = 1.442, time/batch = 0.061\n",
            "2930/7395000 (epoch 1), train_loss = 1.378, time/batch = 0.064\n",
            "2931/7395000 (epoch 1), train_loss = 1.379, time/batch = 0.062\n",
            "2932/7395000 (epoch 1), train_loss = 1.400, time/batch = 0.061\n",
            "2933/7395000 (epoch 1), train_loss = 1.492, time/batch = 0.061\n",
            "2934/7395000 (epoch 1), train_loss = 1.381, time/batch = 0.058\n",
            "2935/7395000 (epoch 1), train_loss = 1.392, time/batch = 0.065\n",
            "2936/7395000 (epoch 1), train_loss = 1.416, time/batch = 0.059\n",
            "2937/7395000 (epoch 1), train_loss = 1.422, time/batch = 0.060\n",
            "2938/7395000 (epoch 1), train_loss = 1.346, time/batch = 0.061\n",
            "2939/7395000 (epoch 1), train_loss = 1.335, time/batch = 0.062\n",
            "2940/7395000 (epoch 1), train_loss = 1.342, time/batch = 0.064\n",
            "2941/7395000 (epoch 1), train_loss = 1.311, time/batch = 0.059\n",
            "2942/7395000 (epoch 1), train_loss = 1.345, time/batch = 0.059\n",
            "2943/7395000 (epoch 1), train_loss = 1.452, time/batch = 0.061\n",
            "2944/7395000 (epoch 1), train_loss = 1.294, time/batch = 0.064\n",
            "2945/7395000 (epoch 1), train_loss = 1.262, time/batch = 0.061\n",
            "2946/7395000 (epoch 1), train_loss = 1.362, time/batch = 0.062\n",
            "2947/7395000 (epoch 1), train_loss = 1.360, time/batch = 0.061\n",
            "2948/7395000 (epoch 1), train_loss = 1.266, time/batch = 0.060\n",
            "2949/7395000 (epoch 1), train_loss = 1.287, time/batch = 0.062\n",
            "2950/7395000 (epoch 1), train_loss = 1.365, time/batch = 0.063\n",
            "2951/7395000 (epoch 1), train_loss = 1.415, time/batch = 0.061\n",
            "2952/7395000 (epoch 1), train_loss = 1.225, time/batch = 0.068\n",
            "2953/7395000 (epoch 1), train_loss = 1.402, time/batch = 0.063\n",
            "2954/7395000 (epoch 1), train_loss = 1.379, time/batch = 0.059\n",
            "2955/7395000 (epoch 1), train_loss = 1.324, time/batch = 0.062\n",
            "2956/7395000 (epoch 1), train_loss = 1.375, time/batch = 0.067\n",
            "2957/7395000 (epoch 1), train_loss = 1.369, time/batch = 0.060\n",
            "2958/7395000 (epoch 2), train_loss = 1.621, time/batch = 0.062\n",
            "2959/7395000 (epoch 2), train_loss = 1.443, time/batch = 0.061\n",
            "2960/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.060\n",
            "2961/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.061\n",
            "2962/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.057\n",
            "2963/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.065\n",
            "2964/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.063\n",
            "2965/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.058\n",
            "2966/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.061\n",
            "2967/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.063\n",
            "2968/7395000 (epoch 2), train_loss = 1.455, time/batch = 0.063\n",
            "2969/7395000 (epoch 2), train_loss = 1.404, time/batch = 0.060\n",
            "2970/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.061\n",
            "2971/7395000 (epoch 2), train_loss = 1.412, time/batch = 0.067\n",
            "2972/7395000 (epoch 2), train_loss = 1.293, time/batch = 0.073\n",
            "2973/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.053\n",
            "2974/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.054\n",
            "2975/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.055\n",
            "2976/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.058\n",
            "2977/7395000 (epoch 2), train_loss = 1.472, time/batch = 0.061\n",
            "2978/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.063\n",
            "2979/7395000 (epoch 2), train_loss = 1.482, time/batch = 0.061\n",
            "2980/7395000 (epoch 2), train_loss = 1.400, time/batch = 0.061\n",
            "2981/7395000 (epoch 2), train_loss = 1.454, time/batch = 0.061\n",
            "2982/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.060\n",
            "2983/7395000 (epoch 2), train_loss = 1.447, time/batch = 0.060\n",
            "2984/7395000 (epoch 2), train_loss = 1.393, time/batch = 0.060\n",
            "2985/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.059\n",
            "2986/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.061\n",
            "2987/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.061\n",
            "2988/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.060\n",
            "2989/7395000 (epoch 2), train_loss = 1.421, time/batch = 0.062\n",
            "2990/7395000 (epoch 2), train_loss = 1.378, time/batch = 0.064\n",
            "2991/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.062\n",
            "2992/7395000 (epoch 2), train_loss = 1.515, time/batch = 0.059\n",
            "2993/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.060\n",
            "2994/7395000 (epoch 2), train_loss = 1.521, time/batch = 0.060\n",
            "2995/7395000 (epoch 2), train_loss = 1.493, time/batch = 0.061\n",
            "2996/7395000 (epoch 2), train_loss = 1.450, time/batch = 0.061\n",
            "2997/7395000 (epoch 2), train_loss = 1.474, time/batch = 0.057\n",
            "2998/7395000 (epoch 2), train_loss = 1.451, time/batch = 0.059\n",
            "2999/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.062\n",
            "3000/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.060\n",
            "model saved to save/model.ckpt\n",
            "3001/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.061\n",
            "3002/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "3003/7395000 (epoch 2), train_loss = 1.388, time/batch = 0.061\n",
            "3004/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.063\n",
            "3005/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.053\n",
            "3006/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.060\n",
            "3007/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.061\n",
            "3008/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.062\n",
            "3009/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.062\n",
            "3010/7395000 (epoch 2), train_loss = 1.413, time/batch = 0.063\n",
            "3011/7395000 (epoch 2), train_loss = 1.429, time/batch = 0.061\n",
            "3012/7395000 (epoch 2), train_loss = 1.492, time/batch = 0.061\n",
            "3013/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.060\n",
            "3014/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.064\n",
            "3015/7395000 (epoch 2), train_loss = 1.278, time/batch = 0.061\n",
            "3016/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.063\n",
            "3017/7395000 (epoch 2), train_loss = 1.411, time/batch = 0.059\n",
            "3018/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.059\n",
            "3019/7395000 (epoch 2), train_loss = 1.378, time/batch = 0.062\n",
            "3020/7395000 (epoch 2), train_loss = 1.410, time/batch = 0.062\n",
            "3021/7395000 (epoch 2), train_loss = 1.444, time/batch = 0.059\n",
            "3022/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.060\n",
            "3023/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.060\n",
            "3024/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.062\n",
            "3025/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.059\n",
            "3026/7395000 (epoch 2), train_loss = 1.464, time/batch = 0.060\n",
            "3027/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.059\n",
            "3028/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.060\n",
            "3029/7395000 (epoch 2), train_loss = 1.459, time/batch = 0.065\n",
            "3030/7395000 (epoch 2), train_loss = 1.412, time/batch = 0.060\n",
            "3031/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.062\n",
            "3032/7395000 (epoch 2), train_loss = 1.468, time/batch = 0.060\n",
            "3033/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.060\n",
            "3034/7395000 (epoch 2), train_loss = 1.411, time/batch = 0.062\n",
            "3035/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.062\n",
            "3036/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.061\n",
            "3037/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.060\n",
            "3038/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.061\n",
            "3039/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.063\n",
            "3040/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.060\n",
            "3041/7395000 (epoch 2), train_loss = 1.345, time/batch = 0.064\n",
            "3042/7395000 (epoch 2), train_loss = 1.419, time/batch = 0.060\n",
            "3043/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.059\n",
            "3044/7395000 (epoch 2), train_loss = 1.426, time/batch = 0.060\n",
            "3045/7395000 (epoch 2), train_loss = 1.466, time/batch = 0.065\n",
            "3046/7395000 (epoch 2), train_loss = 1.461, time/batch = 0.058\n",
            "3047/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.061\n",
            "3048/7395000 (epoch 2), train_loss = 1.523, time/batch = 0.062\n",
            "3049/7395000 (epoch 2), train_loss = 1.454, time/batch = 0.065\n",
            "3050/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.062\n",
            "3051/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.059\n",
            "3052/7395000 (epoch 2), train_loss = 1.393, time/batch = 0.062\n",
            "3053/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.063\n",
            "3054/7395000 (epoch 2), train_loss = 1.413, time/batch = 0.062\n",
            "3055/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.060\n",
            "3056/7395000 (epoch 2), train_loss = 1.439, time/batch = 0.062\n",
            "3057/7395000 (epoch 2), train_loss = 1.418, time/batch = 0.064\n",
            "3058/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.061\n",
            "3059/7395000 (epoch 2), train_loss = 1.350, time/batch = 0.061\n",
            "3060/7395000 (epoch 2), train_loss = 1.394, time/batch = 0.061\n",
            "3061/7395000 (epoch 2), train_loss = 1.440, time/batch = 0.063\n",
            "3062/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.066\n",
            "3063/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.061\n",
            "3064/7395000 (epoch 2), train_loss = 1.505, time/batch = 0.062\n",
            "3065/7395000 (epoch 2), train_loss = 1.482, time/batch = 0.061\n",
            "3066/7395000 (epoch 2), train_loss = 1.516, time/batch = 0.061\n",
            "3067/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.069\n",
            "3068/7395000 (epoch 2), train_loss = 1.427, time/batch = 0.061\n",
            "3069/7395000 (epoch 2), train_loss = 1.519, time/batch = 0.058\n",
            "3070/7395000 (epoch 2), train_loss = 1.582, time/batch = 0.054\n",
            "3071/7395000 (epoch 2), train_loss = 1.489, time/batch = 0.058\n",
            "3072/7395000 (epoch 2), train_loss = 1.470, time/batch = 0.059\n",
            "3073/7395000 (epoch 2), train_loss = 1.419, time/batch = 0.061\n",
            "3074/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.062\n",
            "3075/7395000 (epoch 2), train_loss = 1.468, time/batch = 0.065\n",
            "3076/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.061\n",
            "3077/7395000 (epoch 2), train_loss = 1.404, time/batch = 0.059\n",
            "3078/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.060\n",
            "3079/7395000 (epoch 2), train_loss = 1.356, time/batch = 0.061\n",
            "3080/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.065\n",
            "3081/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.061\n",
            "3082/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.065\n",
            "3083/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.061\n",
            "3084/7395000 (epoch 2), train_loss = 1.265, time/batch = 0.063\n",
            "3085/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.060\n",
            "3086/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.060\n",
            "3087/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.060\n",
            "3088/7395000 (epoch 2), train_loss = 1.361, time/batch = 0.061\n",
            "3089/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.074\n",
            "3090/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.061\n",
            "3091/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.059\n",
            "3092/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.056\n",
            "3093/7395000 (epoch 2), train_loss = 1.401, time/batch = 0.060\n",
            "3094/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.061\n",
            "3095/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.061\n",
            "3096/7395000 (epoch 2), train_loss = 1.425, time/batch = 0.061\n",
            "3097/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.055\n",
            "3098/7395000 (epoch 2), train_loss = 1.483, time/batch = 0.062\n",
            "3099/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.059\n",
            "3100/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.060\n",
            "3101/7395000 (epoch 2), train_loss = 1.388, time/batch = 0.058\n",
            "3102/7395000 (epoch 2), train_loss = 1.423, time/batch = 0.062\n",
            "3103/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.060\n",
            "3104/7395000 (epoch 2), train_loss = 1.477, time/batch = 0.061\n",
            "3105/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.065\n",
            "3106/7395000 (epoch 2), train_loss = 1.443, time/batch = 0.060\n",
            "3107/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.059\n",
            "3108/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.063\n",
            "3109/7395000 (epoch 2), train_loss = 1.446, time/batch = 0.061\n",
            "3110/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.061\n",
            "3111/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.061\n",
            "3112/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.062\n",
            "3113/7395000 (epoch 2), train_loss = 1.249, time/batch = 0.061\n",
            "3114/7395000 (epoch 2), train_loss = 1.415, time/batch = 0.061\n",
            "3115/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.060\n",
            "3116/7395000 (epoch 2), train_loss = 1.300, time/batch = 0.062\n",
            "3117/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.064\n",
            "3118/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.061\n",
            "3119/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.058\n",
            "3120/7395000 (epoch 2), train_loss = 1.351, time/batch = 0.062\n",
            "3121/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.059\n",
            "3122/7395000 (epoch 2), train_loss = 1.273, time/batch = 0.060\n",
            "3123/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.066\n",
            "3124/7395000 (epoch 2), train_loss = 1.222, time/batch = 0.058\n",
            "3125/7395000 (epoch 2), train_loss = 1.436, time/batch = 0.059\n",
            "3126/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.060\n",
            "3127/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "3128/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.060\n",
            "3129/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.062\n",
            "3130/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.060\n",
            "3131/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.061\n",
            "3132/7395000 (epoch 2), train_loss = 1.422, time/batch = 0.060\n",
            "3133/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.060\n",
            "3134/7395000 (epoch 2), train_loss = 1.274, time/batch = 0.068\n",
            "3135/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.055\n",
            "3136/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.061\n",
            "3137/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.058\n",
            "3138/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.061\n",
            "3139/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.062\n",
            "3140/7395000 (epoch 2), train_loss = 1.250, time/batch = 0.058\n",
            "3141/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.061\n",
            "3142/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.062\n",
            "3143/7395000 (epoch 2), train_loss = 1.274, time/batch = 0.067\n",
            "3144/7395000 (epoch 2), train_loss = 1.382, time/batch = 0.060\n",
            "3145/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.062\n",
            "3146/7395000 (epoch 2), train_loss = 1.392, time/batch = 0.060\n",
            "3147/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.060\n",
            "3148/7395000 (epoch 2), train_loss = 1.427, time/batch = 0.061\n",
            "3149/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.062\n",
            "3150/7395000 (epoch 2), train_loss = 1.394, time/batch = 0.061\n",
            "3151/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.060\n",
            "3152/7395000 (epoch 2), train_loss = 1.408, time/batch = 0.062\n",
            "3153/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.059\n",
            "3154/7395000 (epoch 2), train_loss = 1.490, time/batch = 0.062\n",
            "3155/7395000 (epoch 2), train_loss = 1.470, time/batch = 0.061\n",
            "3156/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.062\n",
            "3157/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.061\n",
            "3158/7395000 (epoch 2), train_loss = 1.427, time/batch = 0.059\n",
            "3159/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.059\n",
            "3160/7395000 (epoch 2), train_loss = 1.317, time/batch = 0.062\n",
            "3161/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.061\n",
            "3162/7395000 (epoch 2), train_loss = 1.410, time/batch = 0.061\n",
            "3163/7395000 (epoch 2), train_loss = 1.361, time/batch = 0.059\n",
            "3164/7395000 (epoch 2), train_loss = 1.485, time/batch = 0.061\n",
            "3165/7395000 (epoch 2), train_loss = 1.463, time/batch = 0.060\n",
            "3166/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.061\n",
            "3167/7395000 (epoch 2), train_loss = 1.421, time/batch = 0.061\n",
            "3168/7395000 (epoch 2), train_loss = 1.385, time/batch = 0.059\n",
            "3169/7395000 (epoch 2), train_loss = 1.442, time/batch = 0.063\n",
            "3170/7395000 (epoch 2), train_loss = 1.478, time/batch = 0.060\n",
            "3171/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.062\n",
            "3172/7395000 (epoch 2), train_loss = 1.279, time/batch = 0.056\n",
            "3173/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.060\n",
            "3174/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.061\n",
            "3175/7395000 (epoch 2), train_loss = 1.388, time/batch = 0.061\n",
            "3176/7395000 (epoch 2), train_loss = 1.399, time/batch = 0.066\n",
            "3177/7395000 (epoch 2), train_loss = 1.285, time/batch = 0.059\n",
            "3178/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.056\n",
            "3179/7395000 (epoch 2), train_loss = 1.434, time/batch = 0.061\n",
            "3180/7395000 (epoch 2), train_loss = 1.406, time/batch = 0.061\n",
            "3181/7395000 (epoch 2), train_loss = 1.420, time/batch = 0.060\n",
            "3182/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.061\n",
            "3183/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.059\n",
            "3184/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.066\n",
            "3185/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.057\n",
            "3186/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.066\n",
            "3187/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.060\n",
            "3188/7395000 (epoch 2), train_loss = 1.454, time/batch = 0.061\n",
            "3189/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.069\n",
            "3190/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.059\n",
            "3191/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.061\n",
            "3192/7395000 (epoch 2), train_loss = 1.495, time/batch = 0.060\n",
            "3193/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.059\n",
            "3194/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.066\n",
            "3195/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.065\n",
            "3196/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.054\n",
            "3197/7395000 (epoch 2), train_loss = 1.400, time/batch = 0.059\n",
            "3198/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.060\n",
            "3199/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.065\n",
            "3200/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.062\n",
            "3201/7395000 (epoch 2), train_loss = 1.388, time/batch = 0.068\n",
            "3202/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.060\n",
            "3203/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.061\n",
            "3204/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.066\n",
            "3205/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.060\n",
            "3206/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.062\n",
            "3207/7395000 (epoch 2), train_loss = 1.463, time/batch = 0.064\n",
            "3208/7395000 (epoch 2), train_loss = 1.453, time/batch = 0.059\n",
            "3209/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.062\n",
            "3210/7395000 (epoch 2), train_loss = 1.413, time/batch = 0.063\n",
            "3211/7395000 (epoch 2), train_loss = 1.414, time/batch = 0.059\n",
            "3212/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.060\n",
            "3213/7395000 (epoch 2), train_loss = 1.443, time/batch = 0.064\n",
            "3214/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.055\n",
            "3215/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.061\n",
            "3216/7395000 (epoch 2), train_loss = 1.532, time/batch = 0.061\n",
            "3217/7395000 (epoch 2), train_loss = 1.447, time/batch = 0.064\n",
            "3218/7395000 (epoch 2), train_loss = 1.462, time/batch = 0.059\n",
            "3219/7395000 (epoch 2), train_loss = 1.460, time/batch = 0.062\n",
            "3220/7395000 (epoch 2), train_loss = 1.460, time/batch = 0.063\n",
            "3221/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.059\n",
            "3222/7395000 (epoch 2), train_loss = 1.470, time/batch = 0.063\n",
            "3223/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.059\n",
            "3224/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.059\n",
            "3225/7395000 (epoch 2), train_loss = 1.404, time/batch = 0.068\n",
            "3226/7395000 (epoch 2), train_loss = 1.508, time/batch = 0.058\n",
            "3227/7395000 (epoch 2), train_loss = 1.492, time/batch = 0.060\n",
            "3228/7395000 (epoch 2), train_loss = 1.407, time/batch = 0.060\n",
            "3229/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.056\n",
            "3230/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.062\n",
            "3231/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.058\n",
            "3232/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.062\n",
            "3233/7395000 (epoch 2), train_loss = 1.427, time/batch = 0.059\n",
            "3234/7395000 (epoch 2), train_loss = 1.440, time/batch = 0.065\n",
            "3235/7395000 (epoch 2), train_loss = 1.459, time/batch = 0.063\n",
            "3236/7395000 (epoch 2), train_loss = 1.503, time/batch = 0.060\n",
            "3237/7395000 (epoch 2), train_loss = 1.505, time/batch = 0.058\n",
            "3238/7395000 (epoch 2), train_loss = 1.525, time/batch = 0.061\n",
            "3239/7395000 (epoch 2), train_loss = 1.475, time/batch = 0.058\n",
            "3240/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.062\n",
            "3241/7395000 (epoch 2), train_loss = 1.458, time/batch = 0.058\n",
            "3242/7395000 (epoch 2), train_loss = 1.258, time/batch = 0.062\n",
            "3243/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.061\n",
            "3244/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.059\n",
            "3245/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.059\n",
            "3246/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.059\n",
            "3247/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.060\n",
            "3248/7395000 (epoch 2), train_loss = 1.490, time/batch = 0.062\n",
            "3249/7395000 (epoch 2), train_loss = 1.443, time/batch = 0.064\n",
            "3250/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.060\n",
            "3251/7395000 (epoch 2), train_loss = 1.472, time/batch = 0.062\n",
            "3252/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.061\n",
            "3253/7395000 (epoch 2), train_loss = 1.417, time/batch = 0.060\n",
            "3254/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.061\n",
            "3255/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.061\n",
            "3256/7395000 (epoch 2), train_loss = 1.441, time/batch = 0.061\n",
            "3257/7395000 (epoch 2), train_loss = 1.439, time/batch = 0.064\n",
            "3258/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.060\n",
            "3259/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.057\n",
            "3260/7395000 (epoch 2), train_loss = 1.421, time/batch = 0.060\n",
            "3261/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.061\n",
            "3262/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.067\n",
            "3263/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.061\n",
            "3264/7395000 (epoch 2), train_loss = 1.418, time/batch = 0.061\n",
            "3265/7395000 (epoch 2), train_loss = 1.426, time/batch = 0.059\n",
            "3266/7395000 (epoch 2), train_loss = 1.393, time/batch = 0.062\n",
            "3267/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.068\n",
            "3268/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.066\n",
            "3269/7395000 (epoch 2), train_loss = 1.462, time/batch = 0.061\n",
            "3270/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.060\n",
            "3271/7395000 (epoch 2), train_loss = 1.350, time/batch = 0.062\n",
            "3272/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.061\n",
            "3273/7395000 (epoch 2), train_loss = 1.501, time/batch = 0.065\n",
            "3274/7395000 (epoch 2), train_loss = 1.425, time/batch = 0.059\n",
            "3275/7395000 (epoch 2), train_loss = 1.445, time/batch = 0.060\n",
            "3276/7395000 (epoch 2), train_loss = 1.438, time/batch = 0.060\n",
            "3277/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "3278/7395000 (epoch 2), train_loss = 1.464, time/batch = 0.063\n",
            "3279/7395000 (epoch 2), train_loss = 1.429, time/batch = 0.065\n",
            "3280/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.065\n",
            "3281/7395000 (epoch 2), train_loss = 1.435, time/batch = 0.062\n",
            "3282/7395000 (epoch 2), train_loss = 1.392, time/batch = 0.060\n",
            "3283/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.060\n",
            "3284/7395000 (epoch 2), train_loss = 1.246, time/batch = 0.062\n",
            "3285/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.058\n",
            "3286/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.066\n",
            "3287/7395000 (epoch 2), train_loss = 1.232, time/batch = 0.059\n",
            "3288/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.060\n",
            "3289/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.061\n",
            "3290/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.055\n",
            "3291/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.059\n",
            "3292/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.063\n",
            "3293/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.063\n",
            "3294/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.060\n",
            "3295/7395000 (epoch 2), train_loss = 1.237, time/batch = 0.060\n",
            "3296/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.063\n",
            "3297/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.059\n",
            "3298/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.064\n",
            "3299/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.061\n",
            "3300/7395000 (epoch 2), train_loss = 1.285, time/batch = 0.058\n",
            "3301/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.063\n",
            "3302/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.064\n",
            "3303/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.054\n",
            "3304/7395000 (epoch 2), train_loss = 1.425, time/batch = 0.054\n",
            "3305/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.055\n",
            "3306/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.055\n",
            "3307/7395000 (epoch 2), train_loss = 1.285, time/batch = 0.054\n",
            "3308/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.053\n",
            "3309/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.066\n",
            "3310/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.060\n",
            "3311/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.054\n",
            "3312/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.054\n",
            "3313/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.054\n",
            "3314/7395000 (epoch 2), train_loss = 1.293, time/batch = 0.056\n",
            "3315/7395000 (epoch 2), train_loss = 1.293, time/batch = 0.062\n",
            "3316/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.061\n",
            "3317/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.059\n",
            "3318/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.062\n",
            "3319/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.066\n",
            "3320/7395000 (epoch 2), train_loss = 1.173, time/batch = 0.065\n",
            "3321/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.059\n",
            "3322/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.059\n",
            "3323/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.064\n",
            "3324/7395000 (epoch 2), train_loss = 1.408, time/batch = 0.061\n",
            "3325/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.061\n",
            "3326/7395000 (epoch 2), train_loss = 1.239, time/batch = 0.062\n",
            "3327/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.064\n",
            "3328/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.061\n",
            "3329/7395000 (epoch 2), train_loss = 1.424, time/batch = 0.062\n",
            "3330/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.063\n",
            "3331/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.061\n",
            "3332/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.062\n",
            "3333/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.065\n",
            "3334/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.061\n",
            "3335/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.065\n",
            "3336/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.060\n",
            "3337/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.061\n",
            "3338/7395000 (epoch 2), train_loss = 1.273, time/batch = 0.065\n",
            "3339/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.067\n",
            "3340/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.060\n",
            "3341/7395000 (epoch 2), train_loss = 1.269, time/batch = 0.060\n",
            "3342/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.061\n",
            "3343/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.064\n",
            "3344/7395000 (epoch 2), train_loss = 1.483, time/batch = 0.060\n",
            "3345/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.059\n",
            "3346/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.060\n",
            "3347/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.059\n",
            "3348/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.065\n",
            "3349/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.061\n",
            "3350/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.059\n",
            "3351/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.067\n",
            "3352/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.068\n",
            "3353/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.059\n",
            "3354/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.065\n",
            "3355/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.061\n",
            "3356/7395000 (epoch 2), train_loss = 1.366, time/batch = 0.055\n",
            "3357/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.062\n",
            "3358/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.063\n",
            "3359/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.064\n",
            "3360/7395000 (epoch 2), train_loss = 1.366, time/batch = 0.060\n",
            "3361/7395000 (epoch 2), train_loss = 1.444, time/batch = 0.059\n",
            "3362/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.062\n",
            "3363/7395000 (epoch 2), train_loss = 1.508, time/batch = 0.060\n",
            "3364/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.062\n",
            "3365/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.061\n",
            "3366/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.061\n",
            "3367/7395000 (epoch 2), train_loss = 1.265, time/batch = 0.062\n",
            "3368/7395000 (epoch 2), train_loss = 1.445, time/batch = 0.055\n",
            "3369/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.057\n",
            "3370/7395000 (epoch 2), train_loss = 1.244, time/batch = 0.062\n",
            "3371/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.062\n",
            "3372/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.061\n",
            "3373/7395000 (epoch 2), train_loss = 1.213, time/batch = 0.057\n",
            "3374/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.063\n",
            "3375/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.059\n",
            "3376/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.060\n",
            "3377/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.060\n",
            "3378/7395000 (epoch 2), train_loss = 1.385, time/batch = 0.064\n",
            "3379/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.060\n",
            "3380/7395000 (epoch 2), train_loss = 1.239, time/batch = 0.062\n",
            "3381/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.061\n",
            "3382/7395000 (epoch 2), train_loss = 1.231, time/batch = 0.059\n",
            "3383/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.055\n",
            "3384/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.064\n",
            "3385/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.062\n",
            "3386/7395000 (epoch 2), train_loss = 1.274, time/batch = 0.059\n",
            "3387/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.059\n",
            "3388/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.063\n",
            "3389/7395000 (epoch 2), train_loss = 1.221, time/batch = 0.059\n",
            "3390/7395000 (epoch 2), train_loss = 1.464, time/batch = 0.066\n",
            "3391/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.063\n",
            "3392/7395000 (epoch 2), train_loss = 1.385, time/batch = 0.062\n",
            "3393/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.059\n",
            "3394/7395000 (epoch 2), train_loss = 1.423, time/batch = 0.062\n",
            "3395/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.060\n",
            "3396/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.063\n",
            "3397/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.061\n",
            "3398/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.062\n",
            "3399/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.060\n",
            "3400/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.054\n",
            "3401/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.059\n",
            "3402/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.058\n",
            "3403/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.063\n",
            "3404/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.054\n",
            "3405/7395000 (epoch 2), train_loss = 1.256, time/batch = 0.054\n",
            "3406/7395000 (epoch 2), train_loss = 1.248, time/batch = 0.056\n",
            "3407/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.054\n",
            "3408/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.055\n",
            "3409/7395000 (epoch 2), train_loss = 1.223, time/batch = 0.065\n",
            "3410/7395000 (epoch 2), train_loss = 1.273, time/batch = 0.057\n",
            "3411/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.053\n",
            "3412/7395000 (epoch 2), train_loss = 1.250, time/batch = 0.057\n",
            "3413/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.062\n",
            "3414/7395000 (epoch 2), train_loss = 1.300, time/batch = 0.062\n",
            "3415/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.064\n",
            "3416/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.060\n",
            "3417/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.060\n",
            "3418/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.061\n",
            "3419/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.061\n",
            "3420/7395000 (epoch 2), train_loss = 1.299, time/batch = 0.067\n",
            "3421/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.060\n",
            "3422/7395000 (epoch 2), train_loss = 1.285, time/batch = 0.063\n",
            "3423/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.061\n",
            "3424/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.062\n",
            "3425/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.061\n",
            "3426/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.062\n",
            "3427/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.064\n",
            "3428/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.062\n",
            "3429/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.060\n",
            "3430/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.057\n",
            "3431/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.055\n",
            "3432/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.062\n",
            "3433/7395000 (epoch 2), train_loss = 1.462, time/batch = 0.061\n",
            "3434/7395000 (epoch 2), train_loss = 1.412, time/batch = 0.057\n",
            "3435/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.061\n",
            "3436/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.062\n",
            "3437/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.063\n",
            "3438/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.062\n",
            "3439/7395000 (epoch 2), train_loss = 1.421, time/batch = 0.061\n",
            "3440/7395000 (epoch 2), train_loss = 1.514, time/batch = 0.060\n",
            "3441/7395000 (epoch 2), train_loss = 1.449, time/batch = 0.060\n",
            "3442/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.060\n",
            "3443/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.060\n",
            "3444/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.061\n",
            "3445/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.056\n",
            "3446/7395000 (epoch 2), train_loss = 1.426, time/batch = 0.056\n",
            "3447/7395000 (epoch 2), train_loss = 1.410, time/batch = 0.059\n",
            "3448/7395000 (epoch 2), train_loss = 1.351, time/batch = 0.061\n",
            "3449/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.061\n",
            "3450/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.062\n",
            "3451/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.060\n",
            "3452/7395000 (epoch 2), train_loss = 1.458, time/batch = 0.063\n",
            "3453/7395000 (epoch 2), train_loss = 1.505, time/batch = 0.061\n",
            "3454/7395000 (epoch 2), train_loss = 1.505, time/batch = 0.065\n",
            "3455/7395000 (epoch 2), train_loss = 1.437, time/batch = 0.058\n",
            "3456/7395000 (epoch 2), train_loss = 1.575, time/batch = 0.061\n",
            "3457/7395000 (epoch 2), train_loss = 1.446, time/batch = 0.061\n",
            "3458/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.062\n",
            "3459/7395000 (epoch 2), train_loss = 1.485, time/batch = 0.059\n",
            "3460/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.055\n",
            "3461/7395000 (epoch 2), train_loss = 1.408, time/batch = 0.062\n",
            "3462/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.067\n",
            "3463/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.061\n",
            "3464/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.062\n",
            "3465/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.058\n",
            "3466/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.060\n",
            "3467/7395000 (epoch 2), train_loss = 1.456, time/batch = 0.062\n",
            "3468/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.068\n",
            "3469/7395000 (epoch 2), train_loss = 1.453, time/batch = 0.066\n",
            "3470/7395000 (epoch 2), train_loss = 1.534, time/batch = 0.064\n",
            "3471/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.059\n",
            "3472/7395000 (epoch 2), train_loss = 1.492, time/batch = 0.062\n",
            "3473/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.061\n",
            "3474/7395000 (epoch 2), train_loss = 1.385, time/batch = 0.060\n",
            "3475/7395000 (epoch 2), train_loss = 1.517, time/batch = 0.062\n",
            "3476/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.063\n",
            "3477/7395000 (epoch 2), train_loss = 1.421, time/batch = 0.062\n",
            "3478/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.059\n",
            "3479/7395000 (epoch 2), train_loss = 1.445, time/batch = 0.064\n",
            "3480/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.060\n",
            "3481/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.065\n",
            "3482/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.063\n",
            "3483/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.061\n",
            "3484/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.062\n",
            "3485/7395000 (epoch 2), train_loss = 1.434, time/batch = 0.061\n",
            "3486/7395000 (epoch 2), train_loss = 1.408, time/batch = 0.060\n",
            "3487/7395000 (epoch 2), train_loss = 1.407, time/batch = 0.066\n",
            "3488/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.060\n",
            "3489/7395000 (epoch 2), train_loss = 1.430, time/batch = 0.062\n",
            "3490/7395000 (epoch 2), train_loss = 1.235, time/batch = 0.062\n",
            "3491/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.060\n",
            "3492/7395000 (epoch 2), train_loss = 1.420, time/batch = 0.062\n",
            "3493/7395000 (epoch 2), train_loss = 1.392, time/batch = 0.061\n",
            "3494/7395000 (epoch 2), train_loss = 1.382, time/batch = 0.062\n",
            "3495/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.060\n",
            "3496/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.064\n",
            "3497/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.060\n",
            "3498/7395000 (epoch 2), train_loss = 1.446, time/batch = 0.060\n",
            "3499/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.061\n",
            "3500/7395000 (epoch 2), train_loss = 1.350, time/batch = 0.063\n",
            "3501/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.058\n",
            "3502/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.064\n",
            "3503/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.061\n",
            "3504/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.062\n",
            "3505/7395000 (epoch 2), train_loss = 1.440, time/batch = 0.062\n",
            "3506/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.069\n",
            "3507/7395000 (epoch 2), train_loss = 1.407, time/batch = 0.065\n",
            "3508/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.061\n",
            "3509/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.060\n",
            "3510/7395000 (epoch 2), train_loss = 1.269, time/batch = 0.062\n",
            "3511/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.061\n",
            "3512/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.064\n",
            "3513/7395000 (epoch 2), train_loss = 1.282, time/batch = 0.060\n",
            "3514/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.063\n",
            "3515/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.058\n",
            "3516/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.062\n",
            "3517/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.061\n",
            "3518/7395000 (epoch 2), train_loss = 1.279, time/batch = 0.060\n",
            "3519/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.062\n",
            "3520/7395000 (epoch 2), train_loss = 1.229, time/batch = 0.071\n",
            "3521/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.061\n",
            "3522/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.060\n",
            "3523/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.059\n",
            "3524/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.061\n",
            "3525/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.064\n",
            "3526/7395000 (epoch 2), train_loss = 1.247, time/batch = 0.064\n",
            "3527/7395000 (epoch 2), train_loss = 1.278, time/batch = 0.059\n",
            "3528/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.063\n",
            "3529/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.061\n",
            "3530/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.059\n",
            "3531/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.063\n",
            "3532/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.060\n",
            "3533/7395000 (epoch 2), train_loss = 1.473, time/batch = 0.062\n",
            "3534/7395000 (epoch 2), train_loss = 1.282, time/batch = 0.063\n",
            "3535/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.062\n",
            "3536/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.064\n",
            "3537/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.064\n",
            "3538/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.060\n",
            "3539/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.059\n",
            "3540/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.059\n",
            "3541/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.061\n",
            "3542/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.063\n",
            "3543/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.063\n",
            "3544/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.062\n",
            "3545/7395000 (epoch 2), train_loss = 1.425, time/batch = 0.062\n",
            "3546/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.062\n",
            "3547/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.060\n",
            "3548/7395000 (epoch 2), train_loss = 1.326, time/batch = 0.064\n",
            "3549/7395000 (epoch 2), train_loss = 1.367, time/batch = 0.058\n",
            "3550/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.060\n",
            "3551/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.060\n",
            "3552/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.060\n",
            "3553/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.061\n",
            "3554/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.061\n",
            "3555/7395000 (epoch 2), train_loss = 1.406, time/batch = 0.059\n",
            "3556/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.061\n",
            "3557/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.059\n",
            "3558/7395000 (epoch 2), train_loss = 1.409, time/batch = 0.061\n",
            "3559/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.054\n",
            "3560/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.064\n",
            "3561/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.064\n",
            "3562/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.062\n",
            "3563/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.060\n",
            "3564/7395000 (epoch 2), train_loss = 1.452, time/batch = 0.059\n",
            "3565/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.061\n",
            "3566/7395000 (epoch 2), train_loss = 1.351, time/batch = 0.061\n",
            "3567/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.061\n",
            "3568/7395000 (epoch 2), train_loss = 1.408, time/batch = 0.061\n",
            "3569/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.061\n",
            "3570/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.060\n",
            "3571/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.062\n",
            "3572/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.062\n",
            "3573/7395000 (epoch 2), train_loss = 1.420, time/batch = 0.061\n",
            "3574/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.063\n",
            "3575/7395000 (epoch 2), train_loss = 1.386, time/batch = 0.061\n",
            "3576/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.063\n",
            "3577/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.062\n",
            "3578/7395000 (epoch 2), train_loss = 1.269, time/batch = 0.061\n",
            "3579/7395000 (epoch 2), train_loss = 1.244, time/batch = 0.062\n",
            "3580/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.060\n",
            "3581/7395000 (epoch 2), train_loss = 1.226, time/batch = 0.061\n",
            "3582/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.061\n",
            "3583/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.056\n",
            "3584/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.064\n",
            "3585/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.058\n",
            "3586/7395000 (epoch 2), train_loss = 1.326, time/batch = 0.060\n",
            "3587/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.059\n",
            "3588/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.057\n",
            "3589/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.060\n",
            "3590/7395000 (epoch 2), train_loss = 1.407, time/batch = 0.061\n",
            "3591/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.061\n",
            "3592/7395000 (epoch 2), train_loss = 1.414, time/batch = 0.061\n",
            "3593/7395000 (epoch 2), train_loss = 1.277, time/batch = 0.055\n",
            "3594/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.061\n",
            "3595/7395000 (epoch 2), train_loss = 1.217, time/batch = 0.061\n",
            "3596/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.059\n",
            "3597/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.061\n",
            "3598/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.059\n",
            "3599/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.061\n",
            "3600/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.062\n",
            "3601/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.061\n",
            "3602/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.061\n",
            "3603/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.061\n",
            "3604/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.064\n",
            "3605/7395000 (epoch 2), train_loss = 1.238, time/batch = 0.056\n",
            "3606/7395000 (epoch 2), train_loss = 1.262, time/batch = 0.063\n",
            "3607/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.060\n",
            "3608/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.061\n",
            "3609/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.062\n",
            "3610/7395000 (epoch 2), train_loss = 1.198, time/batch = 0.060\n",
            "3611/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.062\n",
            "3612/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.059\n",
            "3613/7395000 (epoch 2), train_loss = 1.437, time/batch = 0.060\n",
            "3614/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.060\n",
            "3615/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.059\n",
            "3616/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.065\n",
            "3617/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.066\n",
            "3618/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.061\n",
            "3619/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.060\n",
            "3620/7395000 (epoch 2), train_loss = 1.411, time/batch = 0.061\n",
            "3621/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.066\n",
            "3622/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.061\n",
            "3623/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.061\n",
            "3624/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.059\n",
            "3625/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.059\n",
            "3626/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.060\n",
            "3627/7395000 (epoch 2), train_loss = 1.258, time/batch = 0.059\n",
            "3628/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.060\n",
            "3629/7395000 (epoch 2), train_loss = 1.283, time/batch = 0.059\n",
            "3630/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.060\n",
            "3631/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.059\n",
            "3632/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.060\n",
            "3633/7395000 (epoch 2), train_loss = 1.194, time/batch = 0.062\n",
            "3634/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.059\n",
            "3635/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.053\n",
            "3636/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.053\n",
            "3637/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.053\n",
            "3638/7395000 (epoch 2), train_loss = 1.429, time/batch = 0.056\n",
            "3639/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.061\n",
            "3640/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.053\n",
            "3641/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.053\n",
            "3642/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.054\n",
            "3643/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.055\n",
            "3644/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.053\n",
            "3645/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.059\n",
            "3646/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.060\n",
            "3647/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.061\n",
            "3648/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.059\n",
            "3649/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.057\n",
            "3650/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.059\n",
            "3651/7395000 (epoch 2), train_loss = 1.273, time/batch = 0.058\n",
            "3652/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.059\n",
            "3653/7395000 (epoch 2), train_loss = 1.253, time/batch = 0.059\n",
            "3654/7395000 (epoch 2), train_loss = 1.318, time/batch = 0.059\n",
            "3655/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.063\n",
            "3656/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.063\n",
            "3657/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.062\n",
            "3658/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.062\n",
            "3659/7395000 (epoch 2), train_loss = 1.258, time/batch = 0.056\n",
            "3660/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.059\n",
            "3661/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.059\n",
            "3662/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.060\n",
            "3663/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.060\n",
            "3664/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.060\n",
            "3665/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.059\n",
            "3666/7395000 (epoch 2), train_loss = 1.210, time/batch = 0.061\n",
            "3667/7395000 (epoch 2), train_loss = 1.203, time/batch = 0.060\n",
            "3668/7395000 (epoch 2), train_loss = 1.234, time/batch = 0.060\n",
            "3669/7395000 (epoch 2), train_loss = 1.250, time/batch = 0.060\n",
            "3670/7395000 (epoch 2), train_loss = 1.210, time/batch = 0.059\n",
            "3671/7395000 (epoch 2), train_loss = 1.208, time/batch = 0.060\n",
            "3672/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.060\n",
            "3673/7395000 (epoch 2), train_loss = 1.264, time/batch = 0.065\n",
            "3674/7395000 (epoch 2), train_loss = 1.236, time/batch = 0.061\n",
            "3675/7395000 (epoch 2), train_loss = 1.366, time/batch = 0.055\n",
            "3676/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.059\n",
            "3677/7395000 (epoch 2), train_loss = 1.217, time/batch = 0.061\n",
            "3678/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.062\n",
            "3679/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.058\n",
            "3680/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.058\n",
            "3681/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.059\n",
            "3682/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.064\n",
            "3683/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.060\n",
            "3684/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.060\n",
            "3685/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.062\n",
            "3686/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.059\n",
            "3687/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.059\n",
            "3688/7395000 (epoch 2), train_loss = 1.279, time/batch = 0.061\n",
            "3689/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.060\n",
            "3690/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.062\n",
            "3691/7395000 (epoch 2), train_loss = 1.274, time/batch = 0.058\n",
            "3692/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.064\n",
            "3693/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.057\n",
            "3694/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.063\n",
            "3695/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.052\n",
            "3696/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.058\n",
            "3697/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.059\n",
            "3698/7395000 (epoch 2), train_loss = 1.314, time/batch = 0.061\n",
            "3699/7395000 (epoch 2), train_loss = 1.262, time/batch = 0.064\n",
            "3700/7395000 (epoch 2), train_loss = 1.191, time/batch = 0.059\n",
            "3701/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.058\n",
            "3702/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.060\n",
            "3703/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.060\n",
            "3704/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.065\n",
            "3705/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.059\n",
            "3706/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.058\n",
            "3707/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.063\n",
            "3708/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.060\n",
            "3709/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.061\n",
            "3710/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.058\n",
            "3711/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.061\n",
            "3712/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.062\n",
            "3713/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.060\n",
            "3714/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.059\n",
            "3715/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.058\n",
            "3716/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.066\n",
            "3717/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.059\n",
            "3718/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.062\n",
            "3719/7395000 (epoch 2), train_loss = 1.282, time/batch = 0.061\n",
            "3720/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.062\n",
            "3721/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.055\n",
            "3722/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.061\n",
            "3723/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.058\n",
            "3724/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.064\n",
            "3725/7395000 (epoch 2), train_loss = 1.277, time/batch = 0.058\n",
            "3726/7395000 (epoch 2), train_loss = 1.223, time/batch = 0.061\n",
            "3727/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.059\n",
            "3728/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.059\n",
            "3729/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.060\n",
            "3730/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.059\n",
            "3731/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.059\n",
            "3732/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.064\n",
            "3733/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.059\n",
            "3734/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.063\n",
            "3735/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.058\n",
            "3736/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.061\n",
            "3737/7395000 (epoch 2), train_loss = 1.239, time/batch = 0.062\n",
            "3738/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.060\n",
            "3739/7395000 (epoch 2), train_loss = 1.209, time/batch = 0.060\n",
            "3740/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.061\n",
            "3741/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.064\n",
            "3742/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.062\n",
            "3743/7395000 (epoch 2), train_loss = 1.284, time/batch = 0.058\n",
            "3744/7395000 (epoch 2), train_loss = 1.385, time/batch = 0.062\n",
            "3745/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.059\n",
            "3746/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.056\n",
            "3747/7395000 (epoch 2), train_loss = 1.282, time/batch = 0.059\n",
            "3748/7395000 (epoch 2), train_loss = 1.299, time/batch = 0.060\n",
            "3749/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.060\n",
            "3750/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.059\n",
            "3751/7395000 (epoch 2), train_loss = 1.222, time/batch = 0.061\n",
            "3752/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.060\n",
            "3753/7395000 (epoch 2), train_loss = 1.201, time/batch = 0.061\n",
            "3754/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.064\n",
            "3755/7395000 (epoch 2), train_loss = 1.318, time/batch = 0.058\n",
            "3756/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.062\n",
            "3757/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.061\n",
            "3758/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.068\n",
            "3759/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.059\n",
            "3760/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.060\n",
            "3761/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.056\n",
            "3762/7395000 (epoch 2), train_loss = 1.435, time/batch = 0.061\n",
            "3763/7395000 (epoch 2), train_loss = 1.394, time/batch = 0.059\n",
            "3764/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.061\n",
            "3765/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.062\n",
            "3766/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.067\n",
            "3767/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.064\n",
            "3768/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.059\n",
            "3769/7395000 (epoch 2), train_loss = 1.426, time/batch = 0.060\n",
            "3770/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.061\n",
            "3771/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.059\n",
            "3772/7395000 (epoch 2), train_loss = 1.305, time/batch = 0.064\n",
            "3773/7395000 (epoch 2), train_loss = 1.411, time/batch = 0.060\n",
            "3774/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.061\n",
            "3775/7395000 (epoch 2), train_loss = 1.400, time/batch = 0.069\n",
            "3776/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.059\n",
            "3777/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.060\n",
            "3778/7395000 (epoch 2), train_loss = 1.215, time/batch = 0.061\n",
            "3779/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.060\n",
            "3780/7395000 (epoch 2), train_loss = 1.427, time/batch = 0.064\n",
            "3781/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.057\n",
            "3782/7395000 (epoch 2), train_loss = 1.278, time/batch = 0.061\n",
            "3783/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.059\n",
            "3784/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.062\n",
            "3785/7395000 (epoch 2), train_loss = 1.438, time/batch = 0.060\n",
            "3786/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.059\n",
            "3787/7395000 (epoch 2), train_loss = 1.404, time/batch = 0.060\n",
            "3788/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.061\n",
            "3789/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.061\n",
            "3790/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.062\n",
            "3791/7395000 (epoch 2), train_loss = 1.404, time/batch = 0.060\n",
            "3792/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.060\n",
            "3793/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.059\n",
            "3794/7395000 (epoch 2), train_loss = 1.411, time/batch = 0.058\n",
            "3795/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.058\n",
            "3796/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.060\n",
            "3797/7395000 (epoch 2), train_loss = 1.351, time/batch = 0.061\n",
            "3798/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.059\n",
            "3799/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.065\n",
            "3800/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.061\n",
            "3801/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.061\n",
            "3802/7395000 (epoch 2), train_loss = 1.327, time/batch = 0.063\n",
            "3803/7395000 (epoch 2), train_loss = 1.440, time/batch = 0.061\n",
            "3804/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.059\n",
            "3805/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.059\n",
            "3806/7395000 (epoch 2), train_loss = 1.417, time/batch = 0.062\n",
            "3807/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.059\n",
            "3808/7395000 (epoch 2), train_loss = 1.394, time/batch = 0.062\n",
            "3809/7395000 (epoch 2), train_loss = 1.456, time/batch = 0.062\n",
            "3810/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.062\n",
            "3811/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.061\n",
            "3812/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.062\n",
            "3813/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.060\n",
            "3814/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.060\n",
            "3815/7395000 (epoch 2), train_loss = 1.293, time/batch = 0.059\n",
            "3816/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.061\n",
            "3817/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.064\n",
            "3818/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.061\n",
            "3819/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.063\n",
            "3820/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.062\n",
            "3821/7395000 (epoch 2), train_loss = 1.416, time/batch = 0.061\n",
            "3822/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.066\n",
            "3823/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.060\n",
            "3824/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.061\n",
            "3825/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.065\n",
            "3826/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.062\n",
            "3827/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.061\n",
            "3828/7395000 (epoch 2), train_loss = 1.424, time/batch = 0.059\n",
            "3829/7395000 (epoch 2), train_loss = 1.420, time/batch = 0.065\n",
            "3830/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.057\n",
            "3831/7395000 (epoch 2), train_loss = 1.417, time/batch = 0.061\n",
            "3832/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.061\n",
            "3833/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.061\n",
            "3834/7395000 (epoch 2), train_loss = 1.393, time/batch = 0.059\n",
            "3835/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.058\n",
            "3836/7395000 (epoch 2), train_loss = 1.382, time/batch = 0.060\n",
            "3837/7395000 (epoch 2), train_loss = 1.361, time/batch = 0.058\n",
            "3838/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.059\n",
            "3839/7395000 (epoch 2), train_loss = 1.392, time/batch = 0.062\n",
            "3840/7395000 (epoch 2), train_loss = 1.406, time/batch = 0.065\n",
            "3841/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.061\n",
            "3842/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.065\n",
            "3843/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.062\n",
            "3844/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "3845/7395000 (epoch 2), train_loss = 1.229, time/batch = 0.060\n",
            "3846/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.061\n",
            "3847/7395000 (epoch 2), train_loss = 1.384, time/batch = 0.060\n",
            "3848/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.061\n",
            "3849/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.060\n",
            "3850/7395000 (epoch 2), train_loss = 1.213, time/batch = 0.063\n",
            "3851/7395000 (epoch 2), train_loss = 1.300, time/batch = 0.058\n",
            "3852/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.061\n",
            "3853/7395000 (epoch 2), train_loss = 1.224, time/batch = 0.061\n",
            "3854/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.061\n",
            "3855/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.062\n",
            "3856/7395000 (epoch 2), train_loss = 1.214, time/batch = 0.061\n",
            "3857/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.060\n",
            "3858/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.064\n",
            "3859/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.064\n",
            "3860/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.062\n",
            "3861/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.061\n",
            "3862/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.061\n",
            "3863/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.063\n",
            "3864/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.057\n",
            "3865/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.062\n",
            "3866/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.060\n",
            "3867/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.059\n",
            "3868/7395000 (epoch 2), train_loss = 1.305, time/batch = 0.060\n",
            "3869/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.060\n",
            "3870/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.065\n",
            "3871/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.060\n",
            "3872/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.062\n",
            "3873/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.059\n",
            "3874/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.059\n",
            "3875/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.061\n",
            "3876/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.059\n",
            "3877/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.062\n",
            "3878/7395000 (epoch 2), train_loss = 1.314, time/batch = 0.061\n",
            "3879/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.059\n",
            "3880/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.061\n",
            "3881/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.060\n",
            "3882/7395000 (epoch 2), train_loss = 1.360, time/batch = 0.060\n",
            "3883/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.060\n",
            "3884/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.061\n",
            "3885/7395000 (epoch 2), train_loss = 1.350, time/batch = 0.060\n",
            "3886/7395000 (epoch 2), train_loss = 1.329, time/batch = 0.061\n",
            "3887/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.060\n",
            "3888/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.061\n",
            "3889/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.060\n",
            "3890/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.059\n",
            "3891/7395000 (epoch 2), train_loss = 1.417, time/batch = 0.061\n",
            "3892/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.059\n",
            "3893/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.056\n",
            "3894/7395000 (epoch 2), train_loss = 1.318, time/batch = 0.059\n",
            "3895/7395000 (epoch 2), train_loss = 1.214, time/batch = 0.064\n",
            "3896/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.060\n",
            "3897/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.060\n",
            "3898/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.061\n",
            "3899/7395000 (epoch 2), train_loss = 1.185, time/batch = 0.059\n",
            "3900/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.060\n",
            "3901/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.060\n",
            "3902/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.061\n",
            "3903/7395000 (epoch 2), train_loss = 1.283, time/batch = 0.061\n",
            "3904/7395000 (epoch 2), train_loss = 1.367, time/batch = 0.060\n",
            "3905/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.064\n",
            "3906/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.061\n",
            "3907/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.062\n",
            "3908/7395000 (epoch 2), train_loss = 1.262, time/batch = 0.061\n",
            "3909/7395000 (epoch 2), train_loss = 1.227, time/batch = 0.061\n",
            "3910/7395000 (epoch 2), train_loss = 1.205, time/batch = 0.062\n",
            "3911/7395000 (epoch 2), train_loss = 1.224, time/batch = 0.059\n",
            "3912/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.059\n",
            "3913/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.059\n",
            "3914/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.059\n",
            "3915/7395000 (epoch 2), train_loss = 1.350, time/batch = 0.061\n",
            "3916/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.059\n",
            "3917/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.060\n",
            "3918/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.059\n",
            "3919/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.059\n",
            "3920/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.063\n",
            "3921/7395000 (epoch 2), train_loss = 1.290, time/batch = 0.060\n",
            "3922/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.063\n",
            "3923/7395000 (epoch 2), train_loss = 1.378, time/batch = 0.061\n",
            "3924/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.060\n",
            "3925/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.061\n",
            "3926/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.060\n",
            "3927/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.058\n",
            "3928/7395000 (epoch 2), train_loss = 1.328, time/batch = 0.063\n",
            "3929/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.059\n",
            "3930/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.060\n",
            "3931/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.062\n",
            "3932/7395000 (epoch 2), train_loss = 1.258, time/batch = 0.058\n",
            "3933/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.060\n",
            "3934/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.061\n",
            "3935/7395000 (epoch 2), train_loss = 1.402, time/batch = 0.059\n",
            "3936/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.060\n",
            "3937/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.058\n",
            "3938/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.060\n",
            "3939/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.059\n",
            "3940/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.060\n",
            "3941/7395000 (epoch 2), train_loss = 1.400, time/batch = 0.058\n",
            "3942/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.057\n",
            "3943/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.060\n",
            "3944/7395000 (epoch 2), train_loss = 1.413, time/batch = 0.066\n",
            "3945/7395000 (epoch 2), train_loss = 1.397, time/batch = 0.060\n",
            "3946/7395000 (epoch 2), train_loss = 1.317, time/batch = 0.060\n",
            "3947/7395000 (epoch 2), train_loss = 1.401, time/batch = 0.057\n",
            "3948/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.061\n",
            "3949/7395000 (epoch 2), train_loss = 1.423, time/batch = 0.058\n",
            "3950/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.062\n",
            "3951/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.060\n",
            "3952/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.061\n",
            "3953/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.061\n",
            "3954/7395000 (epoch 2), train_loss = 1.401, time/batch = 0.061\n",
            "3955/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.065\n",
            "3956/7395000 (epoch 2), train_loss = 1.454, time/batch = 0.060\n",
            "3957/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.059\n",
            "3958/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.063\n",
            "3959/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.060\n",
            "3960/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.061\n",
            "3961/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.064\n",
            "3962/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.059\n",
            "3963/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "3964/7395000 (epoch 2), train_loss = 1.413, time/batch = 0.061\n",
            "3965/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.060\n",
            "3966/7395000 (epoch 2), train_loss = 1.439, time/batch = 0.061\n",
            "3967/7395000 (epoch 2), train_loss = 1.299, time/batch = 0.058\n",
            "3968/7395000 (epoch 2), train_loss = 1.348, time/batch = 0.062\n",
            "3969/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.053\n",
            "3970/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.061\n",
            "3971/7395000 (epoch 2), train_loss = 1.387, time/batch = 0.055\n",
            "3972/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.057\n",
            "3973/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.053\n",
            "3974/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.054\n",
            "3975/7395000 (epoch 2), train_loss = 1.363, time/batch = 0.061\n",
            "3976/7395000 (epoch 2), train_loss = 1.494, time/batch = 0.059\n",
            "3977/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.059\n",
            "3978/7395000 (epoch 2), train_loss = 1.398, time/batch = 0.063\n",
            "3979/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.060\n",
            "3980/7395000 (epoch 2), train_loss = 1.349, time/batch = 0.060\n",
            "3981/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.057\n",
            "3982/7395000 (epoch 2), train_loss = 1.478, time/batch = 0.066\n",
            "3983/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.058\n",
            "3984/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.059\n",
            "3985/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.065\n",
            "3986/7395000 (epoch 2), train_loss = 1.345, time/batch = 0.064\n",
            "3987/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.061\n",
            "3988/7395000 (epoch 2), train_loss = 1.260, time/batch = 0.062\n",
            "3989/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.060\n",
            "3990/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.060\n",
            "3991/7395000 (epoch 2), train_loss = 1.381, time/batch = 0.057\n",
            "3992/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.062\n",
            "3993/7395000 (epoch 2), train_loss = 1.278, time/batch = 0.059\n",
            "3994/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.056\n",
            "3995/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.067\n",
            "3996/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.059\n",
            "3997/7395000 (epoch 2), train_loss = 1.335, time/batch = 0.059\n",
            "3998/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.060\n",
            "3999/7395000 (epoch 2), train_loss = 1.352, time/batch = 0.056\n",
            "4000/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.062\n",
            "model saved to save/model.ckpt\n",
            "4001/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.060\n",
            "4002/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.064\n",
            "4003/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.058\n",
            "4004/7395000 (epoch 2), train_loss = 1.372, time/batch = 0.066\n",
            "4005/7395000 (epoch 2), train_loss = 1.378, time/batch = 0.060\n",
            "4006/7395000 (epoch 2), train_loss = 1.501, time/batch = 0.058\n",
            "4007/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.064\n",
            "4008/7395000 (epoch 2), train_loss = 1.490, time/batch = 0.057\n",
            "4009/7395000 (epoch 2), train_loss = 1.314, time/batch = 0.060\n",
            "4010/7395000 (epoch 2), train_loss = 1.314, time/batch = 0.061\n",
            "4011/7395000 (epoch 2), train_loss = 1.366, time/batch = 0.059\n",
            "4012/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.061\n",
            "4013/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.060\n",
            "4014/7395000 (epoch 2), train_loss = 1.428, time/batch = 0.065\n",
            "4015/7395000 (epoch 2), train_loss = 1.369, time/batch = 0.060\n",
            "4016/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.059\n",
            "4017/7395000 (epoch 2), train_loss = 1.306, time/batch = 0.067\n",
            "4018/7395000 (epoch 2), train_loss = 1.430, time/batch = 0.061\n",
            "4019/7395000 (epoch 2), train_loss = 1.389, time/batch = 0.060\n",
            "4020/7395000 (epoch 2), train_loss = 1.414, time/batch = 0.061\n",
            "4021/7395000 (epoch 2), train_loss = 1.440, time/batch = 0.061\n",
            "4022/7395000 (epoch 2), train_loss = 1.435, time/batch = 0.060\n",
            "4023/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.059\n",
            "4024/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.070\n",
            "4025/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.061\n",
            "4026/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.060\n",
            "4027/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.065\n",
            "4028/7395000 (epoch 2), train_loss = 1.234, time/batch = 0.061\n",
            "4029/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.058\n",
            "4030/7395000 (epoch 2), train_loss = 1.269, time/batch = 0.064\n",
            "4031/7395000 (epoch 2), train_loss = 1.217, time/batch = 0.060\n",
            "4032/7395000 (epoch 2), train_loss = 1.178, time/batch = 0.061\n",
            "4033/7395000 (epoch 2), train_loss = 1.208, time/batch = 0.061\n",
            "4034/7395000 (epoch 2), train_loss = 1.186, time/batch = 0.061\n",
            "4035/7395000 (epoch 2), train_loss = 1.185, time/batch = 0.064\n",
            "4036/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.061\n",
            "4037/7395000 (epoch 2), train_loss = 1.170, time/batch = 0.060\n",
            "4038/7395000 (epoch 2), train_loss = 1.235, time/batch = 0.062\n",
            "4039/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.058\n",
            "4040/7395000 (epoch 2), train_loss = 1.391, time/batch = 0.059\n",
            "4041/7395000 (epoch 2), train_loss = 1.287, time/batch = 0.061\n",
            "4042/7395000 (epoch 2), train_loss = 1.248, time/batch = 0.062\n",
            "4043/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.059\n",
            "4044/7395000 (epoch 2), train_loss = 1.424, time/batch = 0.056\n",
            "4045/7395000 (epoch 2), train_loss = 1.259, time/batch = 0.062\n",
            "4046/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.061\n",
            "4047/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.059\n",
            "4048/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.060\n",
            "4049/7395000 (epoch 2), train_loss = 1.329, time/batch = 0.064\n",
            "4050/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.056\n",
            "4051/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.059\n",
            "4052/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.058\n",
            "4053/7395000 (epoch 2), train_loss = 1.262, time/batch = 0.060\n",
            "4054/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.059\n",
            "4055/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.059\n",
            "4056/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.061\n",
            "4057/7395000 (epoch 2), train_loss = 1.418, time/batch = 0.059\n",
            "4058/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.062\n",
            "4059/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.060\n",
            "4060/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.063\n",
            "4061/7395000 (epoch 2), train_loss = 1.189, time/batch = 0.061\n",
            "4062/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.063\n",
            "4063/7395000 (epoch 2), train_loss = 1.269, time/batch = 0.061\n",
            "4064/7395000 (epoch 2), train_loss = 1.367, time/batch = 0.055\n",
            "4065/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.059\n",
            "4066/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.059\n",
            "4067/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.061\n",
            "4068/7395000 (epoch 2), train_loss = 1.429, time/batch = 0.062\n",
            "4069/7395000 (epoch 2), train_loss = 1.292, time/batch = 0.059\n",
            "4070/7395000 (epoch 2), train_loss = 1.291, time/batch = 0.062\n",
            "4071/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.060\n",
            "4072/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.063\n",
            "4073/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.060\n",
            "4074/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.058\n",
            "4075/7395000 (epoch 2), train_loss = 1.464, time/batch = 0.064\n",
            "4076/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.059\n",
            "4077/7395000 (epoch 2), train_loss = 1.299, time/batch = 0.064\n",
            "4078/7395000 (epoch 2), train_loss = 1.237, time/batch = 0.059\n",
            "4079/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.057\n",
            "4080/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.062\n",
            "4081/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.061\n",
            "4082/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.065\n",
            "4083/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.059\n",
            "4084/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.066\n",
            "4085/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.060\n",
            "4086/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.058\n",
            "4087/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.060\n",
            "4088/7395000 (epoch 2), train_loss = 1.284, time/batch = 0.061\n",
            "4089/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.061\n",
            "4090/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.062\n",
            "4091/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.062\n",
            "4092/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.057\n",
            "4093/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.064\n",
            "4094/7395000 (epoch 2), train_loss = 1.279, time/batch = 0.058\n",
            "4095/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.061\n",
            "4096/7395000 (epoch 2), train_loss = 1.450, time/batch = 0.059\n",
            "4097/7395000 (epoch 2), train_loss = 1.236, time/batch = 0.060\n",
            "4098/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.062\n",
            "4099/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.059\n",
            "4100/7395000 (epoch 2), train_loss = 1.373, time/batch = 0.060\n",
            "4101/7395000 (epoch 2), train_loss = 1.235, time/batch = 0.061\n",
            "4102/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.060\n",
            "4103/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.060\n",
            "4104/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.057\n",
            "4105/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.059\n",
            "4106/7395000 (epoch 2), train_loss = 1.248, time/batch = 0.061\n",
            "4107/7395000 (epoch 2), train_loss = 1.283, time/batch = 0.064\n",
            "4108/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.062\n",
            "4109/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.057\n",
            "4110/7395000 (epoch 2), train_loss = 1.256, time/batch = 0.060\n",
            "4111/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.060\n",
            "4112/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.061\n",
            "4113/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.060\n",
            "4114/7395000 (epoch 2), train_loss = 1.320, time/batch = 0.059\n",
            "4115/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.062\n",
            "4116/7395000 (epoch 2), train_loss = 1.250, time/batch = 0.060\n",
            "4117/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.061\n",
            "4118/7395000 (epoch 2), train_loss = 1.215, time/batch = 0.059\n",
            "4119/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.058\n",
            "4120/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.065\n",
            "4121/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.060\n",
            "4122/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.060\n",
            "4123/7395000 (epoch 2), train_loss = 1.312, time/batch = 0.060\n",
            "4124/7395000 (epoch 2), train_loss = 1.215, time/batch = 0.057\n",
            "4125/7395000 (epoch 2), train_loss = 1.248, time/batch = 0.066\n",
            "4126/7395000 (epoch 2), train_loss = 1.225, time/batch = 0.054\n",
            "4127/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.061\n",
            "4128/7395000 (epoch 2), train_loss = 1.344, time/batch = 0.060\n",
            "4129/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.059\n",
            "4130/7395000 (epoch 2), train_loss = 1.256, time/batch = 0.065\n",
            "4131/7395000 (epoch 2), train_loss = 1.311, time/batch = 0.061\n",
            "4132/7395000 (epoch 2), train_loss = 1.262, time/batch = 0.060\n",
            "4133/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.061\n",
            "4134/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.063\n",
            "4135/7395000 (epoch 2), train_loss = 1.298, time/batch = 0.062\n",
            "4136/7395000 (epoch 2), train_loss = 1.202, time/batch = 0.061\n",
            "4137/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.059\n",
            "4138/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.061\n",
            "4139/7395000 (epoch 2), train_loss = 1.399, time/batch = 0.061\n",
            "4140/7395000 (epoch 2), train_loss = 1.347, time/batch = 0.062\n",
            "4141/7395000 (epoch 2), train_loss = 1.329, time/batch = 0.061\n",
            "4142/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.067\n",
            "4143/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.059\n",
            "4144/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.059\n",
            "4145/7395000 (epoch 2), train_loss = 1.300, time/batch = 0.061\n",
            "4146/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.061\n",
            "4147/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.059\n",
            "4148/7395000 (epoch 2), train_loss = 1.315, time/batch = 0.061\n",
            "4149/7395000 (epoch 2), train_loss = 1.264, time/batch = 0.062\n",
            "4150/7395000 (epoch 2), train_loss = 1.318, time/batch = 0.065\n",
            "4151/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.060\n",
            "4152/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.058\n",
            "4153/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.060\n",
            "4154/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.062\n",
            "4155/7395000 (epoch 2), train_loss = 1.305, time/batch = 0.064\n",
            "4156/7395000 (epoch 2), train_loss = 1.275, time/batch = 0.064\n",
            "4157/7395000 (epoch 2), train_loss = 1.419, time/batch = 0.059\n",
            "4158/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.061\n",
            "4159/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.066\n",
            "4160/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.060\n",
            "4161/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.059\n",
            "4162/7395000 (epoch 2), train_loss = 1.167, time/batch = 0.059\n",
            "4163/7395000 (epoch 2), train_loss = 1.232, time/batch = 0.063\n",
            "4164/7395000 (epoch 2), train_loss = 1.228, time/batch = 0.061\n",
            "4165/7395000 (epoch 2), train_loss = 1.243, time/batch = 0.061\n",
            "4166/7395000 (epoch 2), train_loss = 1.158, time/batch = 0.061\n",
            "4167/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.060\n",
            "4168/7395000 (epoch 2), train_loss = 1.213, time/batch = 0.061\n",
            "4169/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.059\n",
            "4170/7395000 (epoch 2), train_loss = 1.383, time/batch = 0.061\n",
            "4171/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.058\n",
            "4172/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.061\n",
            "4173/7395000 (epoch 2), train_loss = 1.210, time/batch = 0.064\n",
            "4174/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.060\n",
            "4175/7395000 (epoch 2), train_loss = 1.243, time/batch = 0.061\n",
            "4176/7395000 (epoch 2), train_loss = 1.253, time/batch = 0.056\n",
            "4177/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.060\n",
            "4178/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.066\n",
            "4179/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.059\n",
            "4180/7395000 (epoch 2), train_loss = 1.250, time/batch = 0.059\n",
            "4181/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.059\n",
            "4182/7395000 (epoch 2), train_loss = 1.279, time/batch = 0.062\n",
            "4183/7395000 (epoch 2), train_loss = 1.284, time/batch = 0.061\n",
            "4184/7395000 (epoch 2), train_loss = 1.220, time/batch = 0.059\n",
            "4185/7395000 (epoch 2), train_loss = 1.189, time/batch = 0.059\n",
            "4186/7395000 (epoch 2), train_loss = 1.209, time/batch = 0.059\n",
            "4187/7395000 (epoch 2), train_loss = 1.264, time/batch = 0.064\n",
            "4188/7395000 (epoch 2), train_loss = 1.281, time/batch = 0.061\n",
            "4189/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.059\n",
            "4190/7395000 (epoch 2), train_loss = 1.329, time/batch = 0.061\n",
            "4191/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.058\n",
            "4192/7395000 (epoch 2), train_loss = 1.283, time/batch = 0.060\n",
            "4193/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.062\n",
            "4194/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.060\n",
            "4195/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.060\n",
            "4196/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.059\n",
            "4197/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.059\n",
            "4198/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.060\n",
            "4199/7395000 (epoch 2), train_loss = 1.308, time/batch = 0.061\n",
            "4200/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.060\n",
            "4201/7395000 (epoch 2), train_loss = 1.338, time/batch = 0.058\n",
            "4202/7395000 (epoch 2), train_loss = 1.253, time/batch = 0.057\n",
            "4203/7395000 (epoch 2), train_loss = 1.168, time/batch = 0.062\n",
            "4204/7395000 (epoch 2), train_loss = 1.267, time/batch = 0.061\n",
            "4205/7395000 (epoch 2), train_loss = 1.246, time/batch = 0.061\n",
            "4206/7395000 (epoch 2), train_loss = 1.229, time/batch = 0.060\n",
            "4207/7395000 (epoch 2), train_loss = 1.208, time/batch = 0.057\n",
            "4208/7395000 (epoch 2), train_loss = 1.131, time/batch = 0.062\n",
            "4209/7395000 (epoch 2), train_loss = 1.241, time/batch = 0.058\n",
            "4210/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.063\n",
            "4211/7395000 (epoch 2), train_loss = 1.221, time/batch = 0.058\n",
            "4212/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.055\n",
            "4213/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.059\n",
            "4214/7395000 (epoch 2), train_loss = 1.258, time/batch = 0.059\n",
            "4215/7395000 (epoch 2), train_loss = 1.201, time/batch = 0.061\n",
            "4216/7395000 (epoch 2), train_loss = 1.219, time/batch = 0.062\n",
            "4217/7395000 (epoch 2), train_loss = 1.293, time/batch = 0.056\n",
            "4218/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.062\n",
            "4219/7395000 (epoch 2), train_loss = 1.202, time/batch = 0.056\n",
            "4220/7395000 (epoch 2), train_loss = 1.271, time/batch = 0.066\n",
            "4221/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.060\n",
            "4222/7395000 (epoch 2), train_loss = 1.284, time/batch = 0.061\n",
            "4223/7395000 (epoch 2), train_loss = 1.218, time/batch = 0.063\n",
            "4224/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.061\n",
            "4225/7395000 (epoch 2), train_loss = 1.125, time/batch = 0.059\n",
            "4226/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.059\n",
            "4227/7395000 (epoch 2), train_loss = 1.177, time/batch = 0.066\n",
            "4228/7395000 (epoch 2), train_loss = 1.194, time/batch = 0.063\n",
            "4229/7395000 (epoch 2), train_loss = 1.246, time/batch = 0.060\n",
            "4230/7395000 (epoch 2), train_loss = 1.355, time/batch = 0.063\n",
            "4231/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.062\n",
            "4232/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.059\n",
            "4233/7395000 (epoch 2), train_loss = 1.236, time/batch = 0.067\n",
            "4234/7395000 (epoch 2), train_loss = 1.196, time/batch = 0.059\n",
            "4235/7395000 (epoch 2), train_loss = 1.224, time/batch = 0.061\n",
            "4236/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.068\n",
            "4237/7395000 (epoch 2), train_loss = 1.253, time/batch = 0.061\n",
            "4238/7395000 (epoch 2), train_loss = 1.225, time/batch = 0.066\n",
            "4239/7395000 (epoch 2), train_loss = 1.166, time/batch = 0.060\n",
            "4240/7395000 (epoch 2), train_loss = 1.340, time/batch = 0.060\n",
            "4241/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.061\n",
            "4242/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.060\n",
            "4243/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.064\n",
            "4244/7395000 (epoch 2), train_loss = 1.296, time/batch = 0.062\n",
            "4245/7395000 (epoch 2), train_loss = 1.260, time/batch = 0.060\n",
            "4246/7395000 (epoch 2), train_loss = 1.285, time/batch = 0.062\n",
            "4247/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.058\n",
            "4248/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.062\n",
            "4249/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.059\n",
            "4250/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.060\n",
            "4251/7395000 (epoch 2), train_loss = 1.238, time/batch = 0.065\n",
            "4252/7395000 (epoch 2), train_loss = 1.244, time/batch = 0.060\n",
            "4253/7395000 (epoch 2), train_loss = 1.354, time/batch = 0.060\n",
            "4254/7395000 (epoch 2), train_loss = 1.204, time/batch = 0.062\n",
            "4255/7395000 (epoch 2), train_loss = 1.216, time/batch = 0.060\n",
            "4256/7395000 (epoch 2), train_loss = 1.257, time/batch = 0.060\n",
            "4257/7395000 (epoch 2), train_loss = 1.196, time/batch = 0.059\n",
            "4258/7395000 (epoch 2), train_loss = 1.179, time/batch = 0.061\n",
            "4259/7395000 (epoch 2), train_loss = 1.246, time/batch = 0.062\n",
            "4260/7395000 (epoch 2), train_loss = 1.225, time/batch = 0.069\n",
            "4261/7395000 (epoch 2), train_loss = 1.204, time/batch = 0.061\n",
            "4262/7395000 (epoch 2), train_loss = 1.204, time/batch = 0.062\n",
            "4263/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.059\n",
            "4264/7395000 (epoch 2), train_loss = 1.304, time/batch = 0.062\n",
            "4265/7395000 (epoch 2), train_loss = 1.321, time/batch = 0.058\n",
            "4266/7395000 (epoch 2), train_loss = 1.193, time/batch = 0.060\n",
            "4267/7395000 (epoch 2), train_loss = 1.267, time/batch = 0.059\n",
            "4268/7395000 (epoch 2), train_loss = 1.178, time/batch = 0.058\n",
            "4269/7395000 (epoch 2), train_loss = 1.176, time/batch = 0.062\n",
            "4270/7395000 (epoch 2), train_loss = 1.228, time/batch = 0.060\n",
            "4271/7395000 (epoch 2), train_loss = 1.142, time/batch = 0.062\n",
            "4272/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.060\n",
            "4273/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.058\n",
            "4274/7395000 (epoch 2), train_loss = 1.486, time/batch = 0.062\n",
            "4275/7395000 (epoch 2), train_loss = 1.433, time/batch = 0.060\n",
            "4276/7395000 (epoch 2), train_loss = 1.361, time/batch = 0.059\n",
            "4277/7395000 (epoch 2), train_loss = 1.268, time/batch = 0.072\n",
            "4278/7395000 (epoch 2), train_loss = 1.260, time/batch = 0.061\n",
            "4279/7395000 (epoch 2), train_loss = 1.260, time/batch = 0.061\n",
            "4280/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.062\n",
            "4281/7395000 (epoch 2), train_loss = 1.316, time/batch = 0.060\n",
            "4282/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.063\n",
            "4283/7395000 (epoch 2), train_loss = 1.366, time/batch = 0.058\n",
            "4284/7395000 (epoch 2), train_loss = 1.282, time/batch = 0.064\n",
            "4285/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.061\n",
            "4286/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.060\n",
            "4287/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.064\n",
            "4288/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.061\n",
            "4289/7395000 (epoch 2), train_loss = 1.297, time/batch = 0.061\n",
            "4290/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.062\n",
            "4291/7395000 (epoch 2), train_loss = 1.371, time/batch = 0.060\n",
            "4292/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.065\n",
            "4293/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.063\n",
            "4294/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.057\n",
            "4295/7395000 (epoch 2), train_loss = 1.390, time/batch = 0.061\n",
            "4296/7395000 (epoch 2), train_loss = 1.251, time/batch = 0.060\n",
            "4297/7395000 (epoch 2), train_loss = 1.249, time/batch = 0.066\n",
            "4298/7395000 (epoch 2), train_loss = 1.294, time/batch = 0.061\n",
            "4299/7395000 (epoch 2), train_loss = 1.234, time/batch = 0.058\n",
            "4300/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.062\n",
            "4301/7395000 (epoch 2), train_loss = 1.253, time/batch = 0.059\n",
            "4302/7395000 (epoch 2), train_loss = 1.238, time/batch = 0.065\n",
            "4303/7395000 (epoch 2), train_loss = 1.171, time/batch = 0.060\n",
            "4304/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.060\n",
            "4305/7395000 (epoch 2), train_loss = 1.267, time/batch = 0.061\n",
            "4306/7395000 (epoch 2), train_loss = 1.302, time/batch = 0.061\n",
            "4307/7395000 (epoch 2), train_loss = 1.337, time/batch = 0.065\n",
            "4308/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.065\n",
            "4309/7395000 (epoch 2), train_loss = 1.357, time/batch = 0.059\n",
            "4310/7395000 (epoch 2), train_loss = 1.259, time/batch = 0.068\n",
            "4311/7395000 (epoch 2), train_loss = 1.233, time/batch = 0.061\n",
            "4312/7395000 (epoch 2), train_loss = 1.210, time/batch = 0.060\n",
            "4313/7395000 (epoch 2), train_loss = 1.319, time/batch = 0.061\n",
            "4314/7395000 (epoch 2), train_loss = 1.166, time/batch = 0.061\n",
            "4315/7395000 (epoch 2), train_loss = 1.209, time/batch = 0.065\n",
            "4316/7395000 (epoch 2), train_loss = 1.278, time/batch = 0.061\n",
            "4317/7395000 (epoch 2), train_loss = 1.174, time/batch = 0.059\n",
            "4318/7395000 (epoch 2), train_loss = 1.170, time/batch = 0.060\n",
            "4319/7395000 (epoch 2), train_loss = 1.362, time/batch = 0.059\n",
            "4320/7395000 (epoch 2), train_loss = 1.248, time/batch = 0.060\n",
            "4321/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.059\n",
            "4322/7395000 (epoch 2), train_loss = 1.334, time/batch = 0.061\n",
            "4323/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.061\n",
            "4324/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.061\n",
            "4325/7395000 (epoch 2), train_loss = 1.226, time/batch = 0.065\n",
            "4326/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.061\n",
            "4327/7395000 (epoch 2), train_loss = 1.139, time/batch = 0.065\n",
            "4328/7395000 (epoch 2), train_loss = 1.300, time/batch = 0.066\n",
            "4329/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.061\n",
            "4330/7395000 (epoch 2), train_loss = 1.274, time/batch = 0.059\n",
            "4331/7395000 (epoch 2), train_loss = 1.194, time/batch = 0.059\n",
            "4332/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.060\n",
            "4333/7395000 (epoch 2), train_loss = 1.147, time/batch = 0.062\n",
            "4334/7395000 (epoch 2), train_loss = 1.261, time/batch = 0.060\n",
            "4335/7395000 (epoch 2), train_loss = 1.353, time/batch = 0.061\n",
            "4336/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.060\n",
            "4337/7395000 (epoch 2), train_loss = 1.365, time/batch = 0.061\n",
            "4338/7395000 (epoch 2), train_loss = 1.325, time/batch = 0.067\n",
            "4339/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.060\n",
            "4340/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.061\n",
            "4341/7395000 (epoch 2), train_loss = 1.164, time/batch = 0.061\n",
            "4342/7395000 (epoch 2), train_loss = 1.288, time/batch = 0.061\n",
            "4343/7395000 (epoch 2), train_loss = 1.367, time/batch = 0.064\n",
            "4344/7395000 (epoch 2), train_loss = 1.259, time/batch = 0.062\n",
            "4345/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.060\n",
            "4346/7395000 (epoch 2), train_loss = 1.330, time/batch = 0.063\n",
            "4347/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.059\n",
            "4348/7395000 (epoch 2), train_loss = 1.202, time/batch = 0.060\n",
            "4349/7395000 (epoch 2), train_loss = 1.309, time/batch = 0.062\n",
            "4350/7395000 (epoch 2), train_loss = 1.356, time/batch = 0.061\n",
            "4351/7395000 (epoch 2), train_loss = 1.376, time/batch = 0.069\n",
            "4352/7395000 (epoch 2), train_loss = 1.361, time/batch = 0.061\n",
            "4353/7395000 (epoch 2), train_loss = 1.329, time/batch = 0.057\n",
            "4354/7395000 (epoch 2), train_loss = 1.395, time/batch = 0.060\n",
            "4355/7395000 (epoch 2), train_loss = 1.377, time/batch = 0.061\n",
            "4356/7395000 (epoch 2), train_loss = 1.272, time/batch = 0.060\n",
            "4357/7395000 (epoch 2), train_loss = 1.414, time/batch = 0.059\n",
            "4358/7395000 (epoch 2), train_loss = 1.343, time/batch = 0.057\n",
            "4359/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.060\n",
            "4360/7395000 (epoch 2), train_loss = 1.341, time/batch = 0.065\n",
            "4361/7395000 (epoch 2), train_loss = 1.406, time/batch = 0.061\n",
            "4362/7395000 (epoch 2), train_loss = 1.407, time/batch = 0.062\n",
            "4363/7395000 (epoch 2), train_loss = 1.419, time/batch = 0.055\n",
            "4364/7395000 (epoch 2), train_loss = 1.403, time/batch = 0.062\n",
            "4365/7395000 (epoch 2), train_loss = 1.375, time/batch = 0.062\n",
            "4366/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.061\n",
            "4367/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.059\n",
            "4368/7395000 (epoch 2), train_loss = 1.324, time/batch = 0.055\n",
            "4369/7395000 (epoch 2), train_loss = 1.289, time/batch = 0.056\n",
            "4370/7395000 (epoch 2), train_loss = 1.303, time/batch = 0.062\n",
            "4371/7395000 (epoch 2), train_loss = 1.379, time/batch = 0.059\n",
            "4372/7395000 (epoch 2), train_loss = 1.276, time/batch = 0.060\n",
            "4373/7395000 (epoch 2), train_loss = 1.280, time/batch = 0.062\n",
            "4374/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.060\n",
            "4375/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.059\n",
            "4376/7395000 (epoch 2), train_loss = 1.331, time/batch = 0.063\n",
            "4377/7395000 (epoch 2), train_loss = 1.318, time/batch = 0.065\n",
            "4378/7395000 (epoch 2), train_loss = 1.364, time/batch = 0.056\n",
            "4379/7395000 (epoch 2), train_loss = 1.396, time/batch = 0.060\n",
            "4380/7395000 (epoch 2), train_loss = 1.333, time/batch = 0.060\n",
            "4381/7395000 (epoch 2), train_loss = 1.358, time/batch = 0.060\n",
            "4382/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.059\n",
            "4383/7395000 (epoch 2), train_loss = 1.199, time/batch = 0.060\n",
            "4384/7395000 (epoch 2), train_loss = 1.307, time/batch = 0.062\n",
            "4385/7395000 (epoch 2), train_loss = 1.295, time/batch = 0.061\n",
            "4386/7395000 (epoch 2), train_loss = 1.236, time/batch = 0.063\n",
            "4387/7395000 (epoch 2), train_loss = 1.332, time/batch = 0.060\n",
            "4388/7395000 (epoch 2), train_loss = 1.313, time/batch = 0.061\n",
            "4389/7395000 (epoch 2), train_loss = 1.299, time/batch = 0.061\n",
            "4390/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.061\n",
            "4391/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.063\n",
            "4392/7395000 (epoch 2), train_loss = 1.176, time/batch = 0.062\n",
            "4393/7395000 (epoch 2), train_loss = 1.284, time/batch = 0.061\n",
            "4394/7395000 (epoch 2), train_loss = 1.203, time/batch = 0.071\n",
            "4395/7395000 (epoch 2), train_loss = 1.322, time/batch = 0.059\n",
            "4396/7395000 (epoch 2), train_loss = 1.264, time/batch = 0.061\n",
            "4397/7395000 (epoch 2), train_loss = 1.339, time/batch = 0.061\n",
            "4398/7395000 (epoch 2), train_loss = 1.323, time/batch = 0.062\n",
            "4399/7395000 (epoch 2), train_loss = 1.425, time/batch = 0.064\n",
            "4400/7395000 (epoch 2), train_loss = 1.438, time/batch = 0.061\n",
            "4401/7395000 (epoch 2), train_loss = 1.380, time/batch = 0.058\n",
            "4402/7395000 (epoch 2), train_loss = 1.374, time/batch = 0.063\n",
            "4403/7395000 (epoch 2), train_loss = 1.370, time/batch = 0.059\n",
            "4404/7395000 (epoch 2), train_loss = 1.405, time/batch = 0.065\n",
            "4405/7395000 (epoch 2), train_loss = 1.368, time/batch = 0.059\n",
            "4406/7395000 (epoch 2), train_loss = 1.342, time/batch = 0.061\n",
            "4407/7395000 (epoch 2), train_loss = 1.336, time/batch = 0.062\n",
            "4408/7395000 (epoch 2), train_loss = 1.310, time/batch = 0.062\n",
            "4409/7395000 (epoch 2), train_loss = 1.254, time/batch = 0.060\n",
            "4410/7395000 (epoch 2), train_loss = 1.252, time/batch = 0.060\n",
            "4411/7395000 (epoch 2), train_loss = 1.259, time/batch = 0.058\n",
            "4412/7395000 (epoch 2), train_loss = 1.359, time/batch = 0.062\n",
            "4413/7395000 (epoch 2), train_loss = 1.263, time/batch = 0.061\n",
            "4414/7395000 (epoch 2), train_loss = 1.266, time/batch = 0.062\n",
            "4415/7395000 (epoch 2), train_loss = 1.286, time/batch = 0.062\n",
            "4416/7395000 (epoch 2), train_loss = 1.277, time/batch = 0.062\n",
            "4417/7395000 (epoch 2), train_loss = 1.220, time/batch = 0.059\n",
            "4418/7395000 (epoch 2), train_loss = 1.215, time/batch = 0.062\n",
            "4419/7395000 (epoch 2), train_loss = 1.216, time/batch = 0.061\n",
            "4420/7395000 (epoch 2), train_loss = 1.183, time/batch = 0.060\n",
            "4421/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.059\n",
            "4422/7395000 (epoch 2), train_loss = 1.346, time/batch = 0.068\n",
            "4423/7395000 (epoch 2), train_loss = 1.174, time/batch = 0.064\n",
            "4424/7395000 (epoch 2), train_loss = 1.148, time/batch = 0.064\n",
            "4425/7395000 (epoch 2), train_loss = 1.242, time/batch = 0.059\n",
            "4426/7395000 (epoch 2), train_loss = 1.230, time/batch = 0.061\n",
            "4427/7395000 (epoch 2), train_loss = 1.154, time/batch = 0.068\n",
            "4428/7395000 (epoch 2), train_loss = 1.171, time/batch = 0.062\n",
            "4429/7395000 (epoch 2), train_loss = 1.247, time/batch = 0.058\n",
            "4430/7395000 (epoch 2), train_loss = 1.301, time/batch = 0.064\n",
            "4431/7395000 (epoch 2), train_loss = 1.126, time/batch = 0.060\n",
            "4432/7395000 (epoch 2), train_loss = 1.270, time/batch = 0.061\n",
            "4433/7395000 (epoch 2), train_loss = 1.263, time/batch = 0.059\n",
            "4434/7395000 (epoch 2), train_loss = 1.210, time/batch = 0.063\n",
            "4435/7395000 (epoch 2), train_loss = 1.273, time/batch = 0.059\n",
            "4436/7395000 (epoch 2), train_loss = 1.255, time/batch = 0.061\n",
            "4437/7395000 (epoch 3), train_loss = 1.477, time/batch = 0.058\n",
            "4438/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.061\n",
            "4439/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.060\n",
            "4440/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.063\n",
            "4441/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.064\n",
            "4442/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.055\n",
            "4443/7395000 (epoch 3), train_loss = 1.157, time/batch = 0.059\n",
            "4444/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.053\n",
            "4445/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.058\n",
            "4446/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.061\n",
            "4447/7395000 (epoch 3), train_loss = 1.332, time/batch = 0.053\n",
            "4448/7395000 (epoch 3), train_loss = 1.299, time/batch = 0.054\n",
            "4449/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.054\n",
            "4450/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.056\n",
            "4451/7395000 (epoch 3), train_loss = 1.176, time/batch = 0.053\n",
            "4452/7395000 (epoch 3), train_loss = 1.301, time/batch = 0.054\n",
            "4453/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.054\n",
            "4454/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.054\n",
            "4455/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.054\n",
            "4456/7395000 (epoch 3), train_loss = 1.336, time/batch = 0.057\n",
            "4457/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.055\n",
            "4458/7395000 (epoch 3), train_loss = 1.344, time/batch = 0.054\n",
            "4459/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.059\n",
            "4460/7395000 (epoch 3), train_loss = 1.324, time/batch = 0.063\n",
            "4461/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.060\n",
            "4462/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.060\n",
            "4463/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.062\n",
            "4464/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.059\n",
            "4465/7395000 (epoch 3), train_loss = 1.195, time/batch = 0.058\n",
            "4466/7395000 (epoch 3), train_loss = 1.175, time/batch = 0.061\n",
            "4467/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.059\n",
            "4468/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.057\n",
            "4469/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.060\n",
            "4470/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.060\n",
            "4471/7395000 (epoch 3), train_loss = 1.388, time/batch = 0.061\n",
            "4472/7395000 (epoch 3), train_loss = 1.309, time/batch = 0.061\n",
            "4473/7395000 (epoch 3), train_loss = 1.404, time/batch = 0.059\n",
            "4474/7395000 (epoch 3), train_loss = 1.353, time/batch = 0.059\n",
            "4475/7395000 (epoch 3), train_loss = 1.318, time/batch = 0.061\n",
            "4476/7395000 (epoch 3), train_loss = 1.342, time/batch = 0.060\n",
            "4477/7395000 (epoch 3), train_loss = 1.347, time/batch = 0.067\n",
            "4478/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.061\n",
            "4479/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.061\n",
            "4480/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.061\n",
            "4481/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.060\n",
            "4482/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.059\n",
            "4483/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.060\n",
            "4484/7395000 (epoch 3), train_loss = 1.190, time/batch = 0.065\n",
            "4485/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.060\n",
            "4486/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.060\n",
            "4487/7395000 (epoch 3), train_loss = 1.148, time/batch = 0.058\n",
            "4488/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.063\n",
            "4489/7395000 (epoch 3), train_loss = 1.301, time/batch = 0.058\n",
            "4490/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.059\n",
            "4491/7395000 (epoch 3), train_loss = 1.363, time/batch = 0.063\n",
            "4492/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.061\n",
            "4493/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.057\n",
            "4494/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.067\n",
            "4495/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.061\n",
            "4496/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.059\n",
            "4497/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.058\n",
            "4498/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.060\n",
            "4499/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.059\n",
            "4500/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.060\n",
            "4501/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.062\n",
            "4502/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.061\n",
            "4503/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.058\n",
            "4504/7395000 (epoch 3), train_loss = 1.239, time/batch = 0.062\n",
            "4505/7395000 (epoch 3), train_loss = 1.351, time/batch = 0.058\n",
            "4506/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.061\n",
            "4507/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.059\n",
            "4508/7395000 (epoch 3), train_loss = 1.354, time/batch = 0.066\n",
            "4509/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.061\n",
            "4510/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.062\n",
            "4511/7395000 (epoch 3), train_loss = 1.346, time/batch = 0.061\n",
            "4512/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.060\n",
            "4513/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.060\n",
            "4514/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.062\n",
            "4515/7395000 (epoch 3), train_loss = 1.151, time/batch = 0.060\n",
            "4516/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.056\n",
            "4517/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.061\n",
            "4518/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.059\n",
            "4519/7395000 (epoch 3), train_loss = 1.218, time/batch = 0.064\n",
            "4520/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.061\n",
            "4521/7395000 (epoch 3), train_loss = 1.314, time/batch = 0.060\n",
            "4522/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.062\n",
            "4523/7395000 (epoch 3), train_loss = 1.317, time/batch = 0.060\n",
            "4524/7395000 (epoch 3), train_loss = 1.370, time/batch = 0.067\n",
            "4525/7395000 (epoch 3), train_loss = 1.357, time/batch = 0.059\n",
            "4526/7395000 (epoch 3), train_loss = 1.302, time/batch = 0.061\n",
            "4527/7395000 (epoch 3), train_loss = 1.404, time/batch = 0.061\n",
            "4528/7395000 (epoch 3), train_loss = 1.336, time/batch = 0.058\n",
            "4529/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.060\n",
            "4530/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.061\n",
            "4531/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.060\n",
            "4532/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.060\n",
            "4533/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.065\n",
            "4534/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.066\n",
            "4535/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.061\n",
            "4536/7395000 (epoch 3), train_loss = 1.301, time/batch = 0.061\n",
            "4537/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.062\n",
            "4538/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.061\n",
            "4539/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.061\n",
            "4540/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.062\n",
            "4541/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.060\n",
            "4542/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.063\n",
            "4543/7395000 (epoch 3), train_loss = 1.395, time/batch = 0.061\n",
            "4544/7395000 (epoch 3), train_loss = 1.357, time/batch = 0.073\n",
            "4545/7395000 (epoch 3), train_loss = 1.401, time/batch = 0.061\n",
            "4546/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.061\n",
            "4547/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.060\n",
            "4548/7395000 (epoch 3), train_loss = 1.409, time/batch = 0.061\n",
            "4549/7395000 (epoch 3), train_loss = 1.433, time/batch = 0.061\n",
            "4550/7395000 (epoch 3), train_loss = 1.373, time/batch = 0.064\n",
            "4551/7395000 (epoch 3), train_loss = 1.341, time/batch = 0.060\n",
            "4552/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.063\n",
            "4553/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.062\n",
            "4554/7395000 (epoch 3), train_loss = 1.345, time/batch = 0.065\n",
            "4555/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.062\n",
            "4556/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.062\n",
            "4557/7395000 (epoch 3), train_loss = 1.247, time/batch = 0.063\n",
            "4558/7395000 (epoch 3), train_loss = 1.218, time/batch = 0.062\n",
            "4559/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.060\n",
            "4560/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.066\n",
            "4561/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.062\n",
            "4562/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.062\n",
            "4563/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.061\n",
            "4564/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.064\n",
            "4565/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.065\n",
            "4566/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.062\n",
            "4567/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.062\n",
            "4568/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.061\n",
            "4569/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.063\n",
            "4570/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.062\n",
            "4571/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.061\n",
            "4572/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.064\n",
            "4573/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.058\n",
            "4574/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.062\n",
            "4575/7395000 (epoch 3), train_loss = 1.309, time/batch = 0.060\n",
            "4576/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.068\n",
            "4577/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.072\n",
            "4578/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.062\n",
            "4579/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.063\n",
            "4580/7395000 (epoch 3), train_loss = 1.284, time/batch = 0.063\n",
            "4581/7395000 (epoch 3), train_loss = 1.330, time/batch = 0.061\n",
            "4582/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.063\n",
            "4583/7395000 (epoch 3), train_loss = 1.348, time/batch = 0.062\n",
            "4584/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.062\n",
            "4585/7395000 (epoch 3), train_loss = 1.329, time/batch = 0.063\n",
            "4586/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.063\n",
            "4587/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.061\n",
            "4588/7395000 (epoch 3), train_loss = 1.340, time/batch = 0.064\n",
            "4589/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.061\n",
            "4590/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.061\n",
            "4591/7395000 (epoch 3), train_loss = 1.172, time/batch = 0.062\n",
            "4592/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.063\n",
            "4593/7395000 (epoch 3), train_loss = 1.302, time/batch = 0.064\n",
            "4594/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.062\n",
            "4595/7395000 (epoch 3), train_loss = 1.171, time/batch = 0.060\n",
            "4596/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.064\n",
            "4597/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.059\n",
            "4598/7395000 (epoch 3), train_loss = 1.284, time/batch = 0.063\n",
            "4599/7395000 (epoch 3), train_loss = 1.239, time/batch = 0.062\n",
            "4600/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.063\n",
            "4601/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.061\n",
            "4602/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.063\n",
            "4603/7395000 (epoch 3), train_loss = 1.124, time/batch = 0.064\n",
            "4604/7395000 (epoch 3), train_loss = 1.329, time/batch = 0.076\n",
            "4605/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.066\n",
            "4606/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.064\n",
            "4607/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.060\n",
            "4608/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.064\n",
            "4609/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.072\n",
            "4610/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.064\n",
            "4611/7395000 (epoch 3), train_loss = 1.316, time/batch = 0.063\n",
            "4612/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.062\n",
            "4613/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.060\n",
            "4614/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.064\n",
            "4615/7395000 (epoch 3), train_loss = 1.197, time/batch = 0.061\n",
            "4616/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.059\n",
            "4617/7395000 (epoch 3), train_loss = 1.215, time/batch = 0.060\n",
            "4618/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.064\n",
            "4619/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.064\n",
            "4620/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.062\n",
            "4621/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.060\n",
            "4622/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.066\n",
            "4623/7395000 (epoch 3), train_loss = 1.264, time/batch = 0.060\n",
            "4624/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.064\n",
            "4625/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.066\n",
            "4626/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.058\n",
            "4627/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.063\n",
            "4628/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.064\n",
            "4629/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.059\n",
            "4630/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.064\n",
            "4631/7395000 (epoch 3), train_loss = 1.285, time/batch = 0.062\n",
            "4632/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.064\n",
            "4633/7395000 (epoch 3), train_loss = 1.382, time/batch = 0.062\n",
            "4634/7395000 (epoch 3), train_loss = 1.359, time/batch = 0.064\n",
            "4635/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.064\n",
            "4636/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.063\n",
            "4637/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.062\n",
            "4638/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.064\n",
            "4639/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.060\n",
            "4640/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.063\n",
            "4641/7395000 (epoch 3), train_loss = 1.299, time/batch = 0.063\n",
            "4642/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.066\n",
            "4643/7395000 (epoch 3), train_loss = 1.381, time/batch = 0.063\n",
            "4644/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.063\n",
            "4645/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.062\n",
            "4646/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.062\n",
            "4647/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.061\n",
            "4648/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.061\n",
            "4649/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.063\n",
            "4650/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.064\n",
            "4651/7395000 (epoch 3), train_loss = 1.143, time/batch = 0.062\n",
            "4652/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.064\n",
            "4653/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.063\n",
            "4654/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.065\n",
            "4655/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.061\n",
            "4656/7395000 (epoch 3), train_loss = 1.165, time/batch = 0.060\n",
            "4657/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.065\n",
            "4658/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.067\n",
            "4659/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.063\n",
            "4660/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.065\n",
            "4661/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.062\n",
            "4662/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.068\n",
            "4663/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.062\n",
            "4664/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.060\n",
            "4665/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.067\n",
            "4666/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.062\n",
            "4667/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.060\n",
            "4668/7395000 (epoch 3), train_loss = 1.316, time/batch = 0.064\n",
            "4669/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.063\n",
            "4670/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.065\n",
            "4671/7395000 (epoch 3), train_loss = 1.383, time/batch = 0.060\n",
            "4672/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.057\n",
            "4673/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.060\n",
            "4674/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.065\n",
            "4675/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.059\n",
            "4676/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.065\n",
            "4677/7395000 (epoch 3), train_loss = 1.287, time/batch = 0.060\n",
            "4678/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.064\n",
            "4679/7395000 (epoch 3), train_loss = 1.287, time/batch = 0.062\n",
            "4680/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.061\n",
            "4681/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.063\n",
            "4682/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.062\n",
            "4683/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.063\n",
            "4684/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.064\n",
            "4685/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.062\n",
            "4686/7395000 (epoch 3), train_loss = 1.338, time/batch = 0.066\n",
            "4687/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.062\n",
            "4688/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.060\n",
            "4689/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.062\n",
            "4690/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.063\n",
            "4691/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.066\n",
            "4692/7395000 (epoch 3), train_loss = 1.333, time/batch = 0.057\n",
            "4693/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.062\n",
            "4694/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.062\n",
            "4695/7395000 (epoch 3), train_loss = 1.424, time/batch = 0.061\n",
            "4696/7395000 (epoch 3), train_loss = 1.330, time/batch = 0.067\n",
            "4697/7395000 (epoch 3), train_loss = 1.328, time/batch = 0.062\n",
            "4698/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.064\n",
            "4699/7395000 (epoch 3), train_loss = 1.342, time/batch = 0.064\n",
            "4700/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.065\n",
            "4701/7395000 (epoch 3), train_loss = 1.367, time/batch = 0.056\n",
            "4702/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.064\n",
            "4703/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.061\n",
            "4704/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.063\n",
            "4705/7395000 (epoch 3), train_loss = 1.394, time/batch = 0.061\n",
            "4706/7395000 (epoch 3), train_loss = 1.396, time/batch = 0.056\n",
            "4707/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.065\n",
            "4708/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.065\n",
            "4709/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.062\n",
            "4710/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.062\n",
            "4711/7395000 (epoch 3), train_loss = 1.293, time/batch = 0.064\n",
            "4712/7395000 (epoch 3), train_loss = 1.310, time/batch = 0.063\n",
            "4713/7395000 (epoch 3), train_loss = 1.326, time/batch = 0.061\n",
            "4714/7395000 (epoch 3), train_loss = 1.333, time/batch = 0.061\n",
            "4715/7395000 (epoch 3), train_loss = 1.382, time/batch = 0.061\n",
            "4716/7395000 (epoch 3), train_loss = 1.384, time/batch = 0.062\n",
            "4717/7395000 (epoch 3), train_loss = 1.404, time/batch = 0.061\n",
            "4718/7395000 (epoch 3), train_loss = 1.353, time/batch = 0.063\n",
            "4719/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.064\n",
            "4720/7395000 (epoch 3), train_loss = 1.366, time/batch = 0.063\n",
            "4721/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.060\n",
            "4722/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.063\n",
            "4723/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.066\n",
            "4724/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.058\n",
            "4725/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.060\n",
            "4726/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.063\n",
            "4727/7395000 (epoch 3), train_loss = 1.366, time/batch = 0.059\n",
            "4728/7395000 (epoch 3), train_loss = 1.332, time/batch = 0.063\n",
            "4729/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.062\n",
            "4730/7395000 (epoch 3), train_loss = 1.368, time/batch = 0.064\n",
            "4731/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.062\n",
            "4732/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.061\n",
            "4733/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.063\n",
            "4734/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.063\n",
            "4735/7395000 (epoch 3), train_loss = 1.332, time/batch = 0.063\n",
            "4736/7395000 (epoch 3), train_loss = 1.345, time/batch = 0.063\n",
            "4737/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.061\n",
            "4738/7395000 (epoch 3), train_loss = 1.310, time/batch = 0.065\n",
            "4739/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.067\n",
            "4740/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.069\n",
            "4741/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.067\n",
            "4742/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.062\n",
            "4743/7395000 (epoch 3), train_loss = 1.298, time/batch = 0.061\n",
            "4744/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.062\n",
            "4745/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.063\n",
            "4746/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.064\n",
            "4747/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.063\n",
            "4748/7395000 (epoch 3), train_loss = 1.353, time/batch = 0.063\n",
            "4749/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.061\n",
            "4750/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.063\n",
            "4751/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.061\n",
            "4752/7395000 (epoch 3), train_loss = 1.384, time/batch = 0.063\n",
            "4753/7395000 (epoch 3), train_loss = 1.316, time/batch = 0.066\n",
            "4754/7395000 (epoch 3), train_loss = 1.310, time/batch = 0.064\n",
            "4755/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.063\n",
            "4756/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.072\n",
            "4757/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.062\n",
            "4758/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.068\n",
            "4759/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.060\n",
            "4760/7395000 (epoch 3), train_loss = 1.313, time/batch = 0.063\n",
            "4761/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.062\n",
            "4762/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.063\n",
            "4763/7395000 (epoch 3), train_loss = 1.146, time/batch = 0.063\n",
            "4764/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.064\n",
            "4765/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.068\n",
            "4766/7395000 (epoch 3), train_loss = 1.115, time/batch = 0.062\n",
            "4767/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.062\n",
            "4768/7395000 (epoch 3), train_loss = 1.152, time/batch = 0.066\n",
            "4769/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.060\n",
            "4770/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.069\n",
            "4771/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.063\n",
            "4772/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.068\n",
            "4773/7395000 (epoch 3), train_loss = 1.284, time/batch = 0.061\n",
            "4774/7395000 (epoch 3), train_loss = 1.126, time/batch = 0.069\n",
            "4775/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.062\n",
            "4776/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.066\n",
            "4777/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.059\n",
            "4778/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.063\n",
            "4779/7395000 (epoch 3), train_loss = 1.180, time/batch = 0.060\n",
            "4780/7395000 (epoch 3), train_loss = 1.136, time/batch = 0.064\n",
            "4781/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.063\n",
            "4782/7395000 (epoch 3), train_loss = 1.179, time/batch = 0.065\n",
            "4783/7395000 (epoch 3), train_loss = 1.317, time/batch = 0.062\n",
            "4784/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.064\n",
            "4785/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.066\n",
            "4786/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.064\n",
            "4787/7395000 (epoch 3), train_loss = 1.188, time/batch = 0.062\n",
            "4788/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.069\n",
            "4789/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.062\n",
            "4790/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.064\n",
            "4791/7395000 (epoch 3), train_loss = 1.310, time/batch = 0.062\n",
            "4792/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.064\n",
            "4793/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.061\n",
            "4794/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.062\n",
            "4795/7395000 (epoch 3), train_loss = 1.190, time/batch = 0.063\n",
            "4796/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.064\n",
            "4797/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.062\n",
            "4798/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.068\n",
            "4799/7395000 (epoch 3), train_loss = 1.075, time/batch = 0.063\n",
            "4800/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.064\n",
            "4801/7395000 (epoch 3), train_loss = 1.288, time/batch = 0.063\n",
            "4802/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.070\n",
            "4803/7395000 (epoch 3), train_loss = 1.299, time/batch = 0.061\n",
            "4804/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.070\n",
            "4805/7395000 (epoch 3), train_loss = 1.143, time/batch = 0.063\n",
            "4806/7395000 (epoch 3), train_loss = 1.299, time/batch = 0.063\n",
            "4807/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.061\n",
            "4808/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.062\n",
            "4809/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.063\n",
            "4810/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.065\n",
            "4811/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.064\n",
            "4812/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.063\n",
            "4813/7395000 (epoch 3), train_loss = 1.148, time/batch = 0.061\n",
            "4814/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.064\n",
            "4815/7395000 (epoch 3), train_loss = 1.135, time/batch = 0.066\n",
            "4816/7395000 (epoch 3), train_loss = 1.157, time/batch = 0.062\n",
            "4817/7395000 (epoch 3), train_loss = 1.154, time/batch = 0.063\n",
            "4818/7395000 (epoch 3), train_loss = 1.160, time/batch = 0.065\n",
            "4819/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.063\n",
            "4820/7395000 (epoch 3), train_loss = 1.151, time/batch = 0.073\n",
            "4821/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.065\n",
            "4822/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.062\n",
            "4823/7395000 (epoch 3), train_loss = 1.378, time/batch = 0.061\n",
            "4824/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.064\n",
            "4825/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.065\n",
            "4826/7395000 (epoch 3), train_loss = 1.134, time/batch = 0.064\n",
            "4827/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.064\n",
            "4828/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.063\n",
            "4829/7395000 (epoch 3), train_loss = 1.156, time/batch = 0.062\n",
            "4830/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.063\n",
            "4831/7395000 (epoch 3), train_loss = 1.187, time/batch = 0.061\n",
            "4832/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.063\n",
            "4833/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.060\n",
            "4834/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.066\n",
            "4835/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.063\n",
            "4836/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.066\n",
            "4837/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.065\n",
            "4838/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.062\n",
            "4839/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.064\n",
            "4840/7395000 (epoch 3), train_loss = 1.349, time/batch = 0.064\n",
            "4841/7395000 (epoch 3), train_loss = 1.298, time/batch = 0.060\n",
            "4842/7395000 (epoch 3), train_loss = 1.399, time/batch = 0.064\n",
            "4843/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.060\n",
            "4844/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.062\n",
            "4845/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.063\n",
            "4846/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.062\n",
            "4847/7395000 (epoch 3), train_loss = 1.343, time/batch = 0.061\n",
            "4848/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.064\n",
            "4849/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.067\n",
            "4850/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.063\n",
            "4851/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.060\n",
            "4852/7395000 (epoch 3), train_loss = 1.117, time/batch = 0.064\n",
            "4853/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.060\n",
            "4854/7395000 (epoch 3), train_loss = 1.139, time/batch = 0.072\n",
            "4855/7395000 (epoch 3), train_loss = 1.288, time/batch = 0.060\n",
            "4856/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.069\n",
            "4857/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.061\n",
            "4858/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.063\n",
            "4859/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.062\n",
            "4860/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.059\n",
            "4861/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.063\n",
            "4862/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.064\n",
            "4863/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.064\n",
            "4864/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.066\n",
            "4865/7395000 (epoch 3), train_loss = 1.175, time/batch = 0.061\n",
            "4866/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.062\n",
            "4867/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.060\n",
            "4868/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.060\n",
            "4869/7395000 (epoch 3), train_loss = 1.361, time/batch = 0.062\n",
            "4870/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.062\n",
            "4871/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.061\n",
            "4872/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.062\n",
            "4873/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.060\n",
            "4874/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.063\n",
            "4875/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.063\n",
            "4876/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.062\n",
            "4877/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "4878/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.062\n",
            "4879/7395000 (epoch 3), train_loss = 1.162, time/batch = 0.065\n",
            "4880/7395000 (epoch 3), train_loss = 1.159, time/batch = 0.062\n",
            "4881/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.062\n",
            "4882/7395000 (epoch 3), train_loss = 1.164, time/batch = 0.064\n",
            "4883/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.060\n",
            "4884/7395000 (epoch 3), train_loss = 1.157, time/batch = 0.065\n",
            "4885/7395000 (epoch 3), train_loss = 1.157, time/batch = 0.071\n",
            "4886/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.062\n",
            "4887/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.064\n",
            "4888/7395000 (epoch 3), train_loss = 1.128, time/batch = 0.063\n",
            "4889/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.061\n",
            "4890/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.062\n",
            "4891/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.059\n",
            "4892/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.064\n",
            "4893/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.060\n",
            "4894/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.063\n",
            "4895/7395000 (epoch 3), train_loss = 1.227, time/batch = 0.061\n",
            "4896/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.062\n",
            "4897/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.063\n",
            "4898/7395000 (epoch 3), train_loss = 1.225, time/batch = 0.065\n",
            "4899/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.062\n",
            "4900/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.062\n",
            "4901/7395000 (epoch 3), train_loss = 1.203, time/batch = 0.071\n",
            "4902/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.065\n",
            "4903/7395000 (epoch 3), train_loss = 1.324, time/batch = 0.063\n",
            "4904/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.062\n",
            "4905/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.065\n",
            "4906/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.066\n",
            "4907/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.061\n",
            "4908/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.063\n",
            "4909/7395000 (epoch 3), train_loss = 1.313, time/batch = 0.061\n",
            "4910/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.062\n",
            "4911/7395000 (epoch 3), train_loss = 1.223, time/batch = 0.069\n",
            "4912/7395000 (epoch 3), train_loss = 1.368, time/batch = 0.063\n",
            "4913/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.061\n",
            "4914/7395000 (epoch 3), train_loss = 1.330, time/batch = 0.065\n",
            "4915/7395000 (epoch 3), train_loss = 1.300, time/batch = 0.063\n",
            "4916/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.062\n",
            "4917/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.065\n",
            "4918/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.062\n",
            "4919/7395000 (epoch 3), train_loss = 1.426, time/batch = 0.062\n",
            "4920/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.062\n",
            "4921/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.063\n",
            "4922/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.066\n",
            "4923/7395000 (epoch 3), train_loss = 1.346, time/batch = 0.061\n",
            "4924/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.063\n",
            "4925/7395000 (epoch 3), train_loss = 1.316, time/batch = 0.068\n",
            "4926/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.065\n",
            "4927/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.063\n",
            "4928/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.065\n",
            "4929/7395000 (epoch 3), train_loss = 1.330, time/batch = 0.064\n",
            "4930/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.066\n",
            "4931/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.061\n",
            "4932/7395000 (epoch 3), train_loss = 1.403, time/batch = 0.064\n",
            "4933/7395000 (epoch 3), train_loss = 1.417, time/batch = 0.070\n",
            "4934/7395000 (epoch 3), train_loss = 1.336, time/batch = 0.064\n",
            "4935/7395000 (epoch 3), train_loss = 1.465, time/batch = 0.063\n",
            "4936/7395000 (epoch 3), train_loss = 1.352, time/batch = 0.064\n",
            "4937/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.061\n",
            "4938/7395000 (epoch 3), train_loss = 1.369, time/batch = 0.063\n",
            "4939/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.065\n",
            "4940/7395000 (epoch 3), train_loss = 1.328, time/batch = 0.067\n",
            "4941/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.061\n",
            "4942/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.062\n",
            "4943/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.064\n",
            "4944/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.077\n",
            "4945/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.065\n",
            "4946/7395000 (epoch 3), train_loss = 1.356, time/batch = 0.062\n",
            "4947/7395000 (epoch 3), train_loss = 1.317, time/batch = 0.063\n",
            "4948/7395000 (epoch 3), train_loss = 1.359, time/batch = 0.063\n",
            "4949/7395000 (epoch 3), train_loss = 1.427, time/batch = 0.067\n",
            "4950/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.063\n",
            "4951/7395000 (epoch 3), train_loss = 1.385, time/batch = 0.061\n",
            "4952/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.065\n",
            "4953/7395000 (epoch 3), train_loss = 1.284, time/batch = 0.063\n",
            "4954/7395000 (epoch 3), train_loss = 1.424, time/batch = 0.064\n",
            "4955/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.067\n",
            "4956/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.064\n",
            "4957/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.061\n",
            "4958/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.068\n",
            "4959/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.062\n",
            "4960/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.064\n",
            "4961/7395000 (epoch 3), train_loss = 1.264, time/batch = 0.063\n",
            "4962/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.064\n",
            "4963/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "4964/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.062\n",
            "4965/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.070\n",
            "4966/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.064\n",
            "4967/7395000 (epoch 3), train_loss = 1.247, time/batch = 0.068\n",
            "4968/7395000 (epoch 3), train_loss = 1.346, time/batch = 0.063\n",
            "4969/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.058\n",
            "4970/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.064\n",
            "4971/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.066\n",
            "4972/7395000 (epoch 3), train_loss = 1.288, time/batch = 0.064\n",
            "4973/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.061\n",
            "4974/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.062\n",
            "4975/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.068\n",
            "4976/7395000 (epoch 3), train_loss = 1.172, time/batch = 0.060\n",
            "4977/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.063\n",
            "4978/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.064\n",
            "4979/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.060\n",
            "4980/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.068\n",
            "4981/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.072\n",
            "4982/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.063\n",
            "4983/7395000 (epoch 3), train_loss = 1.160, time/batch = 0.063\n",
            "4984/7395000 (epoch 3), train_loss = 1.335, time/batch = 0.064\n",
            "4985/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.065\n",
            "4986/7395000 (epoch 3), train_loss = 1.310, time/batch = 0.061\n",
            "4987/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.060\n",
            "4988/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.063\n",
            "4989/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.063\n",
            "4990/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.064\n",
            "4991/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.063\n",
            "4992/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.062\n",
            "4993/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.063\n",
            "4994/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.062\n",
            "4995/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.059\n",
            "4996/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.064\n",
            "4997/7395000 (epoch 3), train_loss = 1.190, time/batch = 0.065\n",
            "4998/7395000 (epoch 3), train_loss = 1.175, time/batch = 0.062\n",
            "4999/7395000 (epoch 3), train_loss = 1.145, time/batch = 0.064\n",
            "5000/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.064\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "model saved to save/model.ckpt\n",
            "5001/7395000 (epoch 3), train_loss = 1.167, time/batch = 0.063\n",
            "5002/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.062\n",
            "5003/7395000 (epoch 3), train_loss = 1.180, time/batch = 0.064\n",
            "5004/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.066\n",
            "5005/7395000 (epoch 3), train_loss = 1.160, time/batch = 0.063\n",
            "5006/7395000 (epoch 3), train_loss = 1.180, time/batch = 0.061\n",
            "5007/7395000 (epoch 3), train_loss = 1.178, time/batch = 0.063\n",
            "5008/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.063\n",
            "5009/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.068\n",
            "5010/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.063\n",
            "5011/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.062\n",
            "5012/7395000 (epoch 3), train_loss = 1.376, time/batch = 0.064\n",
            "5013/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.061\n",
            "5014/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.065\n",
            "5015/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.068\n",
            "5016/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.063\n",
            "5017/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.058\n",
            "5018/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.065\n",
            "5019/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.060\n",
            "5020/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.063\n",
            "5021/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.055\n",
            "5022/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.054\n",
            "5023/7395000 (epoch 3), train_loss = 1.279, time/batch = 0.056\n",
            "5024/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.062\n",
            "5025/7395000 (epoch 3), train_loss = 1.300, time/batch = 0.056\n",
            "5026/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.064\n",
            "5027/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.056\n",
            "5028/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.062\n",
            "5029/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.066\n",
            "5030/7395000 (epoch 3), train_loss = 1.215, time/batch = 0.056\n",
            "5031/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.057\n",
            "5032/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.058\n",
            "5033/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.055\n",
            "5034/7395000 (epoch 3), train_loss = 1.313, time/batch = 0.056\n",
            "5035/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.055\n",
            "5036/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.055\n",
            "5037/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.054\n",
            "5038/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.058\n",
            "5039/7395000 (epoch 3), train_loss = 1.279, time/batch = 0.055\n",
            "5040/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.056\n",
            "5041/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.055\n",
            "5042/7395000 (epoch 3), train_loss = 1.300, time/batch = 0.056\n",
            "5043/7395000 (epoch 3), train_loss = 1.354, time/batch = 0.069\n",
            "5044/7395000 (epoch 3), train_loss = 1.287, time/batch = 0.063\n",
            "5045/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.058\n",
            "5046/7395000 (epoch 3), train_loss = 1.300, time/batch = 0.067\n",
            "5047/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.059\n",
            "5048/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.066\n",
            "5049/7395000 (epoch 3), train_loss = 1.197, time/batch = 0.061\n",
            "5050/7395000 (epoch 3), train_loss = 1.289, time/batch = 0.063\n",
            "5051/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.059\n",
            "5052/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.068\n",
            "5053/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.057\n",
            "5054/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.063\n",
            "5055/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.062\n",
            "5056/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.064\n",
            "5057/7395000 (epoch 3), train_loss = 1.176, time/batch = 0.063\n",
            "5058/7395000 (epoch 3), train_loss = 1.153, time/batch = 0.069\n",
            "5059/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.064\n",
            "5060/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.058\n",
            "5061/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.060\n",
            "5062/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.069\n",
            "5063/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.061\n",
            "5064/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.066\n",
            "5065/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.060\n",
            "5066/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.063\n",
            "5067/7395000 (epoch 3), train_loss = 1.331, time/batch = 0.060\n",
            "5068/7395000 (epoch 3), train_loss = 1.279, time/batch = 0.062\n",
            "5069/7395000 (epoch 3), train_loss = 1.318, time/batch = 0.060\n",
            "5070/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.062\n",
            "5071/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.060\n",
            "5072/7395000 (epoch 3), train_loss = 1.178, time/batch = 0.064\n",
            "5073/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.060\n",
            "5074/7395000 (epoch 3), train_loss = 1.131, time/batch = 0.069\n",
            "5075/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.062\n",
            "5076/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.062\n",
            "5077/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.062\n",
            "5078/7395000 (epoch 3), train_loss = 1.279, time/batch = 0.055\n",
            "5079/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.063\n",
            "5080/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.065\n",
            "5081/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.060\n",
            "5082/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.062\n",
            "5083/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.063\n",
            "5084/7395000 (epoch 3), train_loss = 1.135, time/batch = 0.062\n",
            "5085/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.063\n",
            "5086/7395000 (epoch 3), train_loss = 1.218, time/batch = 0.060\n",
            "5087/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.062\n",
            "5088/7395000 (epoch 3), train_loss = 1.129, time/batch = 0.062\n",
            "5089/7395000 (epoch 3), train_loss = 1.097, time/batch = 0.061\n",
            "5090/7395000 (epoch 3), train_loss = 1.150, time/batch = 0.063\n",
            "5091/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.055\n",
            "5092/7395000 (epoch 3), train_loss = 1.346, time/batch = 0.055\n",
            "5093/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.065\n",
            "5094/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.063\n",
            "5095/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.060\n",
            "5096/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.063\n",
            "5097/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.058\n",
            "5098/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.063\n",
            "5099/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.060\n",
            "5100/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.063\n",
            "5101/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.063\n",
            "5102/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.064\n",
            "5103/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.070\n",
            "5104/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.063\n",
            "5105/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.059\n",
            "5106/7395000 (epoch 3), train_loss = 1.175, time/batch = 0.067\n",
            "5107/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.066\n",
            "5108/7395000 (epoch 3), train_loss = 1.195, time/batch = 0.060\n",
            "5109/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.067\n",
            "5110/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.063\n",
            "5111/7395000 (epoch 3), train_loss = 1.146, time/batch = 0.070\n",
            "5112/7395000 (epoch 3), train_loss = 1.106, time/batch = 0.062\n",
            "5113/7395000 (epoch 3), train_loss = 1.175, time/batch = 0.062\n",
            "5114/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.062\n",
            "5115/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.062\n",
            "5116/7395000 (epoch 3), train_loss = 1.314, time/batch = 0.061\n",
            "5117/7395000 (epoch 3), train_loss = 1.341, time/batch = 0.074\n",
            "5118/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.061\n",
            "5119/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.062\n",
            "5120/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.064\n",
            "5121/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.063\n",
            "5122/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.068\n",
            "5123/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.063\n",
            "5124/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.060\n",
            "5125/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.071\n",
            "5126/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.062\n",
            "5127/7395000 (epoch 3), train_loss = 1.223, time/batch = 0.059\n",
            "5128/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.066\n",
            "5129/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.059\n",
            "5130/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.064\n",
            "5131/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.062\n",
            "5132/7395000 (epoch 3), train_loss = 1.169, time/batch = 0.058\n",
            "5133/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.062\n",
            "5134/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.062\n",
            "5135/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.060\n",
            "5136/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.061\n",
            "5137/7395000 (epoch 3), train_loss = 1.247, time/batch = 0.058\n",
            "5138/7395000 (epoch 3), train_loss = 1.171, time/batch = 0.059\n",
            "5139/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.060\n",
            "5140/7395000 (epoch 3), train_loss = 1.179, time/batch = 0.068\n",
            "5141/7395000 (epoch 3), train_loss = 1.197, time/batch = 0.060\n",
            "5142/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.060\n",
            "5143/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.062\n",
            "5144/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.061\n",
            "5145/7395000 (epoch 3), train_loss = 1.135, time/batch = 0.066\n",
            "5146/7395000 (epoch 3), train_loss = 1.131, time/batch = 0.063\n",
            "5147/7395000 (epoch 3), train_loss = 1.164, time/batch = 0.061\n",
            "5148/7395000 (epoch 3), train_loss = 1.162, time/batch = 0.062\n",
            "5149/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.064\n",
            "5150/7395000 (epoch 3), train_loss = 1.128, time/batch = 0.058\n",
            "5151/7395000 (epoch 3), train_loss = 1.173, time/batch = 0.061\n",
            "5152/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.061\n",
            "5153/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.061\n",
            "5154/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.062\n",
            "5155/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.054\n",
            "5156/7395000 (epoch 3), train_loss = 1.134, time/batch = 0.064\n",
            "5157/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.061\n",
            "5158/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.070\n",
            "5159/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.060\n",
            "5160/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.060\n",
            "5161/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.061\n",
            "5162/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.062\n",
            "5163/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.062\n",
            "5164/7395000 (epoch 3), train_loss = 1.318, time/batch = 0.066\n",
            "5165/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.060\n",
            "5166/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.063\n",
            "5167/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.060\n",
            "5168/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.061\n",
            "5169/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.063\n",
            "5170/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.063\n",
            "5171/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.061\n",
            "5172/7395000 (epoch 3), train_loss = 1.247, time/batch = 0.062\n",
            "5173/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.061\n",
            "5174/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.062\n",
            "5175/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.058\n",
            "5176/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.057\n",
            "5177/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.062\n",
            "5178/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.060\n",
            "5179/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.062\n",
            "5180/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.061\n",
            "5181/7395000 (epoch 3), train_loss = 1.180, time/batch = 0.060\n",
            "5182/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.064\n",
            "5183/7395000 (epoch 3), train_loss = 1.197, time/batch = 0.061\n",
            "5184/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.067\n",
            "5185/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.061\n",
            "5186/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.061\n",
            "5187/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.060\n",
            "5188/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.063\n",
            "5189/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.059\n",
            "5190/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.059\n",
            "5191/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.065\n",
            "5192/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.062\n",
            "5193/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.059\n",
            "5194/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.064\n",
            "5195/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.062\n",
            "5196/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.062\n",
            "5197/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.060\n",
            "5198/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.063\n",
            "5199/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.062\n",
            "5200/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.063\n",
            "5201/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.054\n",
            "5202/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.056\n",
            "5203/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.056\n",
            "5204/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.062\n",
            "5205/7395000 (epoch 3), train_loss = 1.145, time/batch = 0.062\n",
            "5206/7395000 (epoch 3), train_loss = 1.225, time/batch = 0.064\n",
            "5207/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.060\n",
            "5208/7395000 (epoch 3), train_loss = 1.304, time/batch = 0.069\n",
            "5209/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.059\n",
            "5210/7395000 (epoch 3), train_loss = 1.264, time/batch = 0.062\n",
            "5211/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.061\n",
            "5212/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.061\n",
            "5213/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.060\n",
            "5214/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.058\n",
            "5215/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.062\n",
            "5216/7395000 (epoch 3), train_loss = 1.160, time/batch = 0.067\n",
            "5217/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.059\n",
            "5218/7395000 (epoch 3), train_loss = 1.131, time/batch = 0.061\n",
            "5219/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.064\n",
            "5220/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.063\n",
            "5221/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.062\n",
            "5222/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.066\n",
            "5223/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.064\n",
            "5224/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.065\n",
            "5225/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.057\n",
            "5226/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.061\n",
            "5227/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.063\n",
            "5228/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.065\n",
            "5229/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.060\n",
            "5230/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.062\n",
            "5231/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.066\n",
            "5232/7395000 (epoch 3), train_loss = 1.108, time/batch = 0.063\n",
            "5233/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.062\n",
            "5234/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.063\n",
            "5235/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.060\n",
            "5236/7395000 (epoch 3), train_loss = 1.209, time/batch = 0.065\n",
            "5237/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.062\n",
            "5238/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.062\n",
            "5239/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.062\n",
            "5240/7395000 (epoch 3), train_loss = 1.293, time/batch = 0.063\n",
            "5241/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.066\n",
            "5242/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.061\n",
            "5243/7395000 (epoch 3), train_loss = 1.171, time/batch = 0.058\n",
            "5244/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.062\n",
            "5245/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.060\n",
            "5246/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.065\n",
            "5247/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.062\n",
            "5248/7395000 (epoch 3), train_loss = 1.344, time/batch = 0.063\n",
            "5249/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.060\n",
            "5250/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.062\n",
            "5251/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.059\n",
            "5252/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.062\n",
            "5253/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.062\n",
            "5254/7395000 (epoch 3), train_loss = 1.333, time/batch = 0.062\n",
            "5255/7395000 (epoch 3), train_loss = 1.293, time/batch = 0.059\n",
            "5256/7395000 (epoch 3), train_loss = 1.358, time/batch = 0.063\n",
            "5257/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.066\n",
            "5258/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.056\n",
            "5259/7395000 (epoch 3), train_loss = 1.343, time/batch = 0.061\n",
            "5260/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.063\n",
            "5261/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.061\n",
            "5262/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.062\n",
            "5263/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.060\n",
            "5264/7395000 (epoch 3), train_loss = 1.356, time/batch = 0.067\n",
            "5265/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.063\n",
            "5266/7395000 (epoch 3), train_loss = 1.314, time/batch = 0.061\n",
            "5267/7395000 (epoch 3), train_loss = 1.247, time/batch = 0.060\n",
            "5268/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.061\n",
            "5269/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.060\n",
            "5270/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.071\n",
            "5271/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.060\n",
            "5272/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.060\n",
            "5273/7395000 (epoch 3), train_loss = 1.335, time/batch = 0.063\n",
            "5274/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.066\n",
            "5275/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.062\n",
            "5276/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.063\n",
            "5277/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.062\n",
            "5278/7395000 (epoch 3), train_loss = 1.223, time/batch = 0.066\n",
            "5279/7395000 (epoch 3), train_loss = 1.302, time/batch = 0.060\n",
            "5280/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.063\n",
            "5281/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.061\n",
            "5282/7395000 (epoch 3), train_loss = 1.338, time/batch = 0.063\n",
            "5283/7395000 (epoch 3), train_loss = 1.317, time/batch = 0.060\n",
            "5284/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.064\n",
            "5285/7395000 (epoch 3), train_loss = 1.329, time/batch = 0.060\n",
            "5286/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.070\n",
            "5287/7395000 (epoch 3), train_loss = 1.319, time/batch = 0.072\n",
            "5288/7395000 (epoch 3), train_loss = 1.368, time/batch = 0.061\n",
            "5289/7395000 (epoch 3), train_loss = 1.327, time/batch = 0.062\n",
            "5290/7395000 (epoch 3), train_loss = 1.215, time/batch = 0.066\n",
            "5291/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.060\n",
            "5292/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.063\n",
            "5293/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.056\n",
            "5294/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.063\n",
            "5295/7395000 (epoch 3), train_loss = 1.278, time/batch = 0.062\n",
            "5296/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.064\n",
            "5297/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.060\n",
            "5298/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.063\n",
            "5299/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.065\n",
            "5300/7395000 (epoch 3), train_loss = 1.326, time/batch = 0.064\n",
            "5301/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.063\n",
            "5302/7395000 (epoch 3), train_loss = 1.264, time/batch = 0.062\n",
            "5303/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.062\n",
            "5304/7395000 (epoch 3), train_loss = 1.325, time/batch = 0.063\n",
            "5305/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.062\n",
            "5306/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.055\n",
            "5307/7395000 (epoch 3), train_loss = 1.340, time/batch = 0.065\n",
            "5308/7395000 (epoch 3), train_loss = 1.345, time/batch = 0.055\n",
            "5309/7395000 (epoch 3), train_loss = 1.169, time/batch = 0.058\n",
            "5310/7395000 (epoch 3), train_loss = 1.338, time/batch = 0.065\n",
            "5311/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.059\n",
            "5312/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.062\n",
            "5313/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.060\n",
            "5314/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.061\n",
            "5315/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.060\n",
            "5316/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.059\n",
            "5317/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.061\n",
            "5318/7395000 (epoch 3), train_loss = 1.309, time/batch = 0.064\n",
            "5319/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.061\n",
            "5320/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.063\n",
            "5321/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.062\n",
            "5322/7395000 (epoch 3), train_loss = 1.314, time/batch = 0.060\n",
            "5323/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.062\n",
            "5324/7395000 (epoch 3), train_loss = 1.154, time/batch = 0.065\n",
            "5325/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.067\n",
            "5326/7395000 (epoch 3), train_loss = 1.299, time/batch = 0.063\n",
            "5327/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.062\n",
            "5328/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.061\n",
            "5329/7395000 (epoch 3), train_loss = 1.132, time/batch = 0.061\n",
            "5330/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.063\n",
            "5331/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.060\n",
            "5332/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.062\n",
            "5333/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.054\n",
            "5334/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.065\n",
            "5335/7395000 (epoch 3), train_loss = 1.143, time/batch = 0.060\n",
            "5336/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.063\n",
            "5337/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.062\n",
            "5338/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.064\n",
            "5339/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.060\n",
            "5340/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.068\n",
            "5341/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.061\n",
            "5342/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.064\n",
            "5343/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.061\n",
            "5344/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.063\n",
            "5345/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.063\n",
            "5346/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.061\n",
            "5347/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.060\n",
            "5348/7395000 (epoch 3), train_loss = 1.135, time/batch = 0.065\n",
            "5349/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.062\n",
            "5350/7395000 (epoch 3), train_loss = 1.160, time/batch = 0.063\n",
            "5351/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.061\n",
            "5352/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.062\n",
            "5353/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.063\n",
            "5354/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.062\n",
            "5355/7395000 (epoch 3), train_loss = 1.242, time/batch = 0.059\n",
            "5356/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.064\n",
            "5357/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.062\n",
            "5358/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.064\n",
            "5359/7395000 (epoch 3), train_loss = 1.281, time/batch = 0.061\n",
            "5360/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.061\n",
            "5361/7395000 (epoch 3), train_loss = 1.293, time/batch = 0.064\n",
            "5362/7395000 (epoch 3), train_loss = 1.285, time/batch = 0.057\n",
            "5363/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.063\n",
            "5364/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.063\n",
            "5365/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.062\n",
            "5366/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.061\n",
            "5367/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.060\n",
            "5368/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.057\n",
            "5369/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.062\n",
            "5370/7395000 (epoch 3), train_loss = 1.317, time/batch = 0.065\n",
            "5371/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.060\n",
            "5372/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.063\n",
            "5373/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.064\n",
            "5374/7395000 (epoch 3), train_loss = 1.134, time/batch = 0.063\n",
            "5375/7395000 (epoch 3), train_loss = 1.156, time/batch = 0.059\n",
            "5376/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.062\n",
            "5377/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.062\n",
            "5378/7395000 (epoch 3), train_loss = 1.120, time/batch = 0.061\n",
            "5379/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.060\n",
            "5380/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.061\n",
            "5381/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.063\n",
            "5382/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.064\n",
            "5383/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.063\n",
            "5384/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.064\n",
            "5385/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.060\n",
            "5386/7395000 (epoch 3), train_loss = 1.201, time/batch = 0.063\n",
            "5387/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.065\n",
            "5388/7395000 (epoch 3), train_loss = 1.156, time/batch = 0.063\n",
            "5389/7395000 (epoch 3), train_loss = 1.131, time/batch = 0.059\n",
            "5390/7395000 (epoch 3), train_loss = 1.146, time/batch = 0.063\n",
            "5391/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.064\n",
            "5392/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.063\n",
            "5393/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.061\n",
            "5394/7395000 (epoch 3), train_loss = 1.270, time/batch = 0.063\n",
            "5395/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.061\n",
            "5396/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.064\n",
            "5397/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.065\n",
            "5398/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.063\n",
            "5399/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.062\n",
            "5400/7395000 (epoch 3), train_loss = 1.218, time/batch = 0.066\n",
            "5401/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.061\n",
            "5402/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.063\n",
            "5403/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.063\n",
            "5404/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.061\n",
            "5405/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.062\n",
            "5406/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.072\n",
            "5407/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.059\n",
            "5408/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.062\n",
            "5409/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.062\n",
            "5410/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.065\n",
            "5411/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.062\n",
            "5412/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.062\n",
            "5413/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.060\n",
            "5414/7395000 (epoch 3), train_loss = 1.328, time/batch = 0.061\n",
            "5415/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.062\n",
            "5416/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.063\n",
            "5417/7395000 (epoch 3), train_loss = 1.239, time/batch = 0.063\n",
            "5418/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.063\n",
            "5419/7395000 (epoch 3), train_loss = 1.304, time/batch = 0.062\n",
            "5420/7395000 (epoch 3), train_loss = 1.324, time/batch = 0.067\n",
            "5421/7395000 (epoch 3), train_loss = 1.301, time/batch = 0.055\n",
            "5422/7395000 (epoch 3), train_loss = 1.289, time/batch = 0.054\n",
            "5423/7395000 (epoch 3), train_loss = 1.335, time/batch = 0.062\n",
            "5424/7395000 (epoch 3), train_loss = 1.325, time/batch = 0.060\n",
            "5425/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.054\n",
            "5426/7395000 (epoch 3), train_loss = 1.329, time/batch = 0.056\n",
            "5427/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.053\n",
            "5428/7395000 (epoch 3), train_loss = 1.343, time/batch = 0.060\n",
            "5429/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.062\n",
            "5430/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.061\n",
            "5431/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.060\n",
            "5432/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.060\n",
            "5433/7395000 (epoch 3), train_loss = 1.325, time/batch = 0.063\n",
            "5434/7395000 (epoch 3), train_loss = 1.227, time/batch = 0.062\n",
            "5435/7395000 (epoch 3), train_loss = 1.353, time/batch = 0.062\n",
            "5436/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.065\n",
            "5437/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.063\n",
            "5438/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.060\n",
            "5439/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.064\n",
            "5440/7395000 (epoch 3), train_loss = 1.203, time/batch = 0.056\n",
            "5441/7395000 (epoch 3), train_loss = 1.268, time/batch = 0.060\n",
            "5442/7395000 (epoch 3), train_loss = 1.319, time/batch = 0.061\n",
            "5443/7395000 (epoch 3), train_loss = 1.348, time/batch = 0.060\n",
            "5444/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.062\n",
            "5445/7395000 (epoch 3), train_loss = 1.364, time/batch = 0.059\n",
            "5446/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.065\n",
            "5447/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.060\n",
            "5448/7395000 (epoch 3), train_loss = 1.179, time/batch = 0.065\n",
            "5449/7395000 (epoch 3), train_loss = 1.225, time/batch = 0.062\n",
            "5450/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.065\n",
            "5451/7395000 (epoch 3), train_loss = 1.218, time/batch = 0.061\n",
            "5452/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.063\n",
            "5453/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.061\n",
            "5454/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.063\n",
            "5455/7395000 (epoch 3), train_loss = 1.415, time/batch = 0.062\n",
            "5456/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.066\n",
            "5457/7395000 (epoch 3), train_loss = 1.311, time/batch = 0.062\n",
            "5458/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.063\n",
            "5459/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.062\n",
            "5460/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.064\n",
            "5461/7395000 (epoch 3), train_loss = 1.394, time/batch = 0.064\n",
            "5462/7395000 (epoch 3), train_loss = 1.295, time/batch = 0.065\n",
            "5463/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.060\n",
            "5464/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.069\n",
            "5465/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.070\n",
            "5466/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "5467/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.062\n",
            "5468/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.061\n",
            "5469/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.057\n",
            "5470/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.065\n",
            "5471/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.063\n",
            "5472/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.069\n",
            "5473/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.062\n",
            "5474/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.062\n",
            "5475/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.057\n",
            "5476/7395000 (epoch 3), train_loss = 1.262, time/batch = 0.062\n",
            "5477/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.060\n",
            "5478/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.067\n",
            "5479/7395000 (epoch 3), train_loss = 1.289, time/batch = 0.063\n",
            "5480/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.063\n",
            "5481/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.065\n",
            "5482/7395000 (epoch 3), train_loss = 1.312, time/batch = 0.064\n",
            "5483/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.059\n",
            "5484/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.064\n",
            "5485/7395000 (epoch 3), train_loss = 1.411, time/batch = 0.063\n",
            "5486/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.065\n",
            "5487/7395000 (epoch 3), train_loss = 1.413, time/batch = 0.061\n",
            "5488/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.069\n",
            "5489/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.061\n",
            "5490/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.064\n",
            "5491/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.058\n",
            "5492/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.061\n",
            "5493/7395000 (epoch 3), train_loss = 1.344, time/batch = 0.059\n",
            "5494/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.064\n",
            "5495/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.060\n",
            "5496/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.068\n",
            "5497/7395000 (epoch 3), train_loss = 1.348, time/batch = 0.063\n",
            "5498/7395000 (epoch 3), train_loss = 1.309, time/batch = 0.062\n",
            "5499/7395000 (epoch 3), train_loss = 1.328, time/batch = 0.061\n",
            "5500/7395000 (epoch 3), train_loss = 1.362, time/batch = 0.062\n",
            "5501/7395000 (epoch 3), train_loss = 1.361, time/batch = 0.060\n",
            "5502/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.063\n",
            "5503/7395000 (epoch 3), train_loss = 1.250, time/batch = 0.060\n",
            "5504/7395000 (epoch 3), train_loss = 1.215, time/batch = 0.062\n",
            "5505/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.064\n",
            "5506/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.062\n",
            "5507/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.065\n",
            "5508/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.062\n",
            "5509/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.060\n",
            "5510/7395000 (epoch 3), train_loss = 1.146, time/batch = 0.061\n",
            "5511/7395000 (epoch 3), train_loss = 1.116, time/batch = 0.061\n",
            "5512/7395000 (epoch 3), train_loss = 1.138, time/batch = 0.063\n",
            "5513/7395000 (epoch 3), train_loss = 1.105, time/batch = 0.063\n",
            "5514/7395000 (epoch 3), train_loss = 1.107, time/batch = 0.062\n",
            "5515/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.060\n",
            "5516/7395000 (epoch 3), train_loss = 1.096, time/batch = 0.061\n",
            "5517/7395000 (epoch 3), train_loss = 1.169, time/batch = 0.060\n",
            "5518/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.061\n",
            "5519/7395000 (epoch 3), train_loss = 1.308, time/batch = 0.060\n",
            "5520/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.061\n",
            "5521/7395000 (epoch 3), train_loss = 1.163, time/batch = 0.062\n",
            "5522/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.059\n",
            "5523/7395000 (epoch 3), train_loss = 1.350, time/batch = 0.062\n",
            "5524/7395000 (epoch 3), train_loss = 1.187, time/batch = 0.063\n",
            "5525/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.063\n",
            "5526/7395000 (epoch 3), train_loss = 1.297, time/batch = 0.062\n",
            "5527/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.061\n",
            "5528/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.062\n",
            "5529/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.060\n",
            "5530/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.063\n",
            "5531/7395000 (epoch 3), train_loss = 1.263, time/batch = 0.062\n",
            "5532/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.063\n",
            "5533/7395000 (epoch 3), train_loss = 1.304, time/batch = 0.065\n",
            "5534/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.063\n",
            "5535/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.061\n",
            "5536/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.061\n",
            "5537/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.060\n",
            "5538/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.071\n",
            "5539/7395000 (epoch 3), train_loss = 1.279, time/batch = 0.063\n",
            "5540/7395000 (epoch 3), train_loss = 1.131, time/batch = 0.063\n",
            "5541/7395000 (epoch 3), train_loss = 1.225, time/batch = 0.063\n",
            "5542/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.062\n",
            "5543/7395000 (epoch 3), train_loss = 1.294, time/batch = 0.059\n",
            "5544/7395000 (epoch 3), train_loss = 1.288, time/batch = 0.061\n",
            "5545/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.062\n",
            "5546/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.060\n",
            "5547/7395000 (epoch 3), train_loss = 1.360, time/batch = 0.062\n",
            "5548/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.064\n",
            "5549/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.061\n",
            "5550/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.068\n",
            "5551/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.061\n",
            "5552/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.064\n",
            "5553/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.055\n",
            "5554/7395000 (epoch 3), train_loss = 1.390, time/batch = 0.057\n",
            "5555/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.060\n",
            "5556/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.054\n",
            "5557/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.059\n",
            "5558/7395000 (epoch 3), train_loss = 1.172, time/batch = 0.061\n",
            "5559/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.060\n",
            "5560/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.061\n",
            "5561/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.064\n",
            "5562/7395000 (epoch 3), train_loss = 1.214, time/batch = 0.062\n",
            "5563/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.059\n",
            "5564/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.062\n",
            "5565/7395000 (epoch 3), train_loss = 1.287, time/batch = 0.060\n",
            "5566/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.061\n",
            "5567/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.060\n",
            "5568/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.061\n",
            "5569/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.065\n",
            "5570/7395000 (epoch 3), train_loss = 1.191, time/batch = 0.061\n",
            "5571/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.065\n",
            "5572/7395000 (epoch 3), train_loss = 1.223, time/batch = 0.062\n",
            "5573/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.059\n",
            "5574/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.066\n",
            "5575/7395000 (epoch 3), train_loss = 1.363, time/batch = 0.060\n",
            "5576/7395000 (epoch 3), train_loss = 1.172, time/batch = 0.064\n",
            "5577/7395000 (epoch 3), train_loss = 1.284, time/batch = 0.058\n",
            "5578/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.062\n",
            "5579/7395000 (epoch 3), train_loss = 1.287, time/batch = 0.062\n",
            "5580/7395000 (epoch 3), train_loss = 1.165, time/batch = 0.061\n",
            "5581/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.062\n",
            "5582/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.060\n",
            "5583/7395000 (epoch 3), train_loss = 1.161, time/batch = 0.059\n",
            "5584/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.061\n",
            "5585/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.060\n",
            "5586/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.064\n",
            "5587/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.066\n",
            "5588/7395000 (epoch 3), train_loss = 1.223, time/batch = 0.062\n",
            "5589/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.061\n",
            "5590/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.067\n",
            "5591/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.062\n",
            "5592/7395000 (epoch 3), train_loss = 1.275, time/batch = 0.063\n",
            "5593/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.063\n",
            "5594/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "5595/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.065\n",
            "5596/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.062\n",
            "5597/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.062\n",
            "5598/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.062\n",
            "5599/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.063\n",
            "5600/7395000 (epoch 3), train_loss = 1.176, time/batch = 0.062\n",
            "5601/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.062\n",
            "5602/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.063\n",
            "5603/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.072\n",
            "5604/7395000 (epoch 3), train_loss = 1.176, time/batch = 0.072\n",
            "5605/7395000 (epoch 3), train_loss = 1.151, time/batch = 0.060\n",
            "5606/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.065\n",
            "5607/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.062\n",
            "5608/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.065\n",
            "5609/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.064\n",
            "5610/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.061\n",
            "5611/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.062\n",
            "5612/7395000 (epoch 3), train_loss = 1.276, time/batch = 0.066\n",
            "5613/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.061\n",
            "5614/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.063\n",
            "5615/7395000 (epoch 3), train_loss = 1.139, time/batch = 0.059\n",
            "5616/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.062\n",
            "5617/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.065\n",
            "5618/7395000 (epoch 3), train_loss = 1.334, time/batch = 0.062\n",
            "5619/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.062\n",
            "5620/7395000 (epoch 3), train_loss = 1.256, time/batch = 0.071\n",
            "5621/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.063\n",
            "5622/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.066\n",
            "5623/7395000 (epoch 3), train_loss = 1.239, time/batch = 0.059\n",
            "5624/7395000 (epoch 3), train_loss = 1.216, time/batch = 0.062\n",
            "5625/7395000 (epoch 3), train_loss = 1.252, time/batch = 0.061\n",
            "5626/7395000 (epoch 3), train_loss = 1.221, time/batch = 0.063\n",
            "5627/7395000 (epoch 3), train_loss = 1.227, time/batch = 0.057\n",
            "5628/7395000 (epoch 3), train_loss = 1.187, time/batch = 0.063\n",
            "5629/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.061\n",
            "5630/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.065\n",
            "5631/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.060\n",
            "5632/7395000 (epoch 3), train_loss = 1.289, time/batch = 0.062\n",
            "5633/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.062\n",
            "5634/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "5635/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.062\n",
            "5636/7395000 (epoch 3), train_loss = 1.345, time/batch = 0.064\n",
            "5637/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.055\n",
            "5638/7395000 (epoch 3), train_loss = 1.243, time/batch = 0.064\n",
            "5639/7395000 (epoch 3), train_loss = 1.179, time/batch = 0.062\n",
            "5640/7395000 (epoch 3), train_loss = 1.169, time/batch = 0.063\n",
            "5641/7395000 (epoch 3), train_loss = 1.102, time/batch = 0.062\n",
            "5642/7395000 (epoch 3), train_loss = 1.162, time/batch = 0.063\n",
            "5643/7395000 (epoch 3), train_loss = 1.146, time/batch = 0.056\n",
            "5644/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.055\n",
            "5645/7395000 (epoch 3), train_loss = 1.093, time/batch = 0.057\n",
            "5646/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.054\n",
            "5647/7395000 (epoch 3), train_loss = 1.139, time/batch = 0.055\n",
            "5648/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.056\n",
            "5649/7395000 (epoch 3), train_loss = 1.320, time/batch = 0.054\n",
            "5650/7395000 (epoch 3), train_loss = 1.225, time/batch = 0.056\n",
            "5651/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.055\n",
            "5652/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.055\n",
            "5653/7395000 (epoch 3), train_loss = 1.217, time/batch = 0.061\n",
            "5654/7395000 (epoch 3), train_loss = 1.168, time/batch = 0.067\n",
            "5655/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.066\n",
            "5656/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.063\n",
            "5657/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.074\n",
            "5658/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.066\n",
            "5659/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.062\n",
            "5660/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.063\n",
            "5661/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.060\n",
            "5662/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.063\n",
            "5663/7395000 (epoch 3), train_loss = 1.128, time/batch = 0.061\n",
            "5664/7395000 (epoch 3), train_loss = 1.114, time/batch = 0.062\n",
            "5665/7395000 (epoch 3), train_loss = 1.126, time/batch = 0.059\n",
            "5666/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.062\n",
            "5667/7395000 (epoch 3), train_loss = 1.206, time/batch = 0.062\n",
            "5668/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.064\n",
            "5669/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.060\n",
            "5670/7395000 (epoch 3), train_loss = 1.171, time/batch = 0.066\n",
            "5671/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.062\n",
            "5672/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.064\n",
            "5673/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.064\n",
            "5674/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.065\n",
            "5675/7395000 (epoch 3), train_loss = 1.188, time/batch = 0.060\n",
            "5676/7395000 (epoch 3), train_loss = 1.253, time/batch = 0.061\n",
            "5677/7395000 (epoch 3), train_loss = 1.179, time/batch = 0.060\n",
            "5678/7395000 (epoch 3), train_loss = 1.249, time/batch = 0.063\n",
            "5679/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.061\n",
            "5680/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.063\n",
            "5681/7395000 (epoch 3), train_loss = 1.176, time/batch = 0.063\n",
            "5682/7395000 (epoch 3), train_loss = 1.091, time/batch = 0.063\n",
            "5683/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.063\n",
            "5684/7395000 (epoch 3), train_loss = 1.173, time/batch = 0.064\n",
            "5685/7395000 (epoch 3), train_loss = 1.167, time/batch = 0.062\n",
            "5686/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.064\n",
            "5687/7395000 (epoch 3), train_loss = 1.058, time/batch = 0.064\n",
            "5688/7395000 (epoch 3), train_loss = 1.165, time/batch = 0.063\n",
            "5689/7395000 (epoch 3), train_loss = 1.244, time/batch = 0.062\n",
            "5690/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.063\n",
            "5691/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.060\n",
            "5692/7395000 (epoch 3), train_loss = 1.337, time/batch = 0.063\n",
            "5693/7395000 (epoch 3), train_loss = 1.195, time/batch = 0.060\n",
            "5694/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.062\n",
            "5695/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.063\n",
            "5696/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.063\n",
            "5697/7395000 (epoch 3), train_loss = 1.171, time/batch = 0.066\n",
            "5698/7395000 (epoch 3), train_loss = 1.125, time/batch = 0.062\n",
            "5699/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.060\n",
            "5700/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.062\n",
            "5701/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.060\n",
            "5702/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.063\n",
            "5703/7395000 (epoch 3), train_loss = 1.248, time/batch = 0.063\n",
            "5704/7395000 (epoch 3), train_loss = 1.054, time/batch = 0.070\n",
            "5705/7395000 (epoch 3), train_loss = 1.166, time/batch = 0.065\n",
            "5706/7395000 (epoch 3), train_loss = 1.092, time/batch = 0.062\n",
            "5707/7395000 (epoch 3), train_loss = 1.127, time/batch = 0.060\n",
            "5708/7395000 (epoch 3), train_loss = 1.190, time/batch = 0.062\n",
            "5709/7395000 (epoch 3), train_loss = 1.286, time/batch = 0.071\n",
            "5710/7395000 (epoch 3), train_loss = 1.241, time/batch = 0.059\n",
            "5711/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.060\n",
            "5712/7395000 (epoch 3), train_loss = 1.172, time/batch = 0.061\n",
            "5713/7395000 (epoch 3), train_loss = 1.128, time/batch = 0.061\n",
            "5714/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.062\n",
            "5715/7395000 (epoch 3), train_loss = 1.192, time/batch = 0.059\n",
            "5716/7395000 (epoch 3), train_loss = 1.182, time/batch = 0.065\n",
            "5717/7395000 (epoch 3), train_loss = 1.159, time/batch = 0.060\n",
            "5718/7395000 (epoch 3), train_loss = 1.092, time/batch = 0.063\n",
            "5719/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.064\n",
            "5720/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.061\n",
            "5721/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.065\n",
            "5722/7395000 (epoch 3), train_loss = 1.230, time/batch = 0.061\n",
            "5723/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.059\n",
            "5724/7395000 (epoch 3), train_loss = 1.195, time/batch = 0.063\n",
            "5725/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.059\n",
            "5726/7395000 (epoch 3), train_loss = 1.232, time/batch = 0.063\n",
            "5727/7395000 (epoch 3), train_loss = 1.204, time/batch = 0.061\n",
            "5728/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.061\n",
            "5729/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.063\n",
            "5730/7395000 (epoch 3), train_loss = 1.158, time/batch = 0.058\n",
            "5731/7395000 (epoch 3), train_loss = 1.177, time/batch = 0.062\n",
            "5732/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.063\n",
            "5733/7395000 (epoch 3), train_loss = 1.145, time/batch = 0.059\n",
            "5734/7395000 (epoch 3), train_loss = 1.152, time/batch = 0.068\n",
            "5735/7395000 (epoch 3), train_loss = 1.196, time/batch = 0.062\n",
            "5736/7395000 (epoch 3), train_loss = 1.141, time/batch = 0.061\n",
            "5737/7395000 (epoch 3), train_loss = 1.106, time/batch = 0.055\n",
            "5738/7395000 (epoch 3), train_loss = 1.187, time/batch = 0.056\n",
            "5739/7395000 (epoch 3), train_loss = 1.155, time/batch = 0.065\n",
            "5740/7395000 (epoch 3), train_loss = 1.141, time/batch = 0.063\n",
            "5741/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.061\n",
            "5742/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.063\n",
            "5743/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.063\n",
            "5744/7395000 (epoch 3), train_loss = 1.261, time/batch = 0.064\n",
            "5745/7395000 (epoch 3), train_loss = 1.130, time/batch = 0.060\n",
            "5746/7395000 (epoch 3), train_loss = 1.193, time/batch = 0.064\n",
            "5747/7395000 (epoch 3), train_loss = 1.114, time/batch = 0.061\n",
            "5748/7395000 (epoch 3), train_loss = 1.112, time/batch = 0.055\n",
            "5749/7395000 (epoch 3), train_loss = 1.157, time/batch = 0.060\n",
            "5750/7395000 (epoch 3), train_loss = 1.089, time/batch = 0.063\n",
            "5751/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.059\n",
            "5752/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.067\n",
            "5753/7395000 (epoch 3), train_loss = 1.401, time/batch = 0.060\n",
            "5754/7395000 (epoch 3), train_loss = 1.370, time/batch = 0.061\n",
            "5755/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.060\n",
            "5756/7395000 (epoch 3), train_loss = 1.203, time/batch = 0.057\n",
            "5757/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.060\n",
            "5758/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.062\n",
            "5759/7395000 (epoch 3), train_loss = 1.184, time/batch = 0.063\n",
            "5760/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.062\n",
            "5761/7395000 (epoch 3), train_loss = 1.224, time/batch = 0.061\n",
            "5762/7395000 (epoch 3), train_loss = 1.307, time/batch = 0.062\n",
            "5763/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.063\n",
            "5764/7395000 (epoch 3), train_loss = 1.266, time/batch = 0.063\n",
            "5765/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.061\n",
            "5766/7395000 (epoch 3), train_loss = 1.269, time/batch = 0.061\n",
            "5767/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.062\n",
            "5768/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.061\n",
            "5769/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.066\n",
            "5770/7395000 (epoch 3), train_loss = 1.298, time/batch = 0.067\n",
            "5771/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.059\n",
            "5772/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.062\n",
            "5773/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.062\n",
            "5774/7395000 (epoch 3), train_loss = 1.304, time/batch = 0.061\n",
            "5775/7395000 (epoch 3), train_loss = 1.178, time/batch = 0.066\n",
            "5776/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.062\n",
            "5777/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.060\n",
            "5778/7395000 (epoch 3), train_loss = 1.154, time/batch = 0.065\n",
            "5779/7395000 (epoch 3), train_loss = 1.163, time/batch = 0.060\n",
            "5780/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.062\n",
            "5781/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.062\n",
            "5782/7395000 (epoch 3), train_loss = 1.103, time/batch = 0.067\n",
            "5783/7395000 (epoch 3), train_loss = 1.322, time/batch = 0.061\n",
            "5784/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.063\n",
            "5785/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.063\n",
            "5786/7395000 (epoch 3), train_loss = 1.257, time/batch = 0.058\n",
            "5787/7395000 (epoch 3), train_loss = 1.237, time/batch = 0.062\n",
            "5788/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.063\n",
            "5789/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.062\n",
            "5790/7395000 (epoch 3), train_loss = 1.165, time/batch = 0.060\n",
            "5791/7395000 (epoch 3), train_loss = 1.152, time/batch = 0.062\n",
            "5792/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.063\n",
            "5793/7395000 (epoch 3), train_loss = 1.101, time/batch = 0.060\n",
            "5794/7395000 (epoch 3), train_loss = 1.140, time/batch = 0.063\n",
            "5795/7395000 (epoch 3), train_loss = 1.207, time/batch = 0.059\n",
            "5796/7395000 (epoch 3), train_loss = 1.108, time/batch = 0.063\n",
            "5797/7395000 (epoch 3), train_loss = 1.095, time/batch = 0.060\n",
            "5798/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.063\n",
            "5799/7395000 (epoch 3), train_loss = 1.183, time/batch = 0.066\n",
            "5800/7395000 (epoch 3), train_loss = 1.233, time/batch = 0.064\n",
            "5801/7395000 (epoch 3), train_loss = 1.258, time/batch = 0.061\n",
            "5802/7395000 (epoch 3), train_loss = 1.265, time/batch = 0.067\n",
            "5803/7395000 (epoch 3), train_loss = 1.259, time/batch = 0.062\n",
            "5804/7395000 (epoch 3), train_loss = 1.151, time/batch = 0.067\n",
            "5805/7395000 (epoch 3), train_loss = 1.239, time/batch = 0.062\n",
            "5806/7395000 (epoch 3), train_loss = 1.083, time/batch = 0.061\n",
            "5807/7395000 (epoch 3), train_loss = 1.234, time/batch = 0.068\n",
            "5808/7395000 (epoch 3), train_loss = 1.226, time/batch = 0.064\n",
            "5809/7395000 (epoch 3), train_loss = 1.210, time/batch = 0.062\n",
            "5810/7395000 (epoch 3), train_loss = 1.122, time/batch = 0.064\n",
            "5811/7395000 (epoch 3), train_loss = 1.188, time/batch = 0.066\n",
            "5812/7395000 (epoch 3), train_loss = 1.076, time/batch = 0.063\n",
            "5813/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.060\n",
            "5814/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.061\n",
            "5815/7395000 (epoch 3), train_loss = 1.208, time/batch = 0.062\n",
            "5816/7395000 (epoch 3), train_loss = 1.291, time/batch = 0.062\n",
            "5817/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.062\n",
            "5818/7395000 (epoch 3), train_loss = 1.174, time/batch = 0.070\n",
            "5819/7395000 (epoch 3), train_loss = 1.222, time/batch = 0.062\n",
            "5820/7395000 (epoch 3), train_loss = 1.087, time/batch = 0.064\n",
            "5821/7395000 (epoch 3), train_loss = 1.211, time/batch = 0.062\n",
            "5822/7395000 (epoch 3), train_loss = 1.290, time/batch = 0.061\n",
            "5823/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.063\n",
            "5824/7395000 (epoch 3), train_loss = 1.235, time/batch = 0.062\n",
            "5825/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.060\n",
            "5826/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.065\n",
            "5827/7395000 (epoch 3), train_loss = 1.142, time/batch = 0.060\n",
            "5828/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.066\n",
            "5829/7395000 (epoch 3), train_loss = 1.280, time/batch = 0.059\n",
            "5830/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.072\n",
            "5831/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.058\n",
            "5832/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.062\n",
            "5833/7395000 (epoch 3), train_loss = 1.321, time/batch = 0.067\n",
            "5834/7395000 (epoch 3), train_loss = 1.305, time/batch = 0.070\n",
            "5835/7395000 (epoch 3), train_loss = 1.199, time/batch = 0.062\n",
            "5836/7395000 (epoch 3), train_loss = 1.336, time/batch = 0.068\n",
            "5837/7395000 (epoch 3), train_loss = 1.264, time/batch = 0.054\n",
            "5838/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.054\n",
            "5839/7395000 (epoch 3), train_loss = 1.274, time/batch = 0.054\n",
            "5840/7395000 (epoch 3), train_loss = 1.340, time/batch = 0.055\n",
            "5841/7395000 (epoch 3), train_loss = 1.345, time/batch = 0.060\n",
            "5842/7395000 (epoch 3), train_loss = 1.352, time/batch = 0.055\n",
            "5843/7395000 (epoch 3), train_loss = 1.335, time/batch = 0.055\n",
            "5844/7395000 (epoch 3), train_loss = 1.306, time/batch = 0.056\n",
            "5845/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.057\n",
            "5846/7395000 (epoch 3), train_loss = 1.267, time/batch = 0.070\n",
            "5847/7395000 (epoch 3), train_loss = 1.254, time/batch = 0.055\n",
            "5848/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.056\n",
            "5849/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.063\n",
            "5850/7395000 (epoch 3), train_loss = 1.303, time/batch = 0.064\n",
            "5851/7395000 (epoch 3), train_loss = 1.198, time/batch = 0.069\n",
            "5852/7395000 (epoch 3), train_loss = 1.202, time/batch = 0.059\n",
            "5853/7395000 (epoch 3), train_loss = 1.212, time/batch = 0.062\n",
            "5854/7395000 (epoch 3), train_loss = 1.189, time/batch = 0.062\n",
            "5855/7395000 (epoch 3), train_loss = 1.271, time/batch = 0.059\n",
            "5856/7395000 (epoch 3), train_loss = 1.246, time/batch = 0.065\n",
            "5857/7395000 (epoch 3), train_loss = 1.296, time/batch = 0.061\n",
            "5858/7395000 (epoch 3), train_loss = 1.323, time/batch = 0.064\n",
            "5859/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.062\n",
            "5860/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.066\n",
            "5861/7395000 (epoch 3), train_loss = 1.238, time/batch = 0.061\n",
            "5862/7395000 (epoch 3), train_loss = 1.136, time/batch = 0.061\n",
            "5863/7395000 (epoch 3), train_loss = 1.240, time/batch = 0.063\n",
            "5864/7395000 (epoch 3), train_loss = 1.219, time/batch = 0.065\n",
            "5865/7395000 (epoch 3), train_loss = 1.164, time/batch = 0.061\n",
            "5866/7395000 (epoch 3), train_loss = 1.260, time/batch = 0.062\n",
            "5867/7395000 (epoch 3), train_loss = 1.245, time/batch = 0.060\n",
            "5868/7395000 (epoch 3), train_loss = 1.228, time/batch = 0.061\n",
            "5869/7395000 (epoch 3), train_loss = 1.283, time/batch = 0.063\n",
            "5870/7395000 (epoch 3), train_loss = 1.236, time/batch = 0.060\n",
            "5871/7395000 (epoch 3), train_loss = 1.111, time/batch = 0.061\n",
            "5872/7395000 (epoch 3), train_loss = 1.220, time/batch = 0.064\n",
            "5873/7395000 (epoch 3), train_loss = 1.139, time/batch = 0.063\n",
            "5874/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.062\n",
            "5875/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.060\n",
            "5876/7395000 (epoch 3), train_loss = 1.272, time/batch = 0.064\n",
            "5877/7395000 (epoch 3), train_loss = 1.251, time/batch = 0.060\n",
            "5878/7395000 (epoch 3), train_loss = 1.351, time/batch = 0.063\n",
            "5879/7395000 (epoch 3), train_loss = 1.379, time/batch = 0.061\n",
            "5880/7395000 (epoch 3), train_loss = 1.313, time/batch = 0.063\n",
            "5881/7395000 (epoch 3), train_loss = 1.300, time/batch = 0.062\n",
            "5882/7395000 (epoch 3), train_loss = 1.292, time/batch = 0.060\n",
            "5883/7395000 (epoch 3), train_loss = 1.330, time/batch = 0.062\n",
            "5884/7395000 (epoch 3), train_loss = 1.289, time/batch = 0.069\n",
            "5885/7395000 (epoch 3), train_loss = 1.273, time/batch = 0.061\n",
            "5886/7395000 (epoch 3), train_loss = 1.255, time/batch = 0.063\n",
            "5887/7395000 (epoch 3), train_loss = 1.231, time/batch = 0.070\n",
            "5888/7395000 (epoch 3), train_loss = 1.183, time/batch = 0.063\n",
            "5889/7395000 (epoch 3), train_loss = 1.185, time/batch = 0.066\n",
            "5890/7395000 (epoch 3), train_loss = 1.188, time/batch = 0.063\n",
            "5891/7395000 (epoch 3), train_loss = 1.277, time/batch = 0.060\n",
            "5892/7395000 (epoch 3), train_loss = 1.187, time/batch = 0.064\n",
            "5893/7395000 (epoch 3), train_loss = 1.197, time/batch = 0.070\n",
            "5894/7395000 (epoch 3), train_loss = 1.213, time/batch = 0.064\n",
            "5895/7395000 (epoch 3), train_loss = 1.190, time/batch = 0.062\n",
            "5896/7395000 (epoch 3), train_loss = 1.144, time/batch = 0.064\n",
            "5897/7395000 (epoch 3), train_loss = 1.153, time/batch = 0.060\n",
            "5898/7395000 (epoch 3), train_loss = 1.149, time/batch = 0.062\n",
            "5899/7395000 (epoch 3), train_loss = 1.110, time/batch = 0.057\n",
            "5900/7395000 (epoch 3), train_loss = 1.165, time/batch = 0.056\n",
            "5901/7395000 (epoch 3), train_loss = 1.282, time/batch = 0.062\n",
            "5902/7395000 (epoch 3), train_loss = 1.110, time/batch = 0.059\n",
            "5903/7395000 (epoch 3), train_loss = 1.079, time/batch = 0.055\n",
            "5904/7395000 (epoch 3), train_loss = 1.170, time/batch = 0.055\n",
            "5905/7395000 (epoch 3), train_loss = 1.148, time/batch = 0.058\n",
            "5906/7395000 (epoch 3), train_loss = 1.094, time/batch = 0.055\n",
            "5907/7395000 (epoch 3), train_loss = 1.105, time/batch = 0.059\n",
            "5908/7395000 (epoch 3), train_loss = 1.181, time/batch = 0.054\n",
            "5909/7395000 (epoch 3), train_loss = 1.229, time/batch = 0.054\n",
            "5910/7395000 (epoch 3), train_loss = 1.069, time/batch = 0.066\n",
            "5911/7395000 (epoch 3), train_loss = 1.194, time/batch = 0.061\n",
            "5912/7395000 (epoch 3), train_loss = 1.200, time/batch = 0.060\n",
            "5913/7395000 (epoch 3), train_loss = 1.143, time/batch = 0.061\n",
            "5914/7395000 (epoch 3), train_loss = 1.205, time/batch = 0.061\n",
            "5915/7395000 (epoch 3), train_loss = 1.186, time/batch = 0.062\n",
            "5916/7395000 (epoch 4), train_loss = 1.386, time/batch = 0.056\n",
            "5917/7395000 (epoch 4), train_loss = 1.249, time/batch = 0.062\n",
            "5918/7395000 (epoch 4), train_loss = 1.185, time/batch = 0.064\n",
            "5919/7395000 (epoch 4), train_loss = 1.171, time/batch = 0.063\n",
            "5920/7395000 (epoch 4), train_loss = 1.147, time/batch = 0.062\n",
            "5921/7395000 (epoch 4), train_loss = 1.137, time/batch = 0.062\n",
            "5922/7395000 (epoch 4), train_loss = 1.082, time/batch = 0.061\n",
            "5923/7395000 (epoch 4), train_loss = 1.183, time/batch = 0.065\n",
            "5924/7395000 (epoch 4), train_loss = 1.169, time/batch = 0.062\n",
            "5925/7395000 (epoch 4), train_loss = 1.135, time/batch = 0.061\n",
            "5926/7395000 (epoch 4), train_loss = 1.260, time/batch = 0.065\n",
            "5927/7395000 (epoch 4), train_loss = 1.228, time/batch = 0.061\n",
            "5928/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.062\n",
            "5929/7395000 (epoch 4), train_loss = 1.223, time/batch = 0.067\n",
            "5930/7395000 (epoch 4), train_loss = 1.114, time/batch = 0.063\n",
            "5931/7395000 (epoch 4), train_loss = 1.230, time/batch = 0.064\n",
            "5932/7395000 (epoch 4), train_loss = 1.098, time/batch = 0.063\n",
            "5933/7395000 (epoch 4), train_loss = 1.208, time/batch = 0.063\n",
            "5934/7395000 (epoch 4), train_loss = 1.196, time/batch = 0.067\n",
            "5935/7395000 (epoch 4), train_loss = 1.265, time/batch = 0.062\n",
            "5936/7395000 (epoch 4), train_loss = 1.172, time/batch = 0.062\n",
            "5937/7395000 (epoch 4), train_loss = 1.268, time/batch = 0.063\n",
            "5938/7395000 (epoch 4), train_loss = 1.219, time/batch = 0.060\n",
            "5939/7395000 (epoch 4), train_loss = 1.251, time/batch = 0.062\n",
            "5940/7395000 (epoch 4), train_loss = 1.148, time/batch = 0.065\n",
            "5941/7395000 (epoch 4), train_loss = 1.245, time/batch = 0.066\n",
            "5942/7395000 (epoch 4), train_loss = 1.196, time/batch = 0.063\n",
            "5943/7395000 (epoch 4), train_loss = 1.201, time/batch = 0.061\n",
            "5944/7395000 (epoch 4), train_loss = 1.128, time/batch = 0.060\n",
            "5945/7395000 (epoch 4), train_loss = 1.104, time/batch = 0.064\n",
            "5946/7395000 (epoch 4), train_loss = 1.199, time/batch = 0.061\n",
            "5947/7395000 (epoch 4), train_loss = 1.235, time/batch = 0.064\n",
            "5948/7395000 (epoch 4), train_loss = 1.199, time/batch = 0.061\n",
            "5949/7395000 (epoch 4), train_loss = 1.169, time/batch = 0.069\n",
            "5950/7395000 (epoch 4), train_loss = 1.317, time/batch = 0.065\n",
            "5951/7395000 (epoch 4), train_loss = 1.247, time/batch = 0.065\n",
            "5952/7395000 (epoch 4), train_loss = 1.322, time/batch = 0.062\n",
            "5953/7395000 (epoch 4), train_loss = 1.262, time/batch = 0.065\n",
            "5954/7395000 (epoch 4), train_loss = 1.241, time/batch = 0.062\n",
            "5955/7395000 (epoch 4), train_loss = 1.265, time/batch = 0.062\n",
            "5956/7395000 (epoch 4), train_loss = 1.276, time/batch = 0.061\n",
            "5957/7395000 (epoch 4), train_loss = 1.151, time/batch = 0.063\n",
            "5958/7395000 (epoch 4), train_loss = 1.176, time/batch = 0.063\n",
            "5959/7395000 (epoch 4), train_loss = 1.146, time/batch = 0.065\n",
            "5960/7395000 (epoch 4), train_loss = 1.199, time/batch = 0.061\n",
            "5961/7395000 (epoch 4), train_loss = 1.160, time/batch = 0.064\n",
            "5962/7395000 (epoch 4), train_loss = 1.187, time/batch = 0.063\n",
            "5963/7395000 (epoch 4), train_loss = 1.120, time/batch = 0.064\n",
            "5964/7395000 (epoch 4), train_loss = 1.134, time/batch = 0.069\n",
            "5965/7395000 (epoch 4), train_loss = 1.171, time/batch = 0.076\n",
            "5966/7395000 (epoch 4), train_loss = 1.073, time/batch = 0.060\n",
            "5967/7395000 (epoch 4), train_loss = 1.145, time/batch = 0.065\n",
            "5968/7395000 (epoch 4), train_loss = 1.232, time/batch = 0.063\n",
            "5969/7395000 (epoch 4), train_loss = 1.243, time/batch = 0.065\n",
            "5970/7395000 (epoch 4), train_loss = 1.291, time/batch = 0.061\n",
            "5971/7395000 (epoch 4), train_loss = 1.171, time/batch = 0.065\n",
            "5972/7395000 (epoch 4), train_loss = 1.233, time/batch = 0.064\n",
            "5973/7395000 (epoch 4), train_loss = 1.122, time/batch = 0.062\n",
            "5974/7395000 (epoch 4), train_loss = 1.185, time/batch = 0.060\n",
            "5975/7395000 (epoch 4), train_loss = 1.241, time/batch = 0.062\n",
            "5976/7395000 (epoch 4), train_loss = 1.204, time/batch = 0.063\n",
            "5977/7395000 (epoch 4), train_loss = 1.169, time/batch = 0.064\n",
            "5978/7395000 (epoch 4), train_loss = 1.228, time/batch = 0.066\n",
            "5979/7395000 (epoch 4), train_loss = 1.233, time/batch = 0.061\n",
            "5980/7395000 (epoch 4), train_loss = 1.138, time/batch = 0.060\n",
            "5981/7395000 (epoch 4), train_loss = 1.161, time/batch = 0.058\n",
            "5982/7395000 (epoch 4), train_loss = 1.167, time/batch = 0.057\n",
            "5983/7395000 (epoch 4), train_loss = 1.168, time/batch = 0.055\n",
            "5984/7395000 (epoch 4), train_loss = 1.287, time/batch = 0.055\n",
            "5985/7395000 (epoch 4), train_loss = 1.135, time/batch = 0.058\n",
            "5986/7395000 (epoch 4), train_loss = 1.212, time/batch = 0.054\n",
            "5987/7395000 (epoch 4), train_loss = 1.286, time/batch = 0.057\n",
            "5988/7395000 (epoch 4), train_loss = 1.206, time/batch = 0.057\n",
            "5989/7395000 (epoch 4), train_loss = 1.170, time/batch = 0.062\n",
            "5990/7395000 (epoch 4), train_loss = 1.282, time/batch = 0.063\n",
            "5991/7395000 (epoch 4), train_loss = 1.202, time/batch = 0.059\n",
            "5992/7395000 (epoch 4), train_loss = 1.205, time/batch = 0.063\n",
            "5993/7395000 (epoch 4), train_loss = 1.163, time/batch = 0.062\n",
            "5994/7395000 (epoch 4), train_loss = 1.089, time/batch = 0.062\n",
            "5995/7395000 (epoch 4), train_loss = 1.155, time/batch = 0.065\n",
            "5996/7395000 (epoch 4), train_loss = 1.149, time/batch = 0.060\n",
            "5997/7395000 (epoch 4), train_loss = 1.142, time/batch = 0.066\n",
            "5998/7395000 (epoch 4), train_loss = 1.146, time/batch = 0.058\n",
            "5999/7395000 (epoch 4), train_loss = 1.144, time/batch = 0.076\n",
            "6000/7395000 (epoch 4), train_loss = 1.249, time/batch = 0.061\n",
            "model saved to save/model.ckpt\n",
            "6001/7395000 (epoch 4), train_loss = 1.187, time/batch = 0.063\n",
            "6002/7395000 (epoch 4), train_loss = 1.258, time/batch = 0.063\n",
            "6003/7395000 (epoch 4), train_loss = 1.316, time/batch = 0.062\n",
            "6004/7395000 (epoch 4), train_loss = 1.299, time/batch = 0.063\n",
            "6005/7395000 (epoch 4), train_loss = 1.243, time/batch = 0.074\n",
            "6006/7395000 (epoch 4), train_loss = 1.341, time/batch = 0.062\n",
            "6007/7395000 (epoch 4), train_loss = 1.281, time/batch = 0.062\n",
            "6008/7395000 (epoch 4), train_loss = 1.204, time/batch = 0.066\n",
            "6009/7395000 (epoch 4), train_loss = 1.210, time/batch = 0.061\n",
            "6010/7395000 (epoch 4), train_loss = 1.215, time/batch = 0.060\n",
            "6011/7395000 (epoch 4), train_loss = 1.167, time/batch = 0.065\n",
            "6012/7395000 (epoch 4), train_loss = 1.215, time/batch = 0.064\n",
            "6013/7395000 (epoch 4), train_loss = 1.218, time/batch = 0.065\n",
            "6014/7395000 (epoch 4), train_loss = 1.257, time/batch = 0.062\n",
            "6015/7395000 (epoch 4), train_loss = 1.237, time/batch = 0.066\n",
            "6016/7395000 (epoch 4), train_loss = 1.243, time/batch = 0.062\n",
            "6017/7395000 (epoch 4), train_loss = 1.188, time/batch = 0.065\n",
            "6018/7395000 (epoch 4), train_loss = 1.206, time/batch = 0.062\n",
            "6019/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.065\n",
            "6020/7395000 (epoch 4), train_loss = 1.170, time/batch = 0.064\n",
            "6021/7395000 (epoch 4), train_loss = 1.193, time/batch = 0.062\n",
            "6022/7395000 (epoch 4), train_loss = 1.335, time/batch = 0.062\n",
            "6023/7395000 (epoch 4), train_loss = 1.290, time/batch = 0.063\n",
            "6024/7395000 (epoch 4), train_loss = 1.336, time/batch = 0.063\n",
            "6025/7395000 (epoch 4), train_loss = 1.200, time/batch = 0.075\n",
            "6026/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.061\n",
            "6027/7395000 (epoch 4), train_loss = 1.340, time/batch = 0.069\n",
            "6028/7395000 (epoch 4), train_loss = 1.358, time/batch = 0.064\n",
            "6029/7395000 (epoch 4), train_loss = 1.308, time/batch = 0.062\n",
            "6030/7395000 (epoch 4), train_loss = 1.274, time/batch = 0.065\n",
            "6031/7395000 (epoch 4), train_loss = 1.206, time/batch = 0.064\n",
            "6032/7395000 (epoch 4), train_loss = 1.196, time/batch = 0.058\n",
            "6033/7395000 (epoch 4), train_loss = 1.276, time/batch = 0.062\n",
            "6034/7395000 (epoch 4), train_loss = 1.195, time/batch = 0.063\n",
            "6035/7395000 (epoch 4), train_loss = 1.205, time/batch = 0.065\n",
            "6036/7395000 (epoch 4), train_loss = 1.186, time/batch = 0.061\n",
            "6037/7395000 (epoch 4), train_loss = 1.138, time/batch = 0.062\n",
            "6038/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.062\n",
            "6039/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.062\n",
            "6040/7395000 (epoch 4), train_loss = 1.126, time/batch = 0.061\n",
            "6041/7395000 (epoch 4), train_loss = 1.094, time/batch = 0.072\n",
            "6042/7395000 (epoch 4), train_loss = 1.071, time/batch = 0.063\n",
            "6043/7395000 (epoch 4), train_loss = 1.161, time/batch = 0.065\n",
            "6044/7395000 (epoch 4), train_loss = 1.192, time/batch = 0.062\n",
            "6045/7395000 (epoch 4), train_loss = 1.172, time/batch = 0.064\n",
            "6046/7395000 (epoch 4), train_loss = 1.144, time/batch = 0.066\n",
            "6047/7395000 (epoch 4), train_loss = 1.199, time/batch = 0.062\n",
            "6048/7395000 (epoch 4), train_loss = 1.157, time/batch = 0.060\n",
            "6049/7395000 (epoch 4), train_loss = 1.133, time/batch = 0.065\n",
            "6050/7395000 (epoch 4), train_loss = 1.165, time/batch = 0.063\n",
            "6051/7395000 (epoch 4), train_loss = 1.245, time/batch = 0.060\n",
            "6052/7395000 (epoch 4), train_loss = 1.163, time/batch = 0.062\n",
            "6053/7395000 (epoch 4), train_loss = 1.166, time/batch = 0.062\n",
            "6054/7395000 (epoch 4), train_loss = 1.251, time/batch = 0.063\n",
            "6055/7395000 (epoch 4), train_loss = 1.089, time/batch = 0.061\n",
            "6056/7395000 (epoch 4), train_loss = 1.272, time/batch = 0.060\n",
            "6057/7395000 (epoch 4), train_loss = 1.198, time/batch = 0.068\n",
            "6058/7395000 (epoch 4), train_loss = 1.191, time/batch = 0.063\n",
            "6059/7395000 (epoch 4), train_loss = 1.218, time/batch = 0.062\n",
            "6060/7395000 (epoch 4), train_loss = 1.267, time/batch = 0.064\n",
            "6061/7395000 (epoch 4), train_loss = 1.194, time/batch = 0.063\n",
            "6062/7395000 (epoch 4), train_loss = 1.278, time/batch = 0.059\n",
            "6063/7395000 (epoch 4), train_loss = 1.186, time/batch = 0.063\n",
            "6064/7395000 (epoch 4), train_loss = 1.255, time/batch = 0.063\n",
            "6065/7395000 (epoch 4), train_loss = 1.195, time/batch = 0.066\n",
            "6066/7395000 (epoch 4), train_loss = 1.099, time/batch = 0.063\n",
            "6067/7395000 (epoch 4), train_loss = 1.283, time/batch = 0.062\n",
            "6068/7395000 (epoch 4), train_loss = 1.168, time/batch = 0.062\n",
            "6069/7395000 (epoch 4), train_loss = 1.107, time/batch = 0.064\n",
            "6070/7395000 (epoch 4), train_loss = 1.108, time/batch = 0.064\n",
            "6071/7395000 (epoch 4), train_loss = 1.082, time/batch = 0.064\n",
            "6072/7395000 (epoch 4), train_loss = 1.227, time/batch = 0.062\n",
            "6073/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.072\n",
            "6074/7395000 (epoch 4), train_loss = 1.099, time/batch = 0.062\n",
            "6075/7395000 (epoch 4), train_loss = 1.114, time/batch = 0.062\n",
            "6076/7395000 (epoch 4), train_loss = 1.203, time/batch = 0.063\n",
            "6077/7395000 (epoch 4), train_loss = 1.216, time/batch = 0.064\n",
            "6078/7395000 (epoch 4), train_loss = 1.170, time/batch = 0.062\n",
            "6079/7395000 (epoch 4), train_loss = 1.161, time/batch = 0.070\n",
            "6080/7395000 (epoch 4), train_loss = 1.115, time/batch = 0.062\n",
            "6081/7395000 (epoch 4), train_loss = 1.209, time/batch = 0.063\n",
            "6082/7395000 (epoch 4), train_loss = 1.070, time/batch = 0.061\n",
            "6083/7395000 (epoch 4), train_loss = 1.264, time/batch = 0.063\n",
            "6084/7395000 (epoch 4), train_loss = 1.159, time/batch = 0.066\n",
            "6085/7395000 (epoch 4), train_loss = 1.210, time/batch = 0.069\n",
            "6086/7395000 (epoch 4), train_loss = 1.195, time/batch = 0.062\n",
            "6087/7395000 (epoch 4), train_loss = 1.200, time/batch = 0.063\n",
            "6088/7395000 (epoch 4), train_loss = 1.187, time/batch = 0.063\n",
            "6089/7395000 (epoch 4), train_loss = 1.217, time/batch = 0.065\n",
            "6090/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.062\n",
            "6091/7395000 (epoch 4), train_loss = 1.211, time/batch = 0.062\n",
            "6092/7395000 (epoch 4), train_loss = 1.133, time/batch = 0.061\n",
            "6093/7395000 (epoch 4), train_loss = 1.204, time/batch = 0.064\n",
            "6094/7395000 (epoch 4), train_loss = 1.142, time/batch = 0.064\n",
            "6095/7395000 (epoch 4), train_loss = 1.192, time/batch = 0.065\n",
            "6096/7395000 (epoch 4), train_loss = 1.163, time/batch = 0.064\n",
            "6097/7395000 (epoch 4), train_loss = 1.157, time/batch = 0.065\n",
            "6098/7395000 (epoch 4), train_loss = 1.078, time/batch = 0.068\n",
            "6099/7395000 (epoch 4), train_loss = 1.187, time/batch = 0.064\n",
            "6100/7395000 (epoch 4), train_loss = 1.134, time/batch = 0.064\n",
            "6101/7395000 (epoch 4), train_loss = 1.110, time/batch = 0.065\n",
            "6102/7395000 (epoch 4), train_loss = 1.193, time/batch = 0.062\n",
            "6103/7395000 (epoch 4), train_loss = 1.152, time/batch = 0.068\n",
            "6104/7395000 (epoch 4), train_loss = 1.190, time/batch = 0.065\n",
            "6105/7395000 (epoch 4), train_loss = 1.202, time/batch = 0.070\n",
            "6106/7395000 (epoch 4), train_loss = 1.229, time/batch = 0.064\n",
            "6107/7395000 (epoch 4), train_loss = 1.116, time/batch = 0.062\n",
            "6108/7395000 (epoch 4), train_loss = 1.215, time/batch = 0.057\n",
            "6109/7395000 (epoch 4), train_loss = 1.150, time/batch = 0.064\n",
            "6110/7395000 (epoch 4), train_loss = 1.220, time/batch = 0.067\n",
            "6111/7395000 (epoch 4), train_loss = 1.196, time/batch = 0.064\n",
            "6112/7395000 (epoch 4), train_loss = 1.320, time/batch = 0.067\n",
            "6113/7395000 (epoch 4), train_loss = 1.294, time/batch = 0.062\n",
            "6114/7395000 (epoch 4), train_loss = 1.200, time/batch = 0.061\n",
            "6115/7395000 (epoch 4), train_loss = 1.231, time/batch = 0.064\n",
            "6116/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.069\n",
            "6117/7395000 (epoch 4), train_loss = 1.179, time/batch = 0.065\n",
            "6118/7395000 (epoch 4), train_loss = 1.139, time/batch = 0.062\n",
            "6119/7395000 (epoch 4), train_loss = 1.195, time/batch = 0.064\n",
            "6120/7395000 (epoch 4), train_loss = 1.230, time/batch = 0.061\n",
            "6121/7395000 (epoch 4), train_loss = 1.168, time/batch = 0.069\n",
            "6122/7395000 (epoch 4), train_loss = 1.323, time/batch = 0.063\n",
            "6123/7395000 (epoch 4), train_loss = 1.295, time/batch = 0.062\n",
            "6124/7395000 (epoch 4), train_loss = 1.169, time/batch = 0.064\n",
            "6125/7395000 (epoch 4), train_loss = 1.205, time/batch = 0.064\n",
            "6126/7395000 (epoch 4), train_loss = 1.197, time/batch = 0.061\n",
            "6127/7395000 (epoch 4), train_loss = 1.243, time/batch = 0.062\n",
            "6128/7395000 (epoch 4), train_loss = 1.282, time/batch = 0.061\n",
            "6129/7395000 (epoch 4), train_loss = 1.179, time/batch = 0.062\n",
            "6130/7395000 (epoch 4), train_loss = 1.072, time/batch = 0.062\n",
            "6131/7395000 (epoch 4), train_loss = 1.176, time/batch = 0.062\n",
            "6132/7395000 (epoch 4), train_loss = 1.130, time/batch = 0.059\n",
            "6133/7395000 (epoch 4), train_loss = 1.204, time/batch = 0.061\n",
            "6134/7395000 (epoch 4), train_loss = 1.212, time/batch = 0.066\n",
            "6135/7395000 (epoch 4), train_loss = 1.100, time/batch = 0.062\n",
            "6136/7395000 (epoch 4), train_loss = 1.202, time/batch = 0.064\n",
            "6137/7395000 (epoch 4), train_loss = 1.246, time/batch = 0.067\n",
            "6138/7395000 (epoch 4), train_loss = 1.227, time/batch = 0.063\n",
            "6139/7395000 (epoch 4), train_loss = 1.251, time/batch = 0.062\n",
            "6140/7395000 (epoch 4), train_loss = 1.134, time/batch = 0.066\n",
            "6141/7395000 (epoch 4), train_loss = 1.192, time/batch = 0.063\n",
            "6142/7395000 (epoch 4), train_loss = 1.145, time/batch = 0.062\n",
            "6143/7395000 (epoch 4), train_loss = 1.139, time/batch = 0.061\n",
            "6144/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.064\n",
            "6145/7395000 (epoch 4), train_loss = 1.174, time/batch = 0.062\n",
            "6146/7395000 (epoch 4), train_loss = 1.283, time/batch = 0.064\n",
            "6147/7395000 (epoch 4), train_loss = 1.250, time/batch = 0.062\n",
            "6148/7395000 (epoch 4), train_loss = 1.208, time/batch = 0.063\n",
            "6149/7395000 (epoch 4), train_loss = 1.137, time/batch = 0.067\n",
            "6150/7395000 (epoch 4), train_loss = 1.314, time/batch = 0.063\n",
            "6151/7395000 (epoch 4), train_loss = 1.179, time/batch = 0.062\n",
            "6152/7395000 (epoch 4), train_loss = 1.119, time/batch = 0.066\n",
            "6153/7395000 (epoch 4), train_loss = 1.180, time/batch = 0.062\n",
            "6154/7395000 (epoch 4), train_loss = 1.084, time/batch = 0.062\n",
            "6155/7395000 (epoch 4), train_loss = 1.231, time/batch = 0.065\n",
            "6156/7395000 (epoch 4), train_loss = 1.222, time/batch = 0.063\n",
            "6157/7395000 (epoch 4), train_loss = 1.169, time/batch = 0.062\n",
            "6158/7395000 (epoch 4), train_loss = 1.228, time/batch = 0.066\n",
            "6159/7395000 (epoch 4), train_loss = 1.209, time/batch = 0.061\n",
            "6160/7395000 (epoch 4), train_loss = 1.190, time/batch = 0.064\n",
            "6161/7395000 (epoch 4), train_loss = 1.175, time/batch = 0.062\n",
            "6162/7395000 (epoch 4), train_loss = 1.059, time/batch = 0.074\n",
            "6163/7395000 (epoch 4), train_loss = 1.149, time/batch = 0.063\n",
            "6164/7395000 (epoch 4), train_loss = 1.206, time/batch = 0.060\n",
            "6165/7395000 (epoch 4), train_loss = 1.261, time/batch = 0.062\n",
            "6166/7395000 (epoch 4), train_loss = 1.249, time/batch = 0.062\n",
            "6167/7395000 (epoch 4), train_loss = 1.214, time/batch = 0.063\n",
            "6168/7395000 (epoch 4), train_loss = 1.231, time/batch = 0.064\n",
            "6169/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.067\n",
            "6170/7395000 (epoch 4), train_loss = 1.194, time/batch = 0.063\n",
            "6171/7395000 (epoch 4), train_loss = 1.269, time/batch = 0.063\n",
            "6172/7395000 (epoch 4), train_loss = 1.208, time/batch = 0.063\n",
            "6173/7395000 (epoch 4), train_loss = 1.198, time/batch = 0.068\n",
            "6174/7395000 (epoch 4), train_loss = 1.363, time/batch = 0.063\n",
            "6175/7395000 (epoch 4), train_loss = 1.261, time/batch = 0.069\n",
            "6176/7395000 (epoch 4), train_loss = 1.254, time/batch = 0.064\n",
            "6177/7395000 (epoch 4), train_loss = 1.291, time/batch = 0.061\n",
            "6178/7395000 (epoch 4), train_loss = 1.270, time/batch = 0.063\n",
            "6179/7395000 (epoch 4), train_loss = 1.192, time/batch = 0.061\n",
            "6180/7395000 (epoch 4), train_loss = 1.307, time/batch = 0.071\n",
            "6181/7395000 (epoch 4), train_loss = 1.202, time/batch = 0.057\n",
            "6182/7395000 (epoch 4), train_loss = 1.215, time/batch = 0.064\n",
            "6183/7395000 (epoch 4), train_loss = 1.246, time/batch = 0.063\n",
            "6184/7395000 (epoch 4), train_loss = 1.328, time/batch = 0.062\n",
            "6185/7395000 (epoch 4), train_loss = 1.336, time/batch = 0.062\n",
            "6186/7395000 (epoch 4), train_loss = 1.230, time/batch = 0.070\n",
            "6187/7395000 (epoch 4), train_loss = 1.214, time/batch = 0.063\n",
            "6188/7395000 (epoch 4), train_loss = 1.208, time/batch = 0.063\n",
            "6189/7395000 (epoch 4), train_loss = 1.206, time/batch = 0.064\n",
            "6190/7395000 (epoch 4), train_loss = 1.237, time/batch = 0.063\n",
            "6191/7395000 (epoch 4), train_loss = 1.237, time/batch = 0.061\n",
            "6192/7395000 (epoch 4), train_loss = 1.262, time/batch = 0.066\n",
            "6193/7395000 (epoch 4), train_loss = 1.261, time/batch = 0.062\n",
            "6194/7395000 (epoch 4), train_loss = 1.316, time/batch = 0.065\n",
            "6195/7395000 (epoch 4), train_loss = 1.321, time/batch = 0.067\n",
            "6196/7395000 (epoch 4), train_loss = 1.336, time/batch = 0.060\n",
            "6197/7395000 (epoch 4), train_loss = 1.286, time/batch = 0.060\n",
            "6198/7395000 (epoch 4), train_loss = 1.208, time/batch = 0.062\n",
            "6199/7395000 (epoch 4), train_loss = 1.308, time/batch = 0.062\n",
            "6200/7395000 (epoch 4), train_loss = 1.105, time/batch = 0.065\n",
            "6201/7395000 (epoch 4), train_loss = 1.170, time/batch = 0.064\n",
            "6202/7395000 (epoch 4), train_loss = 1.172, time/batch = 0.064\n",
            "6203/7395000 (epoch 4), train_loss = 1.202, time/batch = 0.067\n",
            "6204/7395000 (epoch 4), train_loss = 1.207, time/batch = 0.062\n",
            "6205/7395000 (epoch 4), train_loss = 1.173, time/batch = 0.061\n",
            "6206/7395000 (epoch 4), train_loss = 1.300, time/batch = 0.062\n",
            "6207/7395000 (epoch 4), train_loss = 1.272, time/batch = 0.064\n",
            "6208/7395000 (epoch 4), train_loss = 1.158, time/batch = 0.060\n",
            "6209/7395000 (epoch 4), train_loss = 1.301, time/batch = 0.056\n",
            "6210/7395000 (epoch 4), train_loss = 1.241, time/batch = 0.056\n",
            "6211/7395000 (epoch 4), train_loss = 1.238, time/batch = 0.057\n",
            "Traceback (most recent call last):\n",
            "  File \"unconventional-neural-networks/char-rnn-tensorflow/train.py\", line 144, in <module>\n",
            "    train(args)\n",
            "  File \"unconventional-neural-networks/char-rnn-tensorflow/train.py\", line 125, in train\n",
            "    summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUz12vg--AT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dda3950c-93e4-4c6b-d9e8-ae1d03e279b2"
      },
      "source": [
        "!python unconventional-neural-networks/char-rnn-tensorflow/sample.py -n=5000 >> unconventional-neural-networks/char-rnn-tensorflow/out.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:30: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:36: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:39: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:46: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:47: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:86: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:98: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/unconventional-neural-networks/char-rnn-tensorflow/model.py:100: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/sample.py:39: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-09-13 19:37:15.272073: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-13 19:37:15.289808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.290557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-13 19:37:15.290822: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:37:15.292172: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-13 19:37:15.303574: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-13 19:37:15.303961: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-13 19:37:15.305540: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-13 19:37:15.312173: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-13 19:37:15.316484: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-13 19:37:15.316628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.317473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.318209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-13 19:37:15.325023: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-13 19:37:15.325266: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19afdc0 executing computations on platform Host. Devices:\n",
            "2019-09-13 19:37:15.325305: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-13 19:37:15.378201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.379107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19b0840 executing computations on platform CUDA. Devices:\n",
            "2019-09-13 19:37:15.379145: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-13 19:37:15.379351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.380065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-13 19:37:15.380138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:37:15.380175: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-13 19:37:15.380207: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-13 19:37:15.380238: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-13 19:37:15.380269: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-13 19:37:15.380299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-13 19:37:15.380331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-13 19:37:15.380433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.381211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.381886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-13 19:37:15.381956: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-13 19:37:15.383489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-13 19:37:15.383527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-13 19:37:15.383545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-13 19:37:15.383699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.384505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-13 19:37:15.385211: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-13 19:37:15.385271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From unconventional-neural-networks/char-rnn-tensorflow/sample.py:41: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2019-09-13 19:37:15.970704: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z5uKmc1IM34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}